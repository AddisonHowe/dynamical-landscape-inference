Args:
Namespace(name='model_algphiq_1a_v_mmd1', outdir='out/model_training/model_algphiq_1a_v_mmd1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='quadratic', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=0, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[50, 100, 250], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1018066559

Training model...

Saving initial model state to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 5.082181005871461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.082181005871461 | validation: 5.145626604681247]
	TIME [epoch: 99.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 5.036928926450952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.036928926450952 | validation: 5.0921731391525835]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 4.974984230763756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.974984230763756 | validation: 5.016035094139143]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 4.882210702736581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.882210702736581 | validation: 4.8907622364825905]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 4.721788516064636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.721788516064636 | validation: 4.654499769260884]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 4.24730515888208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.24730515888208 | validation: 3.7034373463119414]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 3.528317987686152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.528317987686152 | validation: 3.521854118233338]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 3.0658578656488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0658578656488 | validation: 2.468554467437904]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0984535866346787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0984535866346787 | validation: 1.6005457205384892]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 1.476494730507552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.476494730507552 | validation: 0.6543335739151157]
	TIME [epoch: 4.23 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7884073882641581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7884073882641581 | validation: 0.3784478721966443]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7395526898454875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7395526898454875 | validation: 0.3595618861878977]
	TIME [epoch: 4.19 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7002328894779529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7002328894779529 | validation: 0.3541189378690791]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6295992522730572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6295992522730572 | validation: 0.33214759912152614]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6112387335240609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112387335240609 | validation: 0.34020264262712047]
	TIME [epoch: 4.21 sec]
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5710649114189768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710649114189768 | validation: 0.2978934084761977]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5432295754688075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5432295754688075 | validation: 0.2834273242418265]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.536554909284912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.536554909284912 | validation: 0.28318977857316974]
	TIME [epoch: 4.21 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.513534862190242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.513534862190242 | validation: 0.268211464440006]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4892166260892348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4892166260892348 | validation: 0.2620676866739204]
	TIME [epoch: 4.24 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.47188889897728714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47188889897728714 | validation: 0.2617678066991398]
	TIME [epoch: 4.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4699963513076859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4699963513076859 | validation: 0.2627715892964902]
	TIME [epoch: 4.22 sec]
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4578079682352567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4578079682352567 | validation: 0.2657871067685927]
	TIME [epoch: 4.22 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45952044065199693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45952044065199693 | validation: 0.2660054229215682]
	TIME [epoch: 4.22 sec]
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4506672086872436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4506672086872436 | validation: 0.272088864335993]
	TIME [epoch: 4.22 sec]
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45635855677646264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45635855677646264 | validation: 0.2711740332620377]
	TIME [epoch: 4.22 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4524036406196946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4524036406196946 | validation: 0.28023483283201234]
	TIME [epoch: 4.21 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45115504527822536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45115504527822536 | validation: 0.28005018434579626]
	TIME [epoch: 4.21 sec]
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45693850680151826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45693850680151826 | validation: 0.27837961921313525]
	TIME [epoch: 4.24 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45463310665953893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45463310665953893 | validation: 0.2812146479972266]
	TIME [epoch: 4.23 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44896157842038287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44896157842038287 | validation: 0.28346914703300863]
	TIME [epoch: 4.22 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4488396101917689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4488396101917689 | validation: 0.2888922345644375]
	TIME [epoch: 4.22 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4534661960826498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4534661960826498 | validation: 0.2879664395260012]
	TIME [epoch: 4.2 sec]
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45265638674511427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45265638674511427 | validation: 0.2881526210323836]
	TIME [epoch: 4.22 sec]
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.46106412696994836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46106412696994836 | validation: 0.294323316698047]
	TIME [epoch: 4.19 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45677385753259514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45677385753259514 | validation: 0.2863978024388618]
	TIME [epoch: 4.22 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4559225938144704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4559225938144704 | validation: 0.2862471132236409]
	TIME [epoch: 4.21 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44743849037289607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44743849037289607 | validation: 0.2843820823952693]
	TIME [epoch: 4.22 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4526726083434074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4526726083434074 | validation: 0.2932048335931533]
	TIME [epoch: 4.21 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4550308487741139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4550308487741139 | validation: 0.28471807965893137]
	TIME [epoch: 4.22 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4510649728535616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4510649728535616 | validation: 0.2838247735805906]
	TIME [epoch: 4.25 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4500862859253825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4500862859253825 | validation: 0.2857767557297818]
	TIME [epoch: 4.22 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4449435694646764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4449435694646764 | validation: 0.2791573328038587]
	TIME [epoch: 4.22 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4535073767738278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4535073767738278 | validation: 0.27935295667545723]
	TIME [epoch: 4.23 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4589422780831777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4589422780831777 | validation: 0.2914016387934464]
	TIME [epoch: 4.23 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4469209083751999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4469209083751999 | validation: 0.2903470802779188]
	TIME [epoch: 4.24 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4557919911110983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4557919911110983 | validation: 0.285048559220629]
	TIME [epoch: 4.22 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45744493200213887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45744493200213887 | validation: 0.28728597825012636]
	TIME [epoch: 4.21 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4544783530168047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4544783530168047 | validation: 0.2864643767020989]
	TIME [epoch: 4.22 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4452287381570501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4452287381570501 | validation: 0.2875285384090073]
	TIME [epoch: 4.21 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.45257890280243646		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.45257890280243646 | validation: 0.2853618664965283]
	TIME [epoch: 102 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4458533851791398		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.4458533851791398 | validation: 0.2841890753185147]
	TIME [epoch: 8.3 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4414841223683013		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.4414841223683013 | validation: 0.29195138407718435]
	TIME [epoch: 8.23 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.44149502100130494		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.44149502100130494 | validation: 0.28563599892787384]
	TIME [epoch: 8.23 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43582930736968006		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.43582930736968006 | validation: 0.28306487551942805]
	TIME [epoch: 8.27 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4435905023603681		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.4435905023603681 | validation: 0.2829539551869131]
	TIME [epoch: 8.27 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.43175916478349846		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.43175916478349846 | validation: 0.2821248157958718]
	TIME [epoch: 8.17 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.42500752409083875		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.42500752409083875 | validation: 0.27789156166605544]
	TIME [epoch: 8.14 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4188442673904582		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.4188442673904582 | validation: 0.27853365803049446]
	TIME [epoch: 8.14 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.41333187510052083		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.41333187510052083 | validation: 0.27463649001143514]
	TIME [epoch: 8.15 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.4089132036732104		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.4089132036732104 | validation: 0.2744726194285811]
	TIME [epoch: 8.24 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3820341671671502		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.3820341671671502 | validation: 0.286886083622486]
	TIME [epoch: 8.27 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3776485764030329		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.3776485764030329 | validation: 0.2727775156549017]
	TIME [epoch: 8.23 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3655941187505726		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.3655941187505726 | validation: 0.27330668326591046]
	TIME [epoch: 8.22 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3639032142303833		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.3639032142303833 | validation: 0.2869503964689003]
	TIME [epoch: 8.23 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.36739781880839084		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.36739781880839084 | validation: 0.27183319334376804]
	TIME [epoch: 8.24 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.36169081195409636		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.36169081195409636 | validation: 0.2709214099495063]
	TIME [epoch: 8.24 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3630584078166095		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.3630584078166095 | validation: 0.26875655285350386]
	TIME [epoch: 8.28 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3562217751731933		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.3562217751731933 | validation: 0.26407026781891885]
	TIME [epoch: 8.23 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3534889428832688		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.3534889428832688 | validation: 0.2641956995413897]
	TIME [epoch: 8.25 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3449092967196815		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.3449092967196815 | validation: 0.2658431161101724]
	TIME [epoch: 8.24 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3451752356678762		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.3451752356678762 | validation: 0.2599495927877068]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3453099702945796		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.3453099702945796 | validation: 0.26058290756167035]
	TIME [epoch: 8.25 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3383244501077878		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.3383244501077878 | validation: 0.2639258764981878]
	TIME [epoch: 8.3 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3395998552301138		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.3395998552301138 | validation: 0.2587686865677604]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3317448055964286		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.3317448055964286 | validation: 0.2617624964273871]
	TIME [epoch: 8.21 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3270765846623169		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.3270765846623169 | validation: 0.2599790204305974]
	TIME [epoch: 8.21 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3220141530009314		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.3220141530009314 | validation: 0.2642147640435911]
	TIME [epoch: 8.23 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3297656186319827		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.3297656186319827 | validation: 0.2550865530393515]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3254171358642077		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.3254171358642077 | validation: 0.26064753082570213]
	TIME [epoch: 8.27 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32158521289867104		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.32158521289867104 | validation: 0.2541805977220453]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.32147129918078887		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.32147129918078887 | validation: 0.25904463041368053]
	TIME [epoch: 8.2 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3180165295334869		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.3180165295334869 | validation: 0.2548582452913547]
	TIME [epoch: 8.21 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3132886937722416		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.3132886937722416 | validation: 0.26051592918200983]
	TIME [epoch: 8.19 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.31119677582091604		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.31119677582091604 | validation: 0.2613332683609274]
	TIME [epoch: 8.18 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30660202791950375		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.30660202791950375 | validation: 0.25458568585273755]
	TIME [epoch: 8.25 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3054590141836275		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.3054590141836275 | validation: 0.25151888943236467]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3042770261909962		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.3042770261909962 | validation: 0.24470282996845838]
	TIME [epoch: 8.25 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30188946955050305		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.30188946955050305 | validation: 0.2508862701814941]
	TIME [epoch: 8.24 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.30310784385709244		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.30310784385709244 | validation: 0.25279670962532796]
	TIME [epoch: 8.2 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2990962615627571		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.2990962615627571 | validation: 0.24870742753100547]
	TIME [epoch: 8.19 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.29346149383026965		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.29346149383026965 | validation: 0.2465650951709816]
	TIME [epoch: 8.24 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2940104969487197		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.2940104969487197 | validation: 0.24852020884460507]
	TIME [epoch: 8.2 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.29445423068757837		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.29445423068757837 | validation: 0.24920624827745091]
	TIME [epoch: 8.23 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2947728661551262		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.2947728661551262 | validation: 0.24826298262681706]
	TIME [epoch: 8.21 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2984660520021538		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.2984660520021538 | validation: 0.24864041864742423]
	TIME [epoch: 8.21 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2882270343656086		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.2882270343656086 | validation: 0.2501711863340964]
	TIME [epoch: 8.23 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2866696503278847		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.2866696503278847 | validation: 0.24516520938939457]
	TIME [epoch: 8.24 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2843978777339615		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.2843978777339615 | validation: 0.23994258806906527]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.289359301312919		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.289359301312919 | validation: 0.24329974660738912]
	TIME [epoch: 8.23 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2844595105610428		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.2844595105610428 | validation: 0.24523012009034573]
	TIME [epoch: 112 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.28444223501019955		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.28444223501019955 | validation: 0.24516746984063698]
	TIME [epoch: 18.8 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2808379214552951		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.2808379214552951 | validation: 0.23970176435373342]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27618803995786007		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.27618803995786007 | validation: 0.2446911783014925]
	TIME [epoch: 18.8 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27802902803592583		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.27802902803592583 | validation: 0.23800518024879605]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27680091055660644		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.27680091055660644 | validation: 0.2422683289940475]
	TIME [epoch: 18.8 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27471840724377794		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.27471840724377794 | validation: 0.23963989808687677]
	TIME [epoch: 18.8 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.27730336678872614		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.27730336678872614 | validation: 0.24074485571293353]
	TIME [epoch: 18.8 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2730638727291337		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.2730638727291337 | validation: 0.24406519948240069]
	TIME [epoch: 18.8 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2744939429728691		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.2744939429728691 | validation: 0.24178132717638073]
	TIME [epoch: 18.7 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.266878465227959		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.266878465227959 | validation: 0.24295505763130806]
	TIME [epoch: 18.7 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26710618154883037		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.26710618154883037 | validation: 0.23546918739125436]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2693098282143662		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.2693098282143662 | validation: 0.23556237391811571]
	TIME [epoch: 18.7 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2689829344478908		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.2689829344478908 | validation: 0.23745535656805558]
	TIME [epoch: 18.7 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26665264254004245		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.26665264254004245 | validation: 0.23915898671116126]
	TIME [epoch: 18.7 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2647151745568258		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.2647151745568258 | validation: 0.23865804071842145]
	TIME [epoch: 18.7 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26467719773098003		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.26467719773098003 | validation: 0.2351506021429987]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.26472282967633065		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.26472282967633065 | validation: 0.23670693061695242]
	TIME [epoch: 18.7 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2564510222695256		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.2564510222695256 | validation: 0.23280583703919888]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.25581197761056895		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.25581197761056895 | validation: 0.22754806777809256]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.257118862571468		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.257118862571468 | validation: 0.22733962337868097]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.255564581205939		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.255564581205939 | validation: 0.2268854686270028]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.24467027295683755		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.24467027295683755 | validation: 0.22675285799030168]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23941809685034504		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.23941809685034504 | validation: 0.22865492496080903]
	TIME [epoch: 18.8 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23600563805619829		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.23600563805619829 | validation: 0.2147657134634665]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22821864039930392		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.22821864039930392 | validation: 0.22799366422448547]
	TIME [epoch: 18.7 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22804456650059052		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.22804456650059052 | validation: 0.22977699835908716]
	TIME [epoch: 18.7 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22762691549433905		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.22762691549433905 | validation: 0.225786170355521]
	TIME [epoch: 18.7 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22340169380751332		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.22340169380751332 | validation: 0.2284928829933222]
	TIME [epoch: 18.7 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22412259307060328		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.22412259307060328 | validation: 0.2276397171796941]
	TIME [epoch: 18.6 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22525794177927225		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.22525794177927225 | validation: 0.23061128635251174]
	TIME [epoch: 18.7 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22425280960861674		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.22425280960861674 | validation: 0.2295891803979883]
	TIME [epoch: 18.7 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.22159531841532037		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.22159531841532037 | validation: 0.2353434004741446]
	TIME [epoch: 18.7 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.2196990908553314		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.2196990908553314 | validation: 0.2043314687345007]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20765229948107072		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.20765229948107072 | validation: 0.19758458643333682]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.20059709678752782		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.20059709678752782 | validation: 0.1891443382267419]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19851009980424672		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.19851009980424672 | validation: 0.19447612943355935]
	TIME [epoch: 18.7 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19682950141739677		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.19682950141739677 | validation: 0.19765446274145587]
	TIME [epoch: 18.6 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.19245151757040663		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.19245151757040663 | validation: 0.17555881404269047]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.18226215359565812		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.18226215359565812 | validation: 0.1772912937029291]
	TIME [epoch: 18.7 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17516284182386033		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.17516284182386033 | validation: 0.16633544656800914]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17106793572770296		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.17106793572770296 | validation: 0.15729612375785423]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.16512209548584178		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.16512209548584178 | validation: 0.15661654771932315]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1623996092997354		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.1623996092997354 | validation: 0.15054831612872688]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15662434335951536		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.15662434335951536 | validation: 0.14679114258842274]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15340600672817822		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.15340600672817822 | validation: 0.1418365118570935]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.15019223215229827		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.15019223215229827 | validation: 0.1433365185281769]
	TIME [epoch: 18.7 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1452850991829473		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.1452850991829473 | validation: 0.1346835837294298]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14079280891500515		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.14079280891500515 | validation: 0.13659907679861377]
	TIME [epoch: 18.8 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13910082747586347		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.13910082747586347 | validation: 0.1319006812829865]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.13278290860072592		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.13278290860072592 | validation: 0.12776331493545834]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12934544904503845		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.12934544904503845 | validation: 0.11953000195218097]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12389021245947432		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.12389021245947432 | validation: 0.11341903785422947]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12181943970829748		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.12181943970829748 | validation: 0.11341906126463086]
	TIME [epoch: 18.6 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11807304212130917		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.11807304212130917 | validation: 0.10820283633848643]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11127190377440757		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.11127190377440757 | validation: 0.10247816763091736]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10879831719473368		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.10879831719473368 | validation: 0.10417427342402907]
	TIME [epoch: 18.7 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.1054317911083104		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.1054317911083104 | validation: 0.09492500525082576]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10356330377155291		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.10356330377155291 | validation: 0.09845449717741844]
	TIME [epoch: 18.7 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.10335141081990114		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.10335141081990114 | validation: 0.08964386883093234]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09750508833943297		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.09750508833943297 | validation: 0.0882836656247167]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09611543079508061		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.09611543079508061 | validation: 0.08590546450606974]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.09324726663325829		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.09324726663325829 | validation: 0.08040297798654525]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08719022742242724		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.08719022742242724 | validation: 0.07791707545476384]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08540233368059111		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.08540233368059111 | validation: 0.07672007870086829]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0858197472386179		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.0858197472386179 | validation: 0.07592246067912803]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08326460937680918		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.08326460937680918 | validation: 0.0683230589895501]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08142318416307845		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.08142318416307845 | validation: 0.07245399970172142]
	TIME [epoch: 18.7 sec]
EPOCH 169/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07868436777382654		[learning rate: 0.0016156]
	Learning Rate: 0.0016156
	LOSS [training: 0.07868436777382654 | validation: 0.069631144026294]
	TIME [epoch: 18.7 sec]
EPOCH 170/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07753158545199512		[learning rate: 0.001591]
	Learning Rate: 0.00159099
	LOSS [training: 0.07753158545199512 | validation: 0.06622749106311702]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0731005106768624		[learning rate: 0.0015668]
	Learning Rate: 0.00156675
	LOSS [training: 0.0731005106768624 | validation: 0.06405341990484079]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07186323511903947		[learning rate: 0.0015429]
	Learning Rate: 0.00154288
	LOSS [training: 0.07186323511903947 | validation: 0.06352984062798177]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/500:
	Training over batches...
		[batch 4/4] avg loss: 0.07010030542949798		[learning rate: 0.0015194]
	Learning Rate: 0.00151938
	LOSS [training: 0.07010030542949798 | validation: 0.058302157909673885]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06775005059505168		[learning rate: 0.0014962]
	Learning Rate: 0.00149624
	LOSS [training: 0.06775005059505168 | validation: 0.0570338739614103]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0654436914233632		[learning rate: 0.0014734]
	Learning Rate: 0.00147344
	LOSS [training: 0.0654436914233632 | validation: 0.05735510537094886]
	TIME [epoch: 18.7 sec]
EPOCH 176/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06446399203063191		[learning rate: 0.001451]
	Learning Rate: 0.001451
	LOSS [training: 0.06446399203063191 | validation: 0.053057350487964394]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/500:
	Training over batches...
		[batch 4/4] avg loss: 0.062160435261716204		[learning rate: 0.0014289]
	Learning Rate: 0.00142889
	LOSS [training: 0.062160435261716204 | validation: 0.05125679970227686]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06063220213973046		[learning rate: 0.0014071]
	Learning Rate: 0.00140713
	LOSS [training: 0.06063220213973046 | validation: 0.04902550557622247]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05897182870554119		[learning rate: 0.0013857]
	Learning Rate: 0.00138569
	LOSS [training: 0.05897182870554119 | validation: 0.04759747125993067]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/500:
	Training over batches...
		[batch 4/4] avg loss: 0.057947929406810955		[learning rate: 0.0013646]
	Learning Rate: 0.00136458
	LOSS [training: 0.057947929406810955 | validation: 0.04596450594693463]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/500:
	Training over batches...
		[batch 4/4] avg loss: 0.056421079256593176		[learning rate: 0.0013438]
	Learning Rate: 0.0013438
	LOSS [training: 0.056421079256593176 | validation: 0.04192686160435279]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/500:
	Training over batches...
		[batch 4/4] avg loss: 0.05458809607314457		[learning rate: 0.0013233]
	Learning Rate: 0.00132333
	LOSS [training: 0.05458809607314457 | validation: 0.044352633142124544]
	TIME [epoch: 18.7 sec]
EPOCH 183/500:
	Training over batches...
		[batch 4/4] avg loss: 0.053733258186737694		[learning rate: 0.0013032]
	Learning Rate: 0.00130317
	LOSS [training: 0.053733258186737694 | validation: 0.0428473065793412]
	TIME [epoch: 18.7 sec]
EPOCH 184/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0514708591531521		[learning rate: 0.0012833]
	Learning Rate: 0.00128332
	LOSS [training: 0.0514708591531521 | validation: 0.041580469407212675]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04971249906306105		[learning rate: 0.0012638]
	Learning Rate: 0.00126377
	LOSS [training: 0.04971249906306105 | validation: 0.03898515731731358]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/500:
	Training over batches...
		[batch 4/4] avg loss: 0.048239689298048026		[learning rate: 0.0012445]
	Learning Rate: 0.00124451
	LOSS [training: 0.048239689298048026 | validation: 0.03873535659504708]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/500:
	Training over batches...
		[batch 4/4] avg loss: 0.048581256398405555		[learning rate: 0.0012256]
	Learning Rate: 0.00122556
	LOSS [training: 0.048581256398405555 | validation: 0.034699282807434444]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0462753010802001		[learning rate: 0.0012069]
	Learning Rate: 0.00120689
	LOSS [training: 0.0462753010802001 | validation: 0.03452798300677789]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/500:
	Training over batches...
		[batch 4/4] avg loss: 0.045667780921873055		[learning rate: 0.0011885]
	Learning Rate: 0.0011885
	LOSS [training: 0.045667780921873055 | validation: 0.0345937601549536]
	TIME [epoch: 18.7 sec]
EPOCH 190/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04341656060464112		[learning rate: 0.0011704]
	Learning Rate: 0.0011704
	LOSS [training: 0.04341656060464112 | validation: 0.032215773534782874]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/500:
	Training over batches...
		[batch 4/4] avg loss: 0.042628603237110145		[learning rate: 0.0011526]
	Learning Rate: 0.00115257
	LOSS [training: 0.042628603237110145 | validation: 0.031009638845761522]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/500:
	Training over batches...
		[batch 4/4] avg loss: 0.041097726587567024		[learning rate: 0.001135]
	Learning Rate: 0.00113501
	LOSS [training: 0.041097726587567024 | validation: 0.030435450712628313]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/500:
	Training over batches...
		[batch 4/4] avg loss: 0.040174680275257495		[learning rate: 0.0011177]
	Learning Rate: 0.00111772
	LOSS [training: 0.040174680275257495 | validation: 0.030602877280652526]
	TIME [epoch: 18.7 sec]
EPOCH 194/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03890810856641422		[learning rate: 0.0011007]
	Learning Rate: 0.00110069
	LOSS [training: 0.03890810856641422 | validation: 0.028464804874501946]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03739935680002763		[learning rate: 0.0010839]
	Learning Rate: 0.00108393
	LOSS [training: 0.03739935680002763 | validation: 0.028481842812484438]
	TIME [epoch: 18.7 sec]
EPOCH 196/500:
	Training over batches...
		[batch 4/4] avg loss: 0.037572325214472374		[learning rate: 0.0010674]
	Learning Rate: 0.00106741
	LOSS [training: 0.037572325214472374 | validation: 0.028524330805699428]
	TIME [epoch: 18.7 sec]
EPOCH 197/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03505537138879124		[learning rate: 0.0010512]
	Learning Rate: 0.00105115
	LOSS [training: 0.03505537138879124 | validation: 0.025617966523118776]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03431566654753977		[learning rate: 0.0010351]
	Learning Rate: 0.00103514
	LOSS [training: 0.03431566654753977 | validation: 0.0233364201558411]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/500:
	Training over batches...
		[batch 4/4] avg loss: 0.033029594572775656		[learning rate: 0.0010194]
	Learning Rate: 0.00101937
	LOSS [training: 0.033029594572775656 | validation: 0.023202205862792268]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03189974615919379		[learning rate: 0.0010038]
	Learning Rate: 0.00100384
	LOSS [training: 0.03189974615919379 | validation: 0.022662951756680556]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/500:
	Training over batches...
		[batch 4/4] avg loss: 0.030171223165407438		[learning rate: 0.00098855]
	Learning Rate: 0.000988553
	LOSS [training: 0.030171223165407438 | validation: 0.02206646971247865]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02996329045671015		[learning rate: 0.00097349]
	Learning Rate: 0.000973494
	LOSS [training: 0.02996329045671015 | validation: 0.02204356631791972]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/500:
	Training over batches...
		[batch 4/4] avg loss: 0.028839700178140616		[learning rate: 0.00095866]
	Learning Rate: 0.000958664
	LOSS [training: 0.028839700178140616 | validation: 0.022096885873975965]
	TIME [epoch: 18.7 sec]
EPOCH 204/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02897565068214098		[learning rate: 0.00094406]
	Learning Rate: 0.000944061
	LOSS [training: 0.02897565068214098 | validation: 0.020987914625188106]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/500:
	Training over batches...
		[batch 4/4] avg loss: 0.027816380441372673		[learning rate: 0.00092968]
	Learning Rate: 0.00092968
	LOSS [training: 0.027816380441372673 | validation: 0.018867108763049423]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/500:
	Training over batches...
		[batch 4/4] avg loss: 0.026501187374191392		[learning rate: 0.00091552]
	Learning Rate: 0.000915518
	LOSS [training: 0.026501187374191392 | validation: 0.018910298617011295]
	TIME [epoch: 18.8 sec]
EPOCH 207/500:
	Training over batches...
		[batch 4/4] avg loss: 0.025674042323422505		[learning rate: 0.00090157]
	Learning Rate: 0.000901571
	LOSS [training: 0.025674042323422505 | validation: 0.019225442279089476]
	TIME [epoch: 18.7 sec]
EPOCH 208/500:
	Training over batches...
		[batch 4/4] avg loss: 0.025667156997977066		[learning rate: 0.00088784]
	Learning Rate: 0.000887837
	LOSS [training: 0.025667156997977066 | validation: 0.017556387860801166]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/500:
	Training over batches...
		[batch 4/4] avg loss: 0.024567019744926314		[learning rate: 0.00087431]
	Learning Rate: 0.000874312
	LOSS [training: 0.024567019744926314 | validation: 0.017118083042307948]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02355784485347117		[learning rate: 0.00086099]
	Learning Rate: 0.000860994
	LOSS [training: 0.02355784485347117 | validation: 0.017336143516015565]
	TIME [epoch: 18.6 sec]
EPOCH 211/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02309332966174953		[learning rate: 0.00084788]
	Learning Rate: 0.000847878
	LOSS [training: 0.02309332966174953 | validation: 0.017050631586278203]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02226899895811918		[learning rate: 0.00083496]
	Learning Rate: 0.000834962
	LOSS [training: 0.02226899895811918 | validation: 0.015073181368027906]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02169253254638773		[learning rate: 0.00082224]
	Learning Rate: 0.000822243
	LOSS [training: 0.02169253254638773 | validation: 0.014458948897728181]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/500:
	Training over batches...
		[batch 4/4] avg loss: 0.020334072099992165		[learning rate: 0.00080972]
	Learning Rate: 0.000809717
	LOSS [training: 0.020334072099992165 | validation: 0.013789430650665676]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/500:
	Training over batches...
		[batch 4/4] avg loss: 0.020049176714303073		[learning rate: 0.00079738]
	Learning Rate: 0.000797382
	LOSS [training: 0.020049176714303073 | validation: 0.01384074530691541]
	TIME [epoch: 18.6 sec]
EPOCH 216/500:
	Training over batches...
		[batch 4/4] avg loss: 0.019775833498223167		[learning rate: 0.00078524]
	Learning Rate: 0.000785236
	LOSS [training: 0.019775833498223167 | validation: 0.013901694942193392]
	TIME [epoch: 18.7 sec]
EPOCH 217/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0188648580189647		[learning rate: 0.00077327]
	Learning Rate: 0.000773274
	LOSS [training: 0.0188648580189647 | validation: 0.015058171260559281]
	TIME [epoch: 18.7 sec]
EPOCH 218/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01873666587354539		[learning rate: 0.00076149]
	Learning Rate: 0.000761494
	LOSS [training: 0.01873666587354539 | validation: 0.014537702938222059]
	TIME [epoch: 18.7 sec]
EPOCH 219/500:
	Training over batches...
		[batch 4/4] avg loss: 0.018667668746245654		[learning rate: 0.00074989]
	Learning Rate: 0.000749894
	LOSS [training: 0.018667668746245654 | validation: 0.013767542365364496]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/500:
	Training over batches...
		[batch 4/4] avg loss: 0.017836716607717528		[learning rate: 0.00073847]
	Learning Rate: 0.000738471
	LOSS [training: 0.017836716607717528 | validation: 0.01222505312412957]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01759112238885783		[learning rate: 0.00072722]
	Learning Rate: 0.000727221
	LOSS [training: 0.01759112238885783 | validation: 0.011177638323427861]
	TIME [epoch: 18.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/500:
	Training over batches...
		[batch 4/4] avg loss: 0.016473265560753868		[learning rate: 0.00071614]
	Learning Rate: 0.000716143
	LOSS [training: 0.016473265560753868 | validation: 0.013410609870297144]
	TIME [epoch: 18.7 sec]
EPOCH 223/500:
	Training over batches...
		[batch 4/4] avg loss: 0.016486377333198275		[learning rate: 0.00070523]
	Learning Rate: 0.000705234
	LOSS [training: 0.016486377333198275 | validation: 0.01276985810808579]
	TIME [epoch: 18.6 sec]
EPOCH 224/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01569948164322372		[learning rate: 0.00069449]
	Learning Rate: 0.000694491
	LOSS [training: 0.01569948164322372 | validation: 0.010740278553047256]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015394217241058761		[learning rate: 0.00068391]
	Learning Rate: 0.000683912
	LOSS [training: 0.015394217241058761 | validation: 0.012141681361226803]
	TIME [epoch: 18.7 sec]
EPOCH 226/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015166302439285191		[learning rate: 0.00067349]
	Learning Rate: 0.000673493
	LOSS [training: 0.015166302439285191 | validation: 0.01026534407754513]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01403576437309449		[learning rate: 0.00066323]
	Learning Rate: 0.000663234
	LOSS [training: 0.01403576437309449 | validation: 0.010525249414557084]
	TIME [epoch: 18.7 sec]
EPOCH 228/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01360721957345373		[learning rate: 0.00065313]
	Learning Rate: 0.00065313
	LOSS [training: 0.01360721957345373 | validation: 0.010077652687292287]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/500:
	Training over batches...
		[batch 4/4] avg loss: 0.013430479121992068		[learning rate: 0.00064318]
	Learning Rate: 0.000643181
	LOSS [training: 0.013430479121992068 | validation: 0.01030149583662357]
	TIME [epoch: 18.7 sec]
EPOCH 230/500:
	Training over batches...
		[batch 4/4] avg loss: 0.013301112246301669		[learning rate: 0.00063338]
	Learning Rate: 0.000633383
	LOSS [training: 0.013301112246301669 | validation: 0.008396703770524984]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/500:
	Training over batches...
		[batch 4/4] avg loss: 0.013327172308080275		[learning rate: 0.00062373]
	Learning Rate: 0.000623735
	LOSS [training: 0.013327172308080275 | validation: 0.009220015436571222]
	TIME [epoch: 18.7 sec]
EPOCH 232/500:
	Training over batches...
		[batch 4/4] avg loss: 0.012458066256151947		[learning rate: 0.00061423]
	Learning Rate: 0.000614233
	LOSS [training: 0.012458066256151947 | validation: 0.009909436331110995]
	TIME [epoch: 18.7 sec]
EPOCH 233/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01190961953443368		[learning rate: 0.00060488]
	Learning Rate: 0.000604876
	LOSS [training: 0.01190961953443368 | validation: 0.009215782510032147]
	TIME [epoch: 18.7 sec]
EPOCH 234/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011170867759483224		[learning rate: 0.00059566]
	Learning Rate: 0.000595662
	LOSS [training: 0.011170867759483224 | validation: 0.007838735915820376]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011339798597365018		[learning rate: 0.00058659]
	Learning Rate: 0.000586588
	LOSS [training: 0.011339798597365018 | validation: 0.007437724368482518]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011140772968855272		[learning rate: 0.00057765]
	Learning Rate: 0.000577652
	LOSS [training: 0.011140772968855272 | validation: 0.0069678631305067915]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010256357355554059		[learning rate: 0.00056885]
	Learning Rate: 0.000568853
	LOSS [training: 0.010256357355554059 | validation: 0.005884045068671385]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010028293529557883		[learning rate: 0.00056019]
	Learning Rate: 0.000560187
	LOSS [training: 0.010028293529557883 | validation: 0.007471294592237455]
	TIME [epoch: 18.8 sec]
EPOCH 239/500:
	Training over batches...
		[batch 4/4] avg loss: 0.010467771777486576		[learning rate: 0.00055165]
	Learning Rate: 0.000551654
	LOSS [training: 0.010467771777486576 | validation: 0.006277145408088181]
	TIME [epoch: 18.7 sec]
EPOCH 240/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0099705728454384		[learning rate: 0.00054325]
	Learning Rate: 0.00054325
	LOSS [training: 0.0099705728454384 | validation: 0.005691775188772329]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00962252811325368		[learning rate: 0.00053497]
	Learning Rate: 0.000534975
	LOSS [training: 0.00962252811325368 | validation: 0.007364477754599551]
	TIME [epoch: 18.8 sec]
EPOCH 242/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009640761432625503		[learning rate: 0.00052683]
	Learning Rate: 0.000526825
	LOSS [training: 0.009640761432625503 | validation: 0.00680419871046794]
	TIME [epoch: 18.6 sec]
EPOCH 243/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008907110328521867		[learning rate: 0.0005188]
	Learning Rate: 0.0005188
	LOSS [training: 0.008907110328521867 | validation: 0.006007093965657413]
	TIME [epoch: 18.7 sec]
EPOCH 244/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009053299750965173		[learning rate: 0.0005109]
	Learning Rate: 0.000510897
	LOSS [training: 0.009053299750965173 | validation: 0.006459313430029687]
	TIME [epoch: 18.8 sec]
EPOCH 245/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008691566863178343		[learning rate: 0.00050311]
	Learning Rate: 0.000503114
	LOSS [training: 0.008691566863178343 | validation: 0.005989974417021739]
	TIME [epoch: 18.7 sec]
EPOCH 246/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007817321007742357		[learning rate: 0.00049545]
	Learning Rate: 0.00049545
	LOSS [training: 0.007817321007742357 | validation: 0.005702863534458505]
	TIME [epoch: 18.7 sec]
EPOCH 247/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00834741277342156		[learning rate: 0.0004879]
	Learning Rate: 0.000487903
	LOSS [training: 0.00834741277342156 | validation: 0.0058231849937731405]
	TIME [epoch: 18.7 sec]
EPOCH 248/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008835764199731729		[learning rate: 0.00048047]
	Learning Rate: 0.00048047
	LOSS [training: 0.008835764199731729 | validation: 0.00624528133469821]
	TIME [epoch: 18.7 sec]
EPOCH 249/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00810132809431508		[learning rate: 0.00047315]
	Learning Rate: 0.000473151
	LOSS [training: 0.00810132809431508 | validation: 0.005186780022052551]
	TIME [epoch: 18.8 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007244462149956509		[learning rate: 0.00046594]
	Learning Rate: 0.000465944
	LOSS [training: 0.007244462149956509 | validation: 0.005125687823751961]
	TIME [epoch: 18.7 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007475957581310637		[learning rate: 0.00045885]
	Learning Rate: 0.000458846
	LOSS [training: 0.007475957581310637 | validation: 0.005250152786519043]
	TIME [epoch: 136 sec]
EPOCH 252/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007625594259270037		[learning rate: 0.00045186]
	Learning Rate: 0.000451856
	LOSS [training: 0.007625594259270037 | validation: 0.005191539318838534]
	TIME [epoch: 41.6 sec]
EPOCH 253/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007120494628131452		[learning rate: 0.00044497]
	Learning Rate: 0.000444973
	LOSS [training: 0.007120494628131452 | validation: 0.006038486738303625]
	TIME [epoch: 41.5 sec]
EPOCH 254/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007425918836791465		[learning rate: 0.00043819]
	Learning Rate: 0.000438194
	LOSS [training: 0.007425918836791465 | validation: 0.004654668688006793]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006473611604485461		[learning rate: 0.00043152]
	Learning Rate: 0.000431519
	LOSS [training: 0.006473611604485461 | validation: 0.005078782341517903]
	TIME [epoch: 41.5 sec]
EPOCH 256/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0070411926698652335		[learning rate: 0.00042495]
	Learning Rate: 0.000424946
	LOSS [training: 0.0070411926698652335 | validation: 0.005069241884771137]
	TIME [epoch: 41.5 sec]
EPOCH 257/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0067008856099304995		[learning rate: 0.00041847]
	Learning Rate: 0.000418472
	LOSS [training: 0.0067008856099304995 | validation: 0.004478197347055433]
	TIME [epoch: 41.1 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005800132109230496		[learning rate: 0.0004121]
	Learning Rate: 0.000412098
	LOSS [training: 0.005800132109230496 | validation: 0.004632336566424577]
	TIME [epoch: 41.4 sec]
EPOCH 259/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006807867401231661		[learning rate: 0.00040582]
	Learning Rate: 0.00040582
	LOSS [training: 0.006807867401231661 | validation: 0.0035973970066853407]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00628390373964942		[learning rate: 0.00039964]
	Learning Rate: 0.000399638
	LOSS [training: 0.00628390373964942 | validation: 0.004426274656424882]
	TIME [epoch: 41.3 sec]
EPOCH 261/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006618039598920354		[learning rate: 0.00039355]
	Learning Rate: 0.00039355
	LOSS [training: 0.006618039598920354 | validation: 0.0032067096167346537]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005924119123915085		[learning rate: 0.00038755]
	Learning Rate: 0.000387555
	LOSS [training: 0.005924119123915085 | validation: 0.003946628985718227]
	TIME [epoch: 41.3 sec]
EPOCH 263/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005743893236185392		[learning rate: 0.00038165]
	Learning Rate: 0.000381651
	LOSS [training: 0.005743893236185392 | validation: 0.0028170694334463424]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005537819827400612		[learning rate: 0.00037584]
	Learning Rate: 0.000375837
	LOSS [training: 0.005537819827400612 | validation: 0.002941504645222497]
	TIME [epoch: 41.4 sec]
EPOCH 265/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005403045669333901		[learning rate: 0.00037011]
	Learning Rate: 0.000370112
	LOSS [training: 0.005403045669333901 | validation: 0.0036742336074795335]
	TIME [epoch: 41.4 sec]
EPOCH 266/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005533434020807372		[learning rate: 0.00036447]
	Learning Rate: 0.000364474
	LOSS [training: 0.005533434020807372 | validation: 0.004230836891737344]
	TIME [epoch: 41.4 sec]
EPOCH 267/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005279048244881127		[learning rate: 0.00035892]
	Learning Rate: 0.000358922
	LOSS [training: 0.005279048244881127 | validation: 0.00400272611799258]
	TIME [epoch: 41.4 sec]
EPOCH 268/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005186845488115403		[learning rate: 0.00035345]
	Learning Rate: 0.000353454
	LOSS [training: 0.005186845488115403 | validation: 0.0032451451666232773]
	TIME [epoch: 41.3 sec]
EPOCH 269/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005405668905118978		[learning rate: 0.00034807]
	Learning Rate: 0.00034807
	LOSS [training: 0.005405668905118978 | validation: 0.003483847470618384]
	TIME [epoch: 41.4 sec]
EPOCH 270/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005211759789412284		[learning rate: 0.00034277]
	Learning Rate: 0.000342768
	LOSS [training: 0.005211759789412284 | validation: 0.0030925279990973134]
	TIME [epoch: 41.4 sec]
EPOCH 271/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005407475249505109		[learning rate: 0.00033755]
	Learning Rate: 0.000337546
	LOSS [training: 0.005407475249505109 | validation: 0.0031602353208035486]
	TIME [epoch: 41.4 sec]
EPOCH 272/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004947806752145702		[learning rate: 0.0003324]
	Learning Rate: 0.000332404
	LOSS [training: 0.004947806752145702 | validation: 0.0037264367712368527]
	TIME [epoch: 41.4 sec]
EPOCH 273/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004731714794240588		[learning rate: 0.00032734]
	Learning Rate: 0.000327341
	LOSS [training: 0.004731714794240588 | validation: 0.0031445289594370165]
	TIME [epoch: 41.4 sec]
EPOCH 274/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004833356559867423		[learning rate: 0.00032235]
	Learning Rate: 0.000322354
	LOSS [training: 0.004833356559867423 | validation: 0.0029676045370622705]
	TIME [epoch: 41.4 sec]
EPOCH 275/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004472342619744917		[learning rate: 0.00031744]
	Learning Rate: 0.000317444
	LOSS [training: 0.004472342619744917 | validation: 0.003350246720509632]
	TIME [epoch: 41.5 sec]
EPOCH 276/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004428569716302824		[learning rate: 0.00031261]
	Learning Rate: 0.000312608
	LOSS [training: 0.004428569716302824 | validation: 0.0018906181692732048]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004379286109471525		[learning rate: 0.00030785]
	Learning Rate: 0.000307846
	LOSS [training: 0.004379286109471525 | validation: 0.003097085773065638]
	TIME [epoch: 41.3 sec]
EPOCH 278/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004515805692002593		[learning rate: 0.00030316]
	Learning Rate: 0.000303156
	LOSS [training: 0.004515805692002593 | validation: 0.0030101614305212653]
	TIME [epoch: 41.4 sec]
EPOCH 279/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003863234273059046		[learning rate: 0.00029854]
	Learning Rate: 0.000298538
	LOSS [training: 0.003863234273059046 | validation: 0.00275585025579784]
	TIME [epoch: 41.3 sec]
EPOCH 280/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004110667775749498		[learning rate: 0.00029399]
	Learning Rate: 0.000293991
	LOSS [training: 0.004110667775749498 | validation: 0.002801232089787277]
	TIME [epoch: 41.4 sec]
EPOCH 281/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003688441214850899		[learning rate: 0.00028951]
	Learning Rate: 0.000289512
	LOSS [training: 0.003688441214850899 | validation: 0.0028049662772423947]
	TIME [epoch: 41.4 sec]
EPOCH 282/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0037689110440038893		[learning rate: 0.0002851]
	Learning Rate: 0.000285102
	LOSS [training: 0.0037689110440038893 | validation: 0.0031418974729668907]
	TIME [epoch: 41.4 sec]
EPOCH 283/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0039201286367552485		[learning rate: 0.00028076]
	Learning Rate: 0.000280759
	LOSS [training: 0.0039201286367552485 | validation: 0.0036129438349848508]
	TIME [epoch: 41.4 sec]
EPOCH 284/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004054022156848059		[learning rate: 0.00027648]
	Learning Rate: 0.000276482
	LOSS [training: 0.004054022156848059 | validation: 0.0028894856797830283]
	TIME [epoch: 41.4 sec]
EPOCH 285/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003668562306748755		[learning rate: 0.00027227]
	Learning Rate: 0.00027227
	LOSS [training: 0.003668562306748755 | validation: 0.0011472846041372287]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003622587890766059		[learning rate: 0.00026812]
	Learning Rate: 0.000268123
	LOSS [training: 0.003622587890766059 | validation: 0.0027354355771868335]
	TIME [epoch: 41.4 sec]
EPOCH 287/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003271612480632113		[learning rate: 0.00026404]
	Learning Rate: 0.000264038
	LOSS [training: 0.003271612480632113 | validation: 0.002557370923597264]
	TIME [epoch: 41.4 sec]
EPOCH 288/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033832425073520482		[learning rate: 0.00026002]
	Learning Rate: 0.000260016
	LOSS [training: 0.0033832425073520482 | validation: 0.001884745597959488]
	TIME [epoch: 41.4 sec]
EPOCH 289/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003561461335941832		[learning rate: 0.00025606]
	Learning Rate: 0.000256055
	LOSS [training: 0.003561461335941832 | validation: 0.001963819180361295]
	TIME [epoch: 41.4 sec]
EPOCH 290/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003408402482961918		[learning rate: 0.00025215]
	Learning Rate: 0.000252154
	LOSS [training: 0.003408402482961918 | validation: 0.0025689263628690204]
	TIME [epoch: 41.3 sec]
EPOCH 291/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034882968376811923		[learning rate: 0.00024831]
	Learning Rate: 0.000248313
	LOSS [training: 0.0034882968376811923 | validation: 0.0021534021280716527]
	TIME [epoch: 41.3 sec]
EPOCH 292/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0032607860590721164		[learning rate: 0.00024453]
	Learning Rate: 0.000244531
	LOSS [training: 0.0032607860590721164 | validation: 0.0023555851695974102]
	TIME [epoch: 41.4 sec]
EPOCH 293/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0032077835912049024		[learning rate: 0.00024081]
	Learning Rate: 0.000240806
	LOSS [training: 0.0032077835912049024 | validation: 0.002322438479219509]
	TIME [epoch: 41.4 sec]
EPOCH 294/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029893266045130534		[learning rate: 0.00023714]
	Learning Rate: 0.000237137
	LOSS [training: 0.0029893266045130534 | validation: 0.002325435678658348]
	TIME [epoch: 41.3 sec]
EPOCH 295/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003091051651489653		[learning rate: 0.00023352]
	Learning Rate: 0.000233525
	LOSS [training: 0.003091051651489653 | validation: 0.0023343879002936515]
	TIME [epoch: 41.3 sec]
EPOCH 296/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0029091668607759426		[learning rate: 0.00022997]
	Learning Rate: 0.000229968
	LOSS [training: 0.0029091668607759426 | validation: 0.001058933218344397]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003181629446165826		[learning rate: 0.00022646]
	Learning Rate: 0.000226464
	LOSS [training: 0.003181629446165826 | validation: 0.0019639471049712285]
	TIME [epoch: 41.4 sec]
EPOCH 298/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0035798882828616233		[learning rate: 0.00022301]
	Learning Rate: 0.000223015
	LOSS [training: 0.0035798882828616233 | validation: 0.0021844299534006903]
	TIME [epoch: 41.4 sec]
EPOCH 299/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028373855105919797		[learning rate: 0.00021962]
	Learning Rate: 0.000219617
	LOSS [training: 0.0028373855105919797 | validation: 0.0028281069714115605]
	TIME [epoch: 41.4 sec]
EPOCH 300/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027609210681809842		[learning rate: 0.00021627]
	Learning Rate: 0.000216272
	LOSS [training: 0.0027609210681809842 | validation: 0.0025169940116779336]
	TIME [epoch: 41.4 sec]
EPOCH 301/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00247487891666288		[learning rate: 0.00021298]
	Learning Rate: 0.000212977
	LOSS [training: 0.00247487891666288 | validation: 0.0022020928512512727]
	TIME [epoch: 41.4 sec]
EPOCH 302/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025681829070006846		[learning rate: 0.00020973]
	Learning Rate: 0.000209733
	LOSS [training: 0.0025681829070006846 | validation: 0.0016722252377238483]
	TIME [epoch: 41.4 sec]
EPOCH 303/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025455501142899583		[learning rate: 0.00020654]
	Learning Rate: 0.000206538
	LOSS [training: 0.0025455501142899583 | validation: 0.002657584706195222]
	TIME [epoch: 41.4 sec]
EPOCH 304/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002592766859727582		[learning rate: 0.00020339]
	Learning Rate: 0.000203392
	LOSS [training: 0.002592766859727582 | validation: 0.0017492884635433499]
	TIME [epoch: 41.4 sec]
EPOCH 305/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002590837158357239		[learning rate: 0.00020029]
	Learning Rate: 0.000200293
	LOSS [training: 0.002590837158357239 | validation: 0.002357260281881003]
	TIME [epoch: 41.4 sec]
EPOCH 306/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025048777002174335		[learning rate: 0.00019724]
	Learning Rate: 0.000197242
	LOSS [training: 0.0025048777002174335 | validation: 0.0018361102204318565]
	TIME [epoch: 41.4 sec]
EPOCH 307/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002809041752711986		[learning rate: 0.00019424]
	Learning Rate: 0.000194238
	LOSS [training: 0.002809041752711986 | validation: 0.0016717757222206862]
	TIME [epoch: 41.4 sec]
EPOCH 308/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002360644390609738		[learning rate: 0.00019128]
	Learning Rate: 0.000191279
	LOSS [training: 0.002360644390609738 | validation: 0.0013795889006830014]
	TIME [epoch: 41.4 sec]
EPOCH 309/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021960080982177934		[learning rate: 0.00018836]
	Learning Rate: 0.000188365
	LOSS [training: 0.0021960080982177934 | validation: 0.0019059447391615567]
	TIME [epoch: 41.4 sec]
EPOCH 310/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002274839347911769		[learning rate: 0.0001855]
	Learning Rate: 0.000185495
	LOSS [training: 0.002274839347911769 | validation: 0.0017210015634450048]
	TIME [epoch: 41.5 sec]
EPOCH 311/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025716401552421993		[learning rate: 0.00018267]
	Learning Rate: 0.00018267
	LOSS [training: 0.0025716401552421993 | validation: 0.0014855541513511632]
	TIME [epoch: 41.4 sec]
EPOCH 312/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0024261571655133374		[learning rate: 0.00017989]
	Learning Rate: 0.000179887
	LOSS [training: 0.0024261571655133374 | validation: 0.0012326266553299872]
	TIME [epoch: 41.5 sec]
EPOCH 313/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002667493305067309		[learning rate: 0.00017715]
	Learning Rate: 0.000177147
	LOSS [training: 0.002667493305067309 | validation: 0.002407721290579108]
	TIME [epoch: 41.4 sec]
EPOCH 314/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002272923937501049		[learning rate: 0.00017445]
	Learning Rate: 0.000174448
	LOSS [training: 0.002272923937501049 | validation: 0.002777793749550981]
	TIME [epoch: 41.4 sec]
EPOCH 315/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00237951068710692		[learning rate: 0.00017179]
	Learning Rate: 0.000171791
	LOSS [training: 0.00237951068710692 | validation: 0.002294995632461016]
	TIME [epoch: 41.3 sec]
EPOCH 316/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023149596991500904		[learning rate: 0.00016917]
	Learning Rate: 0.000169174
	LOSS [training: 0.0023149596991500904 | validation: 0.002854222116890342]
	TIME [epoch: 41.4 sec]
EPOCH 317/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021768425151343198		[learning rate: 0.0001666]
	Learning Rate: 0.000166597
	LOSS [training: 0.0021768425151343198 | validation: 0.0017360634572990367]
	TIME [epoch: 41.4 sec]
EPOCH 318/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021765403677429774		[learning rate: 0.00016406]
	Learning Rate: 0.000164059
	LOSS [training: 0.0021765403677429774 | validation: 0.001185354117872116]
	TIME [epoch: 41.4 sec]
EPOCH 319/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021685438336277543		[learning rate: 0.00016156]
	Learning Rate: 0.00016156
	LOSS [training: 0.0021685438336277543 | validation: 0.0017149606872998466]
	TIME [epoch: 41.4 sec]
EPOCH 320/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002033580308716589		[learning rate: 0.0001591]
	Learning Rate: 0.000159099
	LOSS [training: 0.002033580308716589 | validation: 0.001518451486448029]
	TIME [epoch: 41.4 sec]
EPOCH 321/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002416268492848479		[learning rate: 0.00015668]
	Learning Rate: 0.000156675
	LOSS [training: 0.002416268492848479 | validation: 0.0006546235626946598]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017443082000321347		[learning rate: 0.00015429]
	Learning Rate: 0.000154288
	LOSS [training: 0.0017443082000321347 | validation: 0.0005452725367791946]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020058663636615933		[learning rate: 0.00015194]
	Learning Rate: 0.000151938
	LOSS [training: 0.0020058663636615933 | validation: 0.001254359396733253]
	TIME [epoch: 41.5 sec]
EPOCH 324/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021283667715857593		[learning rate: 0.00014962]
	Learning Rate: 0.000149624
	LOSS [training: 0.0021283667715857593 | validation: 0.002398788069016308]
	TIME [epoch: 41.4 sec]
EPOCH 325/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023096772233252836		[learning rate: 0.00014734]
	Learning Rate: 0.000147344
	LOSS [training: 0.0023096772233252836 | validation: 0.0014243627311958237]
	TIME [epoch: 41.5 sec]
EPOCH 326/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002286605276994969		[learning rate: 0.0001451]
	Learning Rate: 0.0001451
	LOSS [training: 0.002286605276994969 | validation: 0.0021663758815279976]
	TIME [epoch: 41.5 sec]
EPOCH 327/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002104771175472345		[learning rate: 0.00014289]
	Learning Rate: 0.000142889
	LOSS [training: 0.002104771175472345 | validation: 0.0015168070121422813]
	TIME [epoch: 41.5 sec]
EPOCH 328/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001992514046368882		[learning rate: 0.00014071]
	Learning Rate: 0.000140713
	LOSS [training: 0.001992514046368882 | validation: 0.0009830689119642196]
	TIME [epoch: 41.5 sec]
EPOCH 329/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002003741045206735		[learning rate: 0.00013857]
	Learning Rate: 0.000138569
	LOSS [training: 0.002003741045206735 | validation: 0.0019303494746339802]
	TIME [epoch: 41.5 sec]
EPOCH 330/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002299184349835749		[learning rate: 0.00013646]
	Learning Rate: 0.000136458
	LOSS [training: 0.002299184349835749 | validation: 0.0017203276160022937]
	TIME [epoch: 41.5 sec]
EPOCH 331/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022578460176558915		[learning rate: 0.00013438]
	Learning Rate: 0.00013438
	LOSS [training: 0.0022578460176558915 | validation: 0.0011971357162929284]
	TIME [epoch: 41.4 sec]
EPOCH 332/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00199732477176265		[learning rate: 0.00013233]
	Learning Rate: 0.000132333
	LOSS [training: 0.00199732477176265 | validation: 0.0012521648290532554]
	TIME [epoch: 41.5 sec]
EPOCH 333/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016804919274023542		[learning rate: 0.00013032]
	Learning Rate: 0.000130317
	LOSS [training: 0.0016804919274023542 | validation: 0.0008198163646735015]
	TIME [epoch: 41.4 sec]
EPOCH 334/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016619633018160101		[learning rate: 0.00012833]
	Learning Rate: 0.000128332
	LOSS [training: 0.0016619633018160101 | validation: 0.0008862064917381094]
	TIME [epoch: 41.5 sec]
EPOCH 335/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016231313197911624		[learning rate: 0.00012638]
	Learning Rate: 0.000126377
	LOSS [training: 0.0016231313197911624 | validation: 0.00011373390138210175]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018770828548739082		[learning rate: 0.00012445]
	Learning Rate: 0.000124451
	LOSS [training: 0.0018770828548739082 | validation: 0.001782167880425989]
	TIME [epoch: 41.4 sec]
EPOCH 337/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016042618897850842		[learning rate: 0.00012256]
	Learning Rate: 0.000122556
	LOSS [training: 0.0016042618897850842 | validation: 0.001768012878066091]
	TIME [epoch: 41.5 sec]
EPOCH 338/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001793213505678933		[learning rate: 0.00012069]
	Learning Rate: 0.000120689
	LOSS [training: 0.001793213505678933 | validation: 0.0008444545414344113]
	TIME [epoch: 41.5 sec]
EPOCH 339/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015210203418538773		[learning rate: 0.00011885]
	Learning Rate: 0.00011885
	LOSS [training: 0.0015210203418538773 | validation: 0.002028566075735732]
	TIME [epoch: 41.5 sec]
EPOCH 340/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017404003122855134		[learning rate: 0.00011704]
	Learning Rate: 0.00011704
	LOSS [training: 0.0017404003122855134 | validation: 0.001134680457352964]
	TIME [epoch: 41.5 sec]
EPOCH 341/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018544875978850075		[learning rate: 0.00011526]
	Learning Rate: 0.000115257
	LOSS [training: 0.0018544875978850075 | validation: 0.0011890984862112427]
	TIME [epoch: 41.4 sec]
EPOCH 342/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015673428944730217		[learning rate: 0.0001135]
	Learning Rate: 0.000113501
	LOSS [training: 0.0015673428944730217 | validation: 0.0012430994982515257]
	TIME [epoch: 41.4 sec]
EPOCH 343/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016971539324475348		[learning rate: 0.00011177]
	Learning Rate: 0.000111772
	LOSS [training: 0.0016971539324475348 | validation: 0.0018475436765286867]
	TIME [epoch: 41.4 sec]
EPOCH 344/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017963447026080714		[learning rate: 0.00011007]
	Learning Rate: 0.000110069
	LOSS [training: 0.0017963447026080714 | validation: 0.0014805678049265084]
	TIME [epoch: 41.4 sec]
EPOCH 345/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00136467183479333		[learning rate: 0.00010839]
	Learning Rate: 0.000108393
	LOSS [training: 0.00136467183479333 | validation: 0.00015678268964954654]
	TIME [epoch: 41.3 sec]
EPOCH 346/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017171997166030807		[learning rate: 0.00010674]
	Learning Rate: 0.000106742
	LOSS [training: 0.0017171997166030807 | validation: 0.0013146931988351623]
	TIME [epoch: 41.5 sec]
EPOCH 347/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016217515908811524		[learning rate: 0.00010512]
	Learning Rate: 0.000105115
	LOSS [training: 0.0016217515908811524 | validation: 0.00086906309549929]
	TIME [epoch: 41.4 sec]
EPOCH 348/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014170960406769994		[learning rate: 0.00010351]
	Learning Rate: 0.000103514
	LOSS [training: 0.0014170960406769994 | validation: 0.0009337203336989784]
	TIME [epoch: 41.5 sec]
EPOCH 349/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016879958464884486		[learning rate: 0.00010194]
	Learning Rate: 0.000101937
	LOSS [training: 0.0016879958464884486 | validation: 0.0014156916012079406]
	TIME [epoch: 41.5 sec]
EPOCH 350/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001553517051017588		[learning rate: 0.00010038]
	Learning Rate: 0.000100385
	LOSS [training: 0.001553517051017588 | validation: 0.0009212859102726637]
	TIME [epoch: 41.5 sec]
EPOCH 351/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017200523647334287		[learning rate: 9.8855e-05]
	Learning Rate: 9.88553e-05
	LOSS [training: 0.0017200523647334287 | validation: 0.000534303552672502]
	TIME [epoch: 41.5 sec]
EPOCH 352/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017337242237933353		[learning rate: 9.7349e-05]
	Learning Rate: 9.73494e-05
	LOSS [training: 0.0017337242237933353 | validation: 0.0006557434390381599]
	TIME [epoch: 41.5 sec]
EPOCH 353/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014301750967474413		[learning rate: 9.5866e-05]
	Learning Rate: 9.58665e-05
	LOSS [training: 0.0014301750967474413 | validation: 0.0005310970796167033]
	TIME [epoch: 41.8 sec]
EPOCH 354/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018530670236550375		[learning rate: 9.4406e-05]
	Learning Rate: 9.44061e-05
	LOSS [training: 0.0018530670236550375 | validation: 0.0006900102846906165]
	TIME [epoch: 41.5 sec]
EPOCH 355/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016340187313346874		[learning rate: 9.2968e-05]
	Learning Rate: 9.2968e-05
	LOSS [training: 0.0016340187313346874 | validation: 0.0010853362930736776]
	TIME [epoch: 41.5 sec]
EPOCH 356/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001691211624546817		[learning rate: 9.1552e-05]
	Learning Rate: 9.15518e-05
	LOSS [training: 0.001691211624546817 | validation: 0.0016691963280959122]
	TIME [epoch: 41.5 sec]
EPOCH 357/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016072699926728972		[learning rate: 9.0157e-05]
	Learning Rate: 9.01571e-05
	LOSS [training: 0.0016072699926728972 | validation: 0.00035881439653230054]
	TIME [epoch: 41.5 sec]
EPOCH 358/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016081241913051716		[learning rate: 8.8784e-05]
	Learning Rate: 8.87837e-05
	LOSS [training: 0.0016081241913051716 | validation: 0.0007448684896954867]
	TIME [epoch: 41.8 sec]
EPOCH 359/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019098621442524915		[learning rate: 8.7431e-05]
	Learning Rate: 8.74312e-05
	LOSS [training: 0.0019098621442524915 | validation: 0.0009577946084697571]
	TIME [epoch: 41.5 sec]
EPOCH 360/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014245742761700292		[learning rate: 8.6099e-05]
	Learning Rate: 8.60994e-05
	LOSS [training: 0.0014245742761700292 | validation: 0.0004767783710940234]
	TIME [epoch: 41.6 sec]
EPOCH 361/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013165747023570799		[learning rate: 8.4788e-05]
	Learning Rate: 8.47878e-05
	LOSS [training: 0.0013165747023570799 | validation: 0.0009159299274905566]
	TIME [epoch: 41.6 sec]
EPOCH 362/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013143479265651516		[learning rate: 8.3496e-05]
	Learning Rate: 8.34962e-05
	LOSS [training: 0.0013143479265651516 | validation: 0.0004673644891280735]
	TIME [epoch: 41.6 sec]
EPOCH 363/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014024610468411644		[learning rate: 8.2224e-05]
	Learning Rate: 8.22243e-05
	LOSS [training: 0.0014024610468411644 | validation: 0.0021911327407673187]
	TIME [epoch: 41.5 sec]
EPOCH 364/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010100389894373905		[learning rate: 8.0972e-05]
	Learning Rate: 8.09717e-05
	LOSS [training: 0.0010100389894373905 | validation: 0.0008729630246108328]
	TIME [epoch: 41.5 sec]
EPOCH 365/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017244082726741272		[learning rate: 7.9738e-05]
	Learning Rate: 7.97382e-05
	LOSS [training: 0.0017244082726741272 | validation: 2.21452432957503e-05]
	TIME [epoch: 41.6 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010327421236448323		[learning rate: 7.8524e-05]
	Learning Rate: 7.85235e-05
	LOSS [training: 0.0010327421236448323 | validation: 0.0011580947825558536]
	TIME [epoch: 41.6 sec]
EPOCH 367/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016067832292384373		[learning rate: 7.7327e-05]
	Learning Rate: 7.73274e-05
	LOSS [training: 0.0016067832292384373 | validation: 0.0006176624545045315]
	TIME [epoch: 41.6 sec]
EPOCH 368/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001371379010661908		[learning rate: 7.6149e-05]
	Learning Rate: 7.61494e-05
	LOSS [training: 0.001371379010661908 | validation: 0.0007377610708997469]
	TIME [epoch: 41.4 sec]
EPOCH 369/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015058808741428005		[learning rate: 7.4989e-05]
	Learning Rate: 7.49894e-05
	LOSS [training: 0.0015058808741428005 | validation: 0.0008310592874867968]
	TIME [epoch: 41.4 sec]
EPOCH 370/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014828594018755502		[learning rate: 7.3847e-05]
	Learning Rate: 7.38471e-05
	LOSS [training: 0.0014828594018755502 | validation: 0.00033459562708436905]
	TIME [epoch: 41.5 sec]
EPOCH 371/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012006819511597446		[learning rate: 7.2722e-05]
	Learning Rate: 7.27221e-05
	LOSS [training: 0.0012006819511597446 | validation: 0.0006478415876604973]
	TIME [epoch: 41.4 sec]
EPOCH 372/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012116970520725393		[learning rate: 7.1614e-05]
	Learning Rate: 7.16143e-05
	LOSS [training: 0.0012116970520725393 | validation: -0.00011422104532698407]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014666512145733066		[learning rate: 7.0523e-05]
	Learning Rate: 7.05234e-05
	LOSS [training: 0.0014666512145733066 | validation: 0.0017377772847807266]
	TIME [epoch: 41.4 sec]
EPOCH 374/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011239709107379724		[learning rate: 6.9449e-05]
	Learning Rate: 6.94491e-05
	LOSS [training: 0.0011239709107379724 | validation: 0.0005724563871230331]
	TIME [epoch: 41.4 sec]
EPOCH 375/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012859036323315536		[learning rate: 6.8391e-05]
	Learning Rate: 6.83912e-05
	LOSS [training: 0.0012859036323315536 | validation: 0.0010865709781633832]
	TIME [epoch: 41.4 sec]
EPOCH 376/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016107642513064537		[learning rate: 6.7349e-05]
	Learning Rate: 6.73493e-05
	LOSS [training: 0.0016107642513064537 | validation: 0.0023018142267213645]
	TIME [epoch: 41.4 sec]
EPOCH 377/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001025613685769735		[learning rate: 6.6323e-05]
	Learning Rate: 6.63234e-05
	LOSS [training: 0.001025613685769735 | validation: -0.00034206708131175436]
	TIME [epoch: 41.4 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010558137607173308		[learning rate: 6.5313e-05]
	Learning Rate: 6.5313e-05
	LOSS [training: 0.0010558137607173308 | validation: 3.9318827272131e-05]
	TIME [epoch: 41.5 sec]
EPOCH 379/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001172725697753531		[learning rate: 6.4318e-05]
	Learning Rate: 6.43181e-05
	LOSS [training: 0.001172725697753531 | validation: 0.00015344574325107938]
	TIME [epoch: 41.4 sec]
EPOCH 380/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015141462108472835		[learning rate: 6.3338e-05]
	Learning Rate: 6.33383e-05
	LOSS [training: 0.0015141462108472835 | validation: 3.70465489424836e-05]
	TIME [epoch: 41.4 sec]
EPOCH 381/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011941519925242105		[learning rate: 6.2373e-05]
	Learning Rate: 6.23735e-05
	LOSS [training: 0.0011941519925242105 | validation: 0.0006133229975136142]
	TIME [epoch: 41.4 sec]
EPOCH 382/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011386278523277283		[learning rate: 6.1423e-05]
	Learning Rate: 6.14233e-05
	LOSS [training: 0.0011386278523277283 | validation: 0.0008950159418286768]
	TIME [epoch: 41.3 sec]
EPOCH 383/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008506139727320241		[learning rate: 6.0488e-05]
	Learning Rate: 6.04876e-05
	LOSS [training: 0.0008506139727320241 | validation: 0.000151019578879664]
	TIME [epoch: 41.3 sec]
EPOCH 384/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014312808596814654		[learning rate: 5.9566e-05]
	Learning Rate: 5.95662e-05
	LOSS [training: 0.0014312808596814654 | validation: 0.0013731577931276524]
	TIME [epoch: 41.4 sec]
EPOCH 385/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011741066890094635		[learning rate: 5.8659e-05]
	Learning Rate: 5.86588e-05
	LOSS [training: 0.0011741066890094635 | validation: 0.0018237420855119991]
	TIME [epoch: 41.5 sec]
EPOCH 386/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010362033532853173		[learning rate: 5.7765e-05]
	Learning Rate: 5.77652e-05
	LOSS [training: 0.0010362033532853173 | validation: 0.0011666822202739914]
	TIME [epoch: 41.4 sec]
EPOCH 387/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012252572232598043		[learning rate: 5.6885e-05]
	Learning Rate: 5.68853e-05
	LOSS [training: 0.0012252572232598043 | validation: 0.0009793330840435367]
	TIME [epoch: 41.4 sec]
EPOCH 388/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010334242855672897		[learning rate: 5.6019e-05]
	Learning Rate: 5.60187e-05
	LOSS [training: 0.0010334242855672897 | validation: 0.000388169020270289]
	TIME [epoch: 41.4 sec]
EPOCH 389/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014051102165982471		[learning rate: 5.5165e-05]
	Learning Rate: 5.51654e-05
	LOSS [training: 0.0014051102165982471 | validation: 0.00015792486969607732]
	TIME [epoch: 41.4 sec]
EPOCH 390/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001155642678020529		[learning rate: 5.4325e-05]
	Learning Rate: 5.4325e-05
	LOSS [training: 0.001155642678020529 | validation: 0.0007191725867042131]
	TIME [epoch: 41.5 sec]
EPOCH 391/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011883211372029894		[learning rate: 5.3497e-05]
	Learning Rate: 5.34975e-05
	LOSS [training: 0.0011883211372029894 | validation: 0.0005556172979160263]
	TIME [epoch: 41.5 sec]
EPOCH 392/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009095402870196181		[learning rate: 5.2683e-05]
	Learning Rate: 5.26825e-05
	LOSS [training: 0.0009095402870196181 | validation: 0.0006445403178089495]
	TIME [epoch: 41.4 sec]
EPOCH 393/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010680386879394906		[learning rate: 5.188e-05]
	Learning Rate: 5.188e-05
	LOSS [training: 0.0010680386879394906 | validation: 0.0014640872727455375]
	TIME [epoch: 41.4 sec]
EPOCH 394/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0014291122315378882		[learning rate: 5.109e-05]
	Learning Rate: 5.10897e-05
	LOSS [training: 0.0014291122315378882 | validation: -0.0001273394943701187]
	TIME [epoch: 41.3 sec]
EPOCH 395/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001264188993709509		[learning rate: 5.0311e-05]
	Learning Rate: 5.03114e-05
	LOSS [training: 0.001264188993709509 | validation: 0.0009527079336644468]
	TIME [epoch: 41.5 sec]
EPOCH 396/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011741173726585261		[learning rate: 4.9545e-05]
	Learning Rate: 4.9545e-05
	LOSS [training: 0.0011741173726585261 | validation: 0.000948360740522848]
	TIME [epoch: 41.5 sec]
EPOCH 397/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007457538506445067		[learning rate: 4.879e-05]
	Learning Rate: 4.87903e-05
	LOSS [training: 0.0007457538506445067 | validation: 0.00043202618519507045]
	TIME [epoch: 41.4 sec]
EPOCH 398/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012130073333756776		[learning rate: 4.8047e-05]
	Learning Rate: 4.8047e-05
	LOSS [training: 0.0012130073333756776 | validation: 0.0009386044465327622]
	TIME [epoch: 41.4 sec]
EPOCH 399/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011118639685906335		[learning rate: 4.7315e-05]
	Learning Rate: 4.73151e-05
	LOSS [training: 0.0011118639685906335 | validation: 0.0006461038087552681]
	TIME [epoch: 41.4 sec]
EPOCH 400/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012466226021395517		[learning rate: 4.6594e-05]
	Learning Rate: 4.65944e-05
	LOSS [training: 0.0012466226021395517 | validation: 0.0010107374501080334]
	TIME [epoch: 41.4 sec]
EPOCH 401/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010545587549642035		[learning rate: 4.5885e-05]
	Learning Rate: 4.58846e-05
	LOSS [training: 0.0010545587549642035 | validation: 4.29136102536285e-05]
	TIME [epoch: 41.4 sec]
EPOCH 402/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000907900007370404		[learning rate: 4.5186e-05]
	Learning Rate: 4.51856e-05
	LOSS [training: 0.000907900007370404 | validation: 0.0005092225265139715]
	TIME [epoch: 41 sec]
EPOCH 403/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001344392455635096		[learning rate: 4.4497e-05]
	Learning Rate: 4.44973e-05
	LOSS [training: 0.001344392455635096 | validation: 0.0009622316175649633]
	TIME [epoch: 41.2 sec]
EPOCH 404/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011594121100991905		[learning rate: 4.3819e-05]
	Learning Rate: 4.38194e-05
	LOSS [training: 0.0011594121100991905 | validation: 0.0016627214102507555]
	TIME [epoch: 41 sec]
EPOCH 405/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012541538503922857		[learning rate: 4.3152e-05]
	Learning Rate: 4.31519e-05
	LOSS [training: 0.0012541538503922857 | validation: 0.00031978198664449976]
	TIME [epoch: 41 sec]
EPOCH 406/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000986227769802193		[learning rate: 4.2495e-05]
	Learning Rate: 4.24946e-05
	LOSS [training: 0.000986227769802193 | validation: 0.0011033496727736133]
	TIME [epoch: 40.9 sec]
EPOCH 407/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012995936346073727		[learning rate: 4.1847e-05]
	Learning Rate: 4.18472e-05
	LOSS [training: 0.0012995936346073727 | validation: 0.001257462851861308]
	TIME [epoch: 41 sec]
EPOCH 408/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013048526031266788		[learning rate: 4.121e-05]
	Learning Rate: 4.12098e-05
	LOSS [training: 0.0013048526031266788 | validation: 0.0016357789384946406]
	TIME [epoch: 41 sec]
EPOCH 409/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009538180204773949		[learning rate: 4.0582e-05]
	Learning Rate: 4.0582e-05
	LOSS [training: 0.0009538180204773949 | validation: 0.0009793633065485518]
	TIME [epoch: 41.1 sec]
EPOCH 410/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008971254769749593		[learning rate: 3.9964e-05]
	Learning Rate: 3.99638e-05
	LOSS [training: 0.0008971254769749593 | validation: 0.0010754817342108907]
	TIME [epoch: 41 sec]
EPOCH 411/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010227959695691183		[learning rate: 3.9355e-05]
	Learning Rate: 3.9355e-05
	LOSS [training: 0.0010227959695691183 | validation: 0.0005373886867225806]
	TIME [epoch: 41 sec]
EPOCH 412/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010653520671739965		[learning rate: 3.8755e-05]
	Learning Rate: 3.87555e-05
	LOSS [training: 0.0010653520671739965 | validation: 0.0007115988475220338]
	TIME [epoch: 41 sec]
EPOCH 413/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013070173081480754		[learning rate: 3.8165e-05]
	Learning Rate: 3.81651e-05
	LOSS [training: 0.0013070173081480754 | validation: 0.0008199402141769392]
	TIME [epoch: 41 sec]
EPOCH 414/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010436925934314619		[learning rate: 3.7584e-05]
	Learning Rate: 3.75837e-05
	LOSS [training: 0.0010436925934314619 | validation: 0.0006366302574507463]
	TIME [epoch: 41 sec]
EPOCH 415/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013432466035595463		[learning rate: 3.7011e-05]
	Learning Rate: 3.70112e-05
	LOSS [training: 0.0013432466035595463 | validation: 0.0005895995366289508]
	TIME [epoch: 41 sec]
EPOCH 416/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010160579267576122		[learning rate: 3.6447e-05]
	Learning Rate: 3.64474e-05
	LOSS [training: 0.0010160579267576122 | validation: 0.0011011248456639611]
	TIME [epoch: 40.9 sec]
EPOCH 417/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012558592429176383		[learning rate: 3.5892e-05]
	Learning Rate: 3.58922e-05
	LOSS [training: 0.0012558592429176383 | validation: 0.000793689865966574]
	TIME [epoch: 41.4 sec]
EPOCH 418/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012506485835567824		[learning rate: 3.5345e-05]
	Learning Rate: 3.53454e-05
	LOSS [training: 0.0012506485835567824 | validation: 0.0014826902277910437]
	TIME [epoch: 41.4 sec]
EPOCH 419/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011841584408280503		[learning rate: 3.4807e-05]
	Learning Rate: 3.4807e-05
	LOSS [training: 0.0011841584408280503 | validation: 0.0013450382093124103]
	TIME [epoch: 41.5 sec]
EPOCH 420/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000917505513406552		[learning rate: 3.4277e-05]
	Learning Rate: 3.42768e-05
	LOSS [training: 0.000917505513406552 | validation: 0.000830556879185294]
	TIME [epoch: 41.5 sec]
EPOCH 421/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009299091986017601		[learning rate: 3.3755e-05]
	Learning Rate: 3.37546e-05
	LOSS [training: 0.0009299091986017601 | validation: 0.0004641240417560977]
	TIME [epoch: 41.6 sec]
EPOCH 422/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012247019966656043		[learning rate: 3.324e-05]
	Learning Rate: 3.32404e-05
	LOSS [training: 0.0012247019966656043 | validation: 0.0003818387326290651]
	TIME [epoch: 41.6 sec]
EPOCH 423/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009844452272659361		[learning rate: 3.2734e-05]
	Learning Rate: 3.27341e-05
	LOSS [training: 0.0009844452272659361 | validation: 0.000528688633175741]
	TIME [epoch: 41.5 sec]
EPOCH 424/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011742297225185018		[learning rate: 3.2235e-05]
	Learning Rate: 3.22354e-05
	LOSS [training: 0.0011742297225185018 | validation: 0.0009471938093903991]
	TIME [epoch: 41.6 sec]
EPOCH 425/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011709441763868996		[learning rate: 3.1744e-05]
	Learning Rate: 3.17444e-05
	LOSS [training: 0.0011709441763868996 | validation: 0.0004490621324928692]
	TIME [epoch: 41.5 sec]
EPOCH 426/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009153821684455869		[learning rate: 3.1261e-05]
	Learning Rate: 3.12608e-05
	LOSS [training: 0.0009153821684455869 | validation: 0.0005602224099839517]
	TIME [epoch: 41.6 sec]
EPOCH 427/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001374786561948953		[learning rate: 3.0785e-05]
	Learning Rate: 3.07846e-05
	LOSS [training: 0.001374786561948953 | validation: 0.00040197278131719427]
	TIME [epoch: 41.6 sec]
EPOCH 428/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006368431963799897		[learning rate: 3.0316e-05]
	Learning Rate: 3.03156e-05
	LOSS [training: 0.0006368431963799897 | validation: 0.0002365133478763779]
	TIME [epoch: 41.5 sec]
EPOCH 429/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013741643164499778		[learning rate: 2.9854e-05]
	Learning Rate: 2.98538e-05
	LOSS [training: 0.0013741643164499778 | validation: 0.0012849888930488533]
	TIME [epoch: 41.4 sec]
EPOCH 430/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008393055818494742		[learning rate: 2.9399e-05]
	Learning Rate: 2.9399e-05
	LOSS [training: 0.0008393055818494742 | validation: 0.0011154275562979322]
	TIME [epoch: 41.4 sec]
EPOCH 431/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011634232100861779		[learning rate: 2.8951e-05]
	Learning Rate: 2.89512e-05
	LOSS [training: 0.0011634232100861779 | validation: 0.0005695824001000047]
	TIME [epoch: 41.4 sec]
EPOCH 432/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001088293054495466		[learning rate: 2.851e-05]
	Learning Rate: 2.85102e-05
	LOSS [training: 0.001088293054495466 | validation: 0.0006580831978404414]
	TIME [epoch: 41.5 sec]
EPOCH 433/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011196907506915489		[learning rate: 2.8076e-05]
	Learning Rate: 2.80759e-05
	LOSS [training: 0.0011196907506915489 | validation: 0.001048056382912061]
	TIME [epoch: 41.4 sec]
EPOCH 434/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010169128446483626		[learning rate: 2.7648e-05]
	Learning Rate: 2.76482e-05
	LOSS [training: 0.0010169128446483626 | validation: 0.0009571797139497789]
	TIME [epoch: 41.4 sec]
EPOCH 435/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010795768734378323		[learning rate: 2.7227e-05]
	Learning Rate: 2.7227e-05
	LOSS [training: 0.0010795768734378323 | validation: 0.0006639943124718531]
	TIME [epoch: 41.5 sec]
EPOCH 436/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009053535256796932		[learning rate: 2.6812e-05]
	Learning Rate: 2.68122e-05
	LOSS [training: 0.0009053535256796932 | validation: 0.0005653547199003347]
	TIME [epoch: 41.5 sec]
EPOCH 437/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008038140470614372		[learning rate: 2.6404e-05]
	Learning Rate: 2.64038e-05
	LOSS [training: 0.0008038140470614372 | validation: 0.0006069906537698593]
	TIME [epoch: 41.3 sec]
EPOCH 438/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012657129587799467		[learning rate: 2.6002e-05]
	Learning Rate: 2.60016e-05
	LOSS [training: 0.0012657129587799467 | validation: -0.00039345418429368806]
	TIME [epoch: 41 sec]
	Saving model to: out/model_training/model_algphiq_1a_v_mmd1_20241015_173508/states/model_algphiq_1a_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0013898704579801745		[learning rate: 2.5605e-05]
	Learning Rate: 2.56055e-05
	LOSS [training: 0.0013898704579801745 | validation: 0.0007860381460529249]
	TIME [epoch: 41 sec]
EPOCH 440/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010488101732779016		[learning rate: 2.5215e-05]
	Learning Rate: 2.52154e-05
	LOSS [training: 0.0010488101732779016 | validation: 0.0005570510464379859]
	TIME [epoch: 41 sec]
EPOCH 441/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000911427307855356		[learning rate: 2.4831e-05]
	Learning Rate: 2.48313e-05
	LOSS [training: 0.000911427307855356 | validation: 0.0004470802141757297]
	TIME [epoch: 40.9 sec]
EPOCH 442/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001122856269676719		[learning rate: 2.4453e-05]
	Learning Rate: 2.44531e-05
	LOSS [training: 0.001122856269676719 | validation: 0.0006134960622877168]
	TIME [epoch: 40.9 sec]
EPOCH 443/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001377903872456948		[learning rate: 2.4081e-05]
	Learning Rate: 2.40806e-05
	LOSS [training: 0.001377903872456948 | validation: 1.4345700139345614e-05]
	TIME [epoch: 40.9 sec]
EPOCH 444/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006893637439919998		[learning rate: 2.3714e-05]
	Learning Rate: 2.37137e-05
	LOSS [training: 0.0006893637439919998 | validation: 5.952255404611189e-05]
	TIME [epoch: 40.9 sec]
EPOCH 445/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010766492819198072		[learning rate: 2.3352e-05]
	Learning Rate: 2.33525e-05
	LOSS [training: 0.0010766492819198072 | validation: 0.0004474876452939749]
	TIME [epoch: 41 sec]
EPOCH 446/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001169794514680726		[learning rate: 2.2997e-05]
	Learning Rate: 2.29968e-05
	LOSS [training: 0.001169794514680726 | validation: 0.0010643294209807595]
	TIME [epoch: 40.9 sec]
EPOCH 447/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011679691530914057		[learning rate: 2.2646e-05]
	Learning Rate: 2.26464e-05
	LOSS [training: 0.0011679691530914057 | validation: 0.0018278010852595312]
	TIME [epoch: 40.8 sec]
EPOCH 448/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00114564364384934		[learning rate: 2.2301e-05]
	Learning Rate: 2.23015e-05
	LOSS [training: 0.00114564364384934 | validation: 0.000443621209998736]
	TIME [epoch: 40.9 sec]
EPOCH 449/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011227800204708968		[learning rate: 2.1962e-05]
	Learning Rate: 2.19617e-05
	LOSS [training: 0.0011227800204708968 | validation: 0.00020565111070377953]
	TIME [epoch: 40.9 sec]
EPOCH 450/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007544465329867982		[learning rate: 2.1627e-05]
	Learning Rate: 2.16272e-05
	LOSS [training: 0.0007544465329867982 | validation: 0.0008711102176127188]
	TIME [epoch: 41 sec]
EPOCH 451/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011797583346836364		[learning rate: 2.1298e-05]
	Learning Rate: 2.12977e-05
	LOSS [training: 0.0011797583346836364 | validation: -0.00023717884890279658]
	TIME [epoch: 41 sec]
EPOCH 452/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001197283654308095		[learning rate: 2.0973e-05]
	Learning Rate: 2.09733e-05
	LOSS [training: 0.001197283654308095 | validation: 0.001440036556704261]
	TIME [epoch: 40.9 sec]
EPOCH 453/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010216047216477986		[learning rate: 2.0654e-05]
	Learning Rate: 2.06538e-05
	LOSS [training: 0.0010216047216477986 | validation: 0.0010162965009448285]
	TIME [epoch: 40.9 sec]
EPOCH 454/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006936559412038137		[learning rate: 2.0339e-05]
	Learning Rate: 2.03392e-05
	LOSS [training: 0.0006936559412038137 | validation: 0.0007174843301516725]
	TIME [epoch: 41 sec]
EPOCH 455/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011929758721714399		[learning rate: 2.0029e-05]
	Learning Rate: 2.00293e-05
	LOSS [training: 0.0011929758721714399 | validation: -0.00012301305287899615]
	TIME [epoch: 40.9 sec]
EPOCH 456/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000859339193298355		[learning rate: 1.9724e-05]
	Learning Rate: 1.97242e-05
	LOSS [training: 0.000859339193298355 | validation: 0.00047127495516204477]
	TIME [epoch: 41 sec]
EPOCH 457/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009535747723695068		[learning rate: 1.9424e-05]
	Learning Rate: 1.94238e-05
	LOSS [training: 0.0009535747723695068 | validation: 0.0011041801137969963]
	TIME [epoch: 41 sec]
EPOCH 458/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0012889096640562065		[learning rate: 1.9128e-05]
	Learning Rate: 1.91279e-05
	LOSS [training: 0.0012889096640562065 | validation: 0.0008007014684420235]
	TIME [epoch: 41 sec]
EPOCH 459/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006963126198128489		[learning rate: 1.8836e-05]
	Learning Rate: 1.88365e-05
	LOSS [training: 0.0006963126198128489 | validation: 0.0011075727829350254]
	TIME [epoch: 41 sec]
EPOCH 460/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010092012221279017		[learning rate: 1.855e-05]
	Learning Rate: 1.85495e-05
	LOSS [training: 0.0010092012221279017 | validation: 4.905522753320168e-05]
	TIME [epoch: 41 sec]
EPOCH 461/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007767853552317467		[learning rate: 1.8267e-05]
	Learning Rate: 1.8267e-05
	LOSS [training: 0.0007767853552317467 | validation: 0.00016559400864014153]
	TIME [epoch: 41.1 sec]
EPOCH 462/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001266729773661897		[learning rate: 1.7989e-05]
	Learning Rate: 1.79887e-05
	LOSS [training: 0.001266729773661897 | validation: 0.00033854432973382355]
	TIME [epoch: 41 sec]
EPOCH 463/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009674683237174444		[learning rate: 1.7715e-05]
	Learning Rate: 1.77147e-05
	LOSS [training: 0.0009674683237174444 | validation: 0.0007273802461968981]
	TIME [epoch: 41 sec]
EPOCH 464/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009537413733580629		[learning rate: 1.7445e-05]
	Learning Rate: 1.74448e-05
	LOSS [training: 0.0009537413733580629 | validation: 0.0001725151281834556]
	TIME [epoch: 41 sec]
EPOCH 465/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010124109537145154		[learning rate: 1.7179e-05]
	Learning Rate: 1.71791e-05
	LOSS [training: 0.0010124109537145154 | validation: 0.0002734863220413235]
	TIME [epoch: 41.1 sec]
EPOCH 466/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010656696959454501		[learning rate: 1.6917e-05]
	Learning Rate: 1.69174e-05
	LOSS [training: 0.0010656696959454501 | validation: 0.00043807784066091896]
	TIME [epoch: 41 sec]
EPOCH 467/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009515875458315638		[learning rate: 1.666e-05]
	Learning Rate: 1.66597e-05
	LOSS [training: 0.0009515875458315638 | validation: 0.0008756632084353409]
	TIME [epoch: 41 sec]
EPOCH 468/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009479887324510446		[learning rate: 1.6406e-05]
	Learning Rate: 1.64059e-05
	LOSS [training: 0.0009479887324510446 | validation: 6.25413180641243e-05]
	TIME [epoch: 41.1 sec]
EPOCH 469/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000750558126454753		[learning rate: 1.6156e-05]
	Learning Rate: 1.6156e-05
	LOSS [training: 0.000750558126454753 | validation: 0.000630883259179003]
	TIME [epoch: 41 sec]
EPOCH 470/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000944289468578934		[learning rate: 1.591e-05]
	Learning Rate: 1.59099e-05
	LOSS [training: 0.000944289468578934 | validation: 0.0009177818505816191]
	TIME [epoch: 41 sec]
EPOCH 471/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008156445283900692		[learning rate: 1.5668e-05]
	Learning Rate: 1.56675e-05
	LOSS [training: 0.0008156445283900692 | validation: 0.00022095492075580305]
	TIME [epoch: 41.1 sec]
EPOCH 472/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008009452625842274		[learning rate: 1.5429e-05]
	Learning Rate: 1.54288e-05
	LOSS [training: 0.0008009452625842274 | validation: -7.81776755791972e-05]
	TIME [epoch: 41.1 sec]
EPOCH 473/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008994253294455634		[learning rate: 1.5194e-05]
	Learning Rate: 1.51938e-05
	LOSS [training: 0.0008994253294455634 | validation: 0.00045593218550925264]
	TIME [epoch: 41 sec]
EPOCH 474/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011028625369179147		[learning rate: 1.4962e-05]
	Learning Rate: 1.49624e-05
	LOSS [training: 0.0011028625369179147 | validation: 0.0004989680258496146]
	TIME [epoch: 41 sec]
EPOCH 475/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011942857663999812		[learning rate: 1.4734e-05]
	Learning Rate: 1.47344e-05
	LOSS [training: 0.0011942857663999812 | validation: 0.00024523657855194883]
	TIME [epoch: 41 sec]
EPOCH 476/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001124752844379742		[learning rate: 1.451e-05]
	Learning Rate: 1.451e-05
	LOSS [training: 0.001124752844379742 | validation: 0.0008979812446885122]
	TIME [epoch: 41 sec]
EPOCH 477/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007921140481962402		[learning rate: 1.4289e-05]
	Learning Rate: 1.42889e-05
	LOSS [training: 0.0007921140481962402 | validation: 0.00043274864445099804]
	TIME [epoch: 40.9 sec]
EPOCH 478/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009880014860827306		[learning rate: 1.4071e-05]
	Learning Rate: 1.40713e-05
	LOSS [training: 0.0009880014860827306 | validation: 0.0007437255838614893]
	TIME [epoch: 41 sec]
EPOCH 479/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000980483791448184		[learning rate: 1.3857e-05]
	Learning Rate: 1.38569e-05
	LOSS [training: 0.000980483791448184 | validation: 0.0010083797744690872]
	TIME [epoch: 41 sec]
EPOCH 480/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010406530021160754		[learning rate: 1.3646e-05]
	Learning Rate: 1.36458e-05
	LOSS [training: 0.0010406530021160754 | validation: 0.0003181310245008455]
	TIME [epoch: 41 sec]
EPOCH 481/500:
	Training over batches...
		[batch 4/4] avg loss: 0.000661028721554787		[learning rate: 1.3438e-05]
	Learning Rate: 1.3438e-05
	LOSS [training: 0.000661028721554787 | validation: 8.751160892368447e-05]
	TIME [epoch: 41 sec]
EPOCH 482/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008143836847508148		[learning rate: 1.3233e-05]
	Learning Rate: 1.32333e-05
	LOSS [training: 0.0008143836847508148 | validation: 0.0005380939268026199]
	TIME [epoch: 41 sec]
EPOCH 483/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009501563804629079		[learning rate: 1.3032e-05]
	Learning Rate: 1.30317e-05
	LOSS [training: 0.0009501563804629079 | validation: 6.692005097143072e-05]
	TIME [epoch: 41 sec]
EPOCH 484/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0008440425286715076		[learning rate: 1.2833e-05]
	Learning Rate: 1.28332e-05
	LOSS [training: 0.0008440425286715076 | validation: 0.0004119067574411051]
	TIME [epoch: 41 sec]
EPOCH 485/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009356028842451348		[learning rate: 1.2638e-05]
	Learning Rate: 1.26377e-05
	LOSS [training: 0.0009356028842451348 | validation: 0.00034497950290317194]
	TIME [epoch: 41 sec]
EPOCH 486/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007906895481406984		[learning rate: 1.2445e-05]
	Learning Rate: 1.24451e-05
	LOSS [training: 0.0007906895481406984 | validation: 0.00027076761991804863]
	TIME [epoch: 41 sec]
EPOCH 487/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009079823650998151		[learning rate: 1.2256e-05]
	Learning Rate: 1.22556e-05
	LOSS [training: 0.0009079823650998151 | validation: -0.00012857552546210503]
	TIME [epoch: 40.9 sec]
EPOCH 488/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009713328888302463		[learning rate: 1.2069e-05]
	Learning Rate: 1.20689e-05
	LOSS [training: 0.0009713328888302463 | validation: -7.16155942966403e-05]
	TIME [epoch: 41 sec]
EPOCH 489/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010141553246467926		[learning rate: 1.1885e-05]
	Learning Rate: 1.1885e-05
	LOSS [training: 0.0010141553246467926 | validation: 0.0008134448694428463]
	TIME [epoch: 41 sec]
EPOCH 490/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006632533188677115		[learning rate: 1.1704e-05]
	Learning Rate: 1.1704e-05
	LOSS [training: 0.0006632533188677115 | validation: -1.9094306062723983e-05]
	TIME [epoch: 41 sec]
EPOCH 491/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006463556045468497		[learning rate: 1.1526e-05]
	Learning Rate: 1.15257e-05
	LOSS [training: 0.0006463556045468497 | validation: 0.0003997012837999865]
	TIME [epoch: 41 sec]
EPOCH 492/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009117595078974844		[learning rate: 1.135e-05]
	Learning Rate: 1.13501e-05
	LOSS [training: 0.0009117595078974844 | validation: 0.0007494846020923425]
	TIME [epoch: 41 sec]
EPOCH 493/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0007890622411337987		[learning rate: 1.1177e-05]
	Learning Rate: 1.11772e-05
	LOSS [training: 0.0007890622411337987 | validation: 0.0007086674400586724]
	TIME [epoch: 41 sec]
EPOCH 494/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0006455742309794963		[learning rate: 1.1007e-05]
	Learning Rate: 1.10069e-05
	LOSS [training: 0.0006455742309794963 | validation: 0.000604067738376072]
	TIME [epoch: 41 sec]
EPOCH 495/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0005271844643466986		[learning rate: 1.0839e-05]
	Learning Rate: 1.08393e-05
	LOSS [training: 0.0005271844643466986 | validation: 0.000654203566905689]
	TIME [epoch: 41.1 sec]
EPOCH 496/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011036287756355366		[learning rate: 1.0674e-05]
	Learning Rate: 1.06741e-05
	LOSS [training: 0.0011036287756355366 | validation: 0.0010829094101156712]
	TIME [epoch: 41 sec]
EPOCH 497/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010997601607391082		[learning rate: 1.0512e-05]
	Learning Rate: 1.05115e-05
	LOSS [training: 0.0010997601607391082 | validation: -1.6048184085774245e-05]
	TIME [epoch: 41 sec]
EPOCH 498/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0009768190163393195		[learning rate: 1.0351e-05]
	Learning Rate: 1.03514e-05
	LOSS [training: 0.0009768190163393195 | validation: 0.0005484887964532935]
	TIME [epoch: 41 sec]
EPOCH 499/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0010154346821725055		[learning rate: 1.0194e-05]
	Learning Rate: 1.01937e-05
	LOSS [training: 0.0010154346821725055 | validation: 0.0006865651801589561]
	TIME [epoch: 41.1 sec]
EPOCH 500/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0011681280975560228		[learning rate: 1.0038e-05]
	Learning Rate: 1.00384e-05
	LOSS [training: 0.0011681280975560228 | validation: 0.0012501209880627067]
	TIME [epoch: 41 sec]
Finished training in 14233.499 seconds.
