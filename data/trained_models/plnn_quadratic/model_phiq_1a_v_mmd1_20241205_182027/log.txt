Args:
Namespace(name='model_phiq_1a_v_mmd1', outdir='out/model_training/model_phiq_1a_v_mmd1', training_data='data/training_data/basic/data_phiq_1a/training', validation_data='data/training_data/basic/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[100, 250, 500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 116579891

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.080883335815248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.080883335815248 | validation: 5.221894007468242]
	TIME [epoch: 424 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.097796004288234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.097796004288234 | validation: 5.046628447948797]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.044001116235831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.044001116235831 | validation: 4.771937603424551]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.894692840323394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.894692840323394 | validation: 4.852165400880696]
	TIME [epoch: 6.23 sec]
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.796653055962399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.796653055962399 | validation: 4.882137902832969]
	TIME [epoch: 6.23 sec]
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.689463171855935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.689463171855935 | validation: 4.6879951002767335]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.7507369462937294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7507369462937294 | validation: 4.847128052851282]
	TIME [epoch: 6.25 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.6492029793429595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6492029793429595 | validation: 4.628325858827376]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.377378509697114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377378509697114 | validation: 4.668989470562581]
	TIME [epoch: 6.22 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.429188379805768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.429188379805768 | validation: 4.397499278451079]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.185716406779459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.185716406779459 | validation: 4.175861571045035]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.116303396222323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.116303396222323 | validation: 4.050434301402454]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9120251918332047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9120251918332047 | validation: 3.8627765185848126]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.745831595541691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.745831595541691 | validation: 3.7554647765432145]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.50052223440894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.50052223440894 | validation: 3.2126357060090918]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0696670427788337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0696670427788337 | validation: 2.9709206518377034]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7909732196175483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7909732196175483 | validation: 2.8487719895115644]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6557920661767804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6557920661767804 | validation: 2.6786834592733317]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.602051874402748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.602051874402748 | validation: 2.5962817131927896]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.458285981222297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.458285981222297 | validation: 2.4443151679010704]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.319312172460908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.319312172460908 | validation: 2.3901103737237337]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2485682590158698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2485682590158698 | validation: 2.1726832804478082]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.06733553342898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.06733553342898 | validation: 2.030515213997388]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9246413395281854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9246413395281854 | validation: 1.8970424054188548]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8267859100346735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8267859100346735 | validation: 1.7941675049879464]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7859071236299677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7859071236299677 | validation: 1.7571169148222685]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7336175650577603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7336175650577603 | validation: 1.7030712461832227]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7079910848706752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7079910848706752 | validation: 1.6871154889994697]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6935080545147518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6935080545147518 | validation: 1.6500496754091385]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.636282884573185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.636282884573185 | validation: 1.6150666422346265]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6242337126111344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6242337126111344 | validation: 1.6260350791518408]
	TIME [epoch: 6.22 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5968475341831312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5968475341831312 | validation: 1.5544772641179763]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5559837164669972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5559837164669972 | validation: 1.5544394955282737]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5552373701886004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5552373701886004 | validation: 1.5303733952650238]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5235465851277321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5235465851277321 | validation: 1.5135686250449198]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5118867168755723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5118867168755723 | validation: 1.4895822018968143]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4808740645707652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4808740645707652 | validation: 1.4518089346597578]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4817158423389656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4817158423389656 | validation: 1.437697948078426]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4379206675505152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4379206675505152 | validation: 1.4403376721480052]
	TIME [epoch: 6.22 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4093389468223658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4093389468223658 | validation: 1.418653006941014]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4349518904242564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4349518904242564 | validation: 1.4091424900500005]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3987237068142968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3987237068142968 | validation: 1.387421771861257]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3638300215011003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3638300215011003 | validation: 1.4587529234217285]
	TIME [epoch: 6.22 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.387106938842239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.387106938842239 | validation: 1.3966248922857598]
	TIME [epoch: 6.22 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3469821091673753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3469821091673753 | validation: 1.3453085664347435]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3166703146888856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3166703146888856 | validation: 1.3339146102920982]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3479563975939457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3479563975939457 | validation: 1.4981185006639879]
	TIME [epoch: 6.23 sec]
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.396830850344386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.396830850344386 | validation: 1.336370985392775]
	TIME [epoch: 6.21 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3144909002161738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3144909002161738 | validation: 1.305970397757282]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3006527696034316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3006527696034316 | validation: 1.2981751874823164]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2926552831813072		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 1.2926552831813072 | validation: 1.2993778694339242]
	TIME [epoch: 6.22 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.288076474797324		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 1.288076474797324 | validation: 1.2946141643143156]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.283916245483787		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 1.283916245483787 | validation: 1.2801564038611963]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2765778906676635		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 1.2765778906676635 | validation: 1.2908257436057826]
	TIME [epoch: 6.22 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2730654985084204		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.2730654985084204 | validation: 1.2926187777544595]
	TIME [epoch: 6.22 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2688191108003801		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 1.2688191108003801 | validation: 1.2928593619413926]
	TIME [epoch: 6.22 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2571011981284936		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 1.2571011981284936 | validation: 1.2486684703084128]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.271461808761262		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 1.271461808761262 | validation: 1.311792178574674]
	TIME [epoch: 6.22 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2594929223847602		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 1.2594929223847602 | validation: 1.2420177538376453]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2322428993297954		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 1.2322428993297954 | validation: 1.231286006117472]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.234680086253444		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 1.234680086253444 | validation: 1.2208490429364889]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.218118809654849		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 1.218118809654849 | validation: 1.2278105009248272]
	TIME [epoch: 6.23 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2514580750854698		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 1.2514580750854698 | validation: 1.262899198885264]
	TIME [epoch: 6.21 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2145869757876018		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 1.2145869757876018 | validation: 1.2196123028475367]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2273863486323653		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 1.2273863486323653 | validation: 1.1834590492505197]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.193642416000391		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 1.193642416000391 | validation: 1.1811046447620512]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2091804859132502		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 1.2091804859132502 | validation: 1.1601279099775725]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1888287479436626		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 1.1888287479436626 | validation: 1.158183364763211]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.15904106533756		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 1.15904106533756 | validation: 1.2949399684965255]
	TIME [epoch: 6.22 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3855836331142106		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 1.3855836331142106 | validation: 1.5562616792809638]
	TIME [epoch: 6.21 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.371873879626818		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 1.371873879626818 | validation: 1.1687353458836456]
	TIME [epoch: 6.22 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2440710832670052		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 1.2440710832670052 | validation: 1.2173076844332527]
	TIME [epoch: 6.22 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2235915919728755		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 1.2235915919728755 | validation: 1.146639421243754]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.200555837895321		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.200555837895321 | validation: 1.1416884674366337]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.180671562658525		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 1.180671562658525 | validation: 1.1264845548743916]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1645560069482244		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 1.1645560069482244 | validation: 1.152222068820139]
	TIME [epoch: 6.22 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1858694300977783		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 1.1858694300977783 | validation: 1.1133699628381926]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1441933172776602		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 1.1441933172776602 | validation: 1.1040581356987484]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1775496620193584		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 1.1775496620193584 | validation: 1.2217277584993056]
	TIME [epoch: 6.24 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1797299685953204		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 1.1797299685953204 | validation: 1.1847097163641451]
	TIME [epoch: 6.21 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1381286597106497		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 1.1381286597106497 | validation: 1.0928693430652847]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1433839750043124		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 1.1433839750043124 | validation: 1.2067383741882667]
	TIME [epoch: 6.21 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1448744740300731		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 1.1448744740300731 | validation: 1.1386344437542717]
	TIME [epoch: 6.21 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0977500876409414		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 1.0977500876409414 | validation: 1.0935800173004862]
	TIME [epoch: 6.21 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1921649811836517		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.1921649811836517 | validation: 1.1798920664299228]
	TIME [epoch: 6.22 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1291875218399867		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 1.1291875218399867 | validation: 1.125618401762491]
	TIME [epoch: 6.22 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0901792894904694		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 1.0901792894904694 | validation: 1.0469447287076956]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1459769063829799		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 1.1459769063829799 | validation: 1.1836898260610513]
	TIME [epoch: 6.22 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1418796519809462		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 1.1418796519809462 | validation: 1.129951739712918]
	TIME [epoch: 6.21 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0913261253715352		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.0913261253715352 | validation: 1.0324816144251225]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1459055452928972		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 1.1459055452928972 | validation: 1.0827024488443235]
	TIME [epoch: 6.22 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1593955993081235		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.1593955993081235 | validation: 1.0625059149640084]
	TIME [epoch: 6.21 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0723274363391286		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 1.0723274363391286 | validation: 1.0568609675663199]
	TIME [epoch: 6.22 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0875404354565168		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.0875404354565168 | validation: 1.0208375206681577]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.065929015559644		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 1.065929015559644 | validation: 0.992110649317788]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.052148465064704		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 1.052148465064704 | validation: 0.9349066512801539]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0426260603809925		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 1.0426260603809925 | validation: 0.9653302459150294]
	TIME [epoch: 6.22 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0326342345505704		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 1.0326342345505704 | validation: 0.9749879273681514]
	TIME [epoch: 6.22 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.052284717460801		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 1.052284717460801 | validation: 1.0630070287081899]
	TIME [epoch: 6.24 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0188489731405215		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 1.0188489731405215 | validation: 1.0934448462473603]
	TIME [epoch: 6.23 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0697456858096208		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 1.0697456858096208 | validation: 1.0623429145097185]
	TIME [epoch: 446 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0220691123528483		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 1.0220691123528483 | validation: 0.968996257740722]
	TIME [epoch: 12.3 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0298270860375087		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 1.0298270860375087 | validation: 1.0973166170448398]
	TIME [epoch: 12.3 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0392470322246319		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 1.0392470322246319 | validation: 0.903874952605924]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.922880168458683		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 0.922880168458683 | validation: 0.8973820736371616]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1129579869311672		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 1.1129579869311672 | validation: 1.1617805911305945]
	TIME [epoch: 12.3 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.045548817398307		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 1.045548817398307 | validation: 0.9192254671133253]
	TIME [epoch: 12.3 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9416444242824981		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 0.9416444242824981 | validation: 0.921594913798465]
	TIME [epoch: 12.3 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9555786795351926		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 0.9555786795351926 | validation: 1.0422803472254487]
	TIME [epoch: 12.3 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0160450058740476		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 1.0160450058740476 | validation: 0.8559571654540541]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0086699132846833		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 1.0086699132846833 | validation: 0.8860896407018335]
	TIME [epoch: 12.3 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8873185553614171		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.8873185553614171 | validation: 1.0786577599698852]
	TIME [epoch: 12.3 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9987001212885659		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 0.9987001212885659 | validation: 0.8124147698086274]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9166408231052123		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 0.9166408231052123 | validation: 0.9567222524003858]
	TIME [epoch: 12.3 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9228730576759558		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 0.9228730576759558 | validation: 1.1074434155399566]
	TIME [epoch: 12.3 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0174098746263502		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 1.0174098746263502 | validation: 0.9735005459009842]
	TIME [epoch: 12.3 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0001405783501902		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 1.0001405783501902 | validation: 0.9912938850948967]
	TIME [epoch: 12.3 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9480437445315073		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 0.9480437445315073 | validation: 1.01843843310015]
	TIME [epoch: 12.3 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9892555526319624		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 0.9892555526319624 | validation: 0.8993235348148009]
	TIME [epoch: 12.2 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9123002301035619		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 0.9123002301035619 | validation: 0.9030912469634379]
	TIME [epoch: 12.3 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.86434237195276		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 0.86434237195276 | validation: 0.9697689074686646]
	TIME [epoch: 12.3 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.902942148906581		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 0.902942148906581 | validation: 0.8550053331711651]
	TIME [epoch: 12.3 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9429184593601806		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 0.9429184593601806 | validation: 0.9447634652078257]
	TIME [epoch: 12.3 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9077578321976344		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 0.9077578321976344 | validation: 0.8642075440029944]
	TIME [epoch: 12.3 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9195046867629231		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 0.9195046867629231 | validation: 0.8161051561750325]
	TIME [epoch: 12.3 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8330680316679281		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 0.8330680316679281 | validation: 0.8767990552088007]
	TIME [epoch: 12.3 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8971866955972094		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 0.8971866955972094 | validation: 0.8425158069441874]
	TIME [epoch: 12.3 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8201733917275503		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 0.8201733917275503 | validation: 0.9803572178026931]
	TIME [epoch: 12.3 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9535403041562851		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 0.9535403041562851 | validation: 0.8146661775652995]
	TIME [epoch: 12.3 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8801199288453103		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 0.8801199288453103 | validation: 0.8520162311876117]
	TIME [epoch: 12.3 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8869436880516459		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.8869436880516459 | validation: 0.9980613578737341]
	TIME [epoch: 12.3 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9185202914922315		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 0.9185202914922315 | validation: 0.7419269265894052]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8140669360849431		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 0.8140669360849431 | validation: 0.8287790299573881]
	TIME [epoch: 12.3 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8396499304600992		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 0.8396499304600992 | validation: 0.9433633656218725]
	TIME [epoch: 12.3 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.156563680240297		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 1.156563680240297 | validation: 1.2118392391341295]
	TIME [epoch: 12.3 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0909449284835335		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 1.0909449284835335 | validation: 0.884922841340985]
	TIME [epoch: 12.3 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9566581342815408		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 0.9566581342815408 | validation: 0.9877448991646838]
	TIME [epoch: 12.3 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8814977248676349		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 0.8814977248676349 | validation: 0.960236560696416]
	TIME [epoch: 12.3 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0023735296378415		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 1.0023735296378415 | validation: 0.8433953367560034]
	TIME [epoch: 12.3 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7564088185990464		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 0.7564088185990464 | validation: 1.1291980265753647]
	TIME [epoch: 12.3 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9958806788232359		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 0.9958806788232359 | validation: 1.0557926746299728]
	TIME [epoch: 12.3 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9553198364753094		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 0.9553198364753094 | validation: 1.00668800232545]
	TIME [epoch: 12.3 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9334117568403301		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 0.9334117568403301 | validation: 0.8058600124848962]
	TIME [epoch: 12.3 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8581780354921823		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 0.8581780354921823 | validation: 0.9805591562211438]
	TIME [epoch: 12.3 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8601306758669126		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 0.8601306758669126 | validation: 0.8258080505268209]
	TIME [epoch: 12.3 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9355765519652441		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 0.9355765519652441 | validation: 0.856217821998875]
	TIME [epoch: 12.3 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8830535920429154		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 0.8830535920429154 | validation: 0.7850494324061439]
	TIME [epoch: 12.3 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8043256002634686		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 0.8043256002634686 | validation: 0.9356365989323987]
	TIME [epoch: 12.3 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8966378577672328		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 0.8966378577672328 | validation: 0.9521016000579798]
	TIME [epoch: 12.3 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8435933851604384		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.8435933851604384 | validation: 0.8862574912218892]
	TIME [epoch: 12.3 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8581942493928307		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 0.8581942493928307 | validation: 0.8605674919156231]
	TIME [epoch: 12.3 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7703192835881558		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 0.7703192835881558 | validation: 0.8727874241270763]
	TIME [epoch: 12.3 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9452623053800988		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 0.9452623053800988 | validation: 0.9176868675077547]
	TIME [epoch: 12.3 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8861022931639062		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 0.8861022931639062 | validation: 1.107046358348878]
	TIME [epoch: 12.3 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8682236895135229		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 0.8682236895135229 | validation: 0.8617241746416902]
	TIME [epoch: 12.3 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7931783066395481		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 0.7931783066395481 | validation: 0.9622818331151709]
	TIME [epoch: 12.3 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7963380889888847		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 0.7963380889888847 | validation: 0.693299444506522]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7079741736741777		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 0.7079741736741777 | validation: 0.8896085798769884]
	TIME [epoch: 12.3 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8930994214819417		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 0.8930994214819417 | validation: 0.7620571093061923]
	TIME [epoch: 12.3 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7953358704322387		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 0.7953358704322387 | validation: 0.8380115257611712]
	TIME [epoch: 12.3 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7910283872253302		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 0.7910283872253302 | validation: 0.7410113979979462]
	TIME [epoch: 12.3 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7581259862093797		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 0.7581259862093797 | validation: 1.062584246470832]
	TIME [epoch: 12.3 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7418357714878347		[learning rate: 0.004405]
	Learning Rate: 0.00440502
	LOSS [training: 0.7418357714878347 | validation: 0.8057042912797234]
	TIME [epoch: 12.3 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.679816062061622		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 0.679816062061622 | validation: 0.9747866232195155]
	TIME [epoch: 12.3 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8831239152031505		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 0.8831239152031505 | validation: 0.8851836240161546]
	TIME [epoch: 12.3 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6907738047322369		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 0.6907738047322369 | validation: 0.8191721241022549]
	TIME [epoch: 12.3 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7731393779778718		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 0.7731393779778718 | validation: 0.7863072685858399]
	TIME [epoch: 12.3 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7234305470435558		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 0.7234305470435558 | validation: 1.1919302079908571]
	TIME [epoch: 12.3 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9774763628846668		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.9774763628846668 | validation: 0.9751380065520179]
	TIME [epoch: 12.3 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9155878396877614		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 0.9155878396877614 | validation: 0.752400870111896]
	TIME [epoch: 12.3 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7711288676296987		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 0.7711288676296987 | validation: 0.7711236058046769]
	TIME [epoch: 12.3 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8755538149970079		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 0.8755538149970079 | validation: 1.11255776917871]
	TIME [epoch: 12.3 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8515225546885647		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 0.8515225546885647 | validation: 0.722661302346039]
	TIME [epoch: 12.3 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7260381485352779		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 0.7260381485352779 | validation: 0.7041679687590904]
	TIME [epoch: 12.3 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.740033516190624		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 0.740033516190624 | validation: 0.8526738326586101]
	TIME [epoch: 12.3 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7646334545941321		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 0.7646334545941321 | validation: 0.894909753977468]
	TIME [epoch: 12.3 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6473823535944514		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 0.6473823535944514 | validation: 0.8068004857837392]
	TIME [epoch: 12.3 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7065589881939836		[learning rate: 0.0039498]
	Learning Rate: 0.00394983
	LOSS [training: 0.7065589881939836 | validation: 0.6447937479779351]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7725622277119955		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 0.7725622277119955 | validation: 0.7347613503282955]
	TIME [epoch: 12.3 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6946838396100483		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 0.6946838396100483 | validation: 0.9102101758333163]
	TIME [epoch: 12.3 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.657106988345896		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 0.657106988345896 | validation: 0.7598256775190376]
	TIME [epoch: 12.2 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7279765287465074		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 0.7279765287465074 | validation: 0.9461530273880527]
	TIME [epoch: 12.3 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7267580834660813		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 0.7267580834660813 | validation: 0.813461426262462]
	TIME [epoch: 12.3 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6922467716021511		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 0.6922467716021511 | validation: 0.7843192421899392]
	TIME [epoch: 12.3 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6795037553829003		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 0.6795037553829003 | validation: 0.7118499567873442]
	TIME [epoch: 12.3 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8278451839522845		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 0.8278451839522845 | validation: 0.7078400793440025]
	TIME [epoch: 12.3 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7214026809293884		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 0.7214026809293884 | validation: 0.6770655614800225]
	TIME [epoch: 12.3 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6608709488856837		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.6608709488856837 | validation: 0.7461971936987277]
	TIME [epoch: 12.3 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7937688753650707		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 0.7937688753650707 | validation: 1.072798818300998]
	TIME [epoch: 12.3 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.755104663424508		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 0.755104663424508 | validation: 1.02267305144224]
	TIME [epoch: 12.3 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8493088687500945		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 0.8493088687500945 | validation: 1.203459213535932]
	TIME [epoch: 12.3 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7735639350338124		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 0.7735639350338124 | validation: 0.8180946989115947]
	TIME [epoch: 12.3 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.661413245436697		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 0.661413245436697 | validation: 0.9470041959635339]
	TIME [epoch: 12.3 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7198557074629108		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 0.7198557074629108 | validation: 0.8327482976069294]
	TIME [epoch: 12.3 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6173178331616123		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 0.6173178331616123 | validation: 0.9328314360107104]
	TIME [epoch: 12.3 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6453940984171395		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 0.6453940984171395 | validation: 0.7597540849774332]
	TIME [epoch: 12.3 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5755932601110361		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 0.5755932601110361 | validation: 0.7531707984590063]
	TIME [epoch: 12.3 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0151803144933709		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 1.0151803144933709 | validation: 1.1756024270594256]
	TIME [epoch: 12.3 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0249103796216879		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 1.0249103796216879 | validation: 1.0061397001621297]
	TIME [epoch: 12.3 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7718169087449893		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 0.7718169087449893 | validation: 0.6537529989692852]
	TIME [epoch: 12.3 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.635097852492751		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 0.635097852492751 | validation: 0.6983315920887907]
	TIME [epoch: 12.3 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6826034912120732		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 0.6826034912120732 | validation: 0.8551528414100598]
	TIME [epoch: 12.3 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7367327744038766		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 0.7367327744038766 | validation: 0.767417276573076]
	TIME [epoch: 12.3 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6913907682758741		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 0.6913907682758741 | validation: 0.6504287368571499]
	TIME [epoch: 12.3 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6342978785409783		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 0.6342978785409783 | validation: 0.6820104521982331]
	TIME [epoch: 12.3 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6600806361537328		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 0.6600806361537328 | validation: 0.6325458228525861]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5771800656233568		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.5771800656233568 | validation: 0.6828562041205132]
	TIME [epoch: 12.3 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.611637218742405		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 0.611637218742405 | validation: 0.6029555106507509]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.597319079146563		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 0.597319079146563 | validation: 0.8646372020860231]
	TIME [epoch: 12.3 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.651809021196057		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 0.651809021196057 | validation: 0.586035016132974]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5507384367055587		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 0.5507384367055587 | validation: 0.6251311790922147]
	TIME [epoch: 12.3 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5808433266949913		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 0.5808433266949913 | validation: 0.6147603092758693]
	TIME [epoch: 12.3 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6100832320842865		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 0.6100832320842865 | validation: 0.5597988353295444]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5222572771433078		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 0.5222572771433078 | validation: 0.6025794526617197]
	TIME [epoch: 12.3 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5782832376681956		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 0.5782832376681956 | validation: 0.5783209767973528]
	TIME [epoch: 12.3 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.609190598890537		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 0.609190598890537 | validation: 0.7629442965608543]
	TIME [epoch: 12.3 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5928908056487963		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 0.5928908056487963 | validation: 0.5702277146320733]
	TIME [epoch: 12.3 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5789916806470319		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 0.5789916806470319 | validation: 0.5434633083949149]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5808533678074732		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 0.5808533678074732 | validation: 0.5950353028185132]
	TIME [epoch: 12.3 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5522809880812128		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 0.5522809880812128 | validation: 0.7451471628622799]
	TIME [epoch: 12.3 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6921488529818557		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 0.6921488529818557 | validation: 0.6160436222348097]
	TIME [epoch: 12.2 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5557958103478646		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 0.5557958103478646 | validation: 0.5551437353217132]
	TIME [epoch: 12.3 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5778775013691043		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 0.5778775013691043 | validation: 0.589948921730548]
	TIME [epoch: 12.3 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5179861934785799		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 0.5179861934785799 | validation: 0.5076488275003764]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4750605375688657		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 0.4750605375688657 | validation: 0.6734573638855954]
	TIME [epoch: 12.3 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6724910818956149		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.6724910818956149 | validation: 0.6242272113180175]
	TIME [epoch: 12.3 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5478089041582351		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 0.5478089041582351 | validation: 0.6860927666485086]
	TIME [epoch: 12.2 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5775356521142325		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 0.5775356521142325 | validation: 0.6058036327837879]
	TIME [epoch: 12.3 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6312780590754167		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 0.6312780590754167 | validation: 0.5676547095477836]
	TIME [epoch: 12.3 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5440517127018236		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 0.5440517127018236 | validation: 0.6420423874814027]
	TIME [epoch: 12.3 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5579858912145063		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 0.5579858912145063 | validation: 0.7659958673367903]
	TIME [epoch: 12.3 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5986785577753704		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 0.5986785577753704 | validation: 0.5411503431711545]
	TIME [epoch: 12.3 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5095624108984984		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 0.5095624108984984 | validation: 0.5247404744572889]
	TIME [epoch: 12.3 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5096728562818199		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 0.5096728562818199 | validation: 0.6352899920677313]
	TIME [epoch: 12.3 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5419658246165642		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 0.5419658246165642 | validation: 0.6691992064340511]
	TIME [epoch: 12.3 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5963285728130588		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 0.5963285728130588 | validation: 0.7703082140611351]
	TIME [epoch: 12.3 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5691332512452293		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 0.5691332512452293 | validation: 0.5307561646636626]
	TIME [epoch: 12.3 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5203608195746007		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 0.5203608195746007 | validation: 0.7035527330869022]
	TIME [epoch: 12.3 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5771431779937866		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 0.5771431779937866 | validation: 0.5403898355100247]
	TIME [epoch: 12.3 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4905265286418422		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 0.4905265286418422 | validation: 0.6086873255033862]
	TIME [epoch: 12.3 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5671774670709027		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 0.5671774670709027 | validation: 0.534238046529947]
	TIME [epoch: 12.3 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5010524563797881		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 0.5010524563797881 | validation: 0.7103090935878963]
	TIME [epoch: 12.3 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5116664672437179		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 0.5116664672437179 | validation: 0.6318835181834006]
	TIME [epoch: 12.3 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5221118984675481		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 0.5221118984675481 | validation: 0.5089513750781662]
	TIME [epoch: 12.3 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.584001058557226		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.584001058557226 | validation: 0.6752916354715235]
	TIME [epoch: 12.3 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6196364952378463		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 0.6196364952378463 | validation: 0.5934184939133821]
	TIME [epoch: 12.3 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4996624310239521		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 0.4996624310239521 | validation: 0.939594259185657]
	TIME [epoch: 12.3 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5409839333922213		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 0.5409839333922213 | validation: 0.5046379203808881]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4816765129250373		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 0.4816765129250373 | validation: 0.5759567452905532]
	TIME [epoch: 12.3 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48427845818549903		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 0.48427845818549903 | validation: 0.6802546646084041]
	TIME [epoch: 12.3 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5369210414109197		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 0.5369210414109197 | validation: 0.5408768780906261]
	TIME [epoch: 466 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5161227104058335		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 0.5161227104058335 | validation: 0.5794875573432388]
	TIME [epoch: 26.3 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.465136604930962		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 0.465136604930962 | validation: 0.5895052941029192]
	TIME [epoch: 26.3 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4304112414499166		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 0.4304112414499166 | validation: 0.7997170153819575]
	TIME [epoch: 26.3 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5730845240133863		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 0.5730845240133863 | validation: 0.4916770773611356]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4811009844019278		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 0.4811009844019278 | validation: 0.5901611324873329]
	TIME [epoch: 26.2 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49327467856724416		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 0.49327467856724416 | validation: 0.5283648905318976]
	TIME [epoch: 26.2 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5616484495735585		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 0.5616484495735585 | validation: 0.5744628783188171]
	TIME [epoch: 26.3 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5546332394196413		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 0.5546332394196413 | validation: 0.5232646999953253]
	TIME [epoch: 26.3 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46174729475820514		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 0.46174729475820514 | validation: 0.4880381371603809]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4724430681111361		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 0.4724430681111361 | validation: 0.541171052734521]
	TIME [epoch: 26.2 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4932798183913538		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 0.4932798183913538 | validation: 0.4620228874736452]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5142867935677147		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 0.5142867935677147 | validation: 0.6295557256925655]
	TIME [epoch: 26.2 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5493724973122912		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.5493724973122912 | validation: 0.5013430573584017]
	TIME [epoch: 26.2 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4867658155894284		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 0.4867658155894284 | validation: 0.5230757268208254]
	TIME [epoch: 26.2 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.429393842411137		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 0.429393842411137 | validation: 0.5194935379851242]
	TIME [epoch: 26.2 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4536935258391134		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 0.4536935258391134 | validation: 0.5854992000394652]
	TIME [epoch: 26.2 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4887251587575224		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 0.4887251587575224 | validation: 0.5672763061995]
	TIME [epoch: 26.3 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4780559078828404		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 0.4780559078828404 | validation: 0.5655661313569978]
	TIME [epoch: 26.2 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4944197469203272		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 0.4944197469203272 | validation: 0.4620617278839482]
	TIME [epoch: 26.2 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43634453870572065		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 0.43634453870572065 | validation: 0.4741477323486637]
	TIME [epoch: 26.2 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48810649361041386		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 0.48810649361041386 | validation: 0.4681369964830039]
	TIME [epoch: 26.3 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4309149766322622		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 0.4309149766322622 | validation: 0.5843683521648041]
	TIME [epoch: 26.3 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5293443570367179		[learning rate: 0.0019653]
	Learning Rate: 0.00196526
	LOSS [training: 0.5293443570367179 | validation: 0.48109857157322317]
	TIME [epoch: 26.3 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44853356548337253		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 0.44853356548337253 | validation: 0.7423723936560362]
	TIME [epoch: 26.3 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5600281123555757		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 0.5600281123555757 | validation: 0.4951507216163919]
	TIME [epoch: 26.2 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44828991764359016		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 0.44828991764359016 | validation: 0.4825021434346869]
	TIME [epoch: 26.2 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4223580529821995		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 0.4223580529821995 | validation: 0.5583806641220674]
	TIME [epoch: 26.2 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4671566444641842		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 0.4671566444641842 | validation: 0.5122094134454964]
	TIME [epoch: 26.2 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4577878749581663		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 0.4577878749581663 | validation: 0.49058988080064425]
	TIME [epoch: 26.2 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46492590712769744		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 0.46492590712769744 | validation: 0.7117375685377196]
	TIME [epoch: 26.3 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5051337403240016		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 0.5051337403240016 | validation: 0.7448315261518512]
	TIME [epoch: 26.2 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5016218513054292		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.5016218513054292 | validation: 0.7314817910257154]
	TIME [epoch: 26.3 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4736553987344901		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 0.4736553987344901 | validation: 0.44546671597166276]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42833044325771163		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 0.42833044325771163 | validation: 0.542659903711133]
	TIME [epoch: 26.3 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4442624973728537		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 0.4442624973728537 | validation: 0.5020906622039796]
	TIME [epoch: 26.3 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4422030154851489		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.4422030154851489 | validation: 0.5296062106792967]
	TIME [epoch: 26.2 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45614334078544594		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.45614334078544594 | validation: 0.507022050588376]
	TIME [epoch: 26.2 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44930073511371815		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.44930073511371815 | validation: 0.5195663512360686]
	TIME [epoch: 26.2 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45697843190000964		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 0.45697843190000964 | validation: 0.4470530204310144]
	TIME [epoch: 26.3 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47763336103264536		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.47763336103264536 | validation: 0.8776200560790138]
	TIME [epoch: 26.2 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5108702195178816		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 0.5108702195178816 | validation: 0.4555761890645067]
	TIME [epoch: 26.2 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4425443691993827		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 0.4425443691993827 | validation: 0.5433597120573891]
	TIME [epoch: 26.2 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4698832557656978		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 0.4698832557656978 | validation: 0.42973966230376615]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3902027267694653		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 0.3902027267694653 | validation: 0.5415073683558997]
	TIME [epoch: 26.2 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43220518872195657		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.43220518872195657 | validation: 0.42435912372691986]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44797151704527427		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 0.44797151704527427 | validation: 0.4586452073821904]
	TIME [epoch: 26.2 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44482619818465274		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.44482619818465274 | validation: 0.4829299555464861]
	TIME [epoch: 26.3 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39990917609636367		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.39990917609636367 | validation: 0.46441661203175894]
	TIME [epoch: 26.2 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5059337935752009		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 0.5059337935752009 | validation: 0.4806532898789876]
	TIME [epoch: 26.2 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3895087429541528		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 0.3895087429541528 | validation: 0.47183636825639785]
	TIME [epoch: 26.3 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4128659265317627		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.4128659265317627 | validation: 0.47236229036309874]
	TIME [epoch: 26.2 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4559179405901749		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 0.4559179405901749 | validation: 0.5526430264227833]
	TIME [epoch: 26.2 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39372031101331		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 0.39372031101331 | validation: 0.5415967159700035]
	TIME [epoch: 26.3 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41148361775423664		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 0.41148361775423664 | validation: 0.4670333240018037]
	TIME [epoch: 26.2 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4002397086236093		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.4002397086236093 | validation: 0.42428430906075904]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4066873227709303		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.4066873227709303 | validation: 0.5732480779683331]
	TIME [epoch: 26.2 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47144605604873213		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.47144605604873213 | validation: 0.43869181019242576]
	TIME [epoch: 26.2 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4372262500730848		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 0.4372262500730848 | validation: 0.47528865425443434]
	TIME [epoch: 26.2 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39735347754160566		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.39735347754160566 | validation: 0.49072963788781926]
	TIME [epoch: 26.2 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39486598112432636		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 0.39486598112432636 | validation: 0.43998899802625757]
	TIME [epoch: 26.2 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4073971524151932		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 0.4073971524151932 | validation: 0.4517713902917423]
	TIME [epoch: 26.2 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39252001491584654		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 0.39252001491584654 | validation: 0.4817070177568233]
	TIME [epoch: 26.3 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4373199722685018		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 0.4373199722685018 | validation: 0.4152125407030475]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38108739285987864		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 0.38108739285987864 | validation: 0.5287712810350009]
	TIME [epoch: 26.2 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43002140662703076		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.43002140662703076 | validation: 0.4295051029877869]
	TIME [epoch: 26.2 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3888935217253329		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 0.3888935217253329 | validation: 0.4602701385506671]
	TIME [epoch: 26.2 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39702380312134705		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 0.39702380312134705 | validation: 0.4087484228233224]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38306423474705376		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.38306423474705376 | validation: 0.45303885162843255]
	TIME [epoch: 26.2 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.439963169701385		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 0.439963169701385 | validation: 0.3981741600927307]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3784293657878426		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.3784293657878426 | validation: 0.44372983728016796]
	TIME [epoch: 26.2 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38276541466588865		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 0.38276541466588865 | validation: 0.44530857747541075]
	TIME [epoch: 26.2 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3866752099363303		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 0.3866752099363303 | validation: 0.4489825206765682]
	TIME [epoch: 26.2 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3958896158396768		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 0.3958896158396768 | validation: 0.4899920245122452]
	TIME [epoch: 26.2 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4600182826499073		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 0.4600182826499073 | validation: 0.4292506281862107]
	TIME [epoch: 26.2 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3591727524665496		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.3591727524665496 | validation: 0.43777859071238445]
	TIME [epoch: 26.3 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37228970668333994		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.37228970668333994 | validation: 0.4086869941885277]
	TIME [epoch: 26.2 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3473675205197665		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 0.3473675205197665 | validation: 0.4077420951369899]
	TIME [epoch: 26.3 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38405262341490864		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.38405262341490864 | validation: 0.432991181863414]
	TIME [epoch: 26.2 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42556154910560995		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.42556154910560995 | validation: 0.45940957208761535]
	TIME [epoch: 26.3 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44246527871011115		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.44246527871011115 | validation: 0.5069020721036115]
	TIME [epoch: 26.2 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3796853419089947		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.3796853419089947 | validation: 0.44290071659833236]
	TIME [epoch: 26.3 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37485831882800646		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.37485831882800646 | validation: 0.41433416001727286]
	TIME [epoch: 26.2 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3481164349310155		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 0.3481164349310155 | validation: 0.4207138387248051]
	TIME [epoch: 26.3 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39642539041859964		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 0.39642539041859964 | validation: 0.6147676208134053]
	TIME [epoch: 26.2 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4251769242335306		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 0.4251769242335306 | validation: 0.47286206857546675]
	TIME [epoch: 26.2 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3694202290640032		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 0.3694202290640032 | validation: 0.4360672542176365]
	TIME [epoch: 26.2 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4160047745800717		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 0.4160047745800717 | validation: 0.5633242851626159]
	TIME [epoch: 26.3 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43256923847732975		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 0.43256923847732975 | validation: 0.47181018374058764]
	TIME [epoch: 26.2 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4033711132060278		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.4033711132060278 | validation: 0.4059152481677334]
	TIME [epoch: 26.2 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35988269042020277		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 0.35988269042020277 | validation: 0.4031205739674135]
	TIME [epoch: 26.2 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3840338998716453		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 0.3840338998716453 | validation: 0.40229156200793337]
	TIME [epoch: 26.2 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3830784786543115		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 0.3830784786543115 | validation: 0.3932600614888738]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3443382094480999		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 0.3443382094480999 | validation: 0.4337084921502819]
	TIME [epoch: 26.2 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3740974887372823		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 0.3740974887372823 | validation: 0.4002808601309373]
	TIME [epoch: 26.2 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34822934867833655		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 0.34822934867833655 | validation: 0.4129207482879253]
	TIME [epoch: 26.3 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3660154144747235		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 0.3660154144747235 | validation: 0.3990294914631195]
	TIME [epoch: 26.3 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36930501998185483		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.36930501998185483 | validation: 0.49413913898441897]
	TIME [epoch: 26.3 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3954198042541309		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.3954198042541309 | validation: 0.4876881177677904]
	TIME [epoch: 26.2 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3763633613198267		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.3763633613198267 | validation: 0.4956046261459038]
	TIME [epoch: 26.3 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3627448382850105		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 0.3627448382850105 | validation: 0.3889047257404902]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33137950108969		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.33137950108969 | validation: 0.38557229371633017]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35952466310114467		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.35952466310114467 | validation: 0.45547532316256506]
	TIME [epoch: 26.2 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3915952052977309		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.3915952052977309 | validation: 0.43019510606248135]
	TIME [epoch: 26.3 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33574245234065747		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.33574245234065747 | validation: 0.3900290175916856]
	TIME [epoch: 26.3 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3658148233135433		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 0.3658148233135433 | validation: 0.429499662674505]
	TIME [epoch: 26.3 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4180707916631647		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 0.4180707916631647 | validation: 0.3870647383998228]
	TIME [epoch: 26.3 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3406915982443438		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.3406915982443438 | validation: 0.4037751577124745]
	TIME [epoch: 26.3 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33427542507021013		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.33427542507021013 | validation: 0.3980974217635741]
	TIME [epoch: 26.3 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34448733717293845		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.34448733717293845 | validation: 0.3853355694819586]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3596792407828777		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.3596792407828777 | validation: 0.40638911947049217]
	TIME [epoch: 26.2 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3426350648356137		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.3426350648356137 | validation: 0.4054799350432857]
	TIME [epoch: 26.2 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3294335787278879		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 0.3294335787278879 | validation: 0.5284318819920609]
	TIME [epoch: 26.2 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3749969938337568		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.3749969938337568 | validation: 0.36385271403620156]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33682267825223183		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.33682267825223183 | validation: 0.42481798006990823]
	TIME [epoch: 26.2 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34506428917394827		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.34506428917394827 | validation: 0.38462714409987797]
	TIME [epoch: 26.2 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3331188192532091		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.3331188192532091 | validation: 0.39406455562586085]
	TIME [epoch: 26.2 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648961308352371		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.3648961308352371 | validation: 0.39602939877277876]
	TIME [epoch: 26.2 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3743343141704825		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.3743343141704825 | validation: 0.5261177105895366]
	TIME [epoch: 26.2 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40612879840339744		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 0.40612879840339744 | validation: 0.4136234161008636]
	TIME [epoch: 26.2 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3636992133712592		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.3636992133712592 | validation: 0.4251178220227466]
	TIME [epoch: 26.3 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.362754129917582		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.362754129917582 | validation: 0.3603452286238068]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33833293524267855		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 0.33833293524267855 | validation: 0.4003531502891492]
	TIME [epoch: 26.3 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3167770772483338		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.3167770772483338 | validation: 0.40988540694593373]
	TIME [epoch: 26.3 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3465105412339696		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 0.3465105412339696 | validation: 0.4095125779398696]
	TIME [epoch: 26.3 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3528048935581152		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.3528048935581152 | validation: 0.36443411919841895]
	TIME [epoch: 26.2 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3181144167227317		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.3181144167227317 | validation: 0.3591153844982854]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30858038083362627		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.30858038083362627 | validation: 0.4077448302461899]
	TIME [epoch: 26.3 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3698209244889634		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.3698209244889634 | validation: 0.3833911282349257]
	TIME [epoch: 26.3 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32368094100341116		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.32368094100341116 | validation: 0.35461897793752045]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3847289906369909		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.3847289906369909 | validation: 0.4521246244625275]
	TIME [epoch: 26.3 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3664964892218996		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 0.3664964892218996 | validation: 0.3603530153774771]
	TIME [epoch: 26.3 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30589221877130246		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 0.30589221877130246 | validation: 0.38758907998683223]
	TIME [epoch: 26.2 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30632059703996434		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 0.30632059703996434 | validation: 0.39196696489921357]
	TIME [epoch: 26.3 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37499706169112096		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 0.37499706169112096 | validation: 0.3720397605166106]
	TIME [epoch: 26.3 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35356750799981096		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 0.35356750799981096 | validation: 0.348733345984543]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.309519614968616		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 0.309519614968616 | validation: 0.36705245433384204]
	TIME [epoch: 26.3 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3084475811069979		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 0.3084475811069979 | validation: 0.3583989140278302]
	TIME [epoch: 26.3 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3246819982217413		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 0.3246819982217413 | validation: 0.3584305494201913]
	TIME [epoch: 26.2 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2966213978534741		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 0.2966213978534741 | validation: 0.4173114819260253]
	TIME [epoch: 26.3 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3456517069676743		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 0.3456517069676743 | validation: 0.3723301361295678]
	TIME [epoch: 26.2 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3022282695324844		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 0.3022282695324844 | validation: 0.3570243339134282]
	TIME [epoch: 26.2 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3022341386322001		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 0.3022341386322001 | validation: 0.353079973305238]
	TIME [epoch: 26.2 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3093872255499376		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 0.3093872255499376 | validation: 0.39738581360839387]
	TIME [epoch: 26.2 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3416613054881506		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 0.3416613054881506 | validation: 0.35936485026655995]
	TIME [epoch: 26.2 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3072513223287535		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 0.3072513223287535 | validation: 0.5394072664843493]
	TIME [epoch: 26.2 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35493899880300384		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.35493899880300384 | validation: 0.42758930553350993]
	TIME [epoch: 26.2 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3293423508829036		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 0.3293423508829036 | validation: 0.350709240322067]
	TIME [epoch: 26.2 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29997250867148295		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 0.29997250867148295 | validation: 0.36417938734704514]
	TIME [epoch: 26.2 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35983346685613166		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 0.35983346685613166 | validation: 0.3649253410461314]
	TIME [epoch: 26.2 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33954879683342787		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 0.33954879683342787 | validation: 0.3823141652320011]
	TIME [epoch: 26.2 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31329988734799735		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 0.31329988734799735 | validation: 0.5324175552081665]
	TIME [epoch: 26.2 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44549105265315814		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 0.44549105265315814 | validation: 0.352393899851394]
	TIME [epoch: 26.2 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3039535061731442		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 0.3039535061731442 | validation: 0.3574166529481561]
	TIME [epoch: 26.2 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919070624247197		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 0.2919070624247197 | validation: 0.34288468879526457]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3033824143972912		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 0.3033824143972912 | validation: 0.3695731540363184]
	TIME [epoch: 26.2 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3160697808675025		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 0.3160697808675025 | validation: 0.3449080976061314]
	TIME [epoch: 26.2 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3700518884082231		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 0.3700518884082231 | validation: 0.37326638492854125]
	TIME [epoch: 26.2 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3062504333553538		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 0.3062504333553538 | validation: 0.3256478089026058]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2868639689718152		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 0.2868639689718152 | validation: 0.408201042851999]
	TIME [epoch: 26.2 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32388135618994907		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 0.32388135618994907 | validation: 0.3505432868995878]
	TIME [epoch: 26.2 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3129916586469015		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 0.3129916586469015 | validation: 0.353640558234811]
	TIME [epoch: 26.2 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3034425952402101		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 0.3034425952402101 | validation: 0.35635826486316347]
	TIME [epoch: 26.2 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29454544737330496		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 0.29454544737330496 | validation: 0.3911211907733808]
	TIME [epoch: 26.2 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38246666487517644		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 0.38246666487517644 | validation: 0.3893624412639465]
	TIME [epoch: 26.2 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32003304949052314		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.32003304949052314 | validation: 0.36329288390784575]
	TIME [epoch: 26.2 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2906016006737955		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 0.2906016006737955 | validation: 0.340051415988974]
	TIME [epoch: 26.2 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856032820118371		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 0.2856032820118371 | validation: 0.3489225015678388]
	TIME [epoch: 26.2 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3018135786293543		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 0.3018135786293543 | validation: 0.3465757433425027]
	TIME [epoch: 26.2 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28303962779786707		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 0.28303962779786707 | validation: 0.32732256210594]
	TIME [epoch: 26.2 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3794970355483712		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 0.3794970355483712 | validation: 0.40889034227559373]
	TIME [epoch: 26.2 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36025029864817304		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 0.36025029864817304 | validation: 0.352882612692713]
	TIME [epoch: 26.2 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919104189082421		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 0.2919104189082421 | validation: 0.33145704126954223]
	TIME [epoch: 26.2 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2777441049021546		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 0.2777441049021546 | validation: 0.33522553009769307]
	TIME [epoch: 26.2 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30533091319921474		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 0.30533091319921474 | validation: 0.3658634467686237]
	TIME [epoch: 26.2 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2914382066240617		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 0.2914382066240617 | validation: 0.33296707061793995]
	TIME [epoch: 26.2 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2957828116372338		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 0.2957828116372338 | validation: 0.3984122957171763]
	TIME [epoch: 26.2 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3088077831101149		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 0.3088077831101149 | validation: 0.34860076700963316]
	TIME [epoch: 26.2 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27633450889643546		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 0.27633450889643546 | validation: 0.33753534134204255]
	TIME [epoch: 26.2 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30408131436865715		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 0.30408131436865715 | validation: 0.32731093435015046]
	TIME [epoch: 26.2 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27141790467028765		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 0.27141790467028765 | validation: 0.3232722599428536]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28164315769391274		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 0.28164315769391274 | validation: 0.32941634319546503]
	TIME [epoch: 26.2 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782224807451606		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 0.2782224807451606 | validation: 0.32763734025203]
	TIME [epoch: 26.2 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2910501391495393		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 0.2910501391495393 | validation: 0.34292145773067007]
	TIME [epoch: 26.2 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27551684189190784		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.27551684189190784 | validation: 0.3547922308831797]
	TIME [epoch: 26.2 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875822695033022		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 0.2875822695033022 | validation: 0.3489315718548925]
	TIME [epoch: 26.2 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29245048884799046		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 0.29245048884799046 | validation: 0.6093099443362706]
	TIME [epoch: 26.2 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4533506474931899		[learning rate: 0.00059638]
	Learning Rate: 0.000596385
	LOSS [training: 0.4533506474931899 | validation: 0.3431165919295156]
	TIME [epoch: 26.2 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30018127907278797		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 0.30018127907278797 | validation: 0.33412759646351237]
	TIME [epoch: 26.2 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31006262424553743		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 0.31006262424553743 | validation: 0.33500507704033744]
	TIME [epoch: 26.2 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29094273036798474		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 0.29094273036798474 | validation: 0.35224060532131773]
	TIME [epoch: 26.2 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929572726242396		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 0.2929572726242396 | validation: 0.35752452351424274]
	TIME [epoch: 26.2 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30429824333820454		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 0.30429824333820454 | validation: 0.3829015268671937]
	TIME [epoch: 26.2 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2842196723559531		[learning rate: 0.00057092]
	Learning Rate: 0.000570925
	LOSS [training: 0.2842196723559531 | validation: 0.3478878244251028]
	TIME [epoch: 26.2 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29183604621463843		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 0.29183604621463843 | validation: 0.32793936628268416]
	TIME [epoch: 26.2 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2687692751505534		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 0.2687692751505534 | validation: 0.3170729630777619]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3009720415877404		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 0.3009720415877404 | validation: 0.4136448007934524]
	TIME [epoch: 26.2 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30156339086781436		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 0.30156339086781436 | validation: 0.34262207511683856]
	TIME [epoch: 26.2 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2756057594945503		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 0.2756057594945503 | validation: 0.3144771987109096]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2663872944927611		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 0.2663872944927611 | validation: 0.31889580505537324]
	TIME [epoch: 26.2 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27492441681768415		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 0.27492441681768415 | validation: 0.3568006564696061]
	TIME [epoch: 26.2 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2816477129022491		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 0.2816477129022491 | validation: 0.3160004829451345]
	TIME [epoch: 26.2 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2758607362632486		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 0.2758607362632486 | validation: 0.3288861950812677]
	TIME [epoch: 26.2 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673437468915899		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.2673437468915899 | validation: 0.32803720328590175]
	TIME [epoch: 26.2 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676162127156435		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 0.2676162127156435 | validation: 0.3136261381986784]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2857039817348237		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 0.2857039817348237 | validation: 0.3514580038628119]
	TIME [epoch: 26.2 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29368521499680605		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 0.29368521499680605 | validation: 0.3147589139446369]
	TIME [epoch: 26.2 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656933423998041		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 0.2656933423998041 | validation: 0.33225227198269536]
	TIME [epoch: 26.2 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27818510123452833		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 0.27818510123452833 | validation: 0.33370572918980757]
	TIME [epoch: 26.2 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2653494714421444		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 0.2653494714421444 | validation: 0.3185367955636854]
	TIME [epoch: 26.2 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26135329840051863		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 0.26135329840051863 | validation: 0.321344116697092]
	TIME [epoch: 26.2 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28980675011490864		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 0.28980675011490864 | validation: 0.3439450016129813]
	TIME [epoch: 26.3 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2672759127649877		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 0.2672759127649877 | validation: 0.3110771453475782]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.257241382657327		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 0.257241382657327 | validation: 0.3246355976722929]
	TIME [epoch: 26.2 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2711267889335074		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 0.2711267889335074 | validation: 0.31023163300484075]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2907906837634957		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 0.2907906837634957 | validation: 0.4299732280184654]
	TIME [epoch: 26.2 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33786041491689345		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 0.33786041491689345 | validation: 0.3393175101443401]
	TIME [epoch: 26.2 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2702932886566948		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 0.2702932886566948 | validation: 0.3109631379119891]
	TIME [epoch: 26.3 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639448571585806		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 0.2639448571585806 | validation: 0.3102468316232164]
	TIME [epoch: 26.2 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577061924886988		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 0.2577061924886988 | validation: 0.32945994899013986]
	TIME [epoch: 26.2 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26342710497871874		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 0.26342710497871874 | validation: 0.31310867146061094]
	TIME [epoch: 26.2 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25646967874729115		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 0.25646967874729115 | validation: 0.3072267393208582]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615485860189187		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.2615485860189187 | validation: 0.31712913626650097]
	TIME [epoch: 26.2 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26620148615131256		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 0.26620148615131256 | validation: 0.2999070350363442]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26810755275019627		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 0.26810755275019627 | validation: 0.31991593280519337]
	TIME [epoch: 26.2 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781542227369156		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 0.2781542227369156 | validation: 0.3982479426363673]
	TIME [epoch: 26.2 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.306184059615344		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 0.306184059615344 | validation: 0.3207775167535586]
	TIME [epoch: 26.3 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650609126224076		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 0.2650609126224076 | validation: 0.31128614450316416]
	TIME [epoch: 26.2 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25184718722313304		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 0.25184718722313304 | validation: 0.30640020301010945]
	TIME [epoch: 26.2 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3989603917215786		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 0.3989603917215786 | validation: 0.4436635556585097]
	TIME [epoch: 26.3 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33381108066617593		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 0.33381108066617593 | validation: 0.3182288172055397]
	TIME [epoch: 26.3 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26726779344548046		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 0.26726779344548046 | validation: 0.29420408053844616]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2496442531303305		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 0.2496442531303305 | validation: 0.4468104124291222]
	TIME [epoch: 26.2 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133835259186958		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 0.3133835259186958 | validation: 0.32476252267087086]
	TIME [epoch: 26.2 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25970785269878544		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 0.25970785269878544 | validation: 0.3064094863687309]
	TIME [epoch: 26.3 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540215652238074		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 0.2540215652238074 | validation: 0.29014005901275375]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463809479662604		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 0.2463809479662604 | validation: 0.29238270393824783]
	TIME [epoch: 26.3 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2534253430404579		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 0.2534253430404579 | validation: 0.29533660456682687]
	TIME [epoch: 26.3 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2504710510789528		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 0.2504710510789528 | validation: 0.3055427743018754]
	TIME [epoch: 26.2 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677114410826231		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 0.2677114410826231 | validation: 0.3061390152457576]
	TIME [epoch: 26.2 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2520826654223842		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 0.2520826654223842 | validation: 0.2955728788077626]
	TIME [epoch: 26.3 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24581872068044308		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.24581872068044308 | validation: 0.3223008097013714]
	TIME [epoch: 26.3 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26735951580329587		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 0.26735951580329587 | validation: 0.32465966837271343]
	TIME [epoch: 26.2 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25753948957153394		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 0.25753948957153394 | validation: 0.2960555182102011]
	TIME [epoch: 26.3 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24539760886171225		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 0.24539760886171225 | validation: 0.2922156402526984]
	TIME [epoch: 26.2 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24970940352804455		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 0.24970940352804455 | validation: 0.30785950092007847]
	TIME [epoch: 26.3 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25610410593493793		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 0.25610410593493793 | validation: 0.2972926111945181]
	TIME [epoch: 26.3 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24596302485991442		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 0.24596302485991442 | validation: 0.30552408286594895]
	TIME [epoch: 26.3 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26801628834394475		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 0.26801628834394475 | validation: 0.3187825125256426]
	TIME [epoch: 26.3 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671613611727479		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 0.2671613611727479 | validation: 0.32232938193925553]
	TIME [epoch: 26.3 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2542894350040418		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 0.2542894350040418 | validation: 0.31245544215302357]
	TIME [epoch: 496 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25296289932309013		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 0.25296289932309013 | validation: 0.29457819281386377]
	TIME [epoch: 55.7 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24782964606569416		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 0.24782964606569416 | validation: 0.29686868213944134]
	TIME [epoch: 55.6 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2446995816620922		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 0.2446995816620922 | validation: 0.2996272533625972]
	TIME [epoch: 55.6 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2441975456859732		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 0.2441975456859732 | validation: 0.2954190150947611]
	TIME [epoch: 55.6 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24802695146179976		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 0.24802695146179976 | validation: 0.33678699038869153]
	TIME [epoch: 55.6 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2590939513500164		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 0.2590939513500164 | validation: 0.38185434760653203]
	TIME [epoch: 55.6 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29063837377583657		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 0.29063837377583657 | validation: 0.31703860727739946]
	TIME [epoch: 55.6 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2539972221294323		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 0.2539972221294323 | validation: 0.28967559739633425]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2447535658230023		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 0.2447535658230023 | validation: 0.31947342394913725]
	TIME [epoch: 55.7 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2592795939508118		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.2592795939508118 | validation: 0.290352922376245]
	TIME [epoch: 55.6 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2887486691713501		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 0.2887486691713501 | validation: 0.31185493447458235]
	TIME [epoch: 55.6 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2737687724840558		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 0.2737687724840558 | validation: 0.2829271206715447]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24476828679046467		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 0.24476828679046467 | validation: 0.28916716214797256]
	TIME [epoch: 55.6 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2441304444081682		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 0.2441304444081682 | validation: 0.2901528947321111]
	TIME [epoch: 55.7 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24256878694451448		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 0.24256878694451448 | validation: 0.28661048048174387]
	TIME [epoch: 55.6 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24314984007368537		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 0.24314984007368537 | validation: 0.2873320068698785]
	TIME [epoch: 55.6 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2602014870358369		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 0.2602014870358369 | validation: 0.2976335823418331]
	TIME [epoch: 55.7 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24454564042444557		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 0.24454564042444557 | validation: 0.28208660786127504]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24156227416597595		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 0.24156227416597595 | validation: 0.2879253448729781]
	TIME [epoch: 55.7 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24458094709815978		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 0.24458094709815978 | validation: 0.2875127497543525]
	TIME [epoch: 55.7 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25399897256401016		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 0.25399897256401016 | validation: 0.2776501500648044]
	TIME [epoch: 55.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_522.pth
	Model improved!!!
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24907091864321076		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 0.24907091864321076 | validation: 0.28673343465808065]
	TIME [epoch: 55.8 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24646954791348535		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 0.24646954791348535 | validation: 0.2718149829326627]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25268398953980253		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 0.25268398953980253 | validation: 0.29814111891929873]
	TIME [epoch: 55.7 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23772123956222968		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 0.23772123956222968 | validation: 0.2855276959451068]
	TIME [epoch: 55.7 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24218792694719377		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 0.24218792694719377 | validation: 0.2776153596378711]
	TIME [epoch: 55.7 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2389566416847793		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 0.2389566416847793 | validation: 0.2847610285586041]
	TIME [epoch: 55.6 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23923424650543482		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 0.23923424650543482 | validation: 0.30995945315724893]
	TIME [epoch: 55.8 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29754376464864285		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.29754376464864285 | validation: 0.29617079886874387]
	TIME [epoch: 55.7 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24840249832953665		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 0.24840249832953665 | validation: 0.2800679728800156]
	TIME [epoch: 55.6 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23731706731078855		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 0.23731706731078855 | validation: 0.2770591768731815]
	TIME [epoch: 55.6 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24499472422895804		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 0.24499472422895804 | validation: 0.2858135731766253]
	TIME [epoch: 55.6 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2559524428163795		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 0.2559524428163795 | validation: 0.2824268825014422]
	TIME [epoch: 55.6 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23947578939217537		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 0.23947578939217537 | validation: 0.2907389232806855]
	TIME [epoch: 55.6 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27759051538868523		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 0.27759051538868523 | validation: 0.2837915058345301]
	TIME [epoch: 55.6 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24150987577214061		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 0.24150987577214061 | validation: 0.2806904584306078]
	TIME [epoch: 55.6 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23529390954553292		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 0.23529390954553292 | validation: 0.28265180597502493]
	TIME [epoch: 55.6 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23573776662608115		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 0.23573776662608115 | validation: 0.2866276204371307]
	TIME [epoch: 55.6 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23653906030771607		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 0.23653906030771607 | validation: 0.28153305839691495]
	TIME [epoch: 55.6 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24072270802319312		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 0.24072270802319312 | validation: 0.3171825778511072]
	TIME [epoch: 55.6 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26183982033777126		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 0.26183982033777126 | validation: 0.2902518966757558]
	TIME [epoch: 55.6 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24663718889847464		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 0.24663718889847464 | validation: 0.28142029010232794]
	TIME [epoch: 55.6 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.237004821164992		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 0.237004821164992 | validation: 0.28106188602217963]
	TIME [epoch: 55.7 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23696950816421808		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 0.23696950816421808 | validation: 0.27814926433366427]
	TIME [epoch: 55.6 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2374706774477505		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 0.2374706774477505 | validation: 0.3202904208635514]
	TIME [epoch: 55.7 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721495351053548		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 0.2721495351053548 | validation: 0.2727926903820841]
	TIME [epoch: 55.7 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2520776696753852		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 0.2520776696753852 | validation: 0.30353059552322725]
	TIME [epoch: 55.7 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2493007389106774		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.2493007389106774 | validation: 0.28139655323907237]
	TIME [epoch: 55.6 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25513880002949524		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 0.25513880002949524 | validation: 0.2879900504519467]
	TIME [epoch: 55.7 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2385984879474995		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 0.2385984879474995 | validation: 0.2916641759437737]
	TIME [epoch: 55.6 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2381236223055844		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 0.2381236223055844 | validation: 0.27970078472884524]
	TIME [epoch: 55.6 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2372173510247682		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 0.2372173510247682 | validation: 0.27617180758400595]
	TIME [epoch: 55.7 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23742537229645616		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 0.23742537229645616 | validation: 0.27202187215499923]
	TIME [epoch: 55.7 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24178994997393372		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 0.24178994997393372 | validation: 0.27301522838245357]
	TIME [epoch: 55.6 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2375527199281664		[learning rate: 0.00025287]
	Learning Rate: 0.000252869
	LOSS [training: 0.2375527199281664 | validation: 0.27719698066397513]
	TIME [epoch: 55.6 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23255972708683662		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 0.23255972708683662 | validation: 0.2990691388245758]
	TIME [epoch: 55.6 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24930621347636492		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 0.24930621347636492 | validation: 0.3089768199754336]
	TIME [epoch: 55.6 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2554303326308173		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 0.2554303326308173 | validation: 0.2879130588173836]
	TIME [epoch: 55.6 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2459016875850054		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 0.2459016875850054 | validation: 0.27229818627876456]
	TIME [epoch: 55.7 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23255656348607176		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 0.23255656348607176 | validation: 0.26827527047740757]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23393452209260812		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 0.23393452209260812 | validation: 0.2834655339146103]
	TIME [epoch: 55.6 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.251022512255301		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 0.251022512255301 | validation: 0.2759913572687722]
	TIME [epoch: 55.7 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605643340389113		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 0.2605643340389113 | validation: 0.2859468253844766]
	TIME [epoch: 55.6 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24103616940677358		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 0.24103616940677358 | validation: 0.27119717759745965]
	TIME [epoch: 55.7 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23263923101642597		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 0.23263923101642597 | validation: 0.28143828229407897]
	TIME [epoch: 55.6 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2399757145092691		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 0.2399757145092691 | validation: 0.2701156998390203]
	TIME [epoch: 55.6 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23864180650603303		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.23864180650603303 | validation: 0.27679358780438523]
	TIME [epoch: 55.7 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24089691668545998		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 0.24089691668545998 | validation: 0.27607921520253365]
	TIME [epoch: 55.6 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23735728538421583		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 0.23735728538421583 | validation: 0.2678518455288404]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2306077965506203		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 0.2306077965506203 | validation: 0.2738609603147974]
	TIME [epoch: 55.6 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23356812397111065		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 0.23356812397111065 | validation: 0.2790787656570709]
	TIME [epoch: 55.6 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23399141740740942		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 0.23399141740740942 | validation: 0.26355474423361547]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2300695475721964		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 0.2300695475721964 | validation: 0.2684444316725438]
	TIME [epoch: 55.6 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2335636567572526		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 0.2335636567572526 | validation: 0.2802326522633558]
	TIME [epoch: 55.7 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2395060147329591		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 0.2395060147329591 | validation: 0.2671181361810436]
	TIME [epoch: 55.7 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23296928099905406		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 0.23296928099905406 | validation: 0.2659529881441828]
	TIME [epoch: 55.6 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23878224991515773		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 0.23878224991515773 | validation: 0.2664861976783714]
	TIME [epoch: 55.6 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23029010181621068		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 0.23029010181621068 | validation: 0.2935238156003648]
	TIME [epoch: 55.7 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24649306673368301		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 0.24649306673368301 | validation: 0.281240153501222]
	TIME [epoch: 55.7 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582752373218744		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 0.2582752373218744 | validation: 0.26997238806004703]
	TIME [epoch: 55.7 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2357446186302582		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 0.2357446186302582 | validation: 0.2642737816224274]
	TIME [epoch: 55.6 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22588107500450566		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 0.22588107500450566 | validation: 0.26395216753202483]
	TIME [epoch: 55.6 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2279867707111508		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 0.2279867707111508 | validation: 0.2785447265594313]
	TIME [epoch: 55.6 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2427162563422845		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 0.2427162563422845 | validation: 0.26084116696690035]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23355849257614164		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 0.23355849257614164 | validation: 0.2779602630445696]
	TIME [epoch: 55.6 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23505464486961553		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.23505464486961553 | validation: 0.25939274299594006]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22831655386568483		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 0.22831655386568483 | validation: 0.2614836406982081]
	TIME [epoch: 55.6 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22604193693686292		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 0.22604193693686292 | validation: 0.26371585504568446]
	TIME [epoch: 55.6 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23048162618107448		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 0.23048162618107448 | validation: 0.28236137577415876]
	TIME [epoch: 55.6 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23570491592001064		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 0.23570491592001064 | validation: 0.2653769683828519]
	TIME [epoch: 55.6 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22892527362688736		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 0.22892527362688736 | validation: 0.2598581659104521]
	TIME [epoch: 55.7 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22857694324997221		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 0.22857694324997221 | validation: 0.2605141522176718]
	TIME [epoch: 55.8 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22682549669265878		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 0.22682549669265878 | validation: 0.2607212620454695]
	TIME [epoch: 55.8 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22803101780021076		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 0.22803101780021076 | validation: 0.2617980484753768]
	TIME [epoch: 55.7 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2330312812687701		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 0.2330312812687701 | validation: 0.25998775692121456]
	TIME [epoch: 55.7 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2473824127553516		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 0.2473824127553516 | validation: 0.2631872730726193]
	TIME [epoch: 55.7 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23297183251416412		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 0.23297183251416412 | validation: 0.26417278036331215]
	TIME [epoch: 55.7 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23754231218454944		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 0.23754231218454944 | validation: 0.26088690426686456]
	TIME [epoch: 55.7 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23159060457462827		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 0.23159060457462827 | validation: 0.27366706212414493]
	TIME [epoch: 55.7 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23228671577418727		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 0.23228671577418727 | validation: 0.26310835978298835]
	TIME [epoch: 55.6 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2315634479980742		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 0.2315634479980742 | validation: 0.27443709823319395]
	TIME [epoch: 55.7 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2370936477988224		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 0.2370936477988224 | validation: 0.2647632028900473]
	TIME [epoch: 55.7 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22815351087212885		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 0.22815351087212885 | validation: 0.2644382858714025]
	TIME [epoch: 55.7 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22496661825256556		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 0.22496661825256556 | validation: 0.26860446020306167]
	TIME [epoch: 55.7 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24237898743568037		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.24237898743568037 | validation: 0.2936596554776114]
	TIME [epoch: 55.7 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24293950301124548		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 0.24293950301124548 | validation: 0.25926468589777135]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23213769356487143		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 0.23213769356487143 | validation: 0.26140783351071506]
	TIME [epoch: 55.7 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23386002510633633		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 0.23386002510633633 | validation: 0.26869676089209843]
	TIME [epoch: 55.7 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22612022956726335		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 0.22612022956726335 | validation: 0.26478053075717073]
	TIME [epoch: 55.6 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22886402578628717		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 0.22886402578628717 | validation: 0.2627627586350274]
	TIME [epoch: 55.6 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22369362319529534		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 0.22369362319529534 | validation: 0.2621350008743557]
	TIME [epoch: 55.6 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22567003685029047		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 0.22567003685029047 | validation: 0.2628105554683708]
	TIME [epoch: 55.7 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22786223196876382		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 0.22786223196876382 | validation: 0.2580440527461353]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22890883081462748		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 0.22890883081462748 | validation: 0.25969076519043044]
	TIME [epoch: 55.6 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2383516756191721		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 0.2383516756191721 | validation: 0.25351384922046116]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23056983156002425		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 0.23056983156002425 | validation: 0.25866127770420067]
	TIME [epoch: 55.6 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23011503522012894		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 0.23011503522012894 | validation: 0.2601079264238258]
	TIME [epoch: 55.6 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22438707026618002		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 0.22438707026618002 | validation: 0.25866525705300025]
	TIME [epoch: 55.6 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23364912757934878		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 0.23364912757934878 | validation: 0.2590058830003326]
	TIME [epoch: 55.6 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2247260395847358		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 0.2247260395847358 | validation: 0.2536284866192844]
	TIME [epoch: 55.7 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22825965414560032		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 0.22825965414560032 | validation: 0.25686191548665593]
	TIME [epoch: 55.6 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22428050521874687		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 0.22428050521874687 | validation: 0.2556277235143086]
	TIME [epoch: 55.6 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22643114929637714		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 0.22643114929637714 | validation: 0.26527704122263324]
	TIME [epoch: 55.6 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2254658583110392		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.2254658583110392 | validation: 0.26437860158119697]
	TIME [epoch: 55.5 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22769948496797204		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 0.22769948496797204 | validation: 0.2690230903350838]
	TIME [epoch: 55.6 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22743821862564428		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 0.22743821862564428 | validation: 0.2599053340688927]
	TIME [epoch: 55.6 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22403519037957856		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 0.22403519037957856 | validation: 0.25481877891869364]
	TIME [epoch: 55.6 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22785155497825196		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 0.22785155497825196 | validation: 0.2779139083420991]
	TIME [epoch: 55.7 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24004434957838025		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 0.24004434957838025 | validation: 0.2585161581598899]
	TIME [epoch: 55.6 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22448964322754045		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 0.22448964322754045 | validation: 0.2617254558824082]
	TIME [epoch: 55.7 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2237009187830461		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 0.2237009187830461 | validation: 0.2602426081494796]
	TIME [epoch: 55.7 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22345868580787015		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 0.22345868580787015 | validation: 0.2517598262502677]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2243314976670303		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 0.2243314976670303 | validation: 0.2525550814572909]
	TIME [epoch: 55.7 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22231829726210367		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 0.22231829726210367 | validation: 0.25797208885264833]
	TIME [epoch: 55.6 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22466739289372065		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 0.22466739289372065 | validation: 0.25777104725657773]
	TIME [epoch: 55.7 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23086741109692482		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 0.23086741109692482 | validation: 0.2611307869000635]
	TIME [epoch: 55.6 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22504955056530587		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 0.22504955056530587 | validation: 0.25644670084700116]
	TIME [epoch: 55.6 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21984767680812362		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 0.21984767680812362 | validation: 0.2591802615483113]
	TIME [epoch: 55.7 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23127886490529978		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 0.23127886490529978 | validation: 0.25796609499920625]
	TIME [epoch: 55.7 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22722259099344266		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 0.22722259099344266 | validation: 0.25458672314562003]
	TIME [epoch: 55.6 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22359812757397846		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 0.22359812757397846 | validation: 0.26519208600416927]
	TIME [epoch: 55.7 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22801436074580822		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 0.22801436074580822 | validation: 0.26173114238326434]
	TIME [epoch: 55.7 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22452698127732681		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.22452698127732681 | validation: 0.25457563631549246]
	TIME [epoch: 55.7 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22092686634191233		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 0.22092686634191233 | validation: 0.25660768766828423]
	TIME [epoch: 55.6 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22449945529743243		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 0.22449945529743243 | validation: 0.2501433760429258]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22523553914376593		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 0.22523553914376593 | validation: 0.27869870485808534]
	TIME [epoch: 55.6 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23350926353051524		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 0.23350926353051524 | validation: 0.25135972538623663]
	TIME [epoch: 55.7 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22275786068950776		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 0.22275786068950776 | validation: 0.2545338545967539]
	TIME [epoch: 55.7 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22244944088308427		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 0.22244944088308427 | validation: 0.25339358805862655]
	TIME [epoch: 55.6 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22571091275078134		[learning rate: 0.00012673]
	Learning Rate: 0.000126734
	LOSS [training: 0.22571091275078134 | validation: 0.2511620236028621]
	TIME [epoch: 55.6 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22144829012678627		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 0.22144829012678627 | validation: 0.27035301826443914]
	TIME [epoch: 55.6 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2369751151782501		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 0.2369751151782501 | validation: 0.2519494159557521]
	TIME [epoch: 55.6 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.222181078059704		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 0.222181078059704 | validation: 0.24886842329649767]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22312051590866983		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 0.22312051590866983 | validation: 0.2531134113179959]
	TIME [epoch: 55.7 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22065350405766634		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 0.22065350405766634 | validation: 0.24959039167052255]
	TIME [epoch: 55.6 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22509015499537569		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 0.22509015499537569 | validation: 0.2502816778328748]
	TIME [epoch: 55.6 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2234406697321242		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 0.2234406697321242 | validation: 0.24763469190051846]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2199664409109581		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 0.2199664409109581 | validation: 0.24935959357627818]
	TIME [epoch: 55.6 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2245490308433588		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 0.2245490308433588 | validation: 0.26128014044256676]
	TIME [epoch: 55.6 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2248028988532435		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 0.2248028988532435 | validation: 0.25196129241671905]
	TIME [epoch: 55.7 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21852206176644598		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 0.21852206176644598 | validation: 0.251575373471697]
	TIME [epoch: 55.7 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22583034998414891		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.22583034998414891 | validation: 0.2548392423038169]
	TIME [epoch: 55.6 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22534205766964893		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 0.22534205766964893 | validation: 0.2532869643827805]
	TIME [epoch: 55.7 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21876849659006978		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 0.21876849659006978 | validation: 0.25004475970510626]
	TIME [epoch: 55.7 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21982450245900542		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 0.21982450245900542 | validation: 0.2500682150478602]
	TIME [epoch: 55.7 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21941680029502095		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 0.21941680029502095 | validation: 0.24729061679128556]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_667.pth
	Model improved!!!
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22378839013597857		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 0.22378839013597857 | validation: 0.24511739163825463]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_668.pth
	Model improved!!!
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2308070068559533		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 0.2308070068559533 | validation: 0.250689959921811]
	TIME [epoch: 55.7 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22617660124010253		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 0.22617660124010253 | validation: 0.2459498998929373]
	TIME [epoch: 55.7 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21880897294888557		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 0.21880897294888557 | validation: 0.25084492691451]
	TIME [epoch: 55.7 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2209277063469293		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 0.2209277063469293 | validation: 0.2546315992239392]
	TIME [epoch: 55.7 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22545281564108122		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 0.22545281564108122 | validation: 0.256414093458473]
	TIME [epoch: 55.6 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22332924688811473		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 0.22332924688811473 | validation: 0.2558658516067014]
	TIME [epoch: 55.6 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2238114282786501		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 0.2238114282786501 | validation: 0.25308503370610097]
	TIME [epoch: 55.6 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2201153436529198		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 0.2201153436529198 | validation: 0.24908636399433556]
	TIME [epoch: 55.7 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22200934248162169		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 0.22200934248162169 | validation: 0.2474026096887234]
	TIME [epoch: 55.7 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22132773986289003		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 0.22132773986289003 | validation: 0.24571530442631762]
	TIME [epoch: 55.6 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22690153817369843		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 0.22690153817369843 | validation: 0.25062221789493494]
	TIME [epoch: 55.6 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22270277173892417		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 0.22270277173892417 | validation: 0.2510886720252435]
	TIME [epoch: 55.6 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22144382764662304		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 0.22144382764662304 | validation: 0.251150430019324]
	TIME [epoch: 55.6 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21982255131540437		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.21982255131540437 | validation: 0.2485037309578204]
	TIME [epoch: 55.7 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2211640856086791		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 0.2211640856086791 | validation: 0.24356841114095942]
	TIME [epoch: 55.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_683.pth
	Model improved!!!
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22431561852188397		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 0.22431561852188397 | validation: 0.24786985904039252]
	TIME [epoch: 55.8 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21835293339054618		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 0.21835293339054618 | validation: 0.2465479591219291]
	TIME [epoch: 55.7 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21988301295564489		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 0.21988301295564489 | validation: 0.24658461264413778]
	TIME [epoch: 55.7 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22209405397515816		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 0.22209405397515816 | validation: 0.24964635567543372]
	TIME [epoch: 55.7 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22002424776142288		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 0.22002424776142288 | validation: 0.24965735423912422]
	TIME [epoch: 55.6 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2207143984287183		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 0.2207143984287183 | validation: 0.2460194668222902]
	TIME [epoch: 55.7 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22046309816432663		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 0.22046309816432663 | validation: 0.25044201461967996]
	TIME [epoch: 55.6 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22054428526913827		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 0.22054428526913827 | validation: 0.2477261826907115]
	TIME [epoch: 55.7 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22587085437008203		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 0.22587085437008203 | validation: 0.27757881413992636]
	TIME [epoch: 55.7 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23641019169150618		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 0.23641019169150618 | validation: 0.2466737560241472]
	TIME [epoch: 55.7 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21853415362644704		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 0.21853415362644704 | validation: 0.24653309135575857]
	TIME [epoch: 55.7 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21941535123403305		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 0.21941535123403305 | validation: 0.24667124770519513]
	TIME [epoch: 55.7 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22148075622111205		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 0.22148075622111205 | validation: 0.25217505880842855]
	TIME [epoch: 55.7 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22008150879434438		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 0.22008150879434438 | validation: 0.24999883921565358]
	TIME [epoch: 55.7 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22089633981829693		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 0.22089633981829693 | validation: 0.2447328430546867]
	TIME [epoch: 55.7 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22242916838225296		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 0.22242916838225296 | validation: 0.2476763740724371]
	TIME [epoch: 55.7 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21850391485439982		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 0.21850391485439982 | validation: 0.24838459119632345]
	TIME [epoch: 55.7 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21801998228561603		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.21801998228561603 | validation: 0.24727431715668924]
	TIME [epoch: 55.7 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.220359878628437		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 0.220359878628437 | validation: 0.24535762371960476]
	TIME [epoch: 55.7 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21888855848525673		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 0.21888855848525673 | validation: 0.2496204149606132]
	TIME [epoch: 55.7 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22038290359728457		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 0.22038290359728457 | validation: 0.24332583513696024]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22589003395333718		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 0.22589003395333718 | validation: 0.2502000425726285]
	TIME [epoch: 55.7 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2187279179259769		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 0.2187279179259769 | validation: 0.24295805310418306]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22067864474767626		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 0.22067864474767626 | validation: 0.24813169867612192]
	TIME [epoch: 55.7 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21814711164938455		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 0.21814711164938455 | validation: 0.24893654821303562]
	TIME [epoch: 55.7 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21973980205887422		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 0.21973980205887422 | validation: 0.2466029911379253]
	TIME [epoch: 55.7 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21601820621193416		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 0.21601820621193416 | validation: 0.24547559843765976]
	TIME [epoch: 55.7 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2241572129615431		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 0.2241572129615431 | validation: 0.2483207618053493]
	TIME [epoch: 55.7 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22100140026881912		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 0.22100140026881912 | validation: 0.24961142989891]
	TIME [epoch: 55.7 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21750492228588006		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 0.21750492228588006 | validation: 0.24660876704910234]
	TIME [epoch: 55.7 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.223979023097369		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 0.223979023097369 | validation: 0.24832120171316424]
	TIME [epoch: 55.7 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21948594407572258		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 0.21948594407572258 | validation: 0.24522414966349565]
	TIME [epoch: 55.7 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21921380391730502		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 0.21921380391730502 | validation: 0.24900614059234547]
	TIME [epoch: 55.7 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21902588822466945		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 0.21902588822466945 | validation: 0.24265600556246392]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21584647014987035		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 0.21584647014987035 | validation: 0.24602952540663314]
	TIME [epoch: 55.7 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2170883950776082		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 0.2170883950776082 | validation: 0.2502327185508902]
	TIME [epoch: 55.6 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22557146930199595		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.22557146930199595 | validation: 0.24373963244328725]
	TIME [epoch: 55.7 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2170738682651478		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 0.2170738682651478 | validation: 0.24442830016945505]
	TIME [epoch: 55.7 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2158502412572944		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 0.2158502412572944 | validation: 0.24632541003451675]
	TIME [epoch: 55.7 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22063957800199835		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 0.22063957800199835 | validation: 0.24675002490884407]
	TIME [epoch: 55.7 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21857220221704307		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 0.21857220221704307 | validation: 0.2452368898182296]
	TIME [epoch: 55.7 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21960108948984325		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 0.21960108948984325 | validation: 0.24660824521251118]
	TIME [epoch: 55.7 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21976379871283627		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 0.21976379871283627 | validation: 0.24597008720122251]
	TIME [epoch: 55.7 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21664284764080452		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 0.21664284764080452 | validation: 0.24500522665651303]
	TIME [epoch: 55.7 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2168740369759193		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 0.2168740369759193 | validation: 0.24024991141776836]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_728.pth
	Model improved!!!
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21810861826944677		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 0.21810861826944677 | validation: 0.2401787456324652]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_729.pth
	Model improved!!!
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2161763061485068		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 0.2161763061485068 | validation: 0.24340357658313258]
	TIME [epoch: 55.7 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21811037187369264		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 0.21811037187369264 | validation: 0.24522677232383241]
	TIME [epoch: 55.7 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21934706987056368		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 0.21934706987056368 | validation: 0.24088506682997538]
	TIME [epoch: 55.7 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2161895817952115		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 0.2161895817952115 | validation: 0.24222827975762867]
	TIME [epoch: 55.7 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22000059470776206		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 0.22000059470776206 | validation: 0.24767593552720907]
	TIME [epoch: 55.7 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22407943298232774		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 0.22407943298232774 | validation: 0.24308091115491093]
	TIME [epoch: 55.7 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22089648083618502		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 0.22089648083618502 | validation: 0.24133805936800912]
	TIME [epoch: 55.7 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2168103410444918		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 0.2168103410444918 | validation: 0.238428762326388]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_737.pth
	Model improved!!!
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2190999870298591		[learning rate: 6.7322e-05]
	Learning Rate: 6.73222e-05
	LOSS [training: 0.2190999870298591 | validation: 0.23991255343975262]
	TIME [epoch: 55.6 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21805034206757856		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.21805034206757856 | validation: 0.24407187843588035]
	TIME [epoch: 55.7 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21652251429713634		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 0.21652251429713634 | validation: 0.2448341452409809]
	TIME [epoch: 55.7 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21828985100011514		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 0.21828985100011514 | validation: 0.2432979532401245]
	TIME [epoch: 55.7 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21598005392975406		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 0.21598005392975406 | validation: 0.24477045951443321]
	TIME [epoch: 55.7 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2175354646270763		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 0.2175354646270763 | validation: 0.24569566921017244]
	TIME [epoch: 55.7 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21817665006799733		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 0.21817665006799733 | validation: 0.2489854155548093]
	TIME [epoch: 55.7 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22047397313257855		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 0.22047397313257855 | validation: 0.24313970768848456]
	TIME [epoch: 55.7 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21821193151230173		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 0.21821193151230173 | validation: 0.24233508379442917]
	TIME [epoch: 55.7 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21776839271705656		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 0.21776839271705656 | validation: 0.2425217660287638]
	TIME [epoch: 55.7 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22009325502071692		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 0.22009325502071692 | validation: 0.24154390970161452]
	TIME [epoch: 55.7 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21629627483024044		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 0.21629627483024044 | validation: 0.251884716190189]
	TIME [epoch: 55.6 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2210118316020525		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 0.2210118316020525 | validation: 0.24076912956125698]
	TIME [epoch: 55.7 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2155693220466926		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 0.2155693220466926 | validation: 0.24194764588291878]
	TIME [epoch: 55.7 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2149285177623289		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 0.2149285177623289 | validation: 0.2375139305014775]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2160921592965946		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 0.2160921592965946 | validation: 0.24266836131575292]
	TIME [epoch: 55.7 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21950232835670272		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 0.21950232835670272 | validation: 0.2509011184631911]
	TIME [epoch: 55.7 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22454744364552748		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 0.22454744364552748 | validation: 0.2386041986781896]
	TIME [epoch: 55.7 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21732309958339188		[learning rate: 5.9063e-05]
	Learning Rate: 5.90631e-05
	LOSS [training: 0.21732309958339188 | validation: 0.24487588273257221]
	TIME [epoch: 55.7 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21720119002448582		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 0.21720119002448582 | validation: 0.24437753598547568]
	TIME [epoch: 55.7 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21644214557115787		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.21644214557115787 | validation: 0.24557460951805687]
	TIME [epoch: 55.7 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2222912713158053		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 0.2222912713158053 | validation: 0.25160105244522757]
	TIME [epoch: 55.7 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21945888411553804		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 0.21945888411553804 | validation: 0.23954251091075354]
	TIME [epoch: 55.7 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2171329183289321		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 0.2171329183289321 | validation: 0.2426988373800262]
	TIME [epoch: 55.7 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21531224590678624		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 0.21531224590678624 | validation: 0.24337198198341128]
	TIME [epoch: 55.7 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21620205933244596		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 0.21620205933244596 | validation: 0.24093447296344978]
	TIME [epoch: 55.7 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2184795698812242		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 0.2184795698812242 | validation: 0.23892023805382767]
	TIME [epoch: 55.7 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21579414298716457		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 0.21579414298716457 | validation: 0.24047362738882527]
	TIME [epoch: 55.6 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21587605918195005		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 0.21587605918195005 | validation: 0.2415298021871642]
	TIME [epoch: 55.7 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21613630397560735		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 0.21613630397560735 | validation: 0.2423156304627985]
	TIME [epoch: 55.7 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21590625970531188		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 0.21590625970531188 | validation: 0.244129626238425]
	TIME [epoch: 55.7 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21886155687207814		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 0.21886155687207814 | validation: 0.24229921605835664]
	TIME [epoch: 55.7 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21498830965315352		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 0.21498830965315352 | validation: 0.2471196069624141]
	TIME [epoch: 55.7 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21990174961336542		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 0.21990174961336542 | validation: 0.2393035364040978]
	TIME [epoch: 55.6 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.215919085141845		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 0.215919085141845 | validation: 0.2425109939925994]
	TIME [epoch: 55.7 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21388601133219193		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 0.21388601133219193 | validation: 0.2441599090928237]
	TIME [epoch: 55.7 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21709488177701303		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 0.21709488177701303 | validation: 0.24116390849096642]
	TIME [epoch: 55.7 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21582579497665072		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 0.21582579497665072 | validation: 0.23693521885310814]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_775.pth
	Model improved!!!
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21675524103780214		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 0.21675524103780214 | validation: 0.23719705798447116]
	TIME [epoch: 55.7 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2148932520592564		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.2148932520592564 | validation: 0.24330350919263083]
	TIME [epoch: 55.7 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21664074784875614		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 0.21664074784875614 | validation: 0.24059161070514262]
	TIME [epoch: 55.7 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21641344274474145		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 0.21641344274474145 | validation: 0.24288736945463618]
	TIME [epoch: 55.7 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21747720210842775		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 0.21747720210842775 | validation: 0.24225989440640577]
	TIME [epoch: 55.7 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21567881986475632		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 0.21567881986475632 | validation: 0.24075597385566777]
	TIME [epoch: 55.7 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2157128553093589		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 0.2157128553093589 | validation: 0.24449864692388465]
	TIME [epoch: 55.7 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21834087206778807		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 0.21834087206778807 | validation: 0.2406679250919042]
	TIME [epoch: 55.7 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21774836118590093		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 0.21774836118590093 | validation: 0.24125298537856804]
	TIME [epoch: 55.7 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21740802808195286		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 0.21740802808195286 | validation: 0.2410873082651627]
	TIME [epoch: 55.7 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21945074258595437		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 0.21945074258595437 | validation: 0.2394848093211376]
	TIME [epoch: 55.7 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2159144830480329		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 0.2159144830480329 | validation: 0.24357991745146734]
	TIME [epoch: 55.7 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21692480530406674		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 0.21692480530406674 | validation: 0.24006703028084192]
	TIME [epoch: 55.7 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21583667071138535		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 0.21583667071138535 | validation: 0.23950236701635114]
	TIME [epoch: 55.7 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.214434058590465		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 0.214434058590465 | validation: 0.23889651245479618]
	TIME [epoch: 55.7 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21395624223566787		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 0.21395624223566787 | validation: 0.23756973025462552]
	TIME [epoch: 55.7 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21624528586957725		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 0.21624528586957725 | validation: 0.23943398277233624]
	TIME [epoch: 55.7 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21496511486733283		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 0.21496511486733283 | validation: 0.23837149976734823]
	TIME [epoch: 55.7 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2153655705990506		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 0.2153655705990506 | validation: 0.2371720511092795]
	TIME [epoch: 55.7 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21580795551051413		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 0.21580795551051413 | validation: 0.23845420169748366]
	TIME [epoch: 55.7 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21565286017347635		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.21565286017347635 | validation: 0.23918088488866004]
	TIME [epoch: 55.7 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21444720654204258		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 0.21444720654204258 | validation: 0.24223254703124814]
	TIME [epoch: 55.7 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21653776241706627		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 0.21653776241706627 | validation: 0.2374715282293191]
	TIME [epoch: 55.7 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21721340271475847		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 0.21721340271475847 | validation: 0.23955003851791629]
	TIME [epoch: 55.7 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21904792043890695		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 0.21904792043890695 | validation: 0.24243147031512227]
	TIME [epoch: 55.7 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21575956583979783		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 0.21575956583979783 | validation: 0.24192909224738846]
	TIME [epoch: 55.6 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21455899520961644		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 0.21455899520961644 | validation: 0.23903769199940123]
	TIME [epoch: 55.7 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21558734025322634		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 0.21558734025322634 | validation: 0.23970586008618439]
	TIME [epoch: 55.7 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21630642643493203		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 0.21630642643493203 | validation: 0.24451704856757664]
	TIME [epoch: 55.7 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2171629297151994		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 0.2171629297151994 | validation: 0.2389873802971873]
	TIME [epoch: 55.7 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21568762965893393		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 0.21568762965893393 | validation: 0.24168793616795733]
	TIME [epoch: 55.7 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21536504402884768		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 0.21536504402884768 | validation: 0.2379811701772161]
	TIME [epoch: 55.7 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21498593391786022		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 0.21498593391786022 | validation: 0.23535429482387865]
	TIME [epoch: 55.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20241205_182027/states/model_phiq_1a_v_mmd1_808.pth
	Model improved!!!
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21519260514598515		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 0.21519260514598515 | validation: 0.24289027767780713]
	TIME [epoch: 55.7 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2137861482655923		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 0.2137861482655923 | validation: 0.24238195895102954]
	TIME [epoch: 55.6 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21501189751463803		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 0.21501189751463803 | validation: 0.23885863376029176]
	TIME [epoch: 55.6 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21440539735384656		[learning rate: 3.9307e-05]
	Learning Rate: 3.93073e-05
	LOSS [training: 0.21440539735384656 | validation: 0.2400102090042958]
	TIME [epoch: 55.6 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21626225567228297		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 0.21626225567228297 | validation: 0.238857806196542]
	TIME [epoch: 55.7 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21364272246238641		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 0.21364272246238641 | validation: 0.24014458258820737]
	TIME [epoch: 55.7 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22122318847943462		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.22122318847943462 | validation: 0.2366284063130289]
	TIME [epoch: 55.6 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21361851608115281		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 0.21361851608115281 | validation: 0.24235522824931927]
	TIME [epoch: 55.6 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21370656663381882		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 0.21370656663381882 | validation: 0.23779609653363215]
	TIME [epoch: 55.7 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2166386687431447		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 0.2166386687431447 | validation: 0.2410899813249759]
	TIME [epoch: 55.7 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21715593697271895		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 0.21715593697271895 | validation: 0.238326578101749]
	TIME [epoch: 55.6 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21432232068829646		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 0.21432232068829646 | validation: 0.24274555138497095]
	TIME [epoch: 55.7 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21257233600266012		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 0.21257233600266012 | validation: 0.23720223412488878]
	TIME [epoch: 55.7 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21605241670753403		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 0.21605241670753403 | validation: 0.23847635939544082]
	TIME [epoch: 55.7 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21533337786273005		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 0.21533337786273005 | validation: 0.23960753937285129]
	TIME [epoch: 55.7 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21748110281416702		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 0.21748110281416702 | validation: 0.24118471953879211]
	TIME [epoch: 55.7 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.21403475111574402		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 0.21403475111574402 | validation: 0.24686074257969998]
	TIME [epoch: 55.7 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22078272273858782		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 0.22078272273858782 | validation: 0.24073294979013266]
	TIME [epoch: 55.7 sec]
EPOCH 827/1000:
	Training over batches...
