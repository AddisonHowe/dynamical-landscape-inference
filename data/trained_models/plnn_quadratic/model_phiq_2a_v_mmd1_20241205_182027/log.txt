Args:
Namespace(name='model_phiq_2a_v_mmd1', outdir='out/model_training/model_phiq_2a_v_mmd1', training_data='data/training_data/basic/data_phiq_2a/training', validation_data='data/training_data/basic/data_phiq_2a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[100, 250, 500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3131077067

Training model...

Saving initial model state to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.805045062365527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.805045062365527 | validation: 6.886312143275177]
	TIME [epoch: 413 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.705550496707242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.705550496707242 | validation: 6.772689979450904]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.647993735085922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.647993735085922 | validation: 6.6633821879969855]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.562494396380062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.562494396380062 | validation: 6.601274984652416]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.48630095035236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.48630095035236 | validation: 6.565356904873043]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.390172462653451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.390172462653451 | validation: 6.516679982663479]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.293194879476681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.293194879476681 | validation: 6.411867401396169]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.183068671060576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.183068671060576 | validation: 6.190445414252288]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 6.104562167103422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.104562167103422 | validation: 6.147964827798273]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.87253868990906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.87253868990906 | validation: 5.98246838867503]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.5776970744268315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5776970744268315 | validation: 5.441924324573466]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.036833817602942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.036833817602942 | validation: 4.956810756660298]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.785682870139841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.785682870139841 | validation: 5.085196906679327]
	TIME [epoch: 6.13 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.691828660220825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.691828660220825 | validation: 4.770760819742802]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.481777158078863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.481777158078863 | validation: 4.597212118878124]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.303465825610366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.303465825610366 | validation: 4.432426143249716]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.168414203604911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.168414203604911 | validation: 4.237138376066415]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9768013041825325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9768013041825325 | validation: 3.9986700864197253]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.88269237893881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.88269237893881 | validation: 4.001231197014205]
	TIME [epoch: 6.14 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.8249404553959714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8249404553959714 | validation: 3.903335463907763]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.76584917588284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.76584917588284 | validation: 3.832870336850722]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.70811167533381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.70811167533381 | validation: 3.8145713694405297]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6762801405008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6762801405008 | validation: 3.7855038596953863]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.63136196696046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.63136196696046 | validation: 3.7621653773032833]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.601738626856405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.601738626856405 | validation: 3.669146553678706]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.538568197713423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.538568197713423 | validation: 3.643517541224255]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.516763143514793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.516763143514793 | validation: 3.593367896715815]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4816620697050262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4816620697050262 | validation: 3.5612671939794787]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.44419939136437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.44419939136437 | validation: 3.531152798203326]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.4172802273435625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4172802273435625 | validation: 3.4998453756113657]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.392724752281789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.392724752281789 | validation: 3.4807123587110427]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.374324410937027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374324410937027 | validation: 3.453286543509292]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.347046403201993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.347046403201993 | validation: 3.4268642399927094]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3159075974562473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3159075974562473 | validation: 3.4074922547553026]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3038401096176084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3038401096176084 | validation: 3.388763673625757]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2915433597908437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2915433597908437 | validation: 3.3762693612589505]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.263527633475105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.263527633475105 | validation: 3.3446587368729217]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.238785929284929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.238785929284929 | validation: 3.326202225419271]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2245661540476314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2245661540476314 | validation: 3.3492319237637904]
	TIME [epoch: 6.16 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2141123391991724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2141123391991724 | validation: 3.3031555156395296]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1906201218964148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1906201218964148 | validation: 3.300308547626527]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1687224329250157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1687224329250157 | validation: 3.2786457135124287]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1550401902901517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1550401902901517 | validation: 3.2844810502675426]
	TIME [epoch: 6.17 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.149795321017381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.149795321017381 | validation: 3.2535957260010884]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.125663221720093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.125663221720093 | validation: 3.225973478274576]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.120405311830335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.120405311830335 | validation: 3.2492597480222516]
	TIME [epoch: 6.15 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.120021972609331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.120021972609331 | validation: 3.2219577548878515]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1138141498136767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1138141498136767 | validation: 3.2115962795435307]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1005882902308994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1005882902308994 | validation: 3.2237179842653765]
	TIME [epoch: 6.16 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.1066379436659517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1066379436659517 | validation: 3.2077651776571687]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0895353380659003		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 3.0895353380659003 | validation: 3.221430298871481]
	TIME [epoch: 6.16 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0929601463274032		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 3.0929601463274032 | validation: 3.196428723979556]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0826770728231416		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 3.0826770728231416 | validation: 3.1665491610424166]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.077645541757411		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 3.077645541757411 | validation: 3.1711177306924787]
	TIME [epoch: 6.17 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0823436971330214		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.0823436971330214 | validation: 3.1727278570175175]
	TIME [epoch: 6.18 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0726267598801975		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 3.0726267598801975 | validation: 3.1802259399411525]
	TIME [epoch: 6.18 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.067515014346905		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 3.067515014346905 | validation: 3.161511850790202]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.066659870549814		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 3.066659870549814 | validation: 3.147262589927074]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.092293970075946		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 3.092293970075946 | validation: 3.1402278268033648]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0551749218713193		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 3.0551749218713193 | validation: 3.130253378865416]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0644493328562548		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 3.0644493328562548 | validation: 3.128837043080827]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.047527460972636		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 3.047527460972636 | validation: 3.1441688320663506]
	TIME [epoch: 6.14 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.037444455225031		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 3.037444455225031 | validation: 3.117380290830278]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.08862206858607		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 3.08862206858607 | validation: 3.1548313199251528]
	TIME [epoch: 6.17 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0509947230441448		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 3.0509947230441448 | validation: 3.0970809091457516]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0298501255300305		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 3.0298501255300305 | validation: 3.089593284533255]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.020312494378723		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 3.020312494378723 | validation: 3.082000240641235]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.031439214897464		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 3.031439214897464 | validation: 3.1017782144060284]
	TIME [epoch: 6.18 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.02416017538922		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 3.02416017538922 | validation: 3.1193865718996854]
	TIME [epoch: 6.18 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9950710118104986		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 2.9950710118104986 | validation: 3.06293339900276]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0596726810638417		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 3.0596726810638417 | validation: 3.0918809484098855]
	TIME [epoch: 6.16 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0576113643712124		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 3.0576113643712124 | validation: 3.093160446136408]
	TIME [epoch: 6.15 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.996183191171399		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 2.996183191171399 | validation: 3.0839150403933435]
	TIME [epoch: 6.15 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9918781769017846		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 2.9918781769017846 | validation: 3.0383078525615]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9735388845935344		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 2.9735388845935344 | validation: 3.0243546779390647]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9783916851676544		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 2.9783916851676544 | validation: 3.084477975043396]
	TIME [epoch: 6.17 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.962011872942723		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 2.962011872942723 | validation: 2.994637095924013]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.016413928706787		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 3.016413928706787 | validation: 3.0986030767998605]
	TIME [epoch: 6.17 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.959012016174087		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 2.959012016174087 | validation: 3.0896822966250532]
	TIME [epoch: 6.17 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9862962329003535		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 2.9862962329003535 | validation: 3.1701734962248356]
	TIME [epoch: 6.17 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9665222522305106		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 2.9665222522305106 | validation: 3.0658878145917994]
	TIME [epoch: 6.18 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9212464263693496		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 2.9212464263693496 | validation: 3.0535400515267233]
	TIME [epoch: 6.17 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9656915645190365		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 2.9656915645190365 | validation: 3.0713464765760086]
	TIME [epoch: 6.18 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.932822134284811		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 2.932822134284811 | validation: 3.003440404418874]
	TIME [epoch: 6.18 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9221301996212534		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 2.9221301996212534 | validation: 3.2669696659872223]
	TIME [epoch: 6.17 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9811066128144392		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 2.9811066128144392 | validation: 3.0939686135104294]
	TIME [epoch: 6.17 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.91290735126512		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 2.91290735126512 | validation: 2.958252662586072]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8970634279841647		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 2.8970634279841647 | validation: 2.9363179881005013]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9334286776230187		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 2.9334286776230187 | validation: 3.0680180493597424]
	TIME [epoch: 6.18 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.872805306082332		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 2.872805306082332 | validation: 2.916813235437722]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.946130732930026		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 2.946130732930026 | validation: 3.027862660841392]
	TIME [epoch: 6.18 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8881749046324017		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 2.8881749046324017 | validation: 2.917280037693019]
	TIME [epoch: 6.17 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8583082906707733		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 2.8583082906707733 | validation: 2.8733607459723496]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8725002980625236		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 2.8725002980625236 | validation: 3.3018521969171077]
	TIME [epoch: 6.13 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.987938572123885		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 2.987938572123885 | validation: 3.135094620481933]
	TIME [epoch: 6.12 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9092983476300645		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 2.9092983476300645 | validation: 3.005582931182566]
	TIME [epoch: 6.12 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.870862689475529		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 2.870862689475529 | validation: 2.94328581019512]
	TIME [epoch: 6.13 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.899157855012837		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 2.899157855012837 | validation: 3.0632984181590572]
	TIME [epoch: 6.13 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.868913915822916		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 2.868913915822916 | validation: 2.895424676737151]
	TIME [epoch: 6.13 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9274061460104464		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 2.9274061460104464 | validation: 3.020891532664825]
	TIME [epoch: 6.13 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8545146821908465		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 2.8545146821908465 | validation: 2.8886811466978806]
	TIME [epoch: 435 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.794545852953238		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 2.794545852953238 | validation: 3.025357488198626]
	TIME [epoch: 12.2 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.838519752231842		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 2.838519752231842 | validation: 2.849131004633941]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.826638607570332		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 2.826638607570332 | validation: 3.2173617056753594]
	TIME [epoch: 12.1 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8373899704715537		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 2.8373899704715537 | validation: 2.871771551907882]
	TIME [epoch: 12.1 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8565766220355666		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 2.8565766220355666 | validation: 2.8153271260267294]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.732817648236158		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 2.732817648236158 | validation: 2.9130012860841887]
	TIME [epoch: 12.1 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8614505425261876		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 2.8614505425261876 | validation: 3.1596868140693037]
	TIME [epoch: 12.1 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.883942561801836		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 2.883942561801836 | validation: 2.9966043003752487]
	TIME [epoch: 12.1 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.777113788888076		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 2.777113788888076 | validation: 2.7779365873702693]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8079497449806934		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 2.8079497449806934 | validation: 2.867467499531391]
	TIME [epoch: 12.1 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7664555035250453		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 2.7664555035250453 | validation: 2.8248386317072827]
	TIME [epoch: 12.2 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9374098881873305		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 2.9374098881873305 | validation: 3.057533823465281]
	TIME [epoch: 12.2 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.845718855319192		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 2.845718855319192 | validation: 2.8427974524510478]
	TIME [epoch: 12.2 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.705471866376124		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 2.705471866376124 | validation: 3.2118294070290263]
	TIME [epoch: 12.2 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.84830517174484		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 2.84830517174484 | validation: 2.8018684343925937]
	TIME [epoch: 12.2 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9502108186752776		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 2.9502108186752776 | validation: 2.951558155484241]
	TIME [epoch: 12.2 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9355680398598993		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 2.9355680398598993 | validation: 3.0499806348704155]
	TIME [epoch: 12.2 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.802346590545526		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 2.802346590545526 | validation: 2.7048034579470412]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.744536888838099		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 2.744536888838099 | validation: 2.802547130321537]
	TIME [epoch: 12.1 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7297096191933448		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 2.7297096191933448 | validation: 3.130224085958806]
	TIME [epoch: 12.1 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8665641106515913		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 2.8665641106515913 | validation: 2.9452199701662045]
	TIME [epoch: 12.1 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.745317283074235		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 2.745317283074235 | validation: 2.720840443247896]
	TIME [epoch: 12.2 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.735064920373748		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 2.735064920373748 | validation: 2.8874408049861495]
	TIME [epoch: 12.2 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7255919744929367		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 2.7255919744929367 | validation: 2.7412207188430093]
	TIME [epoch: 12.2 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6877015396897286		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 2.6877015396897286 | validation: 2.7465210305119214]
	TIME [epoch: 12.2 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.656146533854967		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 2.656146533854967 | validation: 2.8389705499452673]
	TIME [epoch: 12.2 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6586544135909724		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 2.6586544135909724 | validation: 3.1218069964832384]
	TIME [epoch: 12.1 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.736871785378661		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 2.736871785378661 | validation: 2.8818969236876066]
	TIME [epoch: 12.1 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7167831717834336		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 2.7167831717834336 | validation: 2.751992024927028]
	TIME [epoch: 12.1 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.840958245136273		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 2.840958245136273 | validation: 3.026692107263516]
	TIME [epoch: 12.1 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.810754528162236		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 2.810754528162236 | validation: 3.026524656347765]
	TIME [epoch: 12.2 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7601496986343244		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 2.7601496986343244 | validation: 2.810274735278548]
	TIME [epoch: 12.2 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.678222673352785		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 2.678222673352785 | validation: 2.82446610199809]
	TIME [epoch: 12.2 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.683397410468496		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 2.683397410468496 | validation: 2.8078950404738174]
	TIME [epoch: 12.2 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6921059108957692		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 2.6921059108957692 | validation: 3.0935647307132235]
	TIME [epoch: 12.1 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7200330246535502		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 2.7200330246535502 | validation: 2.717694651377373]
	TIME [epoch: 12.1 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6216737882279784		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 2.6216737882279784 | validation: 2.569892731408798]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.583098714205623		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 2.583098714205623 | validation: 2.93560566625979]
	TIME [epoch: 12.1 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6331109671068713		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 2.6331109671068713 | validation: 2.9155127709572324]
	TIME [epoch: 12.1 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.625504398686217		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 2.625504398686217 | validation: 2.5789295166351036]
	TIME [epoch: 12.1 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6163292264821383		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 2.6163292264821383 | validation: 2.8548669180169552]
	TIME [epoch: 12.1 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6142172466724127		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 2.6142172466724127 | validation: 2.785241386084091]
	TIME [epoch: 12.2 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6001179084461197		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 2.6001179084461197 | validation: 2.7012665409964276]
	TIME [epoch: 12.2 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5166535696003964		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 2.5166535696003964 | validation: 2.95068936269742]
	TIME [epoch: 12.2 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.723865311779862		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 2.723865311779862 | validation: 2.718828568288174]
	TIME [epoch: 12.2 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.613041020561446		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 2.613041020561446 | validation: 2.7634940578617013]
	TIME [epoch: 12.2 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.503020497454512		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 2.503020497454512 | validation: 2.9280597775940573]
	TIME [epoch: 12.1 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6510596835675555		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 2.6510596835675555 | validation: 2.843252978655329]
	TIME [epoch: 12.1 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.591838539708448		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 2.591838539708448 | validation: 2.683139058765077]
	TIME [epoch: 12.1 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5342988102876745		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 2.5342988102876745 | validation: 2.8365285965213864]
	TIME [epoch: 12.1 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.776025323075606		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 2.776025323075606 | validation: 2.8681906561410435]
	TIME [epoch: 12.1 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.68706863474808		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 2.68706863474808 | validation: 2.6881403618306883]
	TIME [epoch: 12.1 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4869768172202553		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 2.4869768172202553 | validation: 2.689953758595626]
	TIME [epoch: 12.2 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.645370007995937		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 2.645370007995937 | validation: 2.75783276792785]
	TIME [epoch: 12.2 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.47029056062196		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 2.47029056062196 | validation: 2.8325223056399835]
	TIME [epoch: 12.2 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.567417695725307		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 2.567417695725307 | validation: 2.7888061367580255]
	TIME [epoch: 12.2 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5975120079179175		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 2.5975120079179175 | validation: 2.7319170033624136]
	TIME [epoch: 12.1 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.539749403336569		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 2.539749403336569 | validation: 3.017740921348992]
	TIME [epoch: 12.2 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5222348073312144		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 2.5222348073312144 | validation: 2.6122383519514556]
	TIME [epoch: 12.2 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.504434525391204		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 2.504434525391204 | validation: 2.8784253503249335]
	TIME [epoch: 12.2 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.793314816124222		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 2.793314816124222 | validation: 2.8806275617927963]
	TIME [epoch: 12.2 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.646585800853477		[learning rate: 0.004405]
	Learning Rate: 0.00440502
	LOSS [training: 2.646585800853477 | validation: 2.6157119497181682]
	TIME [epoch: 12.1 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5290076133177353		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 2.5290076133177353 | validation: 2.4680780835389013]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3922471172153275		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 2.3922471172153275 | validation: 2.8471738958861246]
	TIME [epoch: 12.1 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6241802894516435		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 2.6241802894516435 | validation: 3.000939459125261]
	TIME [epoch: 12.2 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.763703324617385		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 2.763703324617385 | validation: 2.7513256544471507]
	TIME [epoch: 12.2 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5196444127429265		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 2.5196444127429265 | validation: 2.526069304812083]
	TIME [epoch: 12.2 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6811537173202145		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 2.6811537173202145 | validation: 3.327265079564688]
	TIME [epoch: 12.2 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.854323604116176		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 2.854323604116176 | validation: 2.775945878036231]
	TIME [epoch: 12.2 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.459431595793365		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 2.459431595793365 | validation: 2.7557938122474095]
	TIME [epoch: 12.2 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.456160522758274		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 2.456160522758274 | validation: 3.021637763555985]
	TIME [epoch: 12.2 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.599562476161145		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 2.599562476161145 | validation: 2.4151198743873725]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.508623881547017		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 2.508623881547017 | validation: 2.827504483837993]
	TIME [epoch: 12.1 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.549140570125492		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 2.549140570125492 | validation: 2.9759799653277486]
	TIME [epoch: 12.1 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.500662551822038		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 2.500662551822038 | validation: 2.42756255507405]
	TIME [epoch: 12.1 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.34476075744522		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 2.34476075744522 | validation: 2.8563215711039414]
	TIME [epoch: 12.1 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5195297093197757		[learning rate: 0.0039498]
	Learning Rate: 0.00394983
	LOSS [training: 2.5195297093197757 | validation: 2.7364124056848684]
	TIME [epoch: 12.1 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.42902836628458		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 2.42902836628458 | validation: 2.6396586607694905]
	TIME [epoch: 12.1 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3461403916610797		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 2.3461403916610797 | validation: 2.5964696036301937]
	TIME [epoch: 12.1 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.356808717259777		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 2.356808717259777 | validation: 2.756088964708621]
	TIME [epoch: 12.1 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5206338379885533		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 2.5206338379885533 | validation: 2.6865733815186554]
	TIME [epoch: 12.1 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4873673768272115		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 2.4873673768272115 | validation: 2.252280822494922]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5532574629192037		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 2.5532574629192037 | validation: 2.5474185608622353]
	TIME [epoch: 12.2 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3638659512128655		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 2.3638659512128655 | validation: 2.6175970788218734]
	TIME [epoch: 12.2 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6943789808098533		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 2.6943789808098533 | validation: 2.4447519890256166]
	TIME [epoch: 12.1 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.48363721838866		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 2.48363721838866 | validation: 3.0231167099922]
	TIME [epoch: 12.1 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.596602700029453		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 2.596602700029453 | validation: 2.55374266791266]
	TIME [epoch: 12.1 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4563346487833306		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 2.4563346487833306 | validation: 2.495757165771769]
	TIME [epoch: 12.1 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3738313002646367		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 2.3738313002646367 | validation: 2.4414409056905018]
	TIME [epoch: 12.1 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3842621353754727		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 2.3842621353754727 | validation: 2.7386737216786132]
	TIME [epoch: 12.2 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5071887158886725		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 2.5071887158886725 | validation: 3.429353573586573]
	TIME [epoch: 12.2 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.9030418487532907		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 2.9030418487532907 | validation: 3.1911962504797504]
	TIME [epoch: 12.2 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.726126989103598		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 2.726126989103598 | validation: 3.1903174350174313]
	TIME [epoch: 12.1 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.552754696149437		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 2.552754696149437 | validation: 3.0599640774689316]
	TIME [epoch: 12.1 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.548517637537379		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 2.548517637537379 | validation: 2.810652372960618]
	TIME [epoch: 12.1 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8384528468721437		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 2.8384528468721437 | validation: 3.261905659345066]
	TIME [epoch: 12.1 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.8478750412218257		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 2.8478750412218257 | validation: 3.040226092817069]
	TIME [epoch: 12.1 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.671324412742562		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 2.671324412742562 | validation: 3.12629521820979]
	TIME [epoch: 12.1 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5188313996193346		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 2.5188313996193346 | validation: 2.54685999305251]
	TIME [epoch: 12.2 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.370740554699023		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 2.370740554699023 | validation: 2.522248600774745]
	TIME [epoch: 12.1 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.433578669798981		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 2.433578669798981 | validation: 2.604918229844203]
	TIME [epoch: 12.1 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4547739701789038		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 2.4547739701789038 | validation: 2.479190525387353]
	TIME [epoch: 12.1 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4881176661173536		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 2.4881176661173536 | validation: 2.7656382394179833]
	TIME [epoch: 12.1 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.670890027512847		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 2.670890027512847 | validation: 3.0507476168145375]
	TIME [epoch: 12.1 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5032384651285664		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 2.5032384651285664 | validation: 2.8950579286886793]
	TIME [epoch: 12.1 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5709937664626206		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 2.5709937664626206 | validation: 2.7344073530455955]
	TIME [epoch: 12.1 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4926174387515285		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 2.4926174387515285 | validation: 2.7116598157252305]
	TIME [epoch: 12.1 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3113726748427323		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 2.3113726748427323 | validation: 2.3366087401356452]
	TIME [epoch: 12.1 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.253440200426441		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 2.253440200426441 | validation: 2.936485401761531]
	TIME [epoch: 12.1 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4738936571540626		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 2.4738936571540626 | validation: 2.7800863520588046]
	TIME [epoch: 12.1 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.6093194821257937		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 2.6093194821257937 | validation: 2.885622314745961]
	TIME [epoch: 12.2 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.488330396555688		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 2.488330396555688 | validation: 2.71968031338112]
	TIME [epoch: 12.1 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.286952750432165		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 2.286952750432165 | validation: 2.572072753603041]
	TIME [epoch: 12.1 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3064705883398764		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 2.3064705883398764 | validation: 2.6973411178042115]
	TIME [epoch: 12.1 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.350322236682607		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 2.350322236682607 | validation: 2.5597037766069652]
	TIME [epoch: 12.1 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.319016712085527		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 2.319016712085527 | validation: 3.0294183936720187]
	TIME [epoch: 12.1 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4881612002781517		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 2.4881612002781517 | validation: 2.7936641366959343]
	TIME [epoch: 12.1 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3212665055728814		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 2.3212665055728814 | validation: 2.437778179749763]
	TIME [epoch: 12.2 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1865740081117804		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 2.1865740081117804 | validation: 2.10623322434422]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.218894717495263		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 2.218894717495263 | validation: 2.613789410820197]
	TIME [epoch: 12.2 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.505820608453351		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 2.505820608453351 | validation: 2.3727966564717136]
	TIME [epoch: 12.2 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.217397715767313		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 2.217397715767313 | validation: 2.070905506917046]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1070079638073675		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 2.1070079638073675 | validation: 2.1823645239638854]
	TIME [epoch: 12.2 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1159269042869115		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 2.1159269042869115 | validation: 2.5626281900428207]
	TIME [epoch: 12.2 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2571794928845383		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 2.2571794928845383 | validation: 2.617169236132799]
	TIME [epoch: 12.2 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4204498290138186		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 2.4204498290138186 | validation: 2.2085709107367606]
	TIME [epoch: 12.2 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2879540216989764		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 2.2879540216989764 | validation: 2.3256017309916883]
	TIME [epoch: 12.2 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.239782113291838		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 2.239782113291838 | validation: 2.4072234870495555]
	TIME [epoch: 12.1 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.211089088622563		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 2.211089088622563 | validation: 2.754379017018273]
	TIME [epoch: 12.1 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3810096718459537		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 2.3810096718459537 | validation: 2.551268711961491]
	TIME [epoch: 12.1 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.4015483082938505		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 2.4015483082938505 | validation: 2.788988594277133]
	TIME [epoch: 12.1 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.225348125897984		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 2.225348125897984 | validation: 2.2836261046660455]
	TIME [epoch: 12.1 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2067534569499783		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 2.2067534569499783 | validation: 2.7046993298901727]
	TIME [epoch: 12.1 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3141083008991616		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 2.3141083008991616 | validation: 2.5891438928299415]
	TIME [epoch: 12.1 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2479643867127908		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 2.2479643867127908 | validation: 2.424780339353276]
	TIME [epoch: 12.1 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1418443037092274		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 2.1418443037092274 | validation: 2.1475925362915724]
	TIME [epoch: 12.2 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.125907811094437		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 2.125907811094437 | validation: 2.5655717482886287]
	TIME [epoch: 12.1 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1580025303734653		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 2.1580025303734653 | validation: 2.286457300322106]
	TIME [epoch: 12.1 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.097392024705546		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 2.097392024705546 | validation: 2.6007080297709]
	TIME [epoch: 12.1 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.232108370921524		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 2.232108370921524 | validation: 2.6826612546628708]
	TIME [epoch: 12.1 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2174747013573253		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 2.2174747013573253 | validation: 2.3039251688733753]
	TIME [epoch: 12.1 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1028853310366245		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 2.1028853310366245 | validation: 2.1628939040368724]
	TIME [epoch: 12.1 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1750991754336497		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 2.1750991754336497 | validation: 2.606982251444661]
	TIME [epoch: 12.1 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1311320230296635		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 2.1311320230296635 | validation: 2.36331984752371]
	TIME [epoch: 12.1 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2704261094014444		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 2.2704261094014444 | validation: 2.2877615315313955]
	TIME [epoch: 12.1 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.194863564311258		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 2.194863564311258 | validation: 2.51957205857474]
	TIME [epoch: 12.1 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2676189728273775		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 2.2676189728273775 | validation: 2.5294020069890037]
	TIME [epoch: 12.1 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1363092572769573		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 2.1363092572769573 | validation: 2.696404334811727]
	TIME [epoch: 12.1 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.186873055025683		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 2.186873055025683 | validation: 2.554564663553524]
	TIME [epoch: 12.1 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.210177367062291		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 2.210177367062291 | validation: 2.450425348938426]
	TIME [epoch: 454 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.164298689257552		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 2.164298689257552 | validation: 2.4998728867223727]
	TIME [epoch: 25.9 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.186720227528871		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 2.186720227528871 | validation: 1.968105510385762]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.082338089021145		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 2.082338089021145 | validation: 2.0548001600482992]
	TIME [epoch: 25.8 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.062968955015796		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 2.062968955015796 | validation: 2.4443259646746025]
	TIME [epoch: 25.9 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.198975917344998		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 2.198975917344998 | validation: 2.660550154430929]
	TIME [epoch: 26 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.149757975724687		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 2.149757975724687 | validation: 2.1760429939349337]
	TIME [epoch: 26 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1016752253312703		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 2.1016752253312703 | validation: 2.1104251415695474]
	TIME [epoch: 26 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.021612469936963		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 2.021612469936963 | validation: 2.2319543672341746]
	TIME [epoch: 25.9 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.036896350724854		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 2.036896350724854 | validation: 2.6208203490911606]
	TIME [epoch: 25.9 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.124933354477174		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 2.124933354477174 | validation: 2.4583449725612576]
	TIME [epoch: 25.9 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0407576285724702		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 2.0407576285724702 | validation: 2.140512614750029]
	TIME [epoch: 26 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9811865250367175		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 1.9811865250367175 | validation: 2.181796985092969]
	TIME [epoch: 25.9 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.099800990596508		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 2.099800990596508 | validation: 2.459608518057798]
	TIME [epoch: 25.9 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.282948897023112		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 2.282948897023112 | validation: 2.167622199580261]
	TIME [epoch: 25.9 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1061667027178137		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 2.1061667027178137 | validation: 2.6185265474587425]
	TIME [epoch: 26 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0970472292279854		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 2.0970472292279854 | validation: 2.2992508540710093]
	TIME [epoch: 26 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1162203900779604		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 2.1162203900779604 | validation: 1.9306220930496902]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0373632758806126		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 2.0373632758806126 | validation: 1.9960633833093389]
	TIME [epoch: 25.9 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1250482299741784		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 2.1250482299741784 | validation: 2.2555478585485966]
	TIME [epoch: 25.8 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.14974514700035		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 2.14974514700035 | validation: 2.500222019517925]
	TIME [epoch: 25.8 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1559103539036206		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 2.1559103539036206 | validation: 1.9983101365005294]
	TIME [epoch: 25.9 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9653161868950149		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 1.9653161868950149 | validation: 2.350439183710405]
	TIME [epoch: 26 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.019571085703619		[learning rate: 0.0019653]
	Learning Rate: 0.00196526
	LOSS [training: 2.019571085703619 | validation: 2.1128846080491326]
	TIME [epoch: 26 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9940055491821604		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 1.9940055491821604 | validation: 2.398933398270798]
	TIME [epoch: 26 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.198025624302704		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 2.198025624302704 | validation: 1.8998856950499468]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.181262907905586		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 2.181262907905586 | validation: 2.7529971060471006]
	TIME [epoch: 25.9 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3547170752504902		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 2.3547170752504902 | validation: 2.7065782136311594]
	TIME [epoch: 25.9 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0864694543440923		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 2.0864694543440923 | validation: 2.1452593026706217]
	TIME [epoch: 26 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0975218291506477		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 2.0975218291506477 | validation: 1.8323710191103149]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.01388771357256		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 2.01388771357256 | validation: 2.0605859678064498]
	TIME [epoch: 26 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.030145982780443		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 2.030145982780443 | validation: 2.204830310904267]
	TIME [epoch: 25.9 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.025376639713131		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 2.025376639713131 | validation: 1.9190133285044688]
	TIME [epoch: 25.8 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.033309472102919		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 2.033309472102919 | validation: 1.9162437236922705]
	TIME [epoch: 25.9 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0103116443630413		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 2.0103116443630413 | validation: 1.978983584612806]
	TIME [epoch: 25.9 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.985965033401742		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 1.985965033401742 | validation: 2.0452015328772397]
	TIME [epoch: 26 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9572449335155693		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 1.9572449335155693 | validation: 2.053831170555262]
	TIME [epoch: 26 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0606223514209736		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 2.0606223514209736 | validation: 2.0209888980996524]
	TIME [epoch: 25.9 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0618400522452855		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 2.0618400522452855 | validation: 2.3465863526873187]
	TIME [epoch: 25.9 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0443451443549168		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 2.0443451443549168 | validation: 2.2904905206159993]
	TIME [epoch: 26 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9489667779715258		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 1.9489667779715258 | validation: 2.033898373164819]
	TIME [epoch: 26 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9970630686688338		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 1.9970630686688338 | validation: 2.076104358601989]
	TIME [epoch: 26 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.892428241526095		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 1.892428241526095 | validation: 2.2030656040292595]
	TIME [epoch: 26.7 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.885328860053145		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 1.885328860053145 | validation: 1.868242822688799]
	TIME [epoch: 26 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9392491616396783		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 1.9392491616396783 | validation: 2.6556457156491318]
	TIME [epoch: 26 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1513770629146483		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 2.1513770629146483 | validation: 2.5885025409563447]
	TIME [epoch: 26 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.2479215964240185		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 2.2479215964240185 | validation: 2.3139893277123145]
	TIME [epoch: 26 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.959222046782242		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 1.959222046782242 | validation: 2.3329123842535138]
	TIME [epoch: 26 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9185607475843152		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 1.9185607475843152 | validation: 1.7741554482235986]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0296447810916467		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 2.0296447810916467 | validation: 1.9722235256696425]
	TIME [epoch: 26 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9265905834273571		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 1.9265905834273571 | validation: 1.9693405222827665]
	TIME [epoch: 26 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9463127733613208		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 1.9463127733613208 | validation: 1.9807734986271897]
	TIME [epoch: 26.1 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.929427908896217		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 1.929427908896217 | validation: 2.225438019225442]
	TIME [epoch: 25.9 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9336849751041516		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 1.9336849751041516 | validation: 1.7578058776606038]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8723578110235028		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 1.8723578110235028 | validation: 2.258204239834516]
	TIME [epoch: 25.9 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9052244077609446		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 1.9052244077609446 | validation: 2.1809409991378184]
	TIME [epoch: 26 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9194408312540319		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 1.9194408312540319 | validation: 2.033006861673739]
	TIME [epoch: 26.1 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8697534841530115		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 1.8697534841530115 | validation: 2.096743856679013]
	TIME [epoch: 25.9 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8261731302506865		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 1.8261731302506865 | validation: 2.125002119139831]
	TIME [epoch: 25.9 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0067214448129413		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 2.0067214448129413 | validation: 1.899445494815374]
	TIME [epoch: 25.9 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8134167643965804		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 1.8134167643965804 | validation: 2.042456621353794]
	TIME [epoch: 26 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.05195769590129		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 2.05195769590129 | validation: 2.6038620517624507]
	TIME [epoch: 25.8 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9213902437758015		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 1.9213902437758015 | validation: 2.1432060789791727]
	TIME [epoch: 25.8 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8669745946996503		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 1.8669745946996503 | validation: 2.121518248938007]
	TIME [epoch: 25.8 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0055416223845555		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 2.0055416223845555 | validation: 2.457648006084691]
	TIME [epoch: 25.8 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9239661201024263		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 1.9239661201024263 | validation: 2.0319161762777282]
	TIME [epoch: 25.9 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8331943600640175		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 1.8331943600640175 | validation: 1.667365971446547]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8365131999115758		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 1.8365131999115758 | validation: 1.790634080973167]
	TIME [epoch: 25.9 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.834656861947951		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 1.834656861947951 | validation: 1.6723759604370598]
	TIME [epoch: 25.8 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9990423957635064		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 1.9990423957635064 | validation: 2.552226809711623]
	TIME [epoch: 25.8 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.173097268851623		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 2.173097268851623 | validation: 3.136214262838612]
	TIME [epoch: 25.8 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1588290364277425		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 2.1588290364277425 | validation: 2.2669996693319927]
	TIME [epoch: 26 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.848321051732212		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 1.848321051732212 | validation: 2.0258827482816777]
	TIME [epoch: 26 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7942117257139287		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 1.7942117257139287 | validation: 2.260324691556505]
	TIME [epoch: 26 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.964398733874826		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 1.964398733874826 | validation: 2.136175022762654]
	TIME [epoch: 25.9 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487054875467617		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 1.8487054875467617 | validation: 2.1685694694313407]
	TIME [epoch: 25.9 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.894391659672139		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 1.894391659672139 | validation: 1.616808416885124]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8088283109007286		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 1.8088283109007286 | validation: 2.811639218522389]
	TIME [epoch: 26 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9609926799706896		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 1.9609926799706896 | validation: 2.1427911562555098]
	TIME [epoch: 25.9 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8306651105903515		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 1.8306651105903515 | validation: 1.787839240552004]
	TIME [epoch: 25.9 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.76508636812291		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 1.76508636812291 | validation: 1.96080850974402]
	TIME [epoch: 25.9 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.756419314587346		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 1.756419314587346 | validation: 1.925070967942974]
	TIME [epoch: 26 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.759693835737709		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 1.759693835737709 | validation: 1.9369481305681755]
	TIME [epoch: 26 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8369536447500667		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 1.8369536447500667 | validation: 1.84267948111686]
	TIME [epoch: 25.9 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.811722335681681		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 1.811722335681681 | validation: 1.9886375564378438]
	TIME [epoch: 25.9 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7963926330257953		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 1.7963926330257953 | validation: 1.5796472004826723]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8202812943174689		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 1.8202812943174689 | validation: 1.896678498575714]
	TIME [epoch: 26 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8688617333025241		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 1.8688617333025241 | validation: 2.0086898913637765]
	TIME [epoch: 26 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.805027063507288		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 1.805027063507288 | validation: 1.9705548587533352]
	TIME [epoch: 25.9 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8579791342385579		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 1.8579791342385579 | validation: 1.9741005055122014]
	TIME [epoch: 25.9 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532129462759308		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 1.8532129462759308 | validation: 1.8870128845795002]
	TIME [epoch: 25.9 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7443338739130951		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 1.7443338739130951 | validation: 1.8808079670444555]
	TIME [epoch: 26 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7425141321991147		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 1.7425141321991147 | validation: 1.7904675917399273]
	TIME [epoch: 25.9 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7312286781001203		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 1.7312286781001203 | validation: 1.8206657677663567]
	TIME [epoch: 25.9 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7686178989752004		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 1.7686178989752004 | validation: 2.0959706262684583]
	TIME [epoch: 25.9 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8706748579907244		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 1.8706748579907244 | validation: 2.0297919993281255]
	TIME [epoch: 26 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7592501974998571		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 1.7592501974998571 | validation: 1.8838467618647567]
	TIME [epoch: 26 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7358419905930447		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 1.7358419905930447 | validation: 2.0086332759736285]
	TIME [epoch: 25.9 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7247805170652621		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 1.7247805170652621 | validation: 1.9600998162438632]
	TIME [epoch: 25.9 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7040987996162418		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 1.7040987996162418 | validation: 1.9819971866089523]
	TIME [epoch: 25.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8860464784091993		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 1.8860464784091993 | validation: 1.7874014913524334]
	TIME [epoch: 25.8 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.795471107038322		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 1.795471107038322 | validation: 1.5565638652024703]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7371091847004467		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 1.7371091847004467 | validation: 1.9020220473984457]
	TIME [epoch: 25.9 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7294609909936192		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 1.7294609909936192 | validation: 1.609118327123659]
	TIME [epoch: 25.9 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6667488719226118		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 1.6667488719226118 | validation: 2.4575589232469666]
	TIME [epoch: 26 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8790495086978907		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 1.8790495086978907 | validation: 2.054952649530032]
	TIME [epoch: 26.1 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7134777051775225		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 1.7134777051775225 | validation: 2.3449698557736918]
	TIME [epoch: 25.9 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.855387643804059		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 1.855387643804059 | validation: 2.3908461590468715]
	TIME [epoch: 25.8 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.88795347000878		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 1.88795347000878 | validation: 2.1467494835422096]
	TIME [epoch: 25.8 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.811867302220094		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 1.811867302220094 | validation: 1.7071222670936153]
	TIME [epoch: 25.8 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7062661875589473		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 1.7062661875589473 | validation: 1.8092965638663339]
	TIME [epoch: 26 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7065683355625196		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 1.7065683355625196 | validation: 1.7354437164247758]
	TIME [epoch: 26 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6838972607782043		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 1.6838972607782043 | validation: 1.6546378011094973]
	TIME [epoch: 25.9 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6309857696345236		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 1.6309857696345236 | validation: 1.9003680035325248]
	TIME [epoch: 25.9 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7936153161351027		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 1.7936153161351027 | validation: 2.4555409638513197]
	TIME [epoch: 25.9 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.914008265632905		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 1.914008265632905 | validation: 1.7492145869841025]
	TIME [epoch: 26 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7084136877573763		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 1.7084136877573763 | validation: 1.7346081774295559]
	TIME [epoch: 26 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7411162396023256		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 1.7411162396023256 | validation: 1.6601403061723599]
	TIME [epoch: 25.9 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6230282800036908		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 1.6230282800036908 | validation: 1.8384420162266553]
	TIME [epoch: 25.9 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6966713027697256		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 1.6966713027697256 | validation: 1.991616993032639]
	TIME [epoch: 25.9 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6642824742062805		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 1.6642824742062805 | validation: 1.6372715095625812]
	TIME [epoch: 26 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6361721711157808		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 1.6361721711157808 | validation: 1.6933064606334538]
	TIME [epoch: 26 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.624536616050653		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 1.624536616050653 | validation: 1.7358077755077561]
	TIME [epoch: 25.9 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.660421469725377		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 1.660421469725377 | validation: 1.6125315401189027]
	TIME [epoch: 25.9 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6287526049986805		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 1.6287526049986805 | validation: 1.7584793627663409]
	TIME [epoch: 26.1 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6996095952231904		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 1.6996095952231904 | validation: 1.9020832588984908]
	TIME [epoch: 26 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.694855637710658		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 1.694855637710658 | validation: 1.7560220314820354]
	TIME [epoch: 26 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.605986279605776		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 1.605986279605776 | validation: 1.901570525922681]
	TIME [epoch: 25.9 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.715995844803013		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 1.715995844803013 | validation: 1.946710106854765]
	TIME [epoch: 26 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6352694383928128		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 1.6352694383928128 | validation: 1.6368262773690838]
	TIME [epoch: 25.9 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6927183071626568		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 1.6927183071626568 | validation: 2.2747375152859792]
	TIME [epoch: 26.1 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8470974579297132		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 1.8470974579297132 | validation: 1.9460903301571264]
	TIME [epoch: 26 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7856804258540033		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 1.7856804258540033 | validation: 1.6853175815426455]
	TIME [epoch: 25.9 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.658660012739421		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 1.658660012739421 | validation: 1.7935714864029375]
	TIME [epoch: 25.9 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.659590499920541		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 1.659590499920541 | validation: 1.8758801472400797]
	TIME [epoch: 25.9 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7337539186756037		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 1.7337539186756037 | validation: 1.871837151934208]
	TIME [epoch: 26 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7203763235336416		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 1.7203763235336416 | validation: 2.0697321570661917]
	TIME [epoch: 26 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8339159499315039		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 1.8339159499315039 | validation: 1.8798253425128106]
	TIME [epoch: 26 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.753022600536652		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 1.753022600536652 | validation: 2.055611263274846]
	TIME [epoch: 26 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7531075477120228		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 1.7531075477120228 | validation: 1.9338067980314608]
	TIME [epoch: 25.9 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7007288607146371		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 1.7007288607146371 | validation: 1.8080866046527269]
	TIME [epoch: 25.9 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.615164871667988		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 1.615164871667988 | validation: 1.8869720406207509]
	TIME [epoch: 26 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6418523102570537		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 1.6418523102570537 | validation: 1.963707613543167]
	TIME [epoch: 25.9 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7378555183121216		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 1.7378555183121216 | validation: 1.9645566131003074]
	TIME [epoch: 26 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.657510601725737		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 1.657510601725737 | validation: 2.102823189073971]
	TIME [epoch: 26 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7405288801972676		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 1.7405288801972676 | validation: 2.329311360289844]
	TIME [epoch: 26 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6960275040854929		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 1.6960275040854929 | validation: 1.6539788670525843]
	TIME [epoch: 25.9 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6926658200145925		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 1.6926658200145925 | validation: 1.5966220980210026]
	TIME [epoch: 25.9 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.63381630857366		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 1.63381630857366 | validation: 1.6661600274548496]
	TIME [epoch: 25.9 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5950037340308274		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 1.5950037340308274 | validation: 1.7267604284246802]
	TIME [epoch: 26 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.629369093809173		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 1.629369093809173 | validation: 1.740641090145839]
	TIME [epoch: 26 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6117742854548878		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 1.6117742854548878 | validation: 1.7693547523069377]
	TIME [epoch: 25.9 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6460374237093354		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 1.6460374237093354 | validation: 1.8047276038210764]
	TIME [epoch: 25.9 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6270493067376155		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 1.6270493067376155 | validation: 1.4928125005243977]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5358988249500132		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 1.5358988249500132 | validation: 1.7558248993684056]
	TIME [epoch: 25.9 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6177786374775138		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 1.6177786374775138 | validation: 1.9074401962535807]
	TIME [epoch: 25.9 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.680647555914386		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 1.680647555914386 | validation: 1.7994631150108782]
	TIME [epoch: 26 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5721692210667855		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 1.5721692210667855 | validation: 1.7497958283200021]
	TIME [epoch: 26.1 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5798522586145372		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 1.5798522586145372 | validation: 1.6382222146590104]
	TIME [epoch: 25.9 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7152288777357723		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 1.7152288777357723 | validation: 1.668221792613346]
	TIME [epoch: 25.8 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6734451787333264		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 1.6734451787333264 | validation: 1.832913806749195]
	TIME [epoch: 25.9 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5960551175158848		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 1.5960551175158848 | validation: 1.5420226977868081]
	TIME [epoch: 25.9 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.662673519172427		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 1.662673519172427 | validation: 1.5696605512989517]
	TIME [epoch: 25.9 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5775631587274508		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 1.5775631587274508 | validation: 1.7463908650238777]
	TIME [epoch: 26 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6784637001493725		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 1.6784637001493725 | validation: 1.7541481735032591]
	TIME [epoch: 26 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6183056697483162		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 1.6183056697483162 | validation: 1.6476388141498368]
	TIME [epoch: 26 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5395812439728547		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 1.5395812439728547 | validation: 1.6384455989234206]
	TIME [epoch: 25.9 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5717400333554865		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 1.5717400333554865 | validation: 1.5826612520101193]
	TIME [epoch: 25.9 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5702226511651745		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 1.5702226511651745 | validation: 1.5774296685316431]
	TIME [epoch: 25.9 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.572845513665002		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 1.572845513665002 | validation: 1.587520895435886]
	TIME [epoch: 25.9 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5755552445237424		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 1.5755552445237424 | validation: 1.8396888228985395]
	TIME [epoch: 26 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6762200137347847		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 1.6762200137347847 | validation: 1.8131226900743254]
	TIME [epoch: 26 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7410054131084092		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 1.7410054131084092 | validation: 1.706982906647231]
	TIME [epoch: 25.9 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6186441187473704		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 1.6186441187473704 | validation: 1.8936178823551004]
	TIME [epoch: 25.9 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6552514280945625		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 1.6552514280945625 | validation: 1.9660224163594449]
	TIME [epoch: 25.9 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6724785120576648		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 1.6724785120576648 | validation: 1.8211596866244328]
	TIME [epoch: 26 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6524431329474218		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 1.6524431329474218 | validation: 1.7537358456167569]
	TIME [epoch: 26 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.540528328252993		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 1.540528328252993 | validation: 1.6455223777269206]
	TIME [epoch: 25.9 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.544136897983023		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 1.544136897983023 | validation: 1.7118874660566714]
	TIME [epoch: 26 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5466458457645713		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 1.5466458457645713 | validation: 1.5995271147986043]
	TIME [epoch: 26 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.513612684254151		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 1.513612684254151 | validation: 1.8585584086222418]
	TIME [epoch: 25.9 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6309162255285619		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 1.6309162255285619 | validation: 1.7866805807684574]
	TIME [epoch: 25.8 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.590818211119689		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 1.590818211119689 | validation: 1.773101150574417]
	TIME [epoch: 25.8 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5452841521859293		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 1.5452841521859293 | validation: 1.6729206626143753]
	TIME [epoch: 25.9 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5603986452065		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 1.5603986452065 | validation: 1.5990016636959725]
	TIME [epoch: 26 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6021260918059284		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 1.6021260918059284 | validation: 1.6777996153160228]
	TIME [epoch: 26 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.571958968340571		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 1.571958968340571 | validation: 1.771760996669595]
	TIME [epoch: 25.8 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6327107300838708		[learning rate: 0.00059638]
	Learning Rate: 0.000596385
	LOSS [training: 1.6327107300838708 | validation: 1.9258011309325134]
	TIME [epoch: 25.8 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.552880352387928		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 1.552880352387928 | validation: 1.6790815807360975]
	TIME [epoch: 25.8 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6486566096312743		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 1.6486566096312743 | validation: 1.9238468372769133]
	TIME [epoch: 25.8 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6111279372752163		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 1.6111279372752163 | validation: 1.6548551129643227]
	TIME [epoch: 25.9 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5516108095388665		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 1.5516108095388665 | validation: 1.5841105338141797]
	TIME [epoch: 26 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.568408099221294		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 1.568408099221294 | validation: 1.4806505013926827]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4840715942847598		[learning rate: 0.00057092]
	Learning Rate: 0.000570925
	LOSS [training: 1.4840715942847598 | validation: 1.7193794202424604]
	TIME [epoch: 25.9 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5692932950069645		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 1.5692932950069645 | validation: 1.5765329896163682]
	TIME [epoch: 25.9 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5232617273049502		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 1.5232617273049502 | validation: 1.610795971694972]
	TIME [epoch: 25.9 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5308186888742865		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 1.5308186888742865 | validation: 1.9893456103673324]
	TIME [epoch: 25.9 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6509020872419127		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 1.6509020872419127 | validation: 1.6184378198775522]
	TIME [epoch: 26 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5307307940234955		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 1.5307307940234955 | validation: 1.5239002975087352]
	TIME [epoch: 26.1 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5305128499473803		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 1.5305128499473803 | validation: 1.5827886400450022]
	TIME [epoch: 25.9 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5238412361218723		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 1.5238412361218723 | validation: 1.5660826700061872]
	TIME [epoch: 25.9 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5672427916756504		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 1.5672427916756504 | validation: 1.8585879086826673]
	TIME [epoch: 25.9 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6680022785999327		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 1.6680022785999327 | validation: 1.4840735200909307]
	TIME [epoch: 25.9 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4861223574283986		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 1.4861223574283986 | validation: 1.5450126190429077]
	TIME [epoch: 26 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5282174894349754		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 1.5282174894349754 | validation: 1.4813059551384025]
	TIME [epoch: 26.1 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5585478414257303		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 1.5585478414257303 | validation: 1.7326588393910856]
	TIME [epoch: 26 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5105210940529268		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 1.5105210940529268 | validation: 1.7032094867813043]
	TIME [epoch: 26 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5343617319569862		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 1.5343617319569862 | validation: 1.8944518063595603]
	TIME [epoch: 26.1 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.560122876166018		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 1.560122876166018 | validation: 1.6472471219421405]
	TIME [epoch: 25.9 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5062753326695633		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 1.5062753326695633 | validation: 1.4967378776136842]
	TIME [epoch: 25.8 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5797812931623212		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 1.5797812931623212 | validation: 1.815512210176577]
	TIME [epoch: 25.8 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6115979422123121		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 1.6115979422123121 | validation: 1.462772489350451]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5701347025663446		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 1.5701347025663446 | validation: 1.6674534297560992]
	TIME [epoch: 26 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5215805391356965		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 1.5215805391356965 | validation: 1.7319051919352992]
	TIME [epoch: 26 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5542918221096054		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 1.5542918221096054 | validation: 1.6789578444334583]
	TIME [epoch: 26 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5213506468490543		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 1.5213506468490543 | validation: 1.587312228861107]
	TIME [epoch: 26 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.524233940504069		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 1.524233940504069 | validation: 1.6311780596922096]
	TIME [epoch: 26 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.535130445683426		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 1.535130445683426 | validation: 1.4613924812363088]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5495244106868875		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 1.5495244106868875 | validation: 1.9200200582466167]
	TIME [epoch: 25.9 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5302659906098777		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 1.5302659906098777 | validation: 1.4668364929803164]
	TIME [epoch: 25.8 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4782714749627461		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 1.4782714749627461 | validation: 1.6677849227912236]
	TIME [epoch: 25.9 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.502868983684345		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 1.502868983684345 | validation: 1.4651902341985443]
	TIME [epoch: 25.9 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5396851128297993		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 1.5396851128297993 | validation: 2.0994311365592337]
	TIME [epoch: 26 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5718049171028774		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 1.5718049171028774 | validation: 1.7856705846914198]
	TIME [epoch: 25.9 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5334097380076364		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 1.5334097380076364 | validation: 1.5739634611683564]
	TIME [epoch: 25.8 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4948214440767602		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 1.4948214440767602 | validation: 1.8113920411388715]
	TIME [epoch: 25.8 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4966835554618907		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 1.4966835554618907 | validation: 1.6468326534810867]
	TIME [epoch: 25.9 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.473781389207578		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 1.473781389207578 | validation: 1.6006765667436942]
	TIME [epoch: 26 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.486950325472725		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 1.486950325472725 | validation: 1.4413679925112388]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4344565333851054		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 1.4344565333851054 | validation: 1.7243065546644267]
	TIME [epoch: 26 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4626700947806617		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 1.4626700947806617 | validation: 1.4752836575435553]
	TIME [epoch: 26 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4656500955818461		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 1.4656500955818461 | validation: 1.4925009572744499]
	TIME [epoch: 26.1 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4658311716509176		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 1.4658311716509176 | validation: 1.4525442636184527]
	TIME [epoch: 26 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4500458222020616		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 1.4500458222020616 | validation: 1.777357247014545]
	TIME [epoch: 25.9 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5613986448372756		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 1.5613986448372756 | validation: 1.908238438418133]
	TIME [epoch: 25.9 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5362892068511436		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 1.5362892068511436 | validation: 1.4392682739472782]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4434615647172184		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 1.4434615647172184 | validation: 1.4107109384768244]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5049724693581377		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 1.5049724693581377 | validation: 1.4768580753785945]
	TIME [epoch: 26.1 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5224486259691052		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 1.5224486259691052 | validation: 1.5925621824981355]
	TIME [epoch: 26 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5303866200730474		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 1.5303866200730474 | validation: 1.6033044194549357]
	TIME [epoch: 25.9 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5262100642707765		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 1.5262100642707765 | validation: 1.6517542513730032]
	TIME [epoch: 25.8 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.48362710499967		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 1.48362710499967 | validation: 1.7050227789903705]
	TIME [epoch: 25.8 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4055725532318015		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 1.4055725532318015 | validation: 1.6701373711397487]
	TIME [epoch: 26 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4816720990767367		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 1.4816720990767367 | validation: 1.693149947405163]
	TIME [epoch: 26 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4840343744675624		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 1.4840343744675624 | validation: 1.5270901529724608]
	TIME [epoch: 25.9 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4327955575092348		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 1.4327955575092348 | validation: 1.9538793697589556]
	TIME [epoch: 25.8 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.477508998630786		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 1.477508998630786 | validation: 1.7549419151679584]
	TIME [epoch: 25.9 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5227454924199808		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 1.5227454924199808 | validation: 1.7278552800171922]
	TIME [epoch: 25.9 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5016197433077065		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 1.5016197433077065 | validation: 1.4664520364865425]
	TIME [epoch: 26 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4675535751646627		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 1.4675535751646627 | validation: 1.5976857849027843]
	TIME [epoch: 26 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4135548976049352		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 1.4135548976049352 | validation: 1.6531968211798391]
	TIME [epoch: 485 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4436147817934077		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 1.4436147817934077 | validation: 1.5589944219455671]
	TIME [epoch: 54.8 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4612824956712558		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 1.4612824956712558 | validation: 1.5456704564732573]
	TIME [epoch: 54.9 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4796216660953585		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 1.4796216660953585 | validation: 1.4735062725550745]
	TIME [epoch: 55.2 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4533787343386473		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 1.4533787343386473 | validation: 1.4071740308195495]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4400507474770023		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 1.4400507474770023 | validation: 1.4815430413963973]
	TIME [epoch: 55.1 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4330833035363215		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 1.4330833035363215 | validation: 1.4815779071807493]
	TIME [epoch: 55.2 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4254994573708397		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 1.4254994573708397 | validation: 1.6595964829346395]
	TIME [epoch: 55.1 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4330241185505725		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 1.4330241185505725 | validation: 1.5157010970140816]
	TIME [epoch: 55 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4580606525339368		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 1.4580606525339368 | validation: 1.5388785203749917]
	TIME [epoch: 55.1 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.434349472413333		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 1.434349472413333 | validation: 1.5638768767811262]
	TIME [epoch: 55.1 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4068780417656932		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 1.4068780417656932 | validation: 1.4752146810295375]
	TIME [epoch: 55.1 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4207841770901521		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 1.4207841770901521 | validation: 1.553190140437081]
	TIME [epoch: 55 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4099694717863804		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 1.4099694717863804 | validation: 1.5466670666157305]
	TIME [epoch: 55 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4381046612635535		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 1.4381046612635535 | validation: 1.7117743247648898]
	TIME [epoch: 54.9 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.419211100943043		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 1.419211100943043 | validation: 1.4464469516519984]
	TIME [epoch: 55 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4112896333970855		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 1.4112896333970855 | validation: 1.3983378488742342]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.379863971644723		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 1.379863971644723 | validation: 1.3694918781737289]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3608641883769068		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 1.3608641883769068 | validation: 1.6946898092738052]
	TIME [epoch: 55.1 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4696344474404004		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 1.4696344474404004 | validation: 1.6134967930692117]
	TIME [epoch: 54.9 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4398221158167654		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 1.4398221158167654 | validation: 1.6098424703419505]
	TIME [epoch: 55.1 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4498075904243217		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 1.4498075904243217 | validation: 1.7840020137078136]
	TIME [epoch: 54.9 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4591759211226845		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 1.4591759211226845 | validation: 1.5086007412719722]
	TIME [epoch: 55 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3796775246746353		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 1.3796775246746353 | validation: 1.3893636481512996]
	TIME [epoch: 55 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.389943223161215		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 1.389943223161215 | validation: 1.4837350548931934]
	TIME [epoch: 55.1 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3854427424162608		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 1.3854427424162608 | validation: 1.6376096921451655]
	TIME [epoch: 54.9 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.477704330225074		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 1.477704330225074 | validation: 1.5128786733418595]
	TIME [epoch: 55 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3909848149405726		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 1.3909848149405726 | validation: 1.438151854752979]
	TIME [epoch: 55 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3853733245014106		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 1.3853733245014106 | validation: 1.5079608191041705]
	TIME [epoch: 55 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.378240132736112		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 1.378240132736112 | validation: 1.493298037194112]
	TIME [epoch: 55 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4070177517105547		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 1.4070177517105547 | validation: 1.7382356439743525]
	TIME [epoch: 54.9 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.371988761803569		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 1.371988761803569 | validation: 1.4874164689706872]
	TIME [epoch: 54.9 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3691948289990408		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 1.3691948289990408 | validation: 1.5215583015485703]
	TIME [epoch: 55 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.33186659810342		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 1.33186659810342 | validation: 1.7051305075376462]
	TIME [epoch: 55 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.462406229705764		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 1.462406229705764 | validation: 1.5803235231317845]
	TIME [epoch: 55 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3836606269175367		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 1.3836606269175367 | validation: 1.5934230750092997]
	TIME [epoch: 55 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3522968578473775		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 1.3522968578473775 | validation: 1.4946921702064355]
	TIME [epoch: 55.1 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3443788719697103		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 1.3443788719697103 | validation: 1.591457846578482]
	TIME [epoch: 55 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.360170979325061		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 1.360170979325061 | validation: 1.502434116024066]
	TIME [epoch: 55 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3817523798965983		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 1.3817523798965983 | validation: 1.401213630244651]
	TIME [epoch: 55 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3487638380131561		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 1.3487638380131561 | validation: 1.4185631041450462]
	TIME [epoch: 55 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3483615560923015		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 1.3483615560923015 | validation: 1.5537277601857415]
	TIME [epoch: 55.1 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3436324993143751		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 1.3436324993143751 | validation: 1.4257143781729233]
	TIME [epoch: 55 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3358390094486206		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 1.3358390094486206 | validation: 1.435122753945488]
	TIME [epoch: 55.1 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3507807585698672		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 1.3507807585698672 | validation: 1.4336992532713555]
	TIME [epoch: 54.9 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.34665023395653		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 1.34665023395653 | validation: 1.3660236399848849]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4209210687176952		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 1.4209210687176952 | validation: 1.407097430504443]
	TIME [epoch: 55.1 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3580362645193906		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 1.3580362645193906 | validation: 1.4465013640330486]
	TIME [epoch: 55.1 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.367050419325692		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 1.367050419325692 | validation: 1.4152995386545617]
	TIME [epoch: 55 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.372126918664922		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 1.372126918664922 | validation: 1.3981527453423073]
	TIME [epoch: 55 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3556066284823136		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 1.3556066284823136 | validation: 1.5111345100179463]
	TIME [epoch: 54.9 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3447054175855166		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 1.3447054175855166 | validation: 1.5528488784818686]
	TIME [epoch: 55 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.334728637300123		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 1.334728637300123 | validation: 1.502137689403485]
	TIME [epoch: 55 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3369461869751396		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 1.3369461869751396 | validation: 1.4590770823873391]
	TIME [epoch: 55 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.342535152074023		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 1.342535152074023 | validation: 1.5338373014779192]
	TIME [epoch: 55.1 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3475325544877614		[learning rate: 0.00025287]
	Learning Rate: 0.000252869
	LOSS [training: 1.3475325544877614 | validation: 1.443787775618665]
	TIME [epoch: 55 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3360983699214928		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 1.3360983699214928 | validation: 1.769976206487893]
	TIME [epoch: 55 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3773302146869777		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 1.3773302146869777 | validation: 1.647425799340549]
	TIME [epoch: 55 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3934004644154576		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 1.3934004644154576 | validation: 1.5611015540690993]
	TIME [epoch: 55 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3653416048398097		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 1.3653416048398097 | validation: 1.5841177009753533]
	TIME [epoch: 55.1 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3954967848340634		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 1.3954967848340634 | validation: 1.5746752154706738]
	TIME [epoch: 55 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3453580941560779		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 1.3453580941560779 | validation: 1.6170298365054827]
	TIME [epoch: 54.9 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.392933429085449		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 1.392933429085449 | validation: 1.485879104355901]
	TIME [epoch: 55 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3408416816339663		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 1.3408416816339663 | validation: 1.463267944365925]
	TIME [epoch: 55.1 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3190085265479443		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 1.3190085265479443 | validation: 1.4747708131257302]
	TIME [epoch: 55 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3254155032835753		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 1.3254155032835753 | validation: 1.3752344648571875]
	TIME [epoch: 55 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3507196784290434		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 1.3507196784290434 | validation: 1.4679032167215977]
	TIME [epoch: 54.9 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3262487028092895		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 1.3262487028092895 | validation: 1.4037973370947046]
	TIME [epoch: 55 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3060732080718702		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 1.3060732080718702 | validation: 1.4180118262806563]
	TIME [epoch: 55 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3407930133797454		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 1.3407930133797454 | validation: 1.5070838460252656]
	TIME [epoch: 55.1 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3165449298127527		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 1.3165449298127527 | validation: 1.5525278501562008]
	TIME [epoch: 55 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3222907052947028		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 1.3222907052947028 | validation: 1.4689321986475106]
	TIME [epoch: 55.1 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3247751573419742		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 1.3247751573419742 | validation: 1.464946286518072]
	TIME [epoch: 55 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3195919566500345		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 1.3195919566500345 | validation: 1.5019682897998652]
	TIME [epoch: 55 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3125431244987966		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 1.3125431244987966 | validation: 1.394741763119329]
	TIME [epoch: 55 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3110244732622807		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 1.3110244732622807 | validation: 1.5106856563145001]
	TIME [epoch: 55.1 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3106440364261107		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 1.3106440364261107 | validation: 1.408546194087085]
	TIME [epoch: 54.9 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.316815931380561		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 1.316815931380561 | validation: 1.521812809670091]
	TIME [epoch: 55.1 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3193862595859116		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 1.3193862595859116 | validation: 1.495577376118205]
	TIME [epoch: 54.9 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.31996684811536		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 1.31996684811536 | validation: 1.5567870188238768]
	TIME [epoch: 55.1 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.320870006787964		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 1.320870006787964 | validation: 1.552197076220802]
	TIME [epoch: 54.9 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3163336065684315		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 1.3163336065684315 | validation: 1.6143133155092833]
	TIME [epoch: 55 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3670546900073453		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 1.3670546900073453 | validation: 1.4902861691780727]
	TIME [epoch: 55 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.328266254725972		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 1.328266254725972 | validation: 1.491276607789412]
	TIME [epoch: 54.9 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3214061090654106		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 1.3214061090654106 | validation: 1.4056527503747773]
	TIME [epoch: 55 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3047482355651256		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 1.3047482355651256 | validation: 1.3983195752994666]
	TIME [epoch: 55 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3101979662298668		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 1.3101979662298668 | validation: 1.3813755140303434]
	TIME [epoch: 55 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3200890955219375		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 1.3200890955219375 | validation: 1.421202431785226]
	TIME [epoch: 54.9 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2925768581736061		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 1.2925768581736061 | validation: 1.4537255111459118]
	TIME [epoch: 55 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3049342111531488		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 1.3049342111531488 | validation: 1.4253214824537053]
	TIME [epoch: 55.1 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.310309811230896		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 1.310309811230896 | validation: 1.4422417543755053]
	TIME [epoch: 55 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3905968952937748		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 1.3905968952937748 | validation: 1.4553090242536977]
	TIME [epoch: 55 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3473444737930633		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 1.3473444737930633 | validation: 1.4758522996493555]
	TIME [epoch: 55 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3399693587369976		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 1.3399693587369976 | validation: 1.399455476045655]
	TIME [epoch: 55 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3209708112973195		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 1.3209708112973195 | validation: 1.4285789002669067]
	TIME [epoch: 55 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3038956751487505		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 1.3038956751487505 | validation: 1.4057730791428358]
	TIME [epoch: 55 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3172490307169609		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 1.3172490307169609 | validation: 1.4024505510547094]
	TIME [epoch: 55 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3065558062141192		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 1.3065558062141192 | validation: 1.4097825896264564]
	TIME [epoch: 55.1 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3141200950974554		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 1.3141200950974554 | validation: 1.3888286580531637]
	TIME [epoch: 55 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2998736379546605		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 1.2998736379546605 | validation: 1.3964231027042502]
	TIME [epoch: 55 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.298029693375632		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 1.298029693375632 | validation: 1.3953244718563313]
	TIME [epoch: 55 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2771121577588236		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 1.2771121577588236 | validation: 1.5517121693469347]
	TIME [epoch: 54.9 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3148314954663203		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 1.3148314954663203 | validation: 1.6209991450334833]
	TIME [epoch: 54.8 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3504077413255549		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 1.3504077413255549 | validation: 1.5142498279695018]
	TIME [epoch: 55 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.323262414494753		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 1.323262414494753 | validation: 1.5880450408929039]
	TIME [epoch: 54.9 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.316056003225104		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 1.316056003225104 | validation: 1.5491949232356172]
	TIME [epoch: 55 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2950387237977803		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 1.2950387237977803 | validation: 1.3778059544729406]
	TIME [epoch: 54.9 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2932994380917917		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 1.2932994380917917 | validation: 1.4534960078298589]
	TIME [epoch: 55.1 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2923421100739994		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 1.2923421100739994 | validation: 1.4031675432835589]
	TIME [epoch: 55.1 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2990291773326104		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 1.2990291773326104 | validation: 1.5524095451356046]
	TIME [epoch: 55 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.317481340270012		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 1.317481340270012 | validation: 1.4493368495171866]
	TIME [epoch: 55 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2989893400019412		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 1.2989893400019412 | validation: 1.4013237938033831]
	TIME [epoch: 55 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2935552825516377		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 1.2935552825516377 | validation: 1.4192793640656243]
	TIME [epoch: 54.9 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2923447837388353		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 1.2923447837388353 | validation: 1.363441022095279]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.325355291093714		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 1.325355291093714 | validation: 1.4542087710300706]
	TIME [epoch: 55.1 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3093779167539161		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 1.3093779167539161 | validation: 1.4570759335668289]
	TIME [epoch: 55.1 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3122635256396014		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 1.3122635256396014 | validation: 1.4522738359806695]
	TIME [epoch: 55.1 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.31314145588674		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 1.31314145588674 | validation: 1.4029173569422373]
	TIME [epoch: 55.2 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2896804728182414		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 1.2896804728182414 | validation: 1.4518295870724809]
	TIME [epoch: 54.9 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3060907728022446		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 1.3060907728022446 | validation: 1.5270619281649727]
	TIME [epoch: 55 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.308549096473254		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 1.308549096473254 | validation: 1.3853180688426807]
	TIME [epoch: 55.1 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3238582728790127		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 1.3238582728790127 | validation: 1.428603818157125]
	TIME [epoch: 54.9 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2969088416501586		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 1.2969088416501586 | validation: 1.401471238746552]
	TIME [epoch: 55.2 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3263420952083085		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 1.3263420952083085 | validation: 1.5095629621095716]
	TIME [epoch: 54.9 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3115930703043468		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 1.3115930703043468 | validation: 1.4060272188201755]
	TIME [epoch: 55 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3075703937248786		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 1.3075703937248786 | validation: 1.4077534786172976]
	TIME [epoch: 55 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.312483408940966		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 1.312483408940966 | validation: 1.4163049352679806]
	TIME [epoch: 55 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3178029004018388		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 1.3178029004018388 | validation: 1.4295382803619474]
	TIME [epoch: 55 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3005200968742743		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 1.3005200968742743 | validation: 1.4211303500185322]
	TIME [epoch: 55.1 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2896431497914944		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 1.2896431497914944 | validation: 1.4029139552109187]
	TIME [epoch: 54.9 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3207150586638678		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 1.3207150586638678 | validation: 1.4373246366876664]
	TIME [epoch: 54.9 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2987038102779778		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 1.2987038102779778 | validation: 1.436429449066749]
	TIME [epoch: 55.1 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.306229730371781		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 1.306229730371781 | validation: 1.4602346918869409]
	TIME [epoch: 55.1 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2940867517994876		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 1.2940867517994876 | validation: 1.3999039544661727]
	TIME [epoch: 55.1 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.276566777815912		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 1.276566777815912 | validation: 1.3873799484349862]
	TIME [epoch: 54.9 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2815502443409617		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 1.2815502443409617 | validation: 1.38535714442081]
	TIME [epoch: 54.9 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2832502666183592		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 1.2832502666183592 | validation: 1.3715470271070849]
	TIME [epoch: 54.9 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.277172494717082		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 1.277172494717082 | validation: 1.402076781392815]
	TIME [epoch: 55.1 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2761555731260823		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 1.2761555731260823 | validation: 1.4019134124439048]
	TIME [epoch: 55 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2762633554514387		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 1.2762633554514387 | validation: 1.4531444176341655]
	TIME [epoch: 55.1 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3018337363229662		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 1.3018337363229662 | validation: 1.449739723809204]
	TIME [epoch: 55 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.318438687622519		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 1.318438687622519 | validation: 1.4773891313029583]
	TIME [epoch: 55.1 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2879420887078807		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 1.2879420887078807 | validation: 1.4446508574899033]
	TIME [epoch: 55 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.296449167757791		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 1.296449167757791 | validation: 1.3929811622157335]
	TIME [epoch: 55 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.283034564769277		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 1.283034564769277 | validation: 1.380535607568786]
	TIME [epoch: 54.8 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2800502178749833		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 1.2800502178749833 | validation: 1.3603581105569758]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2659188008250144		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 1.2659188008250144 | validation: 1.4836425769391217]
	TIME [epoch: 55 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.312425303531936		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 1.312425303531936 | validation: 1.4470050978186708]
	TIME [epoch: 55 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2794626838472503		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 1.2794626838472503 | validation: 1.5462148795115325]
	TIME [epoch: 55 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2937857045717536		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 1.2937857045717536 | validation: 1.4704297705619584]
	TIME [epoch: 55 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2952733791751005		[learning rate: 0.00012673]
	Learning Rate: 0.000126734
	LOSS [training: 1.2952733791751005 | validation: 1.4645504722583622]
	TIME [epoch: 55 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2853361887727053		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 1.2853361887727053 | validation: 1.3960036833085694]
	TIME [epoch: 54.9 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3045494281707606		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 1.3045494281707606 | validation: 1.4155267415342516]
	TIME [epoch: 55 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.288188025384032		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 1.288188025384032 | validation: 1.4538214668711684]
	TIME [epoch: 55 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.284387052597399		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 1.284387052597399 | validation: 1.394884832335634]
	TIME [epoch: 54.9 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2778955339773248		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 1.2778955339773248 | validation: 1.401476148970779]
	TIME [epoch: 55.1 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2829848821849699		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 1.2829848821849699 | validation: 1.4189217044300342]
	TIME [epoch: 55 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2625878847024046		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 1.2625878847024046 | validation: 1.4204604757961192]
	TIME [epoch: 54.9 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.265375834194388		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 1.265375834194388 | validation: 1.4139314901604996]
	TIME [epoch: 55 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.264243712349994		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 1.264243712349994 | validation: 1.4559580070397233]
	TIME [epoch: 55 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2678635242688105		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 1.2678635242688105 | validation: 1.367360166274679]
	TIME [epoch: 54.9 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2768412917920486		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 1.2768412917920486 | validation: 1.3553915853371783]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2954804303218754		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 1.2954804303218754 | validation: 1.342820248444192]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2916164719023415		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 1.2916164719023415 | validation: 1.3412374709683004]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_664.pth
	Model improved!!!
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2859126344035292		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 1.2859126344035292 | validation: 1.3455439145563903]
	TIME [epoch: 54.9 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2670897130809182		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 1.2670897130809182 | validation: 1.3463693492514235]
	TIME [epoch: 55 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2636526916538624		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 1.2636526916538624 | validation: 1.4089129766023758]
	TIME [epoch: 55 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2769087786980065		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 1.2769087786980065 | validation: 1.4064372873497928]
	TIME [epoch: 54.9 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.263728185334232		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 1.263728185334232 | validation: 1.44189205677423]
	TIME [epoch: 54.8 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2704003406224122		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 1.2704003406224122 | validation: 1.4395952916172752]
	TIME [epoch: 55 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2805296649042883		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 1.2805296649042883 | validation: 1.4340899298514895]
	TIME [epoch: 55 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2740063146806961		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 1.2740063146806961 | validation: 1.4136287160371257]
	TIME [epoch: 55 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2819137595228614		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 1.2819137595228614 | validation: 1.390951654729005]
	TIME [epoch: 55 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.294321209380615		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 1.294321209380615 | validation: 1.3930548846483775]
	TIME [epoch: 54.9 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2725377283693162		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 1.2725377283693162 | validation: 1.3872030394554]
	TIME [epoch: 55 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2706528951346883		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 1.2706528951346883 | validation: 1.408026887239806]
	TIME [epoch: 55.1 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2839759665144912		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 1.2839759665144912 | validation: 1.3868481118342135]
	TIME [epoch: 55 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2768204926907676		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 1.2768204926907676 | validation: 1.3704006526961736]
	TIME [epoch: 55 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2780460393271709		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 1.2780460393271709 | validation: 1.3783098553332502]
	TIME [epoch: 55 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2841779484092817		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 1.2841779484092817 | validation: 1.4006274015939153]
	TIME [epoch: 54.9 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2734673597208233		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 1.2734673597208233 | validation: 1.3907601969316472]
	TIME [epoch: 54.9 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2643006381262034		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 1.2643006381262034 | validation: 1.4897319009428647]
	TIME [epoch: 55 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2729262103457635		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 1.2729262103457635 | validation: 1.4179679665638971]
	TIME [epoch: 55 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2674516703876162		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 1.2674516703876162 | validation: 1.3952948778562202]
	TIME [epoch: 55 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2640665778198263		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 1.2640665778198263 | validation: 1.343477802610175]
	TIME [epoch: 55 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.255798290942484		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 1.255798290942484 | validation: 1.3551139242654222]
	TIME [epoch: 54.9 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2654979782753		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 1.2654979782753 | validation: 1.3522702006819578]
	TIME [epoch: 55 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2577866237483681		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 1.2577866237483681 | validation: 1.401832034011867]
	TIME [epoch: 55 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2605825024420751		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 1.2605825024420751 | validation: 1.3811785837300667]
	TIME [epoch: 55.1 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2621994013949362		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 1.2621994013949362 | validation: 1.3519801505291957]
	TIME [epoch: 55 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2653160139862902		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 1.2653160139862902 | validation: 1.3335587677400609]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_691.pth
	Model improved!!!
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2705267447316628		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 1.2705267447316628 | validation: 1.3514684004874318]
	TIME [epoch: 55 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2687417955340319		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 1.2687417955340319 | validation: 1.3585161697623738]
	TIME [epoch: 55 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.272554398997134		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 1.272554398997134 | validation: 1.3417138938795077]
	TIME [epoch: 55.1 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2721789454412502		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 1.2721789454412502 | validation: 1.3516720043962924]
	TIME [epoch: 54.9 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2661306373840067		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 1.2661306373840067 | validation: 1.373759852351249]
	TIME [epoch: 55.1 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2710671753077931		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 1.2710671753077931 | validation: 1.381663672086421]
	TIME [epoch: 55 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2580956459341912		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 1.2580956459341912 | validation: 1.3532113614215213]
	TIME [epoch: 55 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2646217692617228		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 1.2646217692617228 | validation: 1.3594090132716907]
	TIME [epoch: 55.1 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2639320792226219		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 1.2639320792226219 | validation: 1.3701054460497002]
	TIME [epoch: 54.9 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2612714360397008		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 1.2612714360397008 | validation: 1.3598865793084483]
	TIME [epoch: 55.1 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2584335258132033		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 1.2584335258132033 | validation: 1.389164284098993]
	TIME [epoch: 55.1 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2600633462916928		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 1.2600633462916928 | validation: 1.325639950304365]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_703.pth
	Model improved!!!
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2545676221411222		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 1.2545676221411222 | validation: 1.3728063480733659]
	TIME [epoch: 55 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.258172934253492		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 1.258172934253492 | validation: 1.4191326103240378]
	TIME [epoch: 54.8 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.256394909363341		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 1.256394909363341 | validation: 1.3272160884602326]
	TIME [epoch: 55 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2478390712816427		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 1.2478390712816427 | validation: 1.430204468498796]
	TIME [epoch: 54.9 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2664003770523848		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 1.2664003770523848 | validation: 1.3871371348123285]
	TIME [epoch: 55 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2582729784688498		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 1.2582729784688498 | validation: 1.3976565480264438]
	TIME [epoch: 55 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2751340178691088		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 1.2751340178691088 | validation: 1.440671383697198]
	TIME [epoch: 54.9 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2583235761716036		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 1.2583235761716036 | validation: 1.4164713247588487]
	TIME [epoch: 55 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2618219230587204		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 1.2618219230587204 | validation: 1.3303012478420602]
	TIME [epoch: 54.9 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2482281055501188		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 1.2482281055501188 | validation: 1.3225327236046782]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_713.pth
	Model improved!!!
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2634722806615675		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 1.2634722806615675 | validation: 1.3587539178074113]
	TIME [epoch: 54.9 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2437667774999517		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 1.2437667774999517 | validation: 1.333860239490496]
	TIME [epoch: 55.1 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.262522102806365		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 1.262522102806365 | validation: 1.3403941316888583]
	TIME [epoch: 55 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2551947170688553		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 1.2551947170688553 | validation: 1.3638923715406923]
	TIME [epoch: 54.9 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.255025385234442		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 1.255025385234442 | validation: 1.3962104302264726]
	TIME [epoch: 55 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2533255732053792		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 1.2533255732053792 | validation: 1.3584095540757293]
	TIME [epoch: 55 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2536987298995008		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 1.2536987298995008 | validation: 1.392835342586884]
	TIME [epoch: 55 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.252951623969424		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 1.252951623969424 | validation: 1.395625139664015]
	TIME [epoch: 55.1 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2588984727927561		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 1.2588984727927561 | validation: 1.351084878207219]
	TIME [epoch: 55 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2443598582034832		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 1.2443598582034832 | validation: 1.4237851683365028]
	TIME [epoch: 55 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2648271118493661		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 1.2648271118493661 | validation: 1.3817481120243205]
	TIME [epoch: 55 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2510963042919558		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 1.2510963042919558 | validation: 1.354244694317579]
	TIME [epoch: 54.9 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2458972617833783		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 1.2458972617833783 | validation: 1.4230736199039646]
	TIME [epoch: 55 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2570288066486022		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 1.2570288066486022 | validation: 1.3519915631213388]
	TIME [epoch: 54.8 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.251292403413213		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 1.251292403413213 | validation: 1.3413862771007241]
	TIME [epoch: 54.9 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2473575236991836		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 1.2473575236991836 | validation: 1.344234593931753]
	TIME [epoch: 55 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2453209689971898		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 1.2453209689971898 | validation: 1.3363533918287887]
	TIME [epoch: 54.9 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2437650057274154		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 1.2437650057274154 | validation: 1.3526746211607077]
	TIME [epoch: 55 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2492494586828842		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 1.2492494586828842 | validation: 1.364563530029879]
	TIME [epoch: 55.1 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2439785975793505		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 1.2439785975793505 | validation: 1.3301753271783356]
	TIME [epoch: 54.9 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2502110960755695		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 1.2502110960755695 | validation: 1.3282244353726134]
	TIME [epoch: 54.9 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2557966718214477		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 1.2557966718214477 | validation: 1.3652922695470997]
	TIME [epoch: 54.9 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2462518603927077		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 1.2462518603927077 | validation: 1.337906539936053]
	TIME [epoch: 55 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2452365806969472		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 1.2452365806969472 | validation: 1.343160885028934]
	TIME [epoch: 55 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2513974862703834		[learning rate: 6.7322e-05]
	Learning Rate: 6.73222e-05
	LOSS [training: 1.2513974862703834 | validation: 1.366077461308351]
	TIME [epoch: 55 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2456409832662663		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 1.2456409832662663 | validation: 1.3578336674251723]
	TIME [epoch: 54.8 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2438045036827095		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 1.2438045036827095 | validation: 1.3556905059139175]
	TIME [epoch: 54.9 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2355735851793301		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 1.2355735851793301 | validation: 1.3277062036306861]
	TIME [epoch: 55 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2456765902366342		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 1.2456765902366342 | validation: 1.3319482467520327]
	TIME [epoch: 54.9 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.256505328653261		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 1.256505328653261 | validation: 1.3683415661858729]
	TIME [epoch: 55.1 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2412409886478128		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 1.2412409886478128 | validation: 1.3332150407626446]
	TIME [epoch: 55 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2346970480732278		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 1.2346970480732278 | validation: 1.3864332294551187]
	TIME [epoch: 55 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.25604308658091		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 1.25604308658091 | validation: 1.3727866903984554]
	TIME [epoch: 55.1 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2439577467248064		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 1.2439577467248064 | validation: 1.3883840503794493]
	TIME [epoch: 55.1 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2424289695242963		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 1.2424289695242963 | validation: 1.3666467801237006]
	TIME [epoch: 54.9 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2389055625742638		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 1.2389055625742638 | validation: 1.3469716283817563]
	TIME [epoch: 55 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.239022512442545		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 1.239022512442545 | validation: 1.3497724929736779]
	TIME [epoch: 55.1 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2456646854477853		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 1.2456646854477853 | validation: 1.3254913950436082]
	TIME [epoch: 54.8 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.239755720804585		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 1.239755720804585 | validation: 1.3459585624303614]
	TIME [epoch: 55 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2468984734991277		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 1.2468984734991277 | validation: 1.3337321419568278]
	TIME [epoch: 54.9 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.242483806500562		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 1.242483806500562 | validation: 1.3187600374135817]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_754.pth
	Model improved!!!
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2352289846944418		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 1.2352289846944418 | validation: 1.3309797229820624]
	TIME [epoch: 54.9 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2471075477691658		[learning rate: 5.9063e-05]
	Learning Rate: 5.90631e-05
	LOSS [training: 1.2471075477691658 | validation: 1.3184126271477132]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2425403753379398		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 1.2425403753379398 | validation: 1.3642567072044305]
	TIME [epoch: 54.9 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2407725784622319		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 1.2407725784622319 | validation: 1.3625315775699356]
	TIME [epoch: 55.1 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2500189274952813		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 1.2500189274952813 | validation: 1.3366803202571385]
	TIME [epoch: 55 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2439733675355797		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 1.2439733675355797 | validation: 1.3139761707953987]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2339391553088723		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 1.2339391553088723 | validation: 1.3559597000209138]
	TIME [epoch: 54.9 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2400129502564088		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 1.2400129502564088 | validation: 1.3322938276439829]
	TIME [epoch: 55.1 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2416707232361321		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 1.2416707232361321 | validation: 1.3519887517848732]
	TIME [epoch: 55.2 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2355326725094988		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 1.2355326725094988 | validation: 1.3233105584294904]
	TIME [epoch: 54.9 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2420441342899644		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 1.2420441342899644 | validation: 1.3442142461000528]
	TIME [epoch: 55.1 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2402698589477477		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 1.2402698589477477 | validation: 1.3159814204312021]
	TIME [epoch: 55.1 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2400637443719715		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 1.2400637443719715 | validation: 1.337445709226688]
	TIME [epoch: 55 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2367888475133682		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 1.2367888475133682 | validation: 1.3354927003340697]
	TIME [epoch: 55 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.244029619515163		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 1.244029619515163 | validation: 1.345496510749495]
	TIME [epoch: 55.1 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.243314767220796		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 1.243314767220796 | validation: 1.3678961421196583]
	TIME [epoch: 55.1 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2407191675773144		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 1.2407191675773144 | validation: 1.362069970569102]
	TIME [epoch: 54.9 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.245588647405568		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 1.245588647405568 | validation: 1.3212226299261522]
	TIME [epoch: 55 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.237479231866806		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 1.237479231866806 | validation: 1.3174426179673477]
	TIME [epoch: 55 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2438251848706385		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 1.2438251848706385 | validation: 1.3250474738285778]
	TIME [epoch: 55 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2359851611093413		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 1.2359851611093413 | validation: 1.3627831777776036]
	TIME [epoch: 55 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.233782965892566		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 1.233782965892566 | validation: 1.3125903619394723]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phiq_2a_v_mmd1_20241205_182027/states/model_phiq_2a_v_mmd1_776.pth
	Model improved!!!
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2420198712978565		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 1.2420198712978565 | validation: 1.3410005330347188]
	TIME [epoch: 55 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2337009892160715		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 1.2337009892160715 | validation: 1.368628788833435]
	TIME [epoch: 54.9 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.238879686998136		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 1.238879686998136 | validation: 1.3570760980623326]
	TIME [epoch: 55 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2379299612277215		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 1.2379299612277215 | validation: 1.3599613695333264]
	TIME [epoch: 54.9 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2360842621101495		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 1.2360842621101495 | validation: 1.3519073682553122]
	TIME [epoch: 55.1 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.235926911661891		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 1.235926911661891 | validation: 1.3176850373180389]
	TIME [epoch: 54.9 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2346362150311772		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 1.2346362150311772 | validation: 1.3490421387520264]
	TIME [epoch: 55 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2356501839912526		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 1.2356501839912526 | validation: 1.3313055061025583]
	TIME [epoch: 54.9 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2347195822629218		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 1.2347195822629218 | validation: 1.3598768211146481]
	TIME [epoch: 55 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2328995785066108		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 1.2328995785066108 | validation: 1.386758294606926]
	TIME [epoch: 55 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.237957628390733		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 1.237957628390733 | validation: 1.3396403306716667]
	TIME [epoch: 55 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.237588314201314		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 1.237588314201314 | validation: 1.3555975448527153]
	TIME [epoch: 55 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2339663083071288		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 1.2339663083071288 | validation: 1.3374905608308039]
	TIME [epoch: 55 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.235671829891082		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 1.235671829891082 | validation: 1.3217852398213124]
	TIME [epoch: 55.1 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2297984655668373		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 1.2297984655668373 | validation: 1.3439174296050087]
	TIME [epoch: 54.9 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.231817548732177		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 1.231817548732177 | validation: 1.3346647393009]
	TIME [epoch: 55 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2271363884215263		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 1.2271363884215263 | validation: 1.3568221439460735]
	TIME [epoch: 55 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2341915798045715		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 1.2341915798045715 | validation: 1.3375399586945222]
	TIME [epoch: 55 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2285892115461077		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 1.2285892115461077 | validation: 1.3844208121786001]
	TIME [epoch: 55.1 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.240676453443374		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 1.240676453443374 | validation: 1.3715208743880143]
	TIME [epoch: 55 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2307675688920336		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 1.2307675688920336 | validation: 1.369542763241249]
	TIME [epoch: 55 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2320251824872757		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 1.2320251824872757 | validation: 1.3475813315005403]
	TIME [epoch: 54.9 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.230099148226579		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 1.230099148226579 | validation: 1.3220126874124796]
	TIME [epoch: 55 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2275503458800774		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 1.2275503458800774 | validation: 1.3146760811249096]
	TIME [epoch: 55 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2293660369877972		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 1.2293660369877972 | validation: 1.338796056061833]
	TIME [epoch: 55.1 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2328557172456431		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 1.2328557172456431 | validation: 1.3135402010691664]
	TIME [epoch: 55 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2311105985497488		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 1.2311105985497488 | validation: 1.3209523271559211]
	TIME [epoch: 55 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2298013372003331		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 1.2298013372003331 | validation: 1.3300392274844794]
	TIME [epoch: 55 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2264209743928096		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 1.2264209743928096 | validation: 1.3290246427828882]
	TIME [epoch: 55.1 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2339864135657512		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 1.2339864135657512 | validation: 1.334378052279098]
	TIME [epoch: 54.9 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2309207327821992		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 1.2309207327821992 | validation: 1.366554175865749]
	TIME [epoch: 55 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2351983240459945		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 1.2351983240459945 | validation: 1.3953724008361847]
	TIME [epoch: 55 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2317849181523093		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 1.2317849181523093 | validation: 1.3188620626028296]
	TIME [epoch: 55 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.229006488420594		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 1.229006488420594 | validation: 1.3358079882930152]
	TIME [epoch: 55.1 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.229924820172855		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 1.229924820172855 | validation: 1.3281086320046853]
	TIME [epoch: 55 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2335808662570582		[learning rate: 3.9307e-05]
	Learning Rate: 3.93073e-05
	LOSS [training: 1.2335808662570582 | validation: 1.3446207682795899]
	TIME [epoch: 55 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2302038199088678		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 1.2302038199088678 | validation: 1.327469176642417]
	TIME [epoch: 55.1 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2235135624958722		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 1.2235135624958722 | validation: 1.3243375640734865]
	TIME [epoch: 55 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2342890419730894		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 1.2342890419730894 | validation: 1.324203459683794]
	TIME [epoch: 55 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2246665809434103		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 1.2246665809434103 | validation: 1.3260958342557552]
	TIME [epoch: 55.1 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2267296298067534		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 1.2267296298067534 | validation: 1.3230266348124191]
	TIME [epoch: 55 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2227960072844508		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 1.2227960072844508 | validation: 1.331403228023703]
	TIME [epoch: 55 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2273447194168303		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 1.2273447194168303 | validation: 1.3387697782201304]
	TIME [epoch: 55.1 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2294421729192742		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 1.2294421729192742 | validation: 1.332314971411145]
	TIME [epoch: 54.9 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.229016198792653		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 1.229016198792653 | validation: 1.3312948789872827]
	TIME [epoch: 55 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2256950792056345		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 1.2256950792056345 | validation: 1.3357687379356773]
	TIME [epoch: 55 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2258036139364605		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 1.2258036139364605 | validation: 1.3298678747223498]
	TIME [epoch: 55 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2275072272468117		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 1.2275072272468117 | validation: 1.3272602280467778]
	TIME [epoch: 54.9 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.232741219645487		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 1.232741219645487 | validation: 1.3384277277986796]
	TIME [epoch: 55 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2255920017696693		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 1.2255920017696693 | validation: 1.317461234822896]
	TIME [epoch: 54.9 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2214284866494287		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 1.2214284866494287 | validation: 1.329994380672738]
	TIME [epoch: 54.9 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2267280714572142		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 1.2267280714572142 | validation: 1.3268377823575657]
	TIME [epoch: 55.1 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2245302350656022		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 1.2245302350656022 | validation: 1.3384469174910603]
	TIME [epoch: 55.1 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.220429494884009		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 1.220429494884009 | validation: 1.3677870884063572]
	TIME [epoch: 55 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2229640130269275		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 1.2229640130269275 | validation: 1.3275035856402084]
	TIME [epoch: 55.1 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.221062510320578		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 1.221062510320578 | validation: 1.3161284016913217]
	TIME [epoch: 55.1 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2247947659259404		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 1.2247947659259404 | validation: 1.3304100470916]
	TIME [epoch: 55 sec]
EPOCH 834/1000:
	Training over batches...
