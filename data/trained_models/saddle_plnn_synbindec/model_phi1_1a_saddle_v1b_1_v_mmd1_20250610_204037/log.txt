Args:
Namespace(name='model_phi1_1a_saddle_v1b_1_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1b_1/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1b_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05278933048248291, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2073746818

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.378927336914533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.378927336914533 | validation: 6.237560442346339]
	TIME [epoch: 385 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.583053891924195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.583053891924195 | validation: 4.3001856013532]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.639596143520269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.639596143520269 | validation: 3.6588045169474146]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.351372303546865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.351372303546865 | validation: 4.035108675515638]
	TIME [epoch: 6.12 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.03764534812273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.03764534812273 | validation: 3.3246318529487535]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.787841731495414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.787841731495414 | validation: 3.1831751555261962]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6342373386203652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6342373386203652 | validation: 3.1734863559704185]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5095417656260075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5095417656260075 | validation: 3.0226574780554967]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3545294219150024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3545294219150024 | validation: 3.150177345749924]
	TIME [epoch: 6.12 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1915255700092326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1915255700092326 | validation: 2.887025019411582]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0683091417478554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0683091417478554 | validation: 3.195204534067198]
	TIME [epoch: 6.12 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0307004285711283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0307004285711283 | validation: 2.717193177316946]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9012005109090264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9012005109090264 | validation: 2.6445811624835383]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7043461926353016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7043461926353016 | validation: 2.830513376453049]
	TIME [epoch: 6.11 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7312233549950857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7312233549950857 | validation: 2.4558547442950793]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2825618836584143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2825618836584143 | validation: 2.1242777866177005]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.327464344388964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.327464344388964 | validation: 2.036121321666587]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0252785100992376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0252785100992376 | validation: 1.9293860257842872]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9012613010617692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9012613010617692 | validation: 2.3525248557855845]
	TIME [epoch: 6.11 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2014472304960684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2014472304960684 | validation: 1.9710143140439935]
	TIME [epoch: 6.1 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9550865824698744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9550865824698744 | validation: 2.0471739852665567]
	TIME [epoch: 6.11 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8894736217202532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8894736217202532 | validation: 1.7378465214259784]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8474677866547538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8474677866547538 | validation: 4.082770393324385]
	TIME [epoch: 6.1 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.33149594149006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.33149594149006 | validation: 2.666697226920016]
	TIME [epoch: 6.11 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.739125017818622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.739125017818622 | validation: 2.5626846166185997]
	TIME [epoch: 6.11 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.559406683811921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.559406683811921 | validation: 2.427841734766051]
	TIME [epoch: 6.11 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.366266127547988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.366266127547988 | validation: 2.248508790203924]
	TIME [epoch: 6.12 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1119157438821827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1119157438821827 | validation: 2.1978165945126444]
	TIME [epoch: 6.11 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0739491356889057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0739491356889057 | validation: 1.92805442857544]
	TIME [epoch: 6.11 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9023857666939126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9023857666939126 | validation: 1.9152529196152985]
	TIME [epoch: 6.1 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8135481990345186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8135481990345186 | validation: 1.8165134929631273]
	TIME [epoch: 6.12 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8902407395297343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8902407395297343 | validation: 2.0331586847405836]
	TIME [epoch: 6.13 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7002707649456226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7002707649456226 | validation: 1.3868105972554057]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4585687630635744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4585687630635744 | validation: 1.9427644287905603]
	TIME [epoch: 6.13 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4185620131169394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4185620131169394 | validation: 1.4026138084629496]
	TIME [epoch: 6.12 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8081928045828026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8081928045828026 | validation: 1.4320585025882515]
	TIME [epoch: 6.13 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3512721313678588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3512721313678588 | validation: 1.1922506754726743]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1746318924470018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1746318924470018 | validation: 0.9976898738708628]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0441165573734528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0441165573734528 | validation: 1.080929572729062]
	TIME [epoch: 6.13 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0513304087514168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0513304087514168 | validation: 1.3913099495022054]
	TIME [epoch: 6.13 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2794070109000701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2794070109000701 | validation: 1.6559720057486453]
	TIME [epoch: 6.13 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3604372155378335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3604372155378335 | validation: 0.9988352663191188]
	TIME [epoch: 6.14 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9809541800511934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9809541800511934 | validation: 1.04041967144075]
	TIME [epoch: 6.13 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8861440779759945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8861440779759945 | validation: 0.8432055571266739]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8828970089164392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8828970089164392 | validation: 0.8395359207879471]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9074634151870175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074634151870175 | validation: 0.8450788275717709]
	TIME [epoch: 6.11 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9425941681875547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9425941681875547 | validation: 1.226923788277393]
	TIME [epoch: 6.1 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9515762769888403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9515762769888403 | validation: 0.757809436965989]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.804998648523932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804998648523932 | validation: 0.9525234167441506]
	TIME [epoch: 6.1 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.977628365892567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.977628365892567 | validation: 0.8269762167328913]
	TIME [epoch: 6.1 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9112462031808745		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.9112462031808745 | validation: 0.863692019136099]
	TIME [epoch: 6.11 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.984401041598361		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.984401041598361 | validation: 0.752290240902462]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.739101965319777		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.739101965319777 | validation: 0.6326764080529961]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8461759679971648		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.8461759679971648 | validation: 0.8428812398123192]
	TIME [epoch: 6.1 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1484674099304608		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.1484674099304608 | validation: 1.1387913330087993]
	TIME [epoch: 6.1 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8636279738349335		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.8636279738349335 | validation: 0.6112347952622973]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7263680004304959		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.7263680004304959 | validation: 0.5986250051167725]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7006046052521933		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.7006046052521933 | validation: 0.6892730574909374]
	TIME [epoch: 6.11 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7048163209056478		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.7048163209056478 | validation: 0.6047418077379763]
	TIME [epoch: 6.11 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7600584733081945		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.7600584733081945 | validation: 0.9122537352977935]
	TIME [epoch: 6.1 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7327964505441821		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.7327964505441821 | validation: 0.67340322389372]
	TIME [epoch: 6.12 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.81479355513174		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.81479355513174 | validation: 0.9300478857746279]
	TIME [epoch: 6.1 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7789325717410045		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.7789325717410045 | validation: 0.5810710614878883]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.794375534968272		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.794375534968272 | validation: 0.6654422295161935]
	TIME [epoch: 6.09 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6434862119826041		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.6434862119826041 | validation: 1.4360286713719703]
	TIME [epoch: 6.11 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9135903929479383		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.9135903929479383 | validation: 0.7078569406920409]
	TIME [epoch: 6.1 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7478231254791966		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.7478231254791966 | validation: 0.8667068136412668]
	TIME [epoch: 6.1 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7158138354084953		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.7158138354084953 | validation: 0.8762093361981506]
	TIME [epoch: 6.1 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6925368770057856		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.6925368770057856 | validation: 0.5218476161019643]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7837250278361255		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7837250278361255 | validation: 0.7117996347766461]
	TIME [epoch: 6.11 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7052225990692071		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.7052225990692071 | validation: 0.9479847381778798]
	TIME [epoch: 6.09 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7863356607615672		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.7863356607615672 | validation: 0.7466975727255916]
	TIME [epoch: 6.1 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.708104526564903		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.708104526564903 | validation: 0.5518299348026057]
	TIME [epoch: 6.09 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6841787265143349		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.6841787265143349 | validation: 0.5560857807268644]
	TIME [epoch: 6.1 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7096961959250894		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.7096961959250894 | validation: 0.5854704756814457]
	TIME [epoch: 6.1 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8044910101323879		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.8044910101323879 | validation: 0.808685035176768]
	TIME [epoch: 6.1 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.704683833215955		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.704683833215955 | validation: 0.7965333191509594]
	TIME [epoch: 6.1 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6368659249791955		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6368659249791955 | validation: 0.5634428020746718]
	TIME [epoch: 6.1 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6107014820263115		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.6107014820263115 | validation: 0.5327282710791217]
	TIME [epoch: 6.1 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6110370499438244		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.6110370499438244 | validation: 0.826477589742405]
	TIME [epoch: 6.1 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7171758649823857		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.7171758649823857 | validation: 0.4918155688340998]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6024371826469265		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.6024371826469265 | validation: 0.6265600817979944]
	TIME [epoch: 6.1 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6728123392761928		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6728123392761928 | validation: 0.6354000976967453]
	TIME [epoch: 6.09 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6013066114115111		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.6013066114115111 | validation: 0.7084539839887414]
	TIME [epoch: 6.1 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6864221922903766		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.6864221922903766 | validation: 0.6290280026600421]
	TIME [epoch: 6.1 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5432016758943792		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.5432016758943792 | validation: 0.5477062739995899]
	TIME [epoch: 6.1 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6809541981909131		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.6809541981909131 | validation: 0.5422173096377991]
	TIME [epoch: 6.1 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6708089187045538		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.6708089187045538 | validation: 0.6926743340249875]
	TIME [epoch: 6.09 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6912222411459077		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.6912222411459077 | validation: 0.9174189388594072]
	TIME [epoch: 6.1 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6871507845244283		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.6871507845244283 | validation: 0.43403551631336773]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5322369209623177		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5322369209623177 | validation: 0.6577578671186981]
	TIME [epoch: 6.1 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5597439814505503		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.5597439814505503 | validation: 0.6548602708899469]
	TIME [epoch: 6.1 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7230921566522128		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.7230921566522128 | validation: 0.7073222982391718]
	TIME [epoch: 6.1 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5519909790756673		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.5519909790756673 | validation: 0.4211364057538703]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.551031308109419		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.551031308109419 | validation: 0.48064406139897137]
	TIME [epoch: 6.1 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5066727151169429		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5066727151169429 | validation: 0.6113289260179846]
	TIME [epoch: 6.1 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5468656915151094		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.5468656915151094 | validation: 0.46465989737484137]
	TIME [epoch: 6.1 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5878964594230384		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5878964594230384 | validation: 0.41112294067286786]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.494974559238599		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.494974559238599 | validation: 0.8195860000931483]
	TIME [epoch: 6.11 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6862396479727447		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.6862396479727447 | validation: 0.4446659920080751]
	TIME [epoch: 6.11 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46935088053123186		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.46935088053123186 | validation: 0.5791347171827065]
	TIME [epoch: 6.1 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5692879659815593		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.5692879659815593 | validation: 0.520918125599324]
	TIME [epoch: 6.1 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5812213713123917		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.5812213713123917 | validation: 0.4530851064184592]
	TIME [epoch: 6.1 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5020908995348622		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5020908995348622 | validation: 0.46468087695363747]
	TIME [epoch: 6.1 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.605610829947347		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.605610829947347 | validation: 0.4713531414982472]
	TIME [epoch: 6.09 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42296996996971936		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.42296996996971936 | validation: 0.5606677443434105]
	TIME [epoch: 6.1 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46712407377052395		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.46712407377052395 | validation: 0.4107295243416563]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5163146873543598		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.5163146873543598 | validation: 0.4573850599934375]
	TIME [epoch: 6.1 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47872599636582464		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.47872599636582464 | validation: 0.3779305976834286]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4882748946803529		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.4882748946803529 | validation: 0.5300514492254897]
	TIME [epoch: 6.1 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5081116263185423		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5081116263185423 | validation: 0.647214804191143]
	TIME [epoch: 6.11 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5200878803892849		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.5200878803892849 | validation: 0.3833832999980509]
	TIME [epoch: 6.11 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450104823574372		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.5450104823574372 | validation: 0.4226583413485104]
	TIME [epoch: 6.1 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42803805544980883		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.42803805544980883 | validation: 0.42994389158029017]
	TIME [epoch: 6.1 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5246804768771025		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.5246804768771025 | validation: 0.48540085264072075]
	TIME [epoch: 6.1 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5148358700513187		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.5148358700513187 | validation: 0.47840664591684245]
	TIME [epoch: 6.1 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3994388109795695		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.3994388109795695 | validation: 0.5463596242010191]
	TIME [epoch: 6.1 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5414215648283288		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.5414215648283288 | validation: 0.4004548162373871]
	TIME [epoch: 6.1 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4026611135408513		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.4026611135408513 | validation: 0.3784971381431966]
	TIME [epoch: 6.11 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39329242653978824		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.39329242653978824 | validation: 0.4059506391874315]
	TIME [epoch: 6.1 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4716036918841378		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.4716036918841378 | validation: 0.3348079960618864]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4000319862029752		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.4000319862029752 | validation: 0.5264606975019037]
	TIME [epoch: 6.1 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5534443874132531		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.5534443874132531 | validation: 0.4926465024567188]
	TIME [epoch: 6.11 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4053722389569683		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.4053722389569683 | validation: 0.4707048523015558]
	TIME [epoch: 6.1 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48491953550181954		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.48491953550181954 | validation: 0.38838791077437407]
	TIME [epoch: 6.11 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4600197414701322		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.4600197414701322 | validation: 0.43116198573451536]
	TIME [epoch: 6.1 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4149772942732179		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.4149772942732179 | validation: 0.32891702620259144]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3610758981188633		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3610758981188633 | validation: 0.3631770665585678]
	TIME [epoch: 6.11 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39775863427836666		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.39775863427836666 | validation: 0.46257606176931826]
	TIME [epoch: 6.09 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4434144357096118		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.4434144357096118 | validation: 0.37781610373419294]
	TIME [epoch: 6.11 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49432888400512603		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.49432888400512603 | validation: 0.39867095190188373]
	TIME [epoch: 6.09 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37985455603301754		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.37985455603301754 | validation: 0.3393486889456976]
	TIME [epoch: 6.09 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3465746070127403		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3465746070127403 | validation: 0.31969605320114086]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434467811238124		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.3434467811238124 | validation: 0.4093937613913645]
	TIME [epoch: 6.11 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3778669700381888		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3778669700381888 | validation: 0.28974773909136786]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38352184049516147		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.38352184049516147 | validation: 0.2974854168277464]
	TIME [epoch: 6.1 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3811516084229437		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.3811516084229437 | validation: 0.5791578019219837]
	TIME [epoch: 6.1 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3817473247618795		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.3817473247618795 | validation: 0.5250231381416451]
	TIME [epoch: 6.1 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3597097594872483		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.3597097594872483 | validation: 0.5472487630553413]
	TIME [epoch: 6.1 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4283033536539691		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.4283033536539691 | validation: 0.44186906064400033]
	TIME [epoch: 6.09 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38919790836915025		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.38919790836915025 | validation: 0.3695077240921767]
	TIME [epoch: 6.1 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35013195777895423		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.35013195777895423 | validation: 0.36446063938850803]
	TIME [epoch: 6.1 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41459554932221676		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.41459554932221676 | validation: 0.3385672095344052]
	TIME [epoch: 6.11 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086275579401029		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.3086275579401029 | validation: 0.3022293373640837]
	TIME [epoch: 6.1 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3946256983298252		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.3946256983298252 | validation: 0.3820371086123374]
	TIME [epoch: 6.1 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36084117716003633		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.36084117716003633 | validation: 0.4029794234023527]
	TIME [epoch: 6.1 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27664638257959867		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.27664638257959867 | validation: 0.26399631841320637]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38902904661819404		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.38902904661819404 | validation: 0.3074844812896721]
	TIME [epoch: 6.11 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3627485191942407		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.3627485191942407 | validation: 0.4014550928259504]
	TIME [epoch: 6.1 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32244372264659277		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.32244372264659277 | validation: 0.38459854112384106]
	TIME [epoch: 6.1 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676373150636549		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2676373150636549 | validation: 0.23904553989674282]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3020332961215164		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.3020332961215164 | validation: 0.28438655963154613]
	TIME [epoch: 6.1 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3706745419008568		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.3706745419008568 | validation: 0.22660842647768675]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26919511166911864		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.26919511166911864 | validation: 0.37290071996513924]
	TIME [epoch: 6.1 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30381788734960596		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.30381788734960596 | validation: 0.35878993506793033]
	TIME [epoch: 6.09 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3591963748468414		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.3591963748468414 | validation: 0.26726841204313745]
	TIME [epoch: 6.1 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2132837607159032		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.2132837607159032 | validation: 0.3204417001392337]
	TIME [epoch: 6.1 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31148450119411775		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.31148450119411775 | validation: 0.25383891473466474]
	TIME [epoch: 6.1 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2718705051398591		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2718705051398591 | validation: 0.332491550157288]
	TIME [epoch: 6.1 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3003089895660869		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.3003089895660869 | validation: 0.3597085052588919]
	TIME [epoch: 6.1 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2839480110920708		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.2839480110920708 | validation: 0.23887150861046164]
	TIME [epoch: 6.1 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844186997540344		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.2844186997540344 | validation: 0.28317370610054937]
	TIME [epoch: 6.11 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22653656528926813		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.22653656528926813 | validation: 0.344246468228561]
	TIME [epoch: 6.1 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2704010885776833		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.2704010885776833 | validation: 0.4487789056322822]
	TIME [epoch: 6.1 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3155575730553077		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.3155575730553077 | validation: 0.2118150888989544]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810998790341815		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.2810998790341815 | validation: 0.2154160300361527]
	TIME [epoch: 6.1 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23898966125640148		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.23898966125640148 | validation: 0.25001884063702895]
	TIME [epoch: 6.1 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25581519451456025		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.25581519451456025 | validation: 0.416508608084941]
	TIME [epoch: 6.1 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3040004094482005		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.3040004094482005 | validation: 0.34252043547150296]
	TIME [epoch: 6.1 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2219100102200477		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.2219100102200477 | validation: 0.18895144440530623]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23054605161773223		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.23054605161773223 | validation: 0.2772019900510935]
	TIME [epoch: 6.1 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577392960523936		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.2577392960523936 | validation: 0.44705746592910656]
	TIME [epoch: 6.1 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2561334308952104		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.2561334308952104 | validation: 0.18194588679518922]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2130627575974469		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.2130627575974469 | validation: 0.4039511006896519]
	TIME [epoch: 6.09 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3054869430088483		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.3054869430088483 | validation: 0.23518075351888862]
	TIME [epoch: 6.09 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20752827529778256		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.20752827529778256 | validation: 0.3129475584294657]
	TIME [epoch: 6.1 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2518138628625465		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.2518138628625465 | validation: 0.3584039229186735]
	TIME [epoch: 6.1 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3147417701050653		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.3147417701050653 | validation: 0.23843646556429485]
	TIME [epoch: 6.09 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24533446056137898		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.24533446056137898 | validation: 0.22396085939596747]
	TIME [epoch: 6.1 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20552998231858555		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.20552998231858555 | validation: 0.2618807370989162]
	TIME [epoch: 6.1 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19687639545474978		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.19687639545474978 | validation: 0.3288379201491789]
	TIME [epoch: 6.1 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2413832902715028		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.2413832902715028 | validation: 0.41585854773857844]
	TIME [epoch: 6.09 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2568183752342209		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.2568183752342209 | validation: 0.2241103922772631]
	TIME [epoch: 6.1 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16769725941148994		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.16769725941148994 | validation: 0.29768633888193213]
	TIME [epoch: 6.1 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2651727680096158		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.2651727680096158 | validation: 0.32750852305418765]
	TIME [epoch: 6.1 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24233197183662702		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.24233197183662702 | validation: 0.21711260548672082]
	TIME [epoch: 6.1 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22148149513365192		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.22148149513365192 | validation: 0.1953153469081796]
	TIME [epoch: 6.1 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17165511407904419		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.17165511407904419 | validation: 0.193597307031753]
	TIME [epoch: 6.1 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25682452314435317		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.25682452314435317 | validation: 0.19652243251213966]
	TIME [epoch: 6.09 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20828082056723837		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.20828082056723837 | validation: 0.16465819650455735]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16394789321816727		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.16394789321816727 | validation: 0.22824918378844988]
	TIME [epoch: 6.1 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27180739514179164		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.27180739514179164 | validation: 0.32961995321407994]
	TIME [epoch: 6.09 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2234206438324061		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.2234206438324061 | validation: 0.21972054446764344]
	TIME [epoch: 6.1 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2267328072171059		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.2267328072171059 | validation: 0.23900870671069085]
	TIME [epoch: 6.09 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20929306262613584		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.20929306262613584 | validation: 0.2121427585511757]
	TIME [epoch: 6.1 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1637702156222671		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1637702156222671 | validation: 0.1823761604586185]
	TIME [epoch: 6.1 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16532592025039716		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.16532592025039716 | validation: 0.18582635673709458]
	TIME [epoch: 6.09 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25030817913282866		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.25030817913282866 | validation: 0.21489967136571525]
	TIME [epoch: 6.09 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1819267533938653		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.1819267533938653 | validation: 0.2569155922275371]
	TIME [epoch: 6.09 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19053394321090666		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.19053394321090666 | validation: 0.2538267503079241]
	TIME [epoch: 6.09 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21129641395162044		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.21129641395162044 | validation: 0.17039191251142952]
	TIME [epoch: 417 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1792420964983522		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.1792420964983522 | validation: 0.3124743211401998]
	TIME [epoch: 12.1 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23211300398764056		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.23211300398764056 | validation: 0.37528209796190964]
	TIME [epoch: 12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22554450840569537		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.22554450840569537 | validation: 0.19110833938792418]
	TIME [epoch: 12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22051997610745774		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.22051997610745774 | validation: 0.21693395252632608]
	TIME [epoch: 12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17751742835716247		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.17751742835716247 | validation: 0.16148041950945846]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1783581919034777		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.1783581919034777 | validation: 0.1743321302368708]
	TIME [epoch: 12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15792476063733907		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.15792476063733907 | validation: 0.23708132592325643]
	TIME [epoch: 12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20773524033311644		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.20773524033311644 | validation: 0.24395578409879726]
	TIME [epoch: 12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2127204303378394		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.2127204303378394 | validation: 0.36513805820280587]
	TIME [epoch: 12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1939536753953983		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.1939536753953983 | validation: 0.23513357123387385]
	TIME [epoch: 12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17495126612890644		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.17495126612890644 | validation: 0.21780166871948675]
	TIME [epoch: 12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14435259435113276		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.14435259435113276 | validation: 0.1911328027552707]
	TIME [epoch: 12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19569475366766043		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.19569475366766043 | validation: 0.30196248598978603]
	TIME [epoch: 12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19430474813734464		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.19430474813734464 | validation: 0.1376411033863881]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14671444306245604		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.14671444306245604 | validation: 0.1624230461520641]
	TIME [epoch: 12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22183217983301629		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.22183217983301629 | validation: 0.16785816537969633]
	TIME [epoch: 12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18529545700952166		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.18529545700952166 | validation: 0.17445597806386193]
	TIME [epoch: 12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14283075563425893		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.14283075563425893 | validation: 0.13519526667311557]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17769032188502074		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.17769032188502074 | validation: 0.17309975732916427]
	TIME [epoch: 12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14907513773411984		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.14907513773411984 | validation: 0.19613142059798522]
	TIME [epoch: 12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18303764713539414		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.18303764713539414 | validation: 0.22898623276437918]
	TIME [epoch: 12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13289146586395212		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.13289146586395212 | validation: 0.13650643218500152]
	TIME [epoch: 12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13491476501237812		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.13491476501237812 | validation: 0.19589169708709042]
	TIME [epoch: 12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19967485244760017		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.19967485244760017 | validation: 0.16812438480202915]
	TIME [epoch: 12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12241317787127484		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.12241317787127484 | validation: 0.1367239732710504]
	TIME [epoch: 12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20371587386466633		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.20371587386466633 | validation: 0.16496942998296937]
	TIME [epoch: 12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12832797866955067		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.12832797866955067 | validation: 0.19953588364380387]
	TIME [epoch: 12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14363599858879872		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.14363599858879872 | validation: 0.16098759849625455]
	TIME [epoch: 12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14961231174607204		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.14961231174607204 | validation: 0.22337234056265515]
	TIME [epoch: 12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26675400904303637		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.26675400904303637 | validation: 0.16822163390983022]
	TIME [epoch: 12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12911487865895716		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.12911487865895716 | validation: 0.16422640065866967]
	TIME [epoch: 12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13184057373947267		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.13184057373947267 | validation: 0.1755739467825069]
	TIME [epoch: 12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15412289784559574		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.15412289784559574 | validation: 0.23011100589656863]
	TIME [epoch: 12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1506926813076638		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.1506926813076638 | validation: 0.19623241482004344]
	TIME [epoch: 12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1322642636173536		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.1322642636173536 | validation: 0.1294179393091938]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15376097405664446		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.15376097405664446 | validation: 0.1605339641322403]
	TIME [epoch: 12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12327377798277568		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.12327377798277568 | validation: 0.1459493893000831]
	TIME [epoch: 12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1268032804733061		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1268032804733061 | validation: 0.26903596563365617]
	TIME [epoch: 12 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1956968764701575		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.1956968764701575 | validation: 0.12116083287444196]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14768298622292653		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.14768298622292653 | validation: 0.17600981507226243]
	TIME [epoch: 12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13290118244349491		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.13290118244349491 | validation: 0.1790884679055148]
	TIME [epoch: 12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14611337551972123		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.14611337551972123 | validation: 0.12653239703297778]
	TIME [epoch: 12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17601418305331562		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.17601418305331562 | validation: 0.15095943962068564]
	TIME [epoch: 12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10803492617391415		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.10803492617391415 | validation: 0.1354747098576236]
	TIME [epoch: 12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13964095745412802		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.13964095745412802 | validation: 0.203599279262989]
	TIME [epoch: 12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12745986922994973		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.12745986922994973 | validation: 0.1714742044348268]
	TIME [epoch: 12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15794693730947926		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.15794693730947926 | validation: 0.19753918722728328]
	TIME [epoch: 12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12913901426150357		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.12913901426150357 | validation: 0.15838108432856285]
	TIME [epoch: 12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17103452326666452		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.17103452326666452 | validation: 0.1640660726169486]
	TIME [epoch: 12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13335873718628297		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.13335873718628297 | validation: 0.1905412544642569]
	TIME [epoch: 12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11994402679927996		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.11994402679927996 | validation: 0.17519648691197437]
	TIME [epoch: 12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1606220499691406		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.1606220499691406 | validation: 0.17483580355538808]
	TIME [epoch: 12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12191891552361504		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.12191891552361504 | validation: 0.15888842054527907]
	TIME [epoch: 12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12898075508142282		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.12898075508142282 | validation: 0.2697267564343674]
	TIME [epoch: 12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15241179658787016		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.15241179658787016 | validation: 0.13989969970950428]
	TIME [epoch: 12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13469431473619609		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.13469431473619609 | validation: 0.12629512745789068]
	TIME [epoch: 12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1198604582304637		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.1198604582304637 | validation: 0.13590361643802604]
	TIME [epoch: 12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10684185223691103		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.10684185223691103 | validation: 0.3141928007614252]
	TIME [epoch: 12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1578523360140569		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1578523360140569 | validation: 0.16048684472906025]
	TIME [epoch: 12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13756311150495631		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.13756311150495631 | validation: 0.1573626476915072]
	TIME [epoch: 12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1274743800948874		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.1274743800948874 | validation: 0.11360697498531225]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12155538700799037		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.12155538700799037 | validation: 0.170704690666185]
	TIME [epoch: 12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11006264710851388		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.11006264710851388 | validation: 0.15917595752517844]
	TIME [epoch: 12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1267400126227169		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.1267400126227169 | validation: 0.1527554905974896]
	TIME [epoch: 12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13267058460302056		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.13267058460302056 | validation: 0.1718763125471018]
	TIME [epoch: 12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1249718444459868		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.1249718444459868 | validation: 0.15527861490582678]
	TIME [epoch: 12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10237466352755892		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.10237466352755892 | validation: 0.1069831026641935]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11617279670429861		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.11617279670429861 | validation: 0.1696166719611985]
	TIME [epoch: 12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13086187934808996		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.13086187934808996 | validation: 0.14589509510426257]
	TIME [epoch: 12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09711245403432009		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.09711245403432009 | validation: 0.13389684410763159]
	TIME [epoch: 12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1151570269659895		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.1151570269659895 | validation: 0.15561976687801912]
	TIME [epoch: 12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11607072963746323		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.11607072963746323 | validation: 0.14889771331316579]
	TIME [epoch: 12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14210439607246156		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.14210439607246156 | validation: 0.12786127865531952]
	TIME [epoch: 12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0851635515877056		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.0851635515877056 | validation: 0.14408155172294662]
	TIME [epoch: 12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12983365540179476		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.12983365540179476 | validation: 0.13382100373456535]
	TIME [epoch: 12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09662185636815193		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.09662185636815193 | validation: 0.15102071687811128]
	TIME [epoch: 12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10993383626533447		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.10993383626533447 | validation: 0.16142624618783236]
	TIME [epoch: 12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14319794867177554		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.14319794867177554 | validation: 0.17897466856121091]
	TIME [epoch: 12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13562491832972443		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.13562491832972443 | validation: 0.11378965645519254]
	TIME [epoch: 12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08951687248858778		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.08951687248858778 | validation: 0.10744518120448643]
	TIME [epoch: 12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10244191568796984		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.10244191568796984 | validation: 0.1320409638998179]
	TIME [epoch: 12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1072163453838597		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.1072163453838597 | validation: 0.11150680218134336]
	TIME [epoch: 12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1152337805036539		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1152337805036539 | validation: 0.12206529667743966]
	TIME [epoch: 12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09739318058044655		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.09739318058044655 | validation: 0.12186354242593514]
	TIME [epoch: 12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10286977162711855		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.10286977162711855 | validation: 0.10580548223824854]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.142493489144269		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.142493489144269 | validation: 0.09699294306244778]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09043521553006705		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.09043521553006705 | validation: 0.14295513768556017]
	TIME [epoch: 12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11357110738661154		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.11357110738661154 | validation: 0.14349095658459138]
	TIME [epoch: 12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09136997476507376		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.09136997476507376 | validation: 0.1462202448288814]
	TIME [epoch: 12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1273546950178941		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.1273546950178941 | validation: 0.09755126695952637]
	TIME [epoch: 12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09549658345431106		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.09549658345431106 | validation: 0.11242709382784924]
	TIME [epoch: 12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09544681247566957		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.09544681247566957 | validation: 0.13163231550351498]
	TIME [epoch: 12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11318798949011569		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.11318798949011569 | validation: 0.11725212062811805]
	TIME [epoch: 12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10598635394478884		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.10598635394478884 | validation: 0.08634723562470223]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08787154834761812		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.08787154834761812 | validation: 0.12401167325807311]
	TIME [epoch: 12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12421571189047856		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.12421571189047856 | validation: 0.13024681030369756]
	TIME [epoch: 12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0891797058012386		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.0891797058012386 | validation: 0.09001339264493576]
	TIME [epoch: 12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09653666483690672		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.09653666483690672 | validation: 0.1217661927023273]
	TIME [epoch: 12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09412771645255577		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.09412771645255577 | validation: 0.14151873585209565]
	TIME [epoch: 12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14325644884207012		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.14325644884207012 | validation: 0.10418577181642061]
	TIME [epoch: 12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09496965070432872		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.09496965070432872 | validation: 0.11671131598531896]
	TIME [epoch: 12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10073330114508053		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.10073330114508053 | validation: 0.11524535703194906]
	TIME [epoch: 12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08141253469142598		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.08141253469142598 | validation: 0.10068508209478086]
	TIME [epoch: 12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10181639842646281		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.10181639842646281 | validation: 0.16296601578873532]
	TIME [epoch: 12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11101247286957175		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.11101247286957175 | validation: 0.1825653368100696]
	TIME [epoch: 12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09796762888582616		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.09796762888582616 | validation: 0.1120143321570511]
	TIME [epoch: 12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08886473295495696		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.08886473295495696 | validation: 0.10020598328493657]
	TIME [epoch: 12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09169261509042867		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.09169261509042867 | validation: 0.18154369284290306]
	TIME [epoch: 12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11013798919832501		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.11013798919832501 | validation: 0.11899853856569241]
	TIME [epoch: 12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08869887596859052		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.08869887596859052 | validation: 0.10588778310360586]
	TIME [epoch: 12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0974782989266809		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.0974782989266809 | validation: 0.10323619444125953]
	TIME [epoch: 12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09398073089690306		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.09398073089690306 | validation: 0.1186169061308051]
	TIME [epoch: 12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10728010565427698		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.10728010565427698 | validation: 0.09732631352690181]
	TIME [epoch: 12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08674632766941393		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.08674632766941393 | validation: 0.10094055846037545]
	TIME [epoch: 12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08581030588942282		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.08581030588942282 | validation: 0.13980958469978377]
	TIME [epoch: 12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12232151339413777		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.12232151339413777 | validation: 0.12950809064432312]
	TIME [epoch: 12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10056262773237876		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.10056262773237876 | validation: 0.10389157649829371]
	TIME [epoch: 12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07957463403871631		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.07957463403871631 | validation: 0.09238596491935314]
	TIME [epoch: 12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10122834658299065		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.10122834658299065 | validation: 0.14767489380514676]
	TIME [epoch: 12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11669875336666921		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.11669875336666921 | validation: 0.0913484528327802]
	TIME [epoch: 12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07207814601303297		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07207814601303297 | validation: 0.081154573774036]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08712306503828483		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08712306503828483 | validation: 0.1074064236515013]
	TIME [epoch: 12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09244078676660897		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.09244078676660897 | validation: 0.11478894131995118]
	TIME [epoch: 12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10151260159416764		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.10151260159416764 | validation: 0.12601288417454132]
	TIME [epoch: 12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07957077470112436		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.07957077470112436 | validation: 0.1003371127878372]
	TIME [epoch: 12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09947731561915081		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.09947731561915081 | validation: 0.11390609845778227]
	TIME [epoch: 12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.097882769626993		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.097882769626993 | validation: 0.09396308912488854]
	TIME [epoch: 12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07042732809308166		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.07042732809308166 | validation: 0.08704408317849693]
	TIME [epoch: 12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09135768816330997		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.09135768816330997 | validation: 0.37094392880772475]
	TIME [epoch: 12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17124878806474178		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.17124878806474178 | validation: 0.12941988098493346]
	TIME [epoch: 12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08555127962654513		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.08555127962654513 | validation: 0.11330072633480695]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07550227123226318		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.07550227123226318 | validation: 0.08959345613972522]
	TIME [epoch: 12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07660559955884923		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.07660559955884923 | validation: 0.10062213529377753]
	TIME [epoch: 12.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0809632082724239		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.0809632082724239 | validation: 0.1087793716716482]
	TIME [epoch: 12.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08802928949318148		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.08802928949318148 | validation: 0.09151450436162631]
	TIME [epoch: 12.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07947458712098476		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.07947458712098476 | validation: 0.1119541244110488]
	TIME [epoch: 12.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07443405501320333		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.07443405501320333 | validation: 0.10087828047693087]
	TIME [epoch: 12.1 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10479972886630085		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.10479972886630085 | validation: 0.09889203688175391]
	TIME [epoch: 12.1 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07790014856839308		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.07790014856839308 | validation: 0.09522559564841263]
	TIME [epoch: 12.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0692007651035175		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.0692007651035175 | validation: 0.11256420913528745]
	TIME [epoch: 12.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0823267763105458		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.0823267763105458 | validation: 0.09932666251610746]
	TIME [epoch: 12.1 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09062572755863592		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.09062572755863592 | validation: 0.087094000231078]
	TIME [epoch: 12.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06774835114069329		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.06774835114069329 | validation: 0.0976295494469098]
	TIME [epoch: 12.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07551486316432231		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.07551486316432231 | validation: 0.1310833656674248]
	TIME [epoch: 12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09879918406193494		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.09879918406193494 | validation: 0.0800012672327339]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06644097056256948		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.06644097056256948 | validation: 0.07939326649100831]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07903354355664458		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.07903354355664458 | validation: 0.08489607517993941]
	TIME [epoch: 12.1 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07145352023205236		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.07145352023205236 | validation: 0.14348016096706956]
	TIME [epoch: 12.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10816877181804185		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.10816877181804185 | validation: 0.07138439220366788]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07252311629161122		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.07252311629161122 | validation: 0.09360810272709699]
	TIME [epoch: 12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0882063604107157		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.0882063604107157 | validation: 0.07821023198883448]
	TIME [epoch: 12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07570789533879493		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.07570789533879493 | validation: 0.09894286508709753]
	TIME [epoch: 12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08276465040847489		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.08276465040847489 | validation: 0.09550761387100162]
	TIME [epoch: 12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07131694074866154		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.07131694074866154 | validation: 0.07666692943272851]
	TIME [epoch: 12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07750979901759564		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.07750979901759564 | validation: 0.11175857142317183]
	TIME [epoch: 12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06823698861160113		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.06823698861160113 | validation: 0.10187942200178665]
	TIME [epoch: 12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08655724127636436		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.08655724127636436 | validation: 0.3562884839470698]
	TIME [epoch: 12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2081709153910161		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.2081709153910161 | validation: 0.11638954592451267]
	TIME [epoch: 12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08181389616612884		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.08181389616612884 | validation: 0.08393707429552688]
	TIME [epoch: 12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06709786129427864		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.06709786129427864 | validation: 0.10738245907464569]
	TIME [epoch: 12 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08441866581015767		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.08441866581015767 | validation: 0.07208275317249613]
	TIME [epoch: 12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06660446654189604		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.06660446654189604 | validation: 0.09432944141455976]
	TIME [epoch: 12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06674349651423134		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.06674349651423134 | validation: 0.08370575884094328]
	TIME [epoch: 12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08647753075423978		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.08647753075423978 | validation: 0.11780405115002196]
	TIME [epoch: 12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07896504252504576		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.07896504252504576 | validation: 0.1273773753738712]
	TIME [epoch: 12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07543904406051306		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.07543904406051306 | validation: 0.10907512009269354]
	TIME [epoch: 12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07497659581865303		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.07497659581865303 | validation: 0.06973559419688545]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06445992014282642		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.06445992014282642 | validation: 0.08807589544862131]
	TIME [epoch: 12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07546997307361437		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.07546997307361437 | validation: 0.09987285502677812]
	TIME [epoch: 12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07737113628551612		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.07737113628551612 | validation: 0.08330991357326392]
	TIME [epoch: 12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07534728411248293		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.07534728411248293 | validation: 0.11136129227640121]
	TIME [epoch: 12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0915183288416775		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.0915183288416775 | validation: 0.08477910683105228]
	TIME [epoch: 12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06564166982610754		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.06564166982610754 | validation: 0.07994506783274906]
	TIME [epoch: 12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07745917715613877		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.07745917715613877 | validation: 0.06799815998277697]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06931214261734979		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.06931214261734979 | validation: 0.08913731890542687]
	TIME [epoch: 12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07512639447879377		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.07512639447879377 | validation: 0.08317833980982492]
	TIME [epoch: 12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08508281375642836		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.08508281375642836 | validation: 0.07214068056375753]
	TIME [epoch: 12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06943153572682235		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.06943153572682235 | validation: 0.06836036917443501]
	TIME [epoch: 12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06399917063551565		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.06399917063551565 | validation: 0.09998468277133513]
	TIME [epoch: 12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07795454609487898		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.07795454609487898 | validation: 0.09588284097990166]
	TIME [epoch: 12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060978981039073316		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.060978981039073316 | validation: 0.06722876603329365]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0665081562416576		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.0665081562416576 | validation: 0.07974376643930128]
	TIME [epoch: 12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07666253952254795		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.07666253952254795 | validation: 0.08614759503875019]
	TIME [epoch: 12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062687924041628		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.062687924041628 | validation: 0.11189366161473394]
	TIME [epoch: 12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06332711345383059		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.06332711345383059 | validation: 0.08203699562491128]
	TIME [epoch: 12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0805723114202419		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.0805723114202419 | validation: 0.07314948748302508]
	TIME [epoch: 12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05600851108329577		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.05600851108329577 | validation: 0.08104987658433987]
	TIME [epoch: 12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06660987460025077		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.06660987460025077 | validation: 0.07446804694342136]
	TIME [epoch: 12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06417788409623036		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.06417788409623036 | validation: 0.13801404675191833]
	TIME [epoch: 12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07114623637785993		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.07114623637785993 | validation: 0.06747155854712132]
	TIME [epoch: 12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05936624604287721		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.05936624604287721 | validation: 0.07138077495254146]
	TIME [epoch: 12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08253609165132393		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.08253609165132393 | validation: 0.07309385298139824]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05982030207012576		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.05982030207012576 | validation: 0.07125251459841872]
	TIME [epoch: 12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06362192963216096		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.06362192963216096 | validation: 0.08954044556471488]
	TIME [epoch: 12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.068284563798018		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.068284563798018 | validation: 0.0924010057781158]
	TIME [epoch: 12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07366947864332991		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.07366947864332991 | validation: 0.06840613106518302]
	TIME [epoch: 12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0549085087999161		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.0549085087999161 | validation: 0.07045124078337883]
	TIME [epoch: 12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05753174806086165		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.05753174806086165 | validation: 0.09526604425910598]
	TIME [epoch: 12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07740449222731559		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.07740449222731559 | validation: 0.06688642061739483]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06202057603849455		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.06202057603849455 | validation: 0.06493588741939475]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05612479720836507		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.05612479720836507 | validation: 0.07210086398608949]
	TIME [epoch: 12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07815322988776592		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.07815322988776592 | validation: 0.0671074805419124]
	TIME [epoch: 12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0688314342580644		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.0688314342580644 | validation: 0.06399787432413698]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0561040656184955		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.0561040656184955 | validation: 0.11296431230042778]
	TIME [epoch: 12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0696790787911364		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.0696790787911364 | validation: 0.06516435937961802]
	TIME [epoch: 12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0625866391663272		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.0625866391663272 | validation: 0.06843642744591459]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06151981568170199		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.06151981568170199 | validation: 0.08500979598508115]
	TIME [epoch: 12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07654977380741194		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.07654977380741194 | validation: 0.08182555908323869]
	TIME [epoch: 12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05648637341059099		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.05648637341059099 | validation: 0.06169223723978089]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06764323420510499		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.06764323420510499 | validation: 0.08869578666317944]
	TIME [epoch: 12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664263192714029		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.0664263192714029 | validation: 0.06342843511683159]
	TIME [epoch: 12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04745228616422328		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04745228616422328 | validation: 0.06331516126609485]
	TIME [epoch: 12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0634861849889379		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.0634861849889379 | validation: 0.09389160189778922]
	TIME [epoch: 12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06707893757981656		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.06707893757981656 | validation: 0.05344155560527314]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059544892959861845		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.059544892959861845 | validation: 0.07576978286583184]
	TIME [epoch: 12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839384763059005		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.05839384763059005 | validation: 0.06799614122857711]
	TIME [epoch: 12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055603423107505816		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.055603423107505816 | validation: 0.0953641595359459]
	TIME [epoch: 12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07512074781749963		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.07512074781749963 | validation: 0.06558758616779928]
	TIME [epoch: 12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05366706463835582		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.05366706463835582 | validation: 0.06260880178309161]
	TIME [epoch: 12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06269008381327759		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.06269008381327759 | validation: 0.07459706548778033]
	TIME [epoch: 12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06361913173193044		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.06361913173193044 | validation: 0.06229751462126436]
	TIME [epoch: 12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055474694959992236		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.055474694959992236 | validation: 0.0692361553923769]
	TIME [epoch: 12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06225123701417401		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.06225123701417401 | validation: 0.05485371573502737]
	TIME [epoch: 12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05518607692689817		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.05518607692689817 | validation: 0.08020417002135086]
	TIME [epoch: 12 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06910422142948283		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.06910422142948283 | validation: 0.08323174800603339]
	TIME [epoch: 12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052445510157152514		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.052445510157152514 | validation: 0.05431021597873976]
	TIME [epoch: 12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052228862915946334		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.052228862915946334 | validation: 0.05541733124755993]
	TIME [epoch: 12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06622609915123409		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.06622609915123409 | validation: 0.06512593839590392]
	TIME [epoch: 12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048402479172858094		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.048402479172858094 | validation: 0.05227110760940108]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839686222038616		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.05839686222038616 | validation: 0.09116000377729015]
	TIME [epoch: 12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06128050127514144		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.06128050127514144 | validation: 0.08077152578178451]
	TIME [epoch: 12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0659992245501124		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.0659992245501124 | validation: 0.07713141767163603]
	TIME [epoch: 12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05740323329955923		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.05740323329955923 | validation: 0.05936138164332205]
	TIME [epoch: 12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05496048834146066		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.05496048834146066 | validation: 0.06017325224362554]
	TIME [epoch: 12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05892865135203685		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.05892865135203685 | validation: 0.07276845528091798]
	TIME [epoch: 12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0641160990574426		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.0641160990574426 | validation: 0.07486133569575411]
	TIME [epoch: 12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05586293960965139		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.05586293960965139 | validation: 0.06612911839921243]
	TIME [epoch: 12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05495420360538474		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.05495420360538474 | validation: 0.06189896748238906]
	TIME [epoch: 12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05572624864015714		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.05572624864015714 | validation: 0.06846043780607335]
	TIME [epoch: 12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06875594880027201		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.06875594880027201 | validation: 0.06784215638710242]
	TIME [epoch: 12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05692317561218738		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.05692317561218738 | validation: 0.07207785907512192]
	TIME [epoch: 12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05715315180159964		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.05715315180159964 | validation: 0.06461069596423274]
	TIME [epoch: 12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051779514473537805		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.051779514473537805 | validation: 0.04713192660706578]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055427743626134195		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.055427743626134195 | validation: 0.06508338339347283]
	TIME [epoch: 12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06064929440781766		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.06064929440781766 | validation: 0.06382321207341832]
	TIME [epoch: 12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05046169278424994		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.05046169278424994 | validation: 0.06365553762037318]
	TIME [epoch: 12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05428746179254754		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.05428746179254754 | validation: 0.059254644554120356]
	TIME [epoch: 12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058762734393192906		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.058762734393192906 | validation: 0.060768372807013304]
	TIME [epoch: 12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05893793661393579		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.05893793661393579 | validation: 0.05883825659036239]
	TIME [epoch: 12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05038118673860173		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.05038118673860173 | validation: 0.0646556688677099]
	TIME [epoch: 12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05828637601608898		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.05828637601608898 | validation: 0.04983795163890613]
	TIME [epoch: 12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04663534278899733		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.04663534278899733 | validation: 0.06064475652466554]
	TIME [epoch: 12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0592163378679399		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.0592163378679399 | validation: 0.08228377216917171]
	TIME [epoch: 12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06459420380419548		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.06459420380419548 | validation: 0.05474897993755118]
	TIME [epoch: 12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0451188606977688		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.0451188606977688 | validation: 0.0622888602665919]
	TIME [epoch: 12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05460237790713146		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.05460237790713146 | validation: 0.055515352932013165]
	TIME [epoch: 12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05466859220492726		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.05466859220492726 | validation: 0.07539521273572072]
	TIME [epoch: 12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05560831737851783		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.05560831737851783 | validation: 0.09762307590928719]
	TIME [epoch: 12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058055870179316196		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.058055870179316196 | validation: 0.06373638992206881]
	TIME [epoch: 12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048377375467413296		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.048377375467413296 | validation: 0.05943534940430327]
	TIME [epoch: 12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0516731001457163		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.0516731001457163 | validation: 0.060084760537112085]
	TIME [epoch: 12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05578993989633173		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.05578993989633173 | validation: 0.08738188114861789]
	TIME [epoch: 12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05893946207169584		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.05893946207169584 | validation: 0.06994462595231167]
	TIME [epoch: 12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046846589760986594		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.046846589760986594 | validation: 0.05835391367676132]
	TIME [epoch: 12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055628195926956725		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.055628195926956725 | validation: 0.049612887863360634]
	TIME [epoch: 12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04360604814568206		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.04360604814568206 | validation: 0.059374459528134915]
	TIME [epoch: 12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06686867413211063		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.06686867413211063 | validation: 0.0590638057688703]
	TIME [epoch: 12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046462070180602194		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.046462070180602194 | validation: 0.05540270483094317]
	TIME [epoch: 12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04769084621066366		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.04769084621066366 | validation: 0.06880567799954576]
	TIME [epoch: 12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061569912851414145		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.061569912851414145 | validation: 0.06981754253814056]
	TIME [epoch: 12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05495166143411734		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.05495166143411734 | validation: 0.05096659160933998]
	TIME [epoch: 12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04611480554928907		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.04611480554928907 | validation: 0.05735135214295166]
	TIME [epoch: 12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05397923212602011		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.05397923212602011 | validation: 0.05815831220885205]
	TIME [epoch: 12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04646467090652835		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.04646467090652835 | validation: 0.06339127720760308]
	TIME [epoch: 12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060984298979336264		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.060984298979336264 | validation: 0.07303271613062345]
	TIME [epoch: 12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05119127373663061		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.05119127373663061 | validation: 0.05384361166900153]
	TIME [epoch: 12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056813006790973855		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.056813006790973855 | validation: 0.05250039374605722]
	TIME [epoch: 12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053607381406239066		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.053607381406239066 | validation: 0.05841301144495432]
	TIME [epoch: 12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05266303695376631		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.05266303695376631 | validation: 0.08064793303608574]
	TIME [epoch: 12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062425846306672125		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.062425846306672125 | validation: 0.05294439267177094]
	TIME [epoch: 12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045136574532722185		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.045136574532722185 | validation: 0.05231505150167097]
	TIME [epoch: 12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05126948687333324		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.05126948687333324 | validation: 0.04924087857580774]
	TIME [epoch: 12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0993846135511233		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0993846135511233 | validation: 0.06100121846007361]
	TIME [epoch: 12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05571965580513103		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.05571965580513103 | validation: 0.06052013403603626]
	TIME [epoch: 12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043645314308145666		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.043645314308145666 | validation: 0.0496027485631029]
	TIME [epoch: 12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048758143314441074		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.048758143314441074 | validation: 0.05770959065169139]
	TIME [epoch: 12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0477058831567467		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.0477058831567467 | validation: 0.06558046736165155]
	TIME [epoch: 12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05485931693228954		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.05485931693228954 | validation: 0.04824784557129866]
	TIME [epoch: 12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04341696626829658		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.04341696626829658 | validation: 0.050300907611175324]
	TIME [epoch: 12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053987401098747625		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.053987401098747625 | validation: 0.05410727426904207]
	TIME [epoch: 12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05106829073927886		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.05106829073927886 | validation: 0.0675856129534193]
	TIME [epoch: 12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053808293458523046		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.053808293458523046 | validation: 0.04622115676170769]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04800295417184972		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.04800295417184972 | validation: 0.05293533841386697]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04830140089114492		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.04830140089114492 | validation: 0.05573514071598716]
	TIME [epoch: 12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04740354461266559		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.04740354461266559 | validation: 0.054627486299676606]
	TIME [epoch: 12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04921344386178794		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.04921344386178794 | validation: 0.05375575250156582]
	TIME [epoch: 12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05124076404791221		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.05124076404791221 | validation: 0.05860231773282489]
	TIME [epoch: 12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0494906866703053		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0494906866703053 | validation: 0.04828105192387194]
	TIME [epoch: 12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05606360705824791		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.05606360705824791 | validation: 0.05639753296039022]
	TIME [epoch: 12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046700080511689446		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.046700080511689446 | validation: 0.08758915315208284]
	TIME [epoch: 420 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048859681051050034		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.048859681051050034 | validation: 0.06434305072449566]
	TIME [epoch: 25.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043808193115830674		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.043808193115830674 | validation: 0.06604919040100106]
	TIME [epoch: 25.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05426099666917797		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.05426099666917797 | validation: 0.04947030580804082]
	TIME [epoch: 25.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042804559148135427		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.042804559148135427 | validation: 0.055502847500358415]
	TIME [epoch: 25.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05504338996847829		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.05504338996847829 | validation: 0.06868676645341054]
	TIME [epoch: 25.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05244547645798625		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.05244547645798625 | validation: 0.04905486795104465]
	TIME [epoch: 25.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04258900232472086		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.04258900232472086 | validation: 0.050032609058453734]
	TIME [epoch: 25.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04733597844675835		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.04733597844675835 | validation: 0.05221475224161502]
	TIME [epoch: 25.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04950872125247146		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.04950872125247146 | validation: 0.050990114645387115]
	TIME [epoch: 25.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04672297582910932		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.04672297582910932 | validation: 0.0438725765833437]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05340396548891179		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.05340396548891179 | validation: 0.05111454274268964]
	TIME [epoch: 25.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04730109289696265		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.04730109289696265 | validation: 0.05691189299439167]
	TIME [epoch: 25.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05353470217164544		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.05353470217164544 | validation: 0.04753374591492607]
	TIME [epoch: 25.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047911759617537145		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.047911759617537145 | validation: 0.0623382243221854]
	TIME [epoch: 25.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04998972558860734		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.04998972558860734 | validation: 0.06251232254118412]
	TIME [epoch: 25.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047596308041202066		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.047596308041202066 | validation: 0.05448936662867722]
	TIME [epoch: 25.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047144995775718845		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.047144995775718845 | validation: 0.04181275171101116]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04284673983288936		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.04284673983288936 | validation: 0.07224738921130366]
	TIME [epoch: 25.8 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0538039460929889		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.0538039460929889 | validation: 0.0512279975653749]
	TIME [epoch: 25.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04367906981844275		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.04367906981844275 | validation: 0.05792481773710273]
	TIME [epoch: 25.8 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04721117958691502		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.04721117958691502 | validation: 0.0698350942675133]
	TIME [epoch: 25.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053121205622967516		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.053121205622967516 | validation: 0.05607028757069165]
	TIME [epoch: 25.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04471285760717858		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.04471285760717858 | validation: 0.05624455391369691]
	TIME [epoch: 25.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05027821380254027		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.05027821380254027 | validation: 0.05435063224537025]
	TIME [epoch: 25.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04531816613917057		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.04531816613917057 | validation: 0.05336819420057498]
	TIME [epoch: 25.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049368886915354106		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.049368886915354106 | validation: 0.05377842501400075]
	TIME [epoch: 25.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04133819715257378		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.04133819715257378 | validation: 0.052020968534136]
	TIME [epoch: 25.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0544291000576778		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.0544291000576778 | validation: 0.05242977166342105]
	TIME [epoch: 25.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04688875463540256		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.04688875463540256 | validation: 0.04865658962647906]
	TIME [epoch: 25.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04457725650058011		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.04457725650058011 | validation: 0.047743379644858676]
	TIME [epoch: 25.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05288899318524412		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.05288899318524412 | validation: 0.054968429102453364]
	TIME [epoch: 25.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043747842317719894		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.043747842317719894 | validation: 0.04452664596209305]
	TIME [epoch: 25.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043520005822789316		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.043520005822789316 | validation: 0.04439230002709239]
	TIME [epoch: 25.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04954066421017701		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.04954066421017701 | validation: 0.054333558254164305]
	TIME [epoch: 25.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04283567448751051		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.04283567448751051 | validation: 0.05033619519661804]
	TIME [epoch: 25.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04941973046472837		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.04941973046472837 | validation: 0.04700390323426187]
	TIME [epoch: 25.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046023953282149914		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.046023953282149914 | validation: 0.058349447417522654]
	TIME [epoch: 25.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04064548571487934		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.04064548571487934 | validation: 0.0531420454521915]
	TIME [epoch: 25.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049381046797421654		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.049381046797421654 | validation: 0.045272754458545394]
	TIME [epoch: 25.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044403783922753716		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.044403783922753716 | validation: 0.044215530543063306]
	TIME [epoch: 25.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04281389590150214		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.04281389590150214 | validation: 0.05043333677578187]
	TIME [epoch: 25.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05094451215983235		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.05094451215983235 | validation: 0.045675124900873665]
	TIME [epoch: 25.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04179578599759261		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.04179578599759261 | validation: 0.06405883566459948]
	TIME [epoch: 25.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04699021465373344		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.04699021465373344 | validation: 0.04290048724417857]
	TIME [epoch: 25.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04440621801331776		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.04440621801331776 | validation: 0.06372040334458062]
	TIME [epoch: 25.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0517001248567994		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.0517001248567994 | validation: 0.044841654014986014]
	TIME [epoch: 25.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04110119441342643		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.04110119441342643 | validation: 0.05061047503832647]
	TIME [epoch: 25.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04333058433427281		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.04333058433427281 | validation: 0.05902655407638342]
	TIME [epoch: 25.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0486273700593732		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.0486273700593732 | validation: 0.05804975609252555]
	TIME [epoch: 25.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04328881739200433		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.04328881739200433 | validation: 0.05232020034959294]
	TIME [epoch: 25.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04622525661261901		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.04622525661261901 | validation: 0.040301909350289704]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04381477172299298		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.04381477172299298 | validation: 0.04917641413972974]
	TIME [epoch: 25.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042880742313556014		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.042880742313556014 | validation: 0.043220899670295465]
	TIME [epoch: 25.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04426048261305778		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.04426048261305778 | validation: 0.051816810111201206]
	TIME [epoch: 25.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0459182587179241		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.0459182587179241 | validation: 0.04812315295236903]
	TIME [epoch: 25.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04224388586599357		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.04224388586599357 | validation: 0.054255420143256856]
	TIME [epoch: 25.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04259424025755339		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.04259424025755339 | validation: 0.04372283476155755]
	TIME [epoch: 25.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03988237039750514		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.03988237039750514 | validation: 0.06303218576678268]
	TIME [epoch: 25.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04245712949441911		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.04245712949441911 | validation: 0.04216193761676369]
	TIME [epoch: 25.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05712281298964039		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.05712281298964039 | validation: 0.05075876713163924]
	TIME [epoch: 25.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04404972956176364		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.04404972956176364 | validation: 0.049807302432976756]
	TIME [epoch: 25.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042689883932507736		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.042689883932507736 | validation: 0.040398789206009963]
	TIME [epoch: 25.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043272616272338374		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.043272616272338374 | validation: 0.06693386394874117]
	TIME [epoch: 25.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04198989243075262		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.04198989243075262 | validation: 0.043424172774892925]
	TIME [epoch: 25.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04221771473788538		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.04221771473788538 | validation: 0.05840538558439502]
	TIME [epoch: 25.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04229931349834688		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.04229931349834688 | validation: 0.06106521995403891]
	TIME [epoch: 25.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044741764731026215		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.044741764731026215 | validation: 0.05398665272371004]
	TIME [epoch: 25.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043410624033067516		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.043410624033067516 | validation: 0.051247556798644955]
	TIME [epoch: 25.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04939641089985791		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.04939641089985791 | validation: 0.048810888906514346]
	TIME [epoch: 25.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04252461017425344		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.04252461017425344 | validation: 0.06957014039867551]
	TIME [epoch: 25.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046105967735323716		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.046105967735323716 | validation: 0.05039717758235647]
	TIME [epoch: 25.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040278513840672284		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.040278513840672284 | validation: 0.04514112770725308]
	TIME [epoch: 25.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04308879590330574		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.04308879590330574 | validation: 0.04358159372366287]
	TIME [epoch: 25.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04303094198881889		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.04303094198881889 | validation: 0.0454394459896566]
	TIME [epoch: 25.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04413589263106084		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.04413589263106084 | validation: 0.04474860544535187]
	TIME [epoch: 25.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04176551885942073		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.04176551885942073 | validation: 0.041751465580983575]
	TIME [epoch: 25.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038859094656075364		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.038859094656075364 | validation: 0.07200876461022765]
	TIME [epoch: 25.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04608779104308766		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.04608779104308766 | validation: 0.046861144346318884]
	TIME [epoch: 25.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03922103878905905		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.03922103878905905 | validation: 0.04069950040440526]
	TIME [epoch: 25.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04329281368586168		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.04329281368586168 | validation: 0.05647442585540799]
	TIME [epoch: 25.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0431532294262664		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.0431532294262664 | validation: 0.05212077159418378]
	TIME [epoch: 25.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04136743185375459		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.04136743185375459 | validation: 0.04598224138098522]
	TIME [epoch: 25.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03844691173589071		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.03844691173589071 | validation: 0.056745484316425676]
	TIME [epoch: 25.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04172527121365766		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.04172527121365766 | validation: 0.03992324066728803]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04338356409268561		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.04338356409268561 | validation: 0.0667602305773837]
	TIME [epoch: 25.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04657063024569442		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.04657063024569442 | validation: 0.043638086314107566]
	TIME [epoch: 25.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03901373799692457		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.03901373799692457 | validation: 0.045515676077025766]
	TIME [epoch: 25.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04397378333560663		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.04397378333560663 | validation: 0.04663529917317827]
	TIME [epoch: 25.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040640140102904176		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.040640140102904176 | validation: 0.046395501844109094]
	TIME [epoch: 25.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045555308497170266		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.045555308497170266 | validation: 0.057033573104318676]
	TIME [epoch: 25.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04259432660152043		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.04259432660152043 | validation: 0.04515912629642352]
	TIME [epoch: 25.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039302024310026516		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.039302024310026516 | validation: 0.04449002424865055]
	TIME [epoch: 25.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05072709468186992		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.05072709468186992 | validation: 0.052863759466797194]
	TIME [epoch: 25.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048454078518640004		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.048454078518640004 | validation: 0.04397296357796585]
	TIME [epoch: 25.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03866414057462331		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.03866414057462331 | validation: 0.04312020565881053]
	TIME [epoch: 25.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04104491808235271		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.04104491808235271 | validation: 0.04329538046994508]
	TIME [epoch: 25.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0421887602354193		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.0421887602354193 | validation: 0.05498509012792258]
	TIME [epoch: 25.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04578145057992477		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.04578145057992477 | validation: 0.06205601104241133]
	TIME [epoch: 25.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04476498285083018		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.04476498285083018 | validation: 0.05625133631309785]
	TIME [epoch: 25.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0408288386439702		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0408288386439702 | validation: 0.04667325745217975]
	TIME [epoch: 25.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0419415760491565		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0419415760491565 | validation: 0.0464386559172551]
	TIME [epoch: 25.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03990616222905041		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.03990616222905041 | validation: 0.05376844186187266]
	TIME [epoch: 25.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04351940604650842		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.04351940604650842 | validation: 0.04531182401109206]
	TIME [epoch: 25.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03876737089254043		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.03876737089254043 | validation: 0.0391240287428867]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03806753148440515		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.03806753148440515 | validation: 0.04247161918412168]
	TIME [epoch: 25.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04177858453931075		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.04177858453931075 | validation: 0.049268886783696966]
	TIME [epoch: 25.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045130147166782875		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.045130147166782875 | validation: 0.03973812373547089]
	TIME [epoch: 25.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037790906487122224		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.037790906487122224 | validation: 0.04419208753727444]
	TIME [epoch: 25.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038471657984371285		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.038471657984371285 | validation: 0.04562536955008322]
	TIME [epoch: 25.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0411936189160613		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.0411936189160613 | validation: 0.04310188704989419]
	TIME [epoch: 25.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044675012927258353		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.044675012927258353 | validation: 0.04403300852519812]
	TIME [epoch: 25.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04023576000600761		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.04023576000600761 | validation: 0.05076180095488729]
	TIME [epoch: 25.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042109835652503126		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.042109835652503126 | validation: 0.039112674601217444]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03826789269983691		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.03826789269983691 | validation: 0.04461187585307343]
	TIME [epoch: 25.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03842254656689002		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.03842254656689002 | validation: 0.041883756548958234]
	TIME [epoch: 25.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039514664839285306		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.039514664839285306 | validation: 0.051174593399427454]
	TIME [epoch: 25.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040885586816314864		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.040885586816314864 | validation: 0.04064647054312043]
	TIME [epoch: 25.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03677818302974027		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.03677818302974027 | validation: 0.0443289176805864]
	TIME [epoch: 25.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044703317743993394		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.044703317743993394 | validation: 0.041902383227773635]
	TIME [epoch: 25.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04346109157130446		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.04346109157130446 | validation: 0.047323874761452875]
	TIME [epoch: 25.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039930841920674816		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.039930841920674816 | validation: 0.04299053623901025]
	TIME [epoch: 25.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038729895821926964		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.038729895821926964 | validation: 0.04342676222366382]
	TIME [epoch: 25.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03663207679007163		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.03663207679007163 | validation: 0.04102690064518156]
	TIME [epoch: 25.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04192960148557412		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.04192960148557412 | validation: 0.039064888243273276]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04205702942105412		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.04205702942105412 | validation: 0.04751547394816853]
	TIME [epoch: 25.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03753304991994259		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.03753304991994259 | validation: 0.04121770789086127]
	TIME [epoch: 25.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04194565474177107		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.04194565474177107 | validation: 0.04216844431860686]
	TIME [epoch: 25.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042222661369091065		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.042222661369091065 | validation: 0.03870588581461383]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036934318509681846		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.036934318509681846 | validation: 0.04873183293980032]
	TIME [epoch: 25.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045316027125918604		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.045316027125918604 | validation: 0.040195667198075644]
	TIME [epoch: 25.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04046235146335174		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.04046235146335174 | validation: 0.07239368861383128]
	TIME [epoch: 25.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04236034489635357		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.04236034489635357 | validation: 0.04014626325674762]
	TIME [epoch: 25.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03445980004568265		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.03445980004568265 | validation: 0.04290525567274487]
	TIME [epoch: 25.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0433125048245028		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.0433125048245028 | validation: 0.04211202503464347]
	TIME [epoch: 25.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03686708109909762		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.03686708109909762 | validation: 0.04819524114004482]
	TIME [epoch: 25.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03593697363042948		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.03593697363042948 | validation: 0.046058989537167486]
	TIME [epoch: 25.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03897199679457733		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.03897199679457733 | validation: 0.04576796053261835]
	TIME [epoch: 25.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039972610927889216		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.039972610927889216 | validation: 0.045953087969429235]
	TIME [epoch: 25.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04099601211771068		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.04099601211771068 | validation: 0.042430790861904426]
	TIME [epoch: 25.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03490861711789779		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.03490861711789779 | validation: 0.05117977260928998]
	TIME [epoch: 25.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04056340043935171		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.04056340043935171 | validation: 0.041242414459681694]
	TIME [epoch: 25.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03966345789511329		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.03966345789511329 | validation: 0.041759379989894235]
	TIME [epoch: 25.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03853217216594949		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.03853217216594949 | validation: 0.05328964722604631]
	TIME [epoch: 25.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03925882396136284		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.03925882396136284 | validation: 0.03821757977040127]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0370774901050849		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.0370774901050849 | validation: 0.05699179546312987]
	TIME [epoch: 25.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03910975694970825		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.03910975694970825 | validation: 0.046943315148291]
	TIME [epoch: 25.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04032260821464288		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.04032260821464288 | validation: 0.044600659145888774]
	TIME [epoch: 25.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03990178490380365		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.03990178490380365 | validation: 0.03933372149150223]
	TIME [epoch: 25.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038992033833809156		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.038992033833809156 | validation: 0.04279812215196725]
	TIME [epoch: 25.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03837180523327202		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.03837180523327202 | validation: 0.048762978766347026]
	TIME [epoch: 25.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052995724008994424		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.052995724008994424 | validation: 0.050409233388491785]
	TIME [epoch: 25.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04072372706687952		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.04072372706687952 | validation: 0.044889977409973345]
	TIME [epoch: 25.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03797937758329433		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.03797937758329433 | validation: 0.04976571015663868]
	TIME [epoch: 25.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03780797739201679		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.03780797739201679 | validation: 0.043334557988006235]
	TIME [epoch: 25.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03761338381035621		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.03761338381035621 | validation: 0.04668858248937913]
	TIME [epoch: 25.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03838232113837883		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.03838232113837883 | validation: 0.04145183935415091]
	TIME [epoch: 25.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03712598526919075		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.03712598526919075 | validation: 0.04139179228849338]
	TIME [epoch: 25.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03846944678289878		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.03846944678289878 | validation: 0.04718918497749136]
	TIME [epoch: 25.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03871579129220977		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.03871579129220977 | validation: 0.04928547074064922]
	TIME [epoch: 25.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03848828454262285		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.03848828454262285 | validation: 0.03882209901723467]
	TIME [epoch: 25.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040221155016924026		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.040221155016924026 | validation: 0.04122522135812842]
	TIME [epoch: 25.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039821363263295745		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.039821363263295745 | validation: 0.043495330521857445]
	TIME [epoch: 25.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03801701375940658		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.03801701375940658 | validation: 0.04341839461248105]
	TIME [epoch: 25.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041404702797668246		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.041404702797668246 | validation: 0.03976457707610309]
	TIME [epoch: 25.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03511992283864133		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.03511992283864133 | validation: 0.05369726142093095]
	TIME [epoch: 25.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03726454791379179		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.03726454791379179 | validation: 0.04209840009423568]
	TIME [epoch: 25.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04264028982369628		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.04264028982369628 | validation: 0.05097700537713279]
	TIME [epoch: 25.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038570095016308675		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.038570095016308675 | validation: 0.04085242922375977]
	TIME [epoch: 25.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04038745722502723		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.04038745722502723 | validation: 0.04730825322338696]
	TIME [epoch: 25.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03649577300744778		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.03649577300744778 | validation: 0.04287505345107441]
	TIME [epoch: 25.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054438158709899466		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.054438158709899466 | validation: 0.040693853517359976]
	TIME [epoch: 25.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037366016051219135		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.037366016051219135 | validation: 0.04063182464674481]
	TIME [epoch: 25.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0366284264487637		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0366284264487637 | validation: 0.046276295061887955]
	TIME [epoch: 25.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03681829048542222		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.03681829048542222 | validation: 0.03891114418134321]
	TIME [epoch: 25.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03528695885608799		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.03528695885608799 | validation: 0.04320945429820766]
	TIME [epoch: 25.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03570510243626322		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.03570510243626322 | validation: 0.05034484054903532]
	TIME [epoch: 25.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040423041590288755		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.040423041590288755 | validation: 0.04922922529259601]
	TIME [epoch: 25.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0381945278291388		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0381945278291388 | validation: 0.04026575848972247]
	TIME [epoch: 25.7 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038246124945668175		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.038246124945668175 | validation: 0.03756549514487631]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03616391648929988		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.03616391648929988 | validation: 0.04280812914293132]
	TIME [epoch: 25.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035413115223032976		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.035413115223032976 | validation: 0.03555643005883757]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03610100476663547		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.03610100476663547 | validation: 0.041788761933739274]
	TIME [epoch: 25.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03658173130810449		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.03658173130810449 | validation: 0.050271026011853084]
	TIME [epoch: 25.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03899285838561903		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.03899285838561903 | validation: 0.04190466725209677]
	TIME [epoch: 25.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03830858341458182		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.03830858341458182 | validation: 0.04514326236932685]
	TIME [epoch: 25.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03787275602516128		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.03787275602516128 | validation: 0.04317553114890718]
	TIME [epoch: 25.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036507596934709625		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.036507596934709625 | validation: 0.038479937081775585]
	TIME [epoch: 25.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0352206553089177		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.0352206553089177 | validation: 0.04677550070797745]
	TIME [epoch: 25.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039495297134766565		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.039495297134766565 | validation: 0.03912968171606585]
	TIME [epoch: 25.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03649735866636898		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.03649735866636898 | validation: 0.04825946793527587]
	TIME [epoch: 25.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03554196964799852		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.03554196964799852 | validation: 0.04633507545298936]
	TIME [epoch: 25.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03458496972793261		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.03458496972793261 | validation: 0.0393095897555423]
	TIME [epoch: 25.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03736298861544171		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.03736298861544171 | validation: 0.048631526037195144]
	TIME [epoch: 25.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03898699627775841		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.03898699627775841 | validation: 0.03674558867390723]
	TIME [epoch: 25.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036157816331210256		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.036157816331210256 | validation: 0.04438873144899688]
	TIME [epoch: 25.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03682719813210482		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.03682719813210482 | validation: 0.05311887364807012]
	TIME [epoch: 25.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041269759571778974		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.041269759571778974 | validation: 0.04209770631109533]
	TIME [epoch: 25.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037276507882013137		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.037276507882013137 | validation: 0.037413313334327035]
	TIME [epoch: 25.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036280786498016804		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.036280786498016804 | validation: 0.04061832614335452]
	TIME [epoch: 25.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034335728224524374		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.034335728224524374 | validation: 0.03931436921050989]
	TIME [epoch: 25.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03521580151922458		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.03521580151922458 | validation: 0.04311574324721781]
	TIME [epoch: 25.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03725140209757195		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.03725140209757195 | validation: 0.04680259947090272]
	TIME [epoch: 25.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03802450292440103		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.03802450292440103 | validation: 0.04624571109618326]
	TIME [epoch: 25.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03897586580524383		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.03897586580524383 | validation: 0.04002634599496313]
	TIME [epoch: 25.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040354169213796476		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.040354169213796476 | validation: 0.03689794692073446]
	TIME [epoch: 25.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03533404643637952		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.03533404643637952 | validation: 0.04137860478455024]
	TIME [epoch: 25.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03528122147563903		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.03528122147563903 | validation: 0.04165575908449154]
	TIME [epoch: 25.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03728963076198699		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.03728963076198699 | validation: 0.04136461590861639]
	TIME [epoch: 25.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034617550459423115		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.034617550459423115 | validation: 0.03954579338296889]
	TIME [epoch: 25.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03481632776300336		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.03481632776300336 | validation: 0.04216868328462248]
	TIME [epoch: 25.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034769752950088204		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.034769752950088204 | validation: 0.04012528617791718]
	TIME [epoch: 25.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03629999945251981		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.03629999945251981 | validation: 0.036124472490445016]
	TIME [epoch: 25.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03810182160980316		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.03810182160980316 | validation: 0.040035685988882116]
	TIME [epoch: 25.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034648611377919594		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.034648611377919594 | validation: 0.05181983961584127]
	TIME [epoch: 25.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03662878522905863		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.03662878522905863 | validation: 0.0384263919028241]
	TIME [epoch: 25.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034114344185573534		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.034114344185573534 | validation: 0.045414545832930114]
	TIME [epoch: 25.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03692215945396673		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.03692215945396673 | validation: 0.03736050601554439]
	TIME [epoch: 25.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03697201774982363		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.03697201774982363 | validation: 0.0395956210296154]
	TIME [epoch: 25.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03613226066024928		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.03613226066024928 | validation: 0.039070695029777926]
	TIME [epoch: 25.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0350372862470066		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0350372862470066 | validation: 0.0373257412707656]
	TIME [epoch: 25.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036034969992803714		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.036034969992803714 | validation: 0.04066733007332955]
	TIME [epoch: 25.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03501884962070305		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.03501884962070305 | validation: 0.04264144591553173]
	TIME [epoch: 25.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036555860959131437		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.036555860959131437 | validation: 0.039479090296500804]
	TIME [epoch: 25.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033850647785789216		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.033850647785789216 | validation: 0.03866568193764016]
	TIME [epoch: 25.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035270946150636875		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.035270946150636875 | validation: 0.039086571013027684]
	TIME [epoch: 25.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03542228475027622		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.03542228475027622 | validation: 0.04649726819959049]
	TIME [epoch: 25.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03662500523136279		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.03662500523136279 | validation: 0.039036842867705535]
	TIME [epoch: 25.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03541667519126072		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.03541667519126072 | validation: 0.04160500679299633]
	TIME [epoch: 25.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035089425996790546		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.035089425996790546 | validation: 0.04137424981031253]
	TIME [epoch: 25.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036242177463506636		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.036242177463506636 | validation: 0.036790981592732225]
	TIME [epoch: 25.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03479206912287011		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.03479206912287011 | validation: 0.0398671953118442]
	TIME [epoch: 25.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03607490986568565		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.03607490986568565 | validation: 0.03780872319661986]
	TIME [epoch: 25.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036261020173490165		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.036261020173490165 | validation: 0.039955540463209496]
	TIME [epoch: 25.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03426953572476807		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.03426953572476807 | validation: 0.04848252365984927]
	TIME [epoch: 25.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03516641595330167		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.03516641595330167 | validation: 0.03618221003149044]
	TIME [epoch: 25.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0354970292717569		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0354970292717569 | validation: 0.04168362134536235]
	TIME [epoch: 25.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03713527384054849		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.03713527384054849 | validation: 0.03595495796161484]
	TIME [epoch: 25.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03362779545287058		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.03362779545287058 | validation: 0.042409781509431935]
	TIME [epoch: 25.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033630025762116066		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.033630025762116066 | validation: 0.04175847056387699]
	TIME [epoch: 25.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03494140197176388		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.03494140197176388 | validation: 0.04075279767598191]
	TIME [epoch: 25.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03517336054615199		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.03517336054615199 | validation: 0.04418413384219591]
	TIME [epoch: 25.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033838337321514284		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.033838337321514284 | validation: 0.037062233971585146]
	TIME [epoch: 25.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03748111874858487		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.03748111874858487 | validation: 0.04072919737075293]
	TIME [epoch: 25.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03378952266212709		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.03378952266212709 | validation: 0.039753818326778906]
	TIME [epoch: 25.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03469246135522572		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.03469246135522572 | validation: 0.04925329465012655]
	TIME [epoch: 25.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034859862722124296		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.034859862722124296 | validation: 0.03664554296594594]
	TIME [epoch: 25.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036489285279939845		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.036489285279939845 | validation: 0.036460627596728246]
	TIME [epoch: 25.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034855827169115755		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.034855827169115755 | validation: 0.038072664297048406]
	TIME [epoch: 25.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03441374186423788		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.03441374186423788 | validation: 0.042839535964419244]
	TIME [epoch: 25.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03313761108578911		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.03313761108578911 | validation: 0.03759424862347726]
	TIME [epoch: 25.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03707819805419206		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.03707819805419206 | validation: 0.042287501154251145]
	TIME [epoch: 25.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03368554638144866		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.03368554638144866 | validation: 0.04291214648474375]
	TIME [epoch: 25.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03383933802870525		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.03383933802870525 | validation: 0.03692370979697064]
	TIME [epoch: 25.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03308007611452064		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.03308007611452064 | validation: 0.038087769420611456]
	TIME [epoch: 25.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03449035791503659		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.03449035791503659 | validation: 0.043205884934224933]
	TIME [epoch: 25.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0340106082759589		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0340106082759589 | validation: 0.04553450470692107]
	TIME [epoch: 25.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03472443871619357		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.03472443871619357 | validation: 0.036241015576513894]
	TIME [epoch: 25.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03441961070172102		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.03441961070172102 | validation: 0.042411119387745444]
	TIME [epoch: 25.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035191134679510405		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.035191134679510405 | validation: 0.046291725209429556]
	TIME [epoch: 25.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03604173728113836		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.03604173728113836 | validation: 0.035670939546994795]
	TIME [epoch: 25.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0341662716313854		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.0341662716313854 | validation: 0.03630026027302471]
	TIME [epoch: 25.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03387396900982734		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.03387396900982734 | validation: 0.051018656725547465]
	TIME [epoch: 25.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034632561477156494		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.034632561477156494 | validation: 0.041171071588507926]
	TIME [epoch: 25.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04108669898838683		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.04108669898838683 | validation: 0.04415723943837591]
	TIME [epoch: 25.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036374610147165226		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.036374610147165226 | validation: 0.04506327842743611]
	TIME [epoch: 25.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03471159762343229		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.03471159762343229 | validation: 0.03471139407737973]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03455398280542961		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.03455398280542961 | validation: 0.039591301435399545]
	TIME [epoch: 25.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03478935325095836		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.03478935325095836 | validation: 0.039769035355273805]
	TIME [epoch: 25.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329888767147172		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0329888767147172 | validation: 0.04001249241672676]
	TIME [epoch: 25.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03532919518440637		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.03532919518440637 | validation: 0.03629432086220957]
	TIME [epoch: 25.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03348728440745846		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.03348728440745846 | validation: 0.039686236181070034]
	TIME [epoch: 25.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03502731962853877		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.03502731962853877 | validation: 0.04199618546632419]
	TIME [epoch: 25.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035577925643463466		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.035577925643463466 | validation: 0.03671168335142806]
	TIME [epoch: 25.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033014273697991474		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.033014273697991474 | validation: 0.04288516007401078]
	TIME [epoch: 25.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03329492418982797		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.03329492418982797 | validation: 0.04770503055554132]
	TIME [epoch: 25.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03425129519912795		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.03425129519912795 | validation: 0.040757281068662464]
	TIME [epoch: 25.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03355026135267318		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.03355026135267318 | validation: 0.040521887875014775]
	TIME [epoch: 25.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03400740401242391		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.03400740401242391 | validation: 0.04007825689748988]
	TIME [epoch: 25.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034608888336151875		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.034608888336151875 | validation: 0.0404009416386044]
	TIME [epoch: 25.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03774115216852909		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.03774115216852909 | validation: 0.04174660153077897]
	TIME [epoch: 25.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03395534241010807		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.03395534241010807 | validation: 0.040294294317053975]
	TIME [epoch: 25.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03338513854734275		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.03338513854734275 | validation: 0.038216933393279987]
	TIME [epoch: 25.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03233043247254511		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.03233043247254511 | validation: 0.03459209639564478]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03464079931206826		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.03464079931206826 | validation: 0.04362345396765005]
	TIME [epoch: 25.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034199132229321624		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.034199132229321624 | validation: 0.0383052791670868]
	TIME [epoch: 25.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03399590822767277		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.03399590822767277 | validation: 0.033744882084053046]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034502849683178426		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.034502849683178426 | validation: 0.042819169406579156]
	TIME [epoch: 25.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03267031951530551		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.03267031951530551 | validation: 0.038817191827189325]
	TIME [epoch: 25.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03482187008021093		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.03482187008021093 | validation: 0.04154013457046714]
	TIME [epoch: 25.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032652021919892874		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.032652021919892874 | validation: 0.03684469215571089]
	TIME [epoch: 25.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0337756454599029		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0337756454599029 | validation: 0.043662510924035336]
	TIME [epoch: 25.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03382388474070283		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.03382388474070283 | validation: 0.0362734802282043]
	TIME [epoch: 25.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03831880359363875		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.03831880359363875 | validation: 0.04261258165229756]
	TIME [epoch: 25.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03476322614052332		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.03476322614052332 | validation: 0.043415472327691676]
	TIME [epoch: 25.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037018440414928015		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.037018440414928015 | validation: 0.04414690323314156]
	TIME [epoch: 25.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032840411374861996		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.032840411374861996 | validation: 0.04287180206694288]
	TIME [epoch: 25.7 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032143293800751344		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.032143293800751344 | validation: 0.03813902044057791]
	TIME [epoch: 25.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03224467778626485		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.03224467778626485 | validation: 0.038349907712885954]
	TIME [epoch: 25.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032392187141782124		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.032392187141782124 | validation: 0.036645069865467915]
	TIME [epoch: 25.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03363906219943055		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.03363906219943055 | validation: 0.04211508832997608]
	TIME [epoch: 25.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03330328735978809		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.03330328735978809 | validation: 0.037338122139395]
	TIME [epoch: 25.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032130639416430865		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.032130639416430865 | validation: 0.043637778401780125]
	TIME [epoch: 25.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036368212257294114		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.036368212257294114 | validation: 0.04358017136944599]
	TIME [epoch: 25.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0336731135118359		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0336731135118359 | validation: 0.03479049040185733]
	TIME [epoch: 25.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03232550796431745		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.03232550796431745 | validation: 0.04106896555287147]
	TIME [epoch: 25.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032865562371494045		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.032865562371494045 | validation: 0.0444775502094115]
	TIME [epoch: 25.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03717305046589683		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.03717305046589683 | validation: 0.036225011463180815]
	TIME [epoch: 25.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031986286534687366		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.031986286534687366 | validation: 0.03712297906478059]
	TIME [epoch: 25.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03184913839418489		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.03184913839418489 | validation: 0.037576979146725156]
	TIME [epoch: 25.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03172536633051992		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.03172536633051992 | validation: 0.040243417462793765]
	TIME [epoch: 25.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032572015652009165		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.032572015652009165 | validation: 0.039263491630784096]
	TIME [epoch: 25.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03254926269183681		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.03254926269183681 | validation: 0.03777549443317625]
	TIME [epoch: 25.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034452563938788705		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.034452563938788705 | validation: 0.038584214994507676]
	TIME [epoch: 25.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032598497276491806		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.032598497276491806 | validation: 0.03539077497546948]
	TIME [epoch: 25.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032342505278647724		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.032342505278647724 | validation: 0.03858050927965072]
	TIME [epoch: 25.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03298555517197691		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.03298555517197691 | validation: 0.039180828857200985]
	TIME [epoch: 25.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0355729705399558		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.0355729705399558 | validation: 0.04204187690581351]
	TIME [epoch: 25.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033096024035585744		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.033096024035585744 | validation: 0.03862479587700729]
	TIME [epoch: 25.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03221963798996391		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.03221963798996391 | validation: 0.0359075651174854]
	TIME [epoch: 25.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03206113047635577		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.03206113047635577 | validation: 0.037346242094541196]
	TIME [epoch: 25.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034006967872901625		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.034006967872901625 | validation: 0.039099945935270386]
	TIME [epoch: 25.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03272038959066745		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.03272038959066745 | validation: 0.039224438980463204]
	TIME [epoch: 25.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03342022338609074		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.03342022338609074 | validation: 0.03631035358239033]
	TIME [epoch: 25.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03135312178774376		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.03135312178774376 | validation: 0.03771566290477105]
	TIME [epoch: 25.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03354714186063595		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.03354714186063595 | validation: 0.03795876770308078]
	TIME [epoch: 25.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03273866439367085		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.03273866439367085 | validation: 0.03933830379973147]
	TIME [epoch: 25.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031011748253091647		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.031011748253091647 | validation: 0.03954359688264514]
	TIME [epoch: 25.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03260509928285354		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.03260509928285354 | validation: 0.04484037940823818]
	TIME [epoch: 25.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033688334554980146		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.033688334554980146 | validation: 0.0372307898131919]
	TIME [epoch: 25.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03188236045935883		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.03188236045935883 | validation: 0.039151780290968885]
	TIME [epoch: 25.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031764442910453064		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.031764442910453064 | validation: 0.03693259519157631]
	TIME [epoch: 25.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03263830496600503		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.03263830496600503 | validation: 0.04587684468189243]
	TIME [epoch: 25.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03295992481663884		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.03295992481663884 | validation: 0.035330541132649836]
	TIME [epoch: 25.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03455730509824426		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.03455730509824426 | validation: 0.03683952041993703]
	TIME [epoch: 25.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032266411898395256		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.032266411898395256 | validation: 0.037415000482245234]
	TIME [epoch: 25.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031880638755807524		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.031880638755807524 | validation: 0.03749589642107764]
	TIME [epoch: 25.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031266387474648605		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.031266387474648605 | validation: 0.03434350264485098]
	TIME [epoch: 25.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030801838933772894		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.030801838933772894 | validation: 0.03567512731198268]
	TIME [epoch: 25.8 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032348421769676464		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.032348421769676464 | validation: 0.04171539947473357]
	TIME [epoch: 25.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033702204517070344		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.033702204517070344 | validation: 0.03587048226334669]
	TIME [epoch: 25.8 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031858230964033195		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.031858230964033195 | validation: 0.03992226064879998]
	TIME [epoch: 25.8 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03160861352490693		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.03160861352490693 | validation: 0.04156131740515283]
	TIME [epoch: 25.8 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034228805422183844		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.034228805422183844 | validation: 0.038094253273668575]
	TIME [epoch: 25.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145624360616225		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.03145624360616225 | validation: 0.03901865725606985]
	TIME [epoch: 25.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03180187912233377		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.03180187912233377 | validation: 0.03902229202224951]
	TIME [epoch: 25.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031608889111714035		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.031608889111714035 | validation: 0.03729005707980157]
	TIME [epoch: 25.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03156943107738932		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.03156943107738932 | validation: 0.037935363026226326]
	TIME [epoch: 25.7 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032568703838670304		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.032568703838670304 | validation: 0.03526814940246684]
	TIME [epoch: 25.7 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031379047797580564		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.031379047797580564 | validation: 0.040542857553646144]
	TIME [epoch: 25.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035934909265517984		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.035934909265517984 | validation: 0.04389486042345252]
	TIME [epoch: 25.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0342305804851788		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0342305804851788 | validation: 0.04211399722938855]
	TIME [epoch: 25.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03130074987425237		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.03130074987425237 | validation: 0.03966459392611965]
	TIME [epoch: 25.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032299683114056114		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.032299683114056114 | validation: 0.039363696216114435]
	TIME [epoch: 25.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031012300151585737		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.031012300151585737 | validation: 0.041511412438067524]
	TIME [epoch: 25.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0300515849278757		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0300515849278757 | validation: 0.03765413744241541]
	TIME [epoch: 25.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032277531126320855		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.032277531126320855 | validation: 0.03362571161029082]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032128054404795464		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.032128054404795464 | validation: 0.0385255530640215]
	TIME [epoch: 25.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03123867544498325		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.03123867544498325 | validation: 0.036792687352808974]
	TIME [epoch: 25.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03140335557101083		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.03140335557101083 | validation: 0.03709969749121572]
	TIME [epoch: 25.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031196967643606417		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.031196967643606417 | validation: 0.0395532980267706]
	TIME [epoch: 25.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03271104995156141		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.03271104995156141 | validation: 0.03644768342724036]
	TIME [epoch: 25.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03196770735153386		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.03196770735153386 | validation: 0.03676102241653674]
	TIME [epoch: 25.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031529307248699935		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.031529307248699935 | validation: 0.049855962531780054]
	TIME [epoch: 25.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032646137747186575		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.032646137747186575 | validation: 0.040027731715909676]
	TIME [epoch: 25.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031976279429103686		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.031976279429103686 | validation: 0.036734991222034684]
	TIME [epoch: 25.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05714321283082437		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.05714321283082437 | validation: 0.07437161389613245]
	TIME [epoch: 25.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04601643721296414		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.04601643721296414 | validation: 0.05228775080804517]
	TIME [epoch: 25.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03532875871255166		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.03532875871255166 | validation: 0.04645663794836827]
	TIME [epoch: 25.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03304431770464207		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.03304431770464207 | validation: 0.045206958392119675]
	TIME [epoch: 25.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03212528471616448		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.03212528471616448 | validation: 0.04211175015658072]
	TIME [epoch: 25.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031440947230979566		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.031440947230979566 | validation: 0.03832130498491475]
	TIME [epoch: 25.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03103411142760958		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.03103411142760958 | validation: 0.038882065984344065]
	TIME [epoch: 25.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03146216726835406		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.03146216726835406 | validation: 0.037325140169364]
	TIME [epoch: 25.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0306490723844046		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.0306490723844046 | validation: 0.03666656106814811]
	TIME [epoch: 25.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03035882946419166		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.03035882946419166 | validation: 0.037047787291275364]
	TIME [epoch: 25.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03209947911484933		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.03209947911484933 | validation: 0.04658926659472235]
	TIME [epoch: 25.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033834315877172276		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.033834315877172276 | validation: 0.039222504036802754]
	TIME [epoch: 25.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031078284691450693		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.031078284691450693 | validation: 0.03773490209181289]
	TIME [epoch: 25.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03397989512969221		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.03397989512969221 | validation: 0.03458598554436472]
	TIME [epoch: 25.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030946641229547253		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.030946641229547253 | validation: 0.03822851069340534]
	TIME [epoch: 25.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030392144191608628		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.030392144191608628 | validation: 0.0359082679571234]
	TIME [epoch: 25.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03199563775473124		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.03199563775473124 | validation: 0.035658846456460544]
	TIME [epoch: 25.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03096158395232216		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.03096158395232216 | validation: 0.037443311901308576]
	TIME [epoch: 25.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030776284116330432		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.030776284116330432 | validation: 0.03773232839420547]
	TIME [epoch: 25.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03049017248658765		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.03049017248658765 | validation: 0.0369849464483097]
	TIME [epoch: 25.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03348242486654038		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.03348242486654038 | validation: 0.03511486532166432]
	TIME [epoch: 25.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03160753873176929		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.03160753873176929 | validation: 0.03702826413566139]
	TIME [epoch: 25.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03104765472118662		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.03104765472118662 | validation: 0.03601813468419693]
	TIME [epoch: 25.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031116640434169097		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.031116640434169097 | validation: 0.03799104903331041]
	TIME [epoch: 25.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030660622357680363		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.030660622357680363 | validation: 0.03399904211466666]
	TIME [epoch: 25.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030663706010494795		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.030663706010494795 | validation: 0.03773124664989154]
	TIME [epoch: 25.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031139968283238062		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.031139968283238062 | validation: 0.033900056879015414]
	TIME [epoch: 25.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03333141995157476		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.03333141995157476 | validation: 0.03620053593660877]
	TIME [epoch: 25.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030911832538792358		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.030911832538792358 | validation: 0.03954801391594937]
	TIME [epoch: 25.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034126248967437905		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.034126248967437905 | validation: 0.037331055949977265]
	TIME [epoch: 25.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031058651599314133		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.031058651599314133 | validation: 0.04076927218436005]
	TIME [epoch: 25.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030299860412581862		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.030299860412581862 | validation: 0.03627126807931098]
	TIME [epoch: 25.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03186980192174549		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.03186980192174549 | validation: 0.03496016467065928]
	TIME [epoch: 25.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030012951721665758		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.030012951721665758 | validation: 0.03573771028206655]
	TIME [epoch: 25.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030517229441596263		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.030517229441596263 | validation: 0.03397593698229901]
	TIME [epoch: 25.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030888972660599163		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.030888972660599163 | validation: 0.03658099666472081]
	TIME [epoch: 25.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03164652617150215		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.03164652617150215 | validation: 0.037146500110447775]
	TIME [epoch: 25.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03057802802686463		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.03057802802686463 | validation: 0.03489529125593998]
	TIME [epoch: 25.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029277971190439064		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.029277971190439064 | validation: 0.03519014386743467]
	TIME [epoch: 25.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030739954153138103		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.030739954153138103 | validation: 0.03321589028175271]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03111775136179548		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.03111775136179548 | validation: 0.03260128866861017]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030758455038576653		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.030758455038576653 | validation: 0.037520804827123104]
	TIME [epoch: 25.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030040606074462444		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.030040606074462444 | validation: 0.0405763744581686]
	TIME [epoch: 25.7 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03053166523174831		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.03053166523174831 | validation: 0.03785879625281705]
	TIME [epoch: 25.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02999016196047176		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.02999016196047176 | validation: 0.04057328714473422]
	TIME [epoch: 25.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030927557565999023		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.030927557565999023 | validation: 0.037617441100776805]
	TIME [epoch: 25.7 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030785998892818363		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.030785998892818363 | validation: 0.03779862173512945]
	TIME [epoch: 25.7 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029422268147419876		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.029422268147419876 | validation: 0.037585145034780484]
	TIME [epoch: 25.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030114859203048683		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.030114859203048683 | validation: 0.0355749272332504]
	TIME [epoch: 25.7 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02974743382923745		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.02974743382923745 | validation: 0.03731112565159519]
	TIME [epoch: 25.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030156057391849664		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.030156057391849664 | validation: 0.034937320957253146]
	TIME [epoch: 25.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031072342096865817		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.031072342096865817 | validation: 0.03526140487384116]
	TIME [epoch: 25.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030654046534603532		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.030654046534603532 | validation: 0.038597509808346246]
	TIME [epoch: 25.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030704265552035627		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.030704265552035627 | validation: 0.03894693587383169]
	TIME [epoch: 25.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03079323862574154		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.03079323862574154 | validation: 0.03815352018527797]
	TIME [epoch: 25.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03075978105922171		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.03075978105922171 | validation: 0.03800629575105155]
	TIME [epoch: 25.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029415164860107652		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.029415164860107652 | validation: 0.03854218867489168]
	TIME [epoch: 25.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03043737616350663		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.03043737616350663 | validation: 0.039350144775789825]
	TIME [epoch: 25.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030883530967996155		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.030883530967996155 | validation: 0.034863560678332794]
	TIME [epoch: 25.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029906649401761284		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.029906649401761284 | validation: 0.04135251352868421]
	TIME [epoch: 25.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03088004565955147		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.03088004565955147 | validation: 0.04079571838037627]
	TIME [epoch: 25.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030489057030909646		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.030489057030909646 | validation: 0.03957772308180294]
	TIME [epoch: 25.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030275406648352834		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.030275406648352834 | validation: 0.03647044417440294]
	TIME [epoch: 25.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030325649765301464		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.030325649765301464 | validation: 0.03858276614299245]
	TIME [epoch: 25.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03001272986463277		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.03001272986463277 | validation: 0.03787379705604081]
	TIME [epoch: 25.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030767390707945527		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.030767390707945527 | validation: 0.03419779311745959]
	TIME [epoch: 25.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03049598964669018		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.03049598964669018 | validation: 0.035816485563983286]
	TIME [epoch: 25.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030590661766616498		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.030590661766616498 | validation: 0.039430065458079394]
	TIME [epoch: 25.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032221373962427344		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.032221373962427344 | validation: 0.034887971039112356]
	TIME [epoch: 25.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030703913295544907		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.030703913295544907 | validation: 0.03487006886938289]
	TIME [epoch: 25.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029664597819714443		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.029664597819714443 | validation: 0.036312268184491056]
	TIME [epoch: 25.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032009799986871035		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.032009799986871035 | validation: 0.03340143964934983]
	TIME [epoch: 25.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029850846347118497		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.029850846347118497 | validation: 0.03758692513870199]
	TIME [epoch: 25.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030864094052020202		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.030864094052020202 | validation: 0.03427660625230816]
	TIME [epoch: 25.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030557260856261554		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.030557260856261554 | validation: 0.03405659657582515]
	TIME [epoch: 25.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029005031177704154		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.029005031177704154 | validation: 0.034486345056619745]
	TIME [epoch: 25.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029519056309533106		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.029519056309533106 | validation: 0.03655111683625917]
	TIME [epoch: 25.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029353697625711193		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.029353697625711193 | validation: 0.033956434645651554]
	TIME [epoch: 25.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030565920173762903		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.030565920173762903 | validation: 0.03559904044624461]
	TIME [epoch: 25.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0300712639387655		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.0300712639387655 | validation: 0.035718814542974694]
	TIME [epoch: 25.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03170616121190796		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.03170616121190796 | validation: 0.03567649093748208]
	TIME [epoch: 25.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029949821152724446		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.029949821152724446 | validation: 0.03775013948153916]
	TIME [epoch: 25.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03003770278111396		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.03003770278111396 | validation: 0.03576371295757318]
	TIME [epoch: 25.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029278665577622688		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.029278665577622688 | validation: 0.0358612660617657]
	TIME [epoch: 25.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030371054785032752		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.030371054785032752 | validation: 0.03477103453795607]
	TIME [epoch: 25.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03082829298870568		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.03082829298870568 | validation: 0.03977639688560832]
	TIME [epoch: 25.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03020793652153181		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.03020793652153181 | validation: 0.03528384951841545]
	TIME [epoch: 25.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03061770143360654		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.03061770143360654 | validation: 0.038756769559695356]
	TIME [epoch: 25.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03015504772874686		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.03015504772874686 | validation: 0.038663668338178664]
	TIME [epoch: 25.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030031997377268662		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.030031997377268662 | validation: 0.03568702287187084]
	TIME [epoch: 25.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030142276218926355		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.030142276218926355 | validation: 0.04176900507314237]
	TIME [epoch: 25.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03049653869088733		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.03049653869088733 | validation: 0.03633782737248893]
	TIME [epoch: 25.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03011686721060908		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.03011686721060908 | validation: 0.03673731899470288]
	TIME [epoch: 25.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029573641241807464		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.029573641241807464 | validation: 0.03587815276027327]
	TIME [epoch: 25.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028958435261661825		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.028958435261661825 | validation: 0.03424674561776926]
	TIME [epoch: 25.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030584245582887658		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.030584245582887658 | validation: 0.038328711340958194]
	TIME [epoch: 25.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029955066782944392		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.029955066782944392 | validation: 0.03679314486900513]
	TIME [epoch: 25.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03086210210221961		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.03086210210221961 | validation: 0.037423884263860535]
	TIME [epoch: 25.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029405155023176206		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.029405155023176206 | validation: 0.036806817063998035]
	TIME [epoch: 25.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034233856755300884		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.034233856755300884 | validation: 0.03825241026904676]
	TIME [epoch: 25.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03102721378851598		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.03102721378851598 | validation: 0.037559452401161236]
	TIME [epoch: 25.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029872518019320327		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.029872518019320327 | validation: 0.03626412899881617]
	TIME [epoch: 25.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029645778026129603		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.029645778026129603 | validation: 0.03513762761713786]
	TIME [epoch: 25.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029478365380044426		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.029478365380044426 | validation: 0.03498051975052871]
	TIME [epoch: 25.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030168148395683807		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.030168148395683807 | validation: 0.03723886801230436]
	TIME [epoch: 25.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0299859516742797		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0299859516742797 | validation: 0.03564036210045338]
	TIME [epoch: 25.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03591460137612021		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.03591460137612021 | validation: 0.03925489279488393]
	TIME [epoch: 25.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03261199664325445		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.03261199664325445 | validation: 0.037856945376925424]
	TIME [epoch: 25.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029668722967139058		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.029668722967139058 | validation: 0.03580316104616151]
	TIME [epoch: 25.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028445487586270433		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.028445487586270433 | validation: 0.034689065100330985]
	TIME [epoch: 25.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02873149690933445		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.02873149690933445 | validation: 0.03771781322270454]
	TIME [epoch: 25.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0292573859660491		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0292573859660491 | validation: 0.03763751252210684]
	TIME [epoch: 25.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029197073779513368		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.029197073779513368 | validation: 0.0350215382391821]
	TIME [epoch: 25.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0292201984094847		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.0292201984094847 | validation: 0.03629576588197543]
	TIME [epoch: 25.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028776688472406747		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.028776688472406747 | validation: 0.03554906226262421]
	TIME [epoch: 25.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029278183850994628		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.029278183850994628 | validation: 0.03621369920757733]
	TIME [epoch: 25.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030276849452983493		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.030276849452983493 | validation: 0.03560734840925499]
	TIME [epoch: 25.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02831521668815249		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.02831521668815249 | validation: 0.0335631133645673]
	TIME [epoch: 25.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0291914143995815		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0291914143995815 | validation: 0.03621566339458611]
	TIME [epoch: 25.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029184901366145693		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.029184901366145693 | validation: 0.03420321943654788]
	TIME [epoch: 25.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02945759913902557		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.02945759913902557 | validation: 0.03807221977750447]
	TIME [epoch: 25.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0287767652720967		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0287767652720967 | validation: 0.042365872783011516]
	TIME [epoch: 25.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029252445999940174		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.029252445999940174 | validation: 0.03916077459244671]
	TIME [epoch: 25.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028291813346183595		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.028291813346183595 | validation: 0.03620536013470828]
	TIME [epoch: 25.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028795461985849556		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.028795461985849556 | validation: 0.03724997750092238]
	TIME [epoch: 25.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030518939327814708		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.030518939327814708 | validation: 0.03805881202552171]
	TIME [epoch: 25.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029995979239847936		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.029995979239847936 | validation: 0.03774926663751599]
	TIME [epoch: 25.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02852905559072921		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.02852905559072921 | validation: 0.036598865656650506]
	TIME [epoch: 25.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02917410246718672		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.02917410246718672 | validation: 0.03952655027726808]
	TIME [epoch: 25.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029328833195214142		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.029328833195214142 | validation: 0.03527441663952527]
	TIME [epoch: 25.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028913700094177243		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.028913700094177243 | validation: 0.04092603960180564]
	TIME [epoch: 25.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029263961890070703		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.029263961890070703 | validation: 0.03605411930036863]
	TIME [epoch: 25.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029619998508669955		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.029619998508669955 | validation: 0.035564841838360575]
	TIME [epoch: 25.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02877247465851882		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.02877247465851882 | validation: 0.03589406734756291]
	TIME [epoch: 25.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02879334237132602		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.02879334237132602 | validation: 0.03469567274490927]
	TIME [epoch: 430 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028967983996746823		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.028967983996746823 | validation: 0.03492069491056235]
	TIME [epoch: 54.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02834086457286277		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.02834086457286277 | validation: 0.036833324294640375]
	TIME [epoch: 54.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029365098905732204		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.029365098905732204 | validation: 0.03389592108529512]
	TIME [epoch: 54.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028969947841027882		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.028969947841027882 | validation: 0.03436802617748355]
	TIME [epoch: 54.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028761271102089862		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.028761271102089862 | validation: 0.03532850603964431]
	TIME [epoch: 54.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029239588954515703		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.029239588954515703 | validation: 0.03315245654563254]
	TIME [epoch: 54.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029374022626175656		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.029374022626175656 | validation: 0.03551509820433196]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_1_v_mmd1_1008.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 19722.711 seconds.
