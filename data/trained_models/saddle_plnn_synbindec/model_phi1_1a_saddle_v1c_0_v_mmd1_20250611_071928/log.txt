Args:
Namespace(name='model_phi1_1a_saddle_v1c_0_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1c_0/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1c_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05700300633907318, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1262704139

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.222360534251726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.222360534251726 | validation: 6.0285090005140125]
	TIME [epoch: 375 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.818336345231334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.818336345231334 | validation: 5.615255284972975]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.387215340542333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.387215340542333 | validation: 5.382540488645225]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.109018888455361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.109018888455361 | validation: 5.10869632055425]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871631208111823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.871631208111823 | validation: 5.199739743318794]
	TIME [epoch: 6.1 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.776436033921754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.776436033921754 | validation: 4.7778617337204565]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.538188994855842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.538188994855842 | validation: 4.574786384491423]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.362728221491865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.362728221491865 | validation: 4.5260765439032955]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.158920466132806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.158920466132806 | validation: 4.13025971504333]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9630080419781586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9630080419781586 | validation: 3.9879094103667008]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635895381626185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.635895381626185 | validation: 3.705140742190996]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344930320665962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344930320665962 | validation: 3.5150791998846733]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.250222304970661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.250222304970661 | validation: 3.6790093774022044]
	TIME [epoch: 6.09 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1136411159292456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1136411159292456 | validation: 3.2787991028015773]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9131878071410733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9131878071410733 | validation: 3.3372304067472998]
	TIME [epoch: 6.08 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8509232716004513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8509232716004513 | validation: 3.0300594727151093]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7759599216600686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7759599216600686 | validation: 2.893848173292823]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6267580919756734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6267580919756734 | validation: 2.7577423990105343]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.700773054893559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.700773054893559 | validation: 2.9335866996782727]
	TIME [epoch: 6.08 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5083316490372773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5083316490372773 | validation: 2.5790142247664876]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3034594030113476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3034594030113476 | validation: 2.30056056579339]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3105783953031196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3105783953031196 | validation: 2.277675704134152]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1186773930699747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1186773930699747 | validation: 2.0947825057849125]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1712390924966947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1712390924966947 | validation: 2.3753868304452554]
	TIME [epoch: 6.08 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.09542066152867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.09542066152867 | validation: 1.9410700821537006]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9268484914627342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9268484914627342 | validation: 2.041171576746074]
	TIME [epoch: 6.08 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7635579438956273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7635579438956273 | validation: 1.7261782194802588]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6583886248400708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6583886248400708 | validation: 1.6261158015265385]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5624431314487275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5624431314487275 | validation: 1.406276769953634]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4486161712622283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4486161712622283 | validation: 2.2044091576153244]
	TIME [epoch: 6.08 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7113326547025411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7113326547025411 | validation: 1.47333852620169]
	TIME [epoch: 6.08 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.214257762807873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.214257762807873 | validation: 1.2412029950849992]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4632051407242685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4632051407242685 | validation: 1.2740858359481186]
	TIME [epoch: 6.08 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2754145003999313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2754145003999313 | validation: 1.079286849588337]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.009055089454539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.009055089454539 | validation: 1.1821494692629326]
	TIME [epoch: 6.08 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0746925816422088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0746925816422088 | validation: 1.7961394204301433]
	TIME [epoch: 6.08 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3269574852104868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3269574852104868 | validation: 0.7915831047580937]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7282556765521528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282556765521528 | validation: 1.2343329778318677]
	TIME [epoch: 6.08 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4161156391803902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4161156391803902 | validation: 0.7769841895335655]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8525537937355605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8525537937355605 | validation: 0.6427169737205735]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8528408684479689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528408684479689 | validation: 0.9557209100938524]
	TIME [epoch: 6.08 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8526328738796953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526328738796953 | validation: 0.6016122676061211]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0304110371188169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0304110371188169 | validation: 1.4249816383993268]
	TIME [epoch: 6.08 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9983619526736092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9983619526736092 | validation: 0.8536114021957353]
	TIME [epoch: 6.08 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6807175207975124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6807175207975124 | validation: 0.7469365942938619]
	TIME [epoch: 6.07 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.878338514645995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878338514645995 | validation: 0.5146432818449249]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6356715573756349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356715573756349 | validation: 0.8757960505455594]
	TIME [epoch: 6.08 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6506503792176487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6506503792176487 | validation: 0.7399106978415453]
	TIME [epoch: 6.08 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0693029739893343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0693029739893343 | validation: 0.696812321313764]
	TIME [epoch: 6.08 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7004883260217889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004883260217889 | validation: 0.5769912260042006]
	TIME [epoch: 6.07 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7209021557836135		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7209021557836135 | validation: 0.9791711597257176]
	TIME [epoch: 6.08 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9006138383884691		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.9006138383884691 | validation: 0.7977323748832321]
	TIME [epoch: 6.08 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6849647498257835		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6849647498257835 | validation: 0.7244437246374491]
	TIME [epoch: 6.08 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6886770342483581		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.6886770342483581 | validation: 0.5257820561046889]
	TIME [epoch: 6.08 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6736453439204928		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.6736453439204928 | validation: 0.6253651318726801]
	TIME [epoch: 6.08 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5342780262903097		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.5342780262903097 | validation: 0.48176788651925945]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5974272844964964		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.5974272844964964 | validation: 0.8562609542485524]
	TIME [epoch: 6.09 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6096612416629547		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.6096612416629547 | validation: 0.712878002069099]
	TIME [epoch: 6.08 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6180870426985434		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6180870426985434 | validation: 0.423571729434201]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45706535433928636		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.45706535433928636 | validation: 0.7957502369789434]
	TIME [epoch: 6.08 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0554987279907142		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.0554987279907142 | validation: 0.6168813313817173]
	TIME [epoch: 6.08 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5528320760362203		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.5528320760362203 | validation: 0.8428215618955359]
	TIME [epoch: 6.08 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7390366380786544		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.7390366380786544 | validation: 0.6519745770655303]
	TIME [epoch: 6.08 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6076802738932876		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.6076802738932876 | validation: 0.5174923064037806]
	TIME [epoch: 6.08 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4980702226718582		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.4980702226718582 | validation: 0.42146407935622554]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6458961921134103		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.6458961921134103 | validation: 0.7356927129044748]
	TIME [epoch: 6.08 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8177741220167436		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.8177741220167436 | validation: 0.5374549190957005]
	TIME [epoch: 6.08 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5509649897873768		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5509649897873768 | validation: 0.4273489753415607]
	TIME [epoch: 6.08 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4039601943567622		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.4039601943567622 | validation: 0.47911700409270647]
	TIME [epoch: 6.09 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7779374946300652		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7779374946300652 | validation: 0.537066431795018]
	TIME [epoch: 6.08 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6346590196686117		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.6346590196686117 | validation: 0.4644952377435567]
	TIME [epoch: 6.08 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6891525982197686		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6891525982197686 | validation: 0.525142475203489]
	TIME [epoch: 6.07 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43931834945962006		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.43931834945962006 | validation: 0.3753837732137223]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6167239451008617		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.6167239451008617 | validation: 0.8196996611830794]
	TIME [epoch: 6.08 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6320013064868587		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6320013064868587 | validation: 0.47809658565223]
	TIME [epoch: 6.08 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5242576413623614		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5242576413623614 | validation: 0.42030410809691343]
	TIME [epoch: 6.08 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5323678922903639		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5323678922903639 | validation: 0.421311949921889]
	TIME [epoch: 6.08 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6566371535196833		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6566371535196833 | validation: 0.6082707360282582]
	TIME [epoch: 6.08 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5300412859857555		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5300412859857555 | validation: 0.4013353090791009]
	TIME [epoch: 6.08 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4487908135050296		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.4487908135050296 | validation: 0.42932158375476026]
	TIME [epoch: 6.09 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4517262802080787		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.4517262802080787 | validation: 0.7131104492312108]
	TIME [epoch: 6.08 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5562564195567259		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.5562564195567259 | validation: 0.36164115312398315]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4229128807196232		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.4229128807196232 | validation: 0.557400956176002]
	TIME [epoch: 6.08 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4362638365389992		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.4362638365389992 | validation: 0.473162752687723]
	TIME [epoch: 6.07 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5363537818744093		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5363537818744093 | validation: 0.3954169884113068]
	TIME [epoch: 6.08 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41815653552825327		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.41815653552825327 | validation: 0.5246766360994825]
	TIME [epoch: 6.08 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4220759705841791		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.4220759705841791 | validation: 0.5163780047902345]
	TIME [epoch: 6.09 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5520044236807182		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5520044236807182 | validation: 0.5952370279779906]
	TIME [epoch: 6.07 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46118023079532555		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.46118023079532555 | validation: 0.367741177917252]
	TIME [epoch: 6.08 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45933102100249945		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.45933102100249945 | validation: 0.41880393999463617]
	TIME [epoch: 6.07 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4006243809223521		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.4006243809223521 | validation: 0.324264898963168]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47066529007968694		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.47066529007968694 | validation: 0.5212619277450763]
	TIME [epoch: 6.08 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44117806901192247		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.44117806901192247 | validation: 0.41400003042734757]
	TIME [epoch: 6.08 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4305222613847046		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.4305222613847046 | validation: 0.372850214639048]
	TIME [epoch: 6.08 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3534309882240702		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.3534309882240702 | validation: 0.5851168720443044]
	TIME [epoch: 6.08 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48055040913986236		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.48055040913986236 | validation: 0.369826246079058]
	TIME [epoch: 6.07 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38725601452048514		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.38725601452048514 | validation: 0.6132842568132291]
	TIME [epoch: 6.07 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49998116284054844		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.49998116284054844 | validation: 0.31139696587793]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38261976486919186		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.38261976486919186 | validation: 0.3579418856700556]
	TIME [epoch: 6.09 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4166491274145207		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.4166491274145207 | validation: 0.3269443898989408]
	TIME [epoch: 6.08 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463449541651549		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.3463449541651549 | validation: 0.293788028349111]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3537291305389143		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.3537291305389143 | validation: 0.4705651568409654]
	TIME [epoch: 6.07 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40093792281002427		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.40093792281002427 | validation: 0.4099721732217929]
	TIME [epoch: 6.08 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.454724410015381		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.454724410015381 | validation: 0.3973282399087355]
	TIME [epoch: 6.07 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29943850371376346		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.29943850371376346 | validation: 0.3476148601936446]
	TIME [epoch: 6.08 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4338964910285884		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.4338964910285884 | validation: 0.2791433478189663]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2887257085340532		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2887257085340532 | validation: 0.7096714345680244]
	TIME [epoch: 6.08 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4813907959882093		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.4813907959882093 | validation: 0.4460021693936268]
	TIME [epoch: 6.07 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3472193350991345		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3472193350991345 | validation: 0.38639564661726644]
	TIME [epoch: 6.08 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4577483729586259		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.4577483729586259 | validation: 0.38114433645583023]
	TIME [epoch: 6.07 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4522088997100994		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.4522088997100994 | validation: 0.35001059091733683]
	TIME [epoch: 6.08 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28750508199597236		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.28750508199597236 | validation: 0.2846039969191182]
	TIME [epoch: 6.07 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.531876940177268		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.531876940177268 | validation: 0.3879887007401467]
	TIME [epoch: 6.08 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32498639282400044		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.32498639282400044 | validation: 0.41752161442374847]
	TIME [epoch: 6.08 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854455552645649		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2854455552645649 | validation: 0.2728243073072968]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627401466780226		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2627401466780226 | validation: 0.4937475110829835]
	TIME [epoch: 6.08 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3265899729630859		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.3265899729630859 | validation: 0.5055111120835615]
	TIME [epoch: 6.08 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4111971988329076		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.4111971988329076 | validation: 0.26628882430666256]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3494117178710984		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.3494117178710984 | validation: 0.36656255563821816]
	TIME [epoch: 6.09 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.381865193255408		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.381865193255408 | validation: 0.35876591532516156]
	TIME [epoch: 6.08 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38593796970732586		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.38593796970732586 | validation: 0.35668049625765025]
	TIME [epoch: 6.08 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28456441466046184		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.28456441466046184 | validation: 0.24552990071004424]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27885203122726965		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.27885203122726965 | validation: 0.2940771397091555]
	TIME [epoch: 6.08 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3768305212878453		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.3768305212878453 | validation: 0.2761034801124379]
	TIME [epoch: 6.09 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151741501893311		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.3151741501893311 | validation: 0.4105265726956411]
	TIME [epoch: 6.08 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29187165195979786		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.29187165195979786 | validation: 0.3162250486234976]
	TIME [epoch: 6.08 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3287826180574519		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3287826180574519 | validation: 0.26004331249884555]
	TIME [epoch: 6.07 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3033140717066878		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3033140717066878 | validation: 0.25070223414081416]
	TIME [epoch: 6.08 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2687650056345804		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2687650056345804 | validation: 0.47160123501502016]
	TIME [epoch: 6.09 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.407151528290659		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.407151528290659 | validation: 0.23934099669963568]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22521578870617537		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.22521578870617537 | validation: 0.2534565558634351]
	TIME [epoch: 6.09 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28975602945381357		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.28975602945381357 | validation: 0.29219563614252747]
	TIME [epoch: 6.08 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3385351872826919		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3385351872826919 | validation: 0.21860564727917836]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21633728766481392		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.21633728766481392 | validation: 0.33047934633196135]
	TIME [epoch: 6.09 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28167983727349477		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.28167983727349477 | validation: 0.5749791365069397]
	TIME [epoch: 6.07 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35182694602749987		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.35182694602749987 | validation: 0.388053741381459]
	TIME [epoch: 6.08 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24882875592332987		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.24882875592332987 | validation: 0.21229610161046047]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26304644991829707		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.26304644991829707 | validation: 0.259505618004505]
	TIME [epoch: 6.1 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22937166938406586		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.22937166938406586 | validation: 0.35716672707882446]
	TIME [epoch: 6.07 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3559488063203707		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.3559488063203707 | validation: 0.22655602491496357]
	TIME [epoch: 6.07 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22131501110770918		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.22131501110770918 | validation: 0.23345649094008458]
	TIME [epoch: 6.09 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25364340864078394		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.25364340864078394 | validation: 0.2373903838973819]
	TIME [epoch: 6.08 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.234010502363529		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.234010502363529 | validation: 0.24331173916210153]
	TIME [epoch: 6.08 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21979464426927725		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.21979464426927725 | validation: 0.20508795458958792]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26727474917585164		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.26727474917585164 | validation: 0.41780664871017303]
	TIME [epoch: 6.09 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623933843698842		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.2623933843698842 | validation: 0.356395944376775]
	TIME [epoch: 6.08 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2578856152672662		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2578856152672662 | validation: 0.19339466398880753]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20319252445940095		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.20319252445940095 | validation: 0.44052354726547627]
	TIME [epoch: 6.09 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3165893259652589		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.3165893259652589 | validation: 0.24018815685506534]
	TIME [epoch: 6.08 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20364163386887826		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.20364163386887826 | validation: 0.23792258065071542]
	TIME [epoch: 6.08 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3343283357312683		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.3343283357312683 | validation: 0.24177855632113704]
	TIME [epoch: 6.08 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20094339649906695		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.20094339649906695 | validation: 0.32128862638109257]
	TIME [epoch: 6.08 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2634753385944097		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.2634753385944097 | validation: 0.31305375767004673]
	TIME [epoch: 6.08 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21200225527929012		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.21200225527929012 | validation: 0.26801012440862965]
	TIME [epoch: 6.09 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.293462887649366		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.293462887649366 | validation: 0.241559843222307]
	TIME [epoch: 6.08 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1608022981565158		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1608022981565158 | validation: 0.1909978169927155]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23537273764308847		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.23537273764308847 | validation: 0.2633366788341003]
	TIME [epoch: 6.09 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29644868263363333		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.29644868263363333 | validation: 0.23907110948647653]
	TIME [epoch: 6.09 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18710869989684906		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.18710869989684906 | validation: 0.37423347327489176]
	TIME [epoch: 6.07 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2279020548616978		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2279020548616978 | validation: 0.2611652546856843]
	TIME [epoch: 6.08 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18382904974272712		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.18382904974272712 | validation: 0.17782175049489973]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26939553329823096		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.26939553329823096 | validation: 0.2868649391998813]
	TIME [epoch: 6.08 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2019877387152041		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2019877387152041 | validation: 0.30679147091554293]
	TIME [epoch: 6.08 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2123627575129841		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.2123627575129841 | validation: 0.34720316420500263]
	TIME [epoch: 6.08 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21954979985654516		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.21954979985654516 | validation: 0.2775202217691917]
	TIME [epoch: 6.08 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17399406862604883		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.17399406862604883 | validation: 0.2535246531901211]
	TIME [epoch: 6.08 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19505890122207248		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.19505890122207248 | validation: 0.29276484514870266]
	TIME [epoch: 6.08 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23903743545663428		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.23903743545663428 | validation: 0.20216095412380775]
	TIME [epoch: 6.08 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14541216135349075		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.14541216135349075 | validation: 0.29329533847978434]
	TIME [epoch: 6.07 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25985123726220577		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.25985123726220577 | validation: 0.3387244223109682]
	TIME [epoch: 6.08 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1788447813932067		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.1788447813932067 | validation: 0.16300589065171522]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18115292499582802		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.18115292499582802 | validation: 0.2425722066868058]
	TIME [epoch: 6.09 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2288319410406639		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.2288319410406639 | validation: 0.16124911274062964]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12691730761556963		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.12691730761556963 | validation: 0.22713977158409573]
	TIME [epoch: 6.08 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2168622406246084		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.2168622406246084 | validation: 0.24192250611217464]
	TIME [epoch: 6.08 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17984939185279983		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.17984939185279983 | validation: 0.320216226034122]
	TIME [epoch: 6.08 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17849711581100255		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.17849711581100255 | validation: 0.16836386059941413]
	TIME [epoch: 6.08 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14540843867111108		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.14540843867111108 | validation: 0.2530650259423086]
	TIME [epoch: 6.08 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22880355639809596		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.22880355639809596 | validation: 0.1749835708494945]
	TIME [epoch: 6.08 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15235689755556922		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.15235689755556922 | validation: 0.1669398808486383]
	TIME [epoch: 6.08 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16186217130040093		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.16186217130040093 | validation: 0.16512637021265958]
	TIME [epoch: 6.08 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19408457615167288		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.19408457615167288 | validation: 0.1837114380561395]
	TIME [epoch: 6.08 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1991211701067061		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.1991211701067061 | validation: 0.14275392500942402]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10830357536974314		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.10830357536974314 | validation: 0.15166428530856813]
	TIME [epoch: 6.09 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16136286981910922		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.16136286981910922 | validation: 0.18403101709096711]
	TIME [epoch: 6.08 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2266107404042148		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.2266107404042148 | validation: 0.14537565185091295]
	TIME [epoch: 6.08 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13512813323758208		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.13512813323758208 | validation: 0.21403593453893993]
	TIME [epoch: 6.09 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14794090358441941		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.14794090358441941 | validation: 0.11863413194760454]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19395485904633822		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.19395485904633822 | validation: 0.31596679862583843]
	TIME [epoch: 6.08 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2203545642557931		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.2203545642557931 | validation: 0.16062150744012993]
	TIME [epoch: 6.08 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15718486393315154		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.15718486393315154 | validation: 0.15501423224005362]
	TIME [epoch: 6.07 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14780426923972165		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.14780426923972165 | validation: 0.19721876086300513]
	TIME [epoch: 6.08 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14740687846141007		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.14740687846141007 | validation: 0.15706609446515685]
	TIME [epoch: 6.07 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15792921646250707		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.15792921646250707 | validation: 0.23967904974187978]
	TIME [epoch: 6.07 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17088484865476333		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.17088484865476333 | validation: 0.1727115246537323]
	TIME [epoch: 6.07 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12769074608672681		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.12769074608672681 | validation: 0.3833151826488151]
	TIME [epoch: 6.07 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1780470436266268		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.1780470436266268 | validation: 0.1203236097622842]
	TIME [epoch: 6.07 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08589401603930526		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.08589401603930526 | validation: 0.24527517842826238]
	TIME [epoch: 6.08 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1655489512095238		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.1655489512095238 | validation: 0.14550164359576667]
	TIME [epoch: 6.08 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20386027675467913		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.20386027675467913 | validation: 0.1670236104412552]
	TIME [epoch: 6.07 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11501321034553488		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.11501321034553488 | validation: 0.12936089187941796]
	TIME [epoch: 398 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13124569735759173		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.13124569735759173 | validation: 0.1681957477218619]
	TIME [epoch: 12 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10916895205206385		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.10916895205206385 | validation: 0.18082965921687433]
	TIME [epoch: 12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1337759758614467		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.1337759758614467 | validation: 0.2117279916143443]
	TIME [epoch: 12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16299540751535208		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.16299540751535208 | validation: 0.20258627740282634]
	TIME [epoch: 12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11573861797007692		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.11573861797007692 | validation: 0.15857777464144987]
	TIME [epoch: 12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20478446951206558		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.20478446951206558 | validation: 0.15399152560078117]
	TIME [epoch: 12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1332486444262131		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.1332486444262131 | validation: 0.10672093994183557]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09329629866136155		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.09329629866136155 | validation: 0.18204816486511427]
	TIME [epoch: 12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18090746887727605		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.18090746887727605 | validation: 0.10349902665646767]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09744298758677242		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.09744298758677242 | validation: 0.13629660497578527]
	TIME [epoch: 12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1402751359856612		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1402751359856612 | validation: 0.23508159404141316]
	TIME [epoch: 12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13730946729833488		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.13730946729833488 | validation: 0.14553537029291913]
	TIME [epoch: 12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10741215542628588		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.10741215542628588 | validation: 0.15322437737502526]
	TIME [epoch: 12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12572174930374122		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.12572174930374122 | validation: 0.15383708310465694]
	TIME [epoch: 12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1765559054112747		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.1765559054112747 | validation: 0.13793907908679742]
	TIME [epoch: 12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09648315345914664		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.09648315345914664 | validation: 0.12213122571463876]
	TIME [epoch: 12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12981277993810145		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.12981277993810145 | validation: 0.1390968760766052]
	TIME [epoch: 12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09291676339171677		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.09291676339171677 | validation: 0.103525512465519]
	TIME [epoch: 12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10908704927738334		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.10908704927738334 | validation: 0.2669954414565366]
	TIME [epoch: 12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488967562848334		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.1488967562848334 | validation: 0.14556142871024452]
	TIME [epoch: 12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11781603053276482		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.11781603053276482 | validation: 0.14942395482268828]
	TIME [epoch: 12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1377197601501271		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.1377197601501271 | validation: 0.11769952755150928]
	TIME [epoch: 12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09011879395979183		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.09011879395979183 | validation: 0.1181673775565263]
	TIME [epoch: 12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1235419195115999		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.1235419195115999 | validation: 0.1479421781534545]
	TIME [epoch: 12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10860901734073738		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.10860901734073738 | validation: 0.1385691411171811]
	TIME [epoch: 12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13215350649049218		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.13215350649049218 | validation: 0.1545141901821476]
	TIME [epoch: 12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09509896199260003		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.09509896199260003 | validation: 0.15681436828292444]
	TIME [epoch: 12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08933538492754099		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.08933538492754099 | validation: 0.06809348685314541]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08945303675268128		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.08945303675268128 | validation: 0.25252070973116575]
	TIME [epoch: 12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17221351529985965		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.17221351529985965 | validation: 0.0893793118307342]
	TIME [epoch: 12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10316321069065564		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.10316321069065564 | validation: 0.1551762066174266]
	TIME [epoch: 12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10681852824937202		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.10681852824937202 | validation: 0.10710457361901476]
	TIME [epoch: 12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10680010792105973		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.10680010792105973 | validation: 0.11183737929381458]
	TIME [epoch: 12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0947497609824257		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.0947497609824257 | validation: 0.10452749349276311]
	TIME [epoch: 12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10426251540126998		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.10426251540126998 | validation: 0.10994989589339012]
	TIME [epoch: 12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09416551774807982		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.09416551774807982 | validation: 0.15892845504112496]
	TIME [epoch: 12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1410856256171753		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1410856256171753 | validation: 0.08912866708145573]
	TIME [epoch: 12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07422682145064441		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.07422682145064441 | validation: 0.12999194549970308]
	TIME [epoch: 12 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13540388191967395		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.13540388191967395 | validation: 0.15562785855334493]
	TIME [epoch: 12 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07749870980500967		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.07749870980500967 | validation: 0.091843648173314]
	TIME [epoch: 12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12056482021601816		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.12056482021601816 | validation: 0.08654978119299012]
	TIME [epoch: 12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05460651677043624		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.05460651677043624 | validation: 0.09588700540469508]
	TIME [epoch: 12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15059759070361775		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.15059759070361775 | validation: 0.14966114948437276]
	TIME [epoch: 12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.106001930259776		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.106001930259776 | validation: 0.13266390292459165]
	TIME [epoch: 12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1308262564249611		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.1308262564249611 | validation: 0.12413159911038255]
	TIME [epoch: 12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0971703943482208		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.0971703943482208 | validation: 0.09728861878169445]
	TIME [epoch: 12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08202279489359683		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.08202279489359683 | validation: 0.11452151583635872]
	TIME [epoch: 12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10511513346253266		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.10511513346253266 | validation: 0.12065444095272422]
	TIME [epoch: 12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07456758522775644		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.07456758522775644 | validation: 0.06881802721919505]
	TIME [epoch: 12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06393170970680805		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.06393170970680805 | validation: 0.2289283359325364]
	TIME [epoch: 12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19569054605674238		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.19569054605674238 | validation: 0.09613358638773636]
	TIME [epoch: 12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06762159263441733		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.06762159263441733 | validation: 0.08609218061136661]
	TIME [epoch: 12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08259306154841003		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.08259306154841003 | validation: 0.1698458752647988]
	TIME [epoch: 12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12323156424099868		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.12323156424099868 | validation: 0.08043605150071959]
	TIME [epoch: 12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05931426216251477		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.05931426216251477 | validation: 0.08736130097380156]
	TIME [epoch: 12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11891587430299262		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.11891587430299262 | validation: 0.09692354081334273]
	TIME [epoch: 12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08838664653039896		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.08838664653039896 | validation: 0.0924315753546034]
	TIME [epoch: 12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09092249703705414		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.09092249703705414 | validation: 0.08892021987844503]
	TIME [epoch: 12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10585033607859787		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.10585033607859787 | validation: 0.08375596850757275]
	TIME [epoch: 12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0697821665311627		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.0697821665311627 | validation: 0.09251327847781624]
	TIME [epoch: 12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07839644823390371		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.07839644823390371 | validation: 0.15192495232089803]
	TIME [epoch: 12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07951225077259325		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.07951225077259325 | validation: 0.10565165269588425]
	TIME [epoch: 12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12182608198529714		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.12182608198529714 | validation: 0.1311469677178267]
	TIME [epoch: 12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0882944652894729		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.0882944652894729 | validation: 0.07552615595825656]
	TIME [epoch: 12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09245276534110625		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.09245276534110625 | validation: 0.13923626870742023]
	TIME [epoch: 12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08321569953316885		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.08321569953316885 | validation: 0.07704380712346377]
	TIME [epoch: 12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07381880702836778		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.07381880702836778 | validation: 0.1623820548587906]
	TIME [epoch: 12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0845235483012716		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.0845235483012716 | validation: 0.11634210379674226]
	TIME [epoch: 12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08369548618693355		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.08369548618693355 | validation: 0.09460582532624029]
	TIME [epoch: 12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09889598911105585		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.09889598911105585 | validation: 0.15659268739436732]
	TIME [epoch: 12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08599996794167958		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.08599996794167958 | validation: 0.09801472945919751]
	TIME [epoch: 12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07825604727847499		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.07825604727847499 | validation: 0.12731777751990211]
	TIME [epoch: 12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07366709933341813		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.07366709933341813 | validation: 0.13091336710542034]
	TIME [epoch: 12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1066603157591572		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.1066603157591572 | validation: 0.0829139719874241]
	TIME [epoch: 12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054342793094121135		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.054342793094121135 | validation: 0.08720539418090203]
	TIME [epoch: 12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06881384461054506		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.06881384461054506 | validation: 0.08044387898484014]
	TIME [epoch: 12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10126720078424195		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.10126720078424195 | validation: 0.12822174347624024]
	TIME [epoch: 12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10461591519931457		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.10461591519931457 | validation: 0.06185355349992239]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08855143960597085		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.08855143960597085 | validation: 0.07917309742764372]
	TIME [epoch: 12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07408724086389872		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.07408724086389872 | validation: 0.15080029486838203]
	TIME [epoch: 12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08671483781123442		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.08671483781123442 | validation: 0.07954248108926812]
	TIME [epoch: 12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06295870336921049		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.06295870336921049 | validation: 0.1191058273935561]
	TIME [epoch: 12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10807033282736417		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.10807033282736417 | validation: 0.08529560882528386]
	TIME [epoch: 12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05439353584913722		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.05439353584913722 | validation: 0.09022993319126237]
	TIME [epoch: 12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06357850401381038		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.06357850401381038 | validation: 0.1194577411836521]
	TIME [epoch: 12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10963343942895014		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.10963343942895014 | validation: 0.07609576009239499]
	TIME [epoch: 12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05796003076731282		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.05796003076731282 | validation: 0.07447730994684443]
	TIME [epoch: 12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09593301958145457		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.09593301958145457 | validation: 0.09484405040659301]
	TIME [epoch: 12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06625340495420029		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.06625340495420029 | validation: 0.09773511398153989]
	TIME [epoch: 12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06376197724605012		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.06376197724605012 | validation: 0.05836038147787233]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06273923512317423		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.06273923512317423 | validation: 0.14929816316113875]
	TIME [epoch: 12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0978174921115097		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.0978174921115097 | validation: 0.07032725236412968]
	TIME [epoch: 12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058496367090795816		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.058496367090795816 | validation: 0.08162573424660154]
	TIME [epoch: 12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06965655345523761		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.06965655345523761 | validation: 0.11880314865485461]
	TIME [epoch: 12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08115530268347212		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.08115530268347212 | validation: 0.09843674829135007]
	TIME [epoch: 12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07161098237061199		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.07161098237061199 | validation: 0.10460523769989793]
	TIME [epoch: 12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08248064305428032		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.08248064305428032 | validation: 0.06323644768917916]
	TIME [epoch: 12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05623967600439475		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.05623967600439475 | validation: 0.08361646549253826]
	TIME [epoch: 12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08048384782389853		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.08048384782389853 | validation: 0.09842918575763139]
	TIME [epoch: 12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06720752805313657		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.06720752805313657 | validation: 0.05314979042265039]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04331526048485834		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.04331526048485834 | validation: 0.1651084434191556]
	TIME [epoch: 12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08830671702989792		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.08830671702989792 | validation: 0.05544382209120077]
	TIME [epoch: 12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03973752264963879		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.03973752264963879 | validation: 0.05708450542659587]
	TIME [epoch: 12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07481881798056192		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.07481881798056192 | validation: 0.12361870408978579]
	TIME [epoch: 12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08229571658328455		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.08229571658328455 | validation: 0.11968667944289016]
	TIME [epoch: 12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09158222890990156		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.09158222890990156 | validation: 0.054363641292684464]
	TIME [epoch: 12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061500614717524604		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.061500614717524604 | validation: 0.059076667011428906]
	TIME [epoch: 12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05752789715013179		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.05752789715013179 | validation: 0.125395141240732]
	TIME [epoch: 12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0809953534381097		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.0809953534381097 | validation: 0.058136451700053]
	TIME [epoch: 12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06171781241940755		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.06171781241940755 | validation: 0.10981375267542053]
	TIME [epoch: 12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08368439120488083		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.08368439120488083 | validation: 0.0805171345646426]
	TIME [epoch: 12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05488162648038343		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.05488162648038343 | validation: 0.05847004407552102]
	TIME [epoch: 12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05847751926890785		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.05847751926890785 | validation: 0.06831598555942486]
	TIME [epoch: 12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336845431058821		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.06336845431058821 | validation: 0.06164860551249207]
	TIME [epoch: 12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06518718133464807		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.06518718133464807 | validation: 0.1087864103623058]
	TIME [epoch: 12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05949312100166976		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.05949312100166976 | validation: 0.0724433171341341]
	TIME [epoch: 12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0923359717912009		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.0923359717912009 | validation: 0.07134173272449873]
	TIME [epoch: 12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048594968364693146		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.048594968364693146 | validation: 0.05278664694519735]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05969527438913839		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.05969527438913839 | validation: 0.09895115504319643]
	TIME [epoch: 12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06102897782401826		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.06102897782401826 | validation: 0.06631043013535866]
	TIME [epoch: 12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05502609247118307		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.05502609247118307 | validation: 0.07860203122850476]
	TIME [epoch: 12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08470735233927977		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08470735233927977 | validation: 0.06262955070989612]
	TIME [epoch: 12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050578291855801025		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.050578291855801025 | validation: 0.06889036503646194]
	TIME [epoch: 12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08406577700373527		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.08406577700373527 | validation: 0.0751930485305087]
	TIME [epoch: 12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1159126567576281		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.1159126567576281 | validation: 0.04841277857835283]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0482118542550794		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.0482118542550794 | validation: 0.04363952863954128]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03482273740756103		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03482273740756103 | validation: 0.06732704361630987]
	TIME [epoch: 12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06976315594955419		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.06976315594955419 | validation: 0.08233038785195061]
	TIME [epoch: 12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05831013749292514		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.05831013749292514 | validation: 0.2662281576861305]
	TIME [epoch: 12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10905885326302507		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.10905885326302507 | validation: 0.055019502327371315]
	TIME [epoch: 12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03724892551660609		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.03724892551660609 | validation: 0.07451215075843119]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06233670746803928		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.06233670746803928 | validation: 0.0792847202364362]
	TIME [epoch: 12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04679428859702732		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.04679428859702732 | validation: 0.046432751973465136]
	TIME [epoch: 12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04344889199045336		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.04344889199045336 | validation: 0.11207946395026468]
	TIME [epoch: 12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07028698801577031		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.07028698801577031 | validation: 0.09634598165480374]
	TIME [epoch: 12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06770655816858603		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.06770655816858603 | validation: 0.04890815076166374]
	TIME [epoch: 12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043112930302710725		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.043112930302710725 | validation: 0.0895629182012598]
	TIME [epoch: 12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07886178321265801		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.07886178321265801 | validation: 0.07228650414523119]
	TIME [epoch: 12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06560474107865384		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.06560474107865384 | validation: 0.09683489557868155]
	TIME [epoch: 12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10621367248398256		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.10621367248398256 | validation: 0.1165016625161266]
	TIME [epoch: 12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06608508898364265		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.06608508898364265 | validation: 0.056053034084317305]
	TIME [epoch: 12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03343444556789824		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.03343444556789824 | validation: 0.0483695865134757]
	TIME [epoch: 12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04036293876194308		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.04036293876194308 | validation: 0.07267033634668672]
	TIME [epoch: 12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07010530396368594		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.07010530396368594 | validation: 0.06202608442007661]
	TIME [epoch: 12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03697985612436057		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.03697985612436057 | validation: 0.051149596555014734]
	TIME [epoch: 12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0568203096082109		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.0568203096082109 | validation: 0.08971580034358012]
	TIME [epoch: 12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04982631406625128		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.04982631406625128 | validation: 0.08136906062067671]
	TIME [epoch: 12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06198332834327084		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.06198332834327084 | validation: 0.054253573727635514]
	TIME [epoch: 12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03996341971365367		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.03996341971365367 | validation: 0.0496905011175969]
	TIME [epoch: 12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05073722965795565		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.05073722965795565 | validation: 0.08063367958799078]
	TIME [epoch: 12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05385006200271904		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.05385006200271904 | validation: 0.077048439393591]
	TIME [epoch: 12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062323882305416786		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.062323882305416786 | validation: 0.0815867459499247]
	TIME [epoch: 12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05070063300200418		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.05070063300200418 | validation: 0.04948483844261164]
	TIME [epoch: 12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05391257668060147		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.05391257668060147 | validation: 0.07387621222390353]
	TIME [epoch: 12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06277952674919281		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.06277952674919281 | validation: 0.06270755456580338]
	TIME [epoch: 12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0349028287662874		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.0349028287662874 | validation: 0.046279973897586396]
	TIME [epoch: 12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05957744303323624		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.05957744303323624 | validation: 0.07249200599887926]
	TIME [epoch: 12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07342559018374066		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.07342559018374066 | validation: 0.054558349571496335]
	TIME [epoch: 12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047487438052606815		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.047487438052606815 | validation: 0.06966521348482924]
	TIME [epoch: 12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041010462617613465		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.041010462617613465 | validation: 0.04230546033061018]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0421599336274105		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.0421599336274105 | validation: 0.11107785388501848]
	TIME [epoch: 12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08004190961480298		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.08004190961480298 | validation: 0.05550466600616685]
	TIME [epoch: 12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03450162014094574		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.03450162014094574 | validation: 0.052343540299787744]
	TIME [epoch: 12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05337524889604472		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.05337524889604472 | validation: 0.06286334675732341]
	TIME [epoch: 12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05845114758342399		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.05845114758342399 | validation: 0.08814667579041038]
	TIME [epoch: 12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04104002011761382		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.04104002011761382 | validation: 0.07925634369932658]
	TIME [epoch: 12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05930094405341094		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.05930094405341094 | validation: 0.041149557640163904]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035767210221435634		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.035767210221435634 | validation: 0.06765167216012011]
	TIME [epoch: 12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057220269995435905		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.057220269995435905 | validation: 0.09767332141606713]
	TIME [epoch: 12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05444984727936634		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.05444984727936634 | validation: 0.10761980747719926]
	TIME [epoch: 12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06707702765435197		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.06707702765435197 | validation: 0.05063891942143246]
	TIME [epoch: 12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032103324014809004		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.032103324014809004 | validation: 0.06891610207580243]
	TIME [epoch: 12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0406945655976638		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.0406945655976638 | validation: 0.08769022629954348]
	TIME [epoch: 12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04714408973895515		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.04714408973895515 | validation: 0.0770481866011343]
	TIME [epoch: 12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06114845333574566		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.06114845333574566 | validation: 0.05476015299114345]
	TIME [epoch: 12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04602236063410633		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.04602236063410633 | validation: 0.048012214471886375]
	TIME [epoch: 12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03769680541515211		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.03769680541515211 | validation: 0.06808038552296565]
	TIME [epoch: 12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043099936194946946		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.043099936194946946 | validation: 0.057078975420907896]
	TIME [epoch: 12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03770679078971301		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.03770679078971301 | validation: 0.05620873028961067]
	TIME [epoch: 12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06257433978094265		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.06257433978094265 | validation: 0.07317283859546861]
	TIME [epoch: 12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03753008471088157		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.03753008471088157 | validation: 0.06034283658819839]
	TIME [epoch: 12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05231721976419419		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.05231721976419419 | validation: 0.04266040004876257]
	TIME [epoch: 12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03296439746095645		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.03296439746095645 | validation: 0.09926807604828113]
	TIME [epoch: 12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060741004287174286		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.060741004287174286 | validation: 0.061865208546672296]
	TIME [epoch: 12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03211279294417047		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.03211279294417047 | validation: 0.05455202493727759]
	TIME [epoch: 12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05990325366354861		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.05990325366354861 | validation: 0.06563178530677019]
	TIME [epoch: 12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03962785053656953		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.03962785053656953 | validation: 0.07572114564120258]
	TIME [epoch: 12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05919835647931885		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.05919835647931885 | validation: 0.0517110099970755]
	TIME [epoch: 12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04061473162629381		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.04061473162629381 | validation: 0.057978103134211985]
	TIME [epoch: 12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03714264421149853		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.03714264421149853 | validation: 0.04926917784447475]
	TIME [epoch: 12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04889321873406564		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.04889321873406564 | validation: 0.05413409308771962]
	TIME [epoch: 12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041120153882810664		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.041120153882810664 | validation: 0.04598815469226572]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03720504739810852		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.03720504739810852 | validation: 0.04887177725229119]
	TIME [epoch: 12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03459762055144068		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.03459762055144068 | validation: 0.06911457998573847]
	TIME [epoch: 12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049739921150169286		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.049739921150169286 | validation: 0.08373780606291911]
	TIME [epoch: 12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05934437464586659		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.05934437464586659 | validation: 0.0696890749238504]
	TIME [epoch: 12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03494322104592051		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.03494322104592051 | validation: 0.03882773068488247]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03465659079087635		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.03465659079087635 | validation: 0.10393904203256571]
	TIME [epoch: 12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17161982034677684		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.17161982034677684 | validation: 0.18388147922946632]
	TIME [epoch: 12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1377715965794629		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.1377715965794629 | validation: 0.09042706216831614]
	TIME [epoch: 12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05865624647338397		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.05865624647338397 | validation: 0.035633719126948335]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03134329759308061		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.03134329759308061 | validation: 0.048047318829483564]
	TIME [epoch: 12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03690954227156168		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.03690954227156168 | validation: 0.03632282457125885]
	TIME [epoch: 12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04389978469653776		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.04389978469653776 | validation: 0.06804269233395477]
	TIME [epoch: 12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04882335033885881		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.04882335033885881 | validation: 0.04304115320083414]
	TIME [epoch: 12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02731208801803746		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.02731208801803746 | validation: 0.04164531905646766]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02940600551490164		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.02940600551490164 | validation: 0.06129555665845336]
	TIME [epoch: 12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04422656950298575		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.04422656950298575 | validation: 0.05273937566162121]
	TIME [epoch: 12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035549861903582496		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.035549861903582496 | validation: 0.06306130063204976]
	TIME [epoch: 12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047216648939312975		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.047216648939312975 | validation: 0.04002598109155757]
	TIME [epoch: 12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041667741424738836		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.041667741424738836 | validation: 0.06812425171965014]
	TIME [epoch: 12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028153199647044408		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.028153199647044408 | validation: 0.0442300071911519]
	TIME [epoch: 12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04481860100259475		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.04481860100259475 | validation: 0.04893737820892009]
	TIME [epoch: 12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044713511576693926		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.044713511576693926 | validation: 0.04987928640182668]
	TIME [epoch: 12 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03204009054650196		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.03204009054650196 | validation: 0.04234012248267992]
	TIME [epoch: 12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04320733412284325		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.04320733412284325 | validation: 0.06065640197196243]
	TIME [epoch: 12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03813404364232188		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.03813404364232188 | validation: 0.03706959879997403]
	TIME [epoch: 12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038416767725025405		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.038416767725025405 | validation: 0.052864321794636224]
	TIME [epoch: 12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03853225160155875		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.03853225160155875 | validation: 0.05608518093077221]
	TIME [epoch: 12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037516246957445674		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.037516246957445674 | validation: 0.045998098638027056]
	TIME [epoch: 12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04026805458357682		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.04026805458357682 | validation: 0.06726946440427627]
	TIME [epoch: 12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03703608872080002		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.03703608872080002 | validation: 0.0511204350653027]
	TIME [epoch: 12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03029277967555139		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.03029277967555139 | validation: 0.049041841361854334]
	TIME [epoch: 12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0396160675401044		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.0396160675401044 | validation: 0.08084474460555582]
	TIME [epoch: 12 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06133237167817253		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.06133237167817253 | validation: 0.05395526946614386]
	TIME [epoch: 12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029936145842472665		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.029936145842472665 | validation: 0.03815314772856804]
	TIME [epoch: 12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028574111814995112		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.028574111814995112 | validation: 0.06363923969568905]
	TIME [epoch: 12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049862412159774216		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.049862412159774216 | validation: 0.06470497502454052]
	TIME [epoch: 12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035783949406237296		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.035783949406237296 | validation: 0.065612807528516]
	TIME [epoch: 12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027003553462103307		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.027003553462103307 | validation: 0.038104794902947575]
	TIME [epoch: 12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030839087889265288		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.030839087889265288 | validation: 0.11212599495210865]
	TIME [epoch: 12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06320125254713174		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.06320125254713174 | validation: 0.046393106360601594]
	TIME [epoch: 12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030027132686496084		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.030027132686496084 | validation: 0.03927686426094546]
	TIME [epoch: 12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0436169161192749		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.0436169161192749 | validation: 0.07098599435832323]
	TIME [epoch: 12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051067548490715944		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.051067548490715944 | validation: 0.04425517970061557]
	TIME [epoch: 12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03760068912611811		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.03760068912611811 | validation: 0.04492806006195604]
	TIME [epoch: 12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026296558874060308		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.026296558874060308 | validation: 0.03952778705411429]
	TIME [epoch: 12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03143396618527979		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.03143396618527979 | validation: 0.055596030901152296]
	TIME [epoch: 12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045147619354371935		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.045147619354371935 | validation: 0.041179773179553764]
	TIME [epoch: 12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03112080216166567		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.03112080216166567 | validation: 0.07546530293630589]
	TIME [epoch: 12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03621916326528667		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.03621916326528667 | validation: 0.043460205076230204]
	TIME [epoch: 12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030816297466198656		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.030816297466198656 | validation: 0.06746262502328806]
	TIME [epoch: 12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04181679671827428		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.04181679671827428 | validation: 0.036102921663056786]
	TIME [epoch: 12 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03131830524131836		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.03131830524131836 | validation: 0.04871443503812708]
	TIME [epoch: 12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033335210476730104		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.033335210476730104 | validation: 0.05066111368541106]
	TIME [epoch: 12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0408860797308421		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.0408860797308421 | validation: 0.049000549599205676]
	TIME [epoch: 12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026501937618151367		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.026501937618151367 | validation: 0.03516589608840995]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023975528322850985		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.023975528322850985 | validation: 0.06117054451297907]
	TIME [epoch: 12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05195556265021646		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.05195556265021646 | validation: 0.03742853957016706]
	TIME [epoch: 12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02587956588538809		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.02587956588538809 | validation: 0.05410621989982545]
	TIME [epoch: 12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03401077293592477		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.03401077293592477 | validation: 0.048346376061914345]
	TIME [epoch: 12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03501678952176154		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03501678952176154 | validation: 0.04571396851694267]
	TIME [epoch: 12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04591631462028792		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.04591631462028792 | validation: 0.03670775471403175]
	TIME [epoch: 12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029497274669221966		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.029497274669221966 | validation: 0.039277965094743586]
	TIME [epoch: 12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038175633449883714		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.038175633449883714 | validation: 0.03524542619649906]
	TIME [epoch: 12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029686258795311234		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.029686258795311234 | validation: 0.03990463009115624]
	TIME [epoch: 12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03014485909503266		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.03014485909503266 | validation: 0.06903699649395867]
	TIME [epoch: 12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03879723298184018		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03879723298184018 | validation: 0.0399982663662596]
	TIME [epoch: 12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0365040702217645		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.0365040702217645 | validation: 0.044363287152757516]
	TIME [epoch: 12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035512460357242176		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.035512460357242176 | validation: 0.048630029254901924]
	TIME [epoch: 12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029770619913299316		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.029770619913299316 | validation: 0.03984336103976473]
	TIME [epoch: 12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04142352176920432		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.04142352176920432 | validation: 0.04523421637762037]
	TIME [epoch: 12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02675444914292132		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.02675444914292132 | validation: 0.046525888243237515]
	TIME [epoch: 12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03665032609083932		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.03665032609083932 | validation: 0.050719300493347295]
	TIME [epoch: 12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03104538046163586		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.03104538046163586 | validation: 0.04029915690117933]
	TIME [epoch: 12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02897159503446542		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.02897159503446542 | validation: 0.05112045564376122]
	TIME [epoch: 12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030387110257667514		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.030387110257667514 | validation: 0.05445266572688115]
	TIME [epoch: 12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033228661479084		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.033228661479084 | validation: 0.041934776561576]
	TIME [epoch: 12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028933531624602674		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.028933531624602674 | validation: 0.056952818181876495]
	TIME [epoch: 12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037451673660490875		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.037451673660490875 | validation: 0.038413586853266915]
	TIME [epoch: 12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021209606363466662		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.021209606363466662 | validation: 0.053843057807717795]
	TIME [epoch: 12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03790515559987318		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.03790515559987318 | validation: 0.046006825817322225]
	TIME [epoch: 12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562614870971071		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.03562614870971071 | validation: 0.0394755520279048]
	TIME [epoch: 12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02466718490198531		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.02466718490198531 | validation: 0.04018226243515498]
	TIME [epoch: 12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029640243127672983		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.029640243127672983 | validation: 0.0752556188790196]
	TIME [epoch: 12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03716609259222099		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.03716609259222099 | validation: 0.04875450571251616]
	TIME [epoch: 12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039499388129417994		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.039499388129417994 | validation: 0.03648133334080432]
	TIME [epoch: 12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024481327364648062		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.024481327364648062 | validation: 0.03323438485382696]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023067099316087872		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.023067099316087872 | validation: 0.04319309430856653]
	TIME [epoch: 12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03541311799545689		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03541311799545689 | validation: 0.05171577560426946]
	TIME [epoch: 12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03075590856190124		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.03075590856190124 | validation: 0.0613173290135845]
	TIME [epoch: 12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033407243678301404		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.033407243678301404 | validation: 0.05593689222600051]
	TIME [epoch: 12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03341217889828664		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.03341217889828664 | validation: 0.03466536540966007]
	TIME [epoch: 12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026068053814115547		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.026068053814115547 | validation: 0.06193011526013169]
	TIME [epoch: 12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03522852911619225		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.03522852911619225 | validation: 0.033628891542309275]
	TIME [epoch: 12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023653322398902984		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.023653322398902984 | validation: 0.04703507060770012]
	TIME [epoch: 12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03066340797520017		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.03066340797520017 | validation: 0.03443659844651539]
	TIME [epoch: 12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024045598739458603		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.024045598739458603 | validation: 0.07656413465980808]
	TIME [epoch: 12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04385447145403955		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.04385447145403955 | validation: 0.08919553845633077]
	TIME [epoch: 12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07025838228054938		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.07025838228054938 | validation: 0.038369934579227535]
	TIME [epoch: 12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030725322169415613		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.030725322169415613 | validation: 0.048696772454176064]
	TIME [epoch: 12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02824691614657387		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.02824691614657387 | validation: 0.04180639075457257]
	TIME [epoch: 12 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022081183725023493		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.022081183725023493 | validation: 0.0436403099337682]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03457833590521979		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.03457833590521979 | validation: 0.03635227547498489]
	TIME [epoch: 12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021516320224090127		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.021516320224090127 | validation: 0.04039479755693947]
	TIME [epoch: 12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04548990759394604		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.04548990759394604 | validation: 0.034349480221707085]
	TIME [epoch: 12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02932296552917372		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.02932296552917372 | validation: 0.11058500366142962]
	TIME [epoch: 12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05113349212520639		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.05113349212520639 | validation: 0.035408778646818906]
	TIME [epoch: 12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027935684810507934		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.027935684810507934 | validation: 0.0452710156809769]
	TIME [epoch: 12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028365753148494995		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.028365753148494995 | validation: 0.03378883291525196]
	TIME [epoch: 403 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020935325478972852		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.020935325478972852 | validation: 0.04165478925656612]
	TIME [epoch: 25.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026944097754667735		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.026944097754667735 | validation: 0.04679519154339238]
	TIME [epoch: 25.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026378794366824884		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.026378794366824884 | validation: 0.061427135281832954]
	TIME [epoch: 25.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03624017881175649		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.03624017881175649 | validation: 0.04128902308857349]
	TIME [epoch: 25.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022799250315709778		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.022799250315709778 | validation: 0.03467150537890781]
	TIME [epoch: 25.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03054563205944445		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.03054563205944445 | validation: 0.053059231105085064]
	TIME [epoch: 25.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029162217976950833		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.029162217976950833 | validation: 0.0339004772478705]
	TIME [epoch: 25.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019340822403201608		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.019340822403201608 | validation: 0.06726077533674943]
	TIME [epoch: 25.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038531009901077086		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.038531009901077086 | validation: 0.03697580508885971]
	TIME [epoch: 25.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02245439636552869		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.02245439636552869 | validation: 0.1084770405130768]
	TIME [epoch: 25.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051137453654048606		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.051137453654048606 | validation: 0.03418797743021983]
	TIME [epoch: 25.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02195302810673084		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.02195302810673084 | validation: 0.044008085917011]
	TIME [epoch: 25.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02921177078425851		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.02921177078425851 | validation: 0.04289254270491459]
	TIME [epoch: 25.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029197095406599677		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.029197095406599677 | validation: 0.03569320255101136]
	TIME [epoch: 25.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028873026091618806		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.028873026091618806 | validation: 0.03277952857525681]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02041703474497991		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.02041703474497991 | validation: 0.03948551920255315]
	TIME [epoch: 25.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025961321426372225		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.025961321426372225 | validation: 0.04183074706605258]
	TIME [epoch: 25.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025948678863663126		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.025948678863663126 | validation: 0.05456417012197176]
	TIME [epoch: 25.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021629569099032558		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.021629569099032558 | validation: 0.04178369838486237]
	TIME [epoch: 25.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029780361107576564		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.029780361107576564 | validation: 0.055445859945286974]
	TIME [epoch: 25.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03698867338570827		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.03698867338570827 | validation: 0.033762200341077436]
	TIME [epoch: 25.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01928263573206411		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.01928263573206411 | validation: 0.033423674119960146]
	TIME [epoch: 25.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019842981701268247		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.019842981701268247 | validation: 0.03821512886041252]
	TIME [epoch: 25.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02854410985107561		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.02854410985107561 | validation: 0.042114173847620444]
	TIME [epoch: 25.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024065530559120422		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.024065530559120422 | validation: 0.037847344937551756]
	TIME [epoch: 25.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018204558089060192		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.018204558089060192 | validation: 0.049046167009017136]
	TIME [epoch: 25.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02855314397751903		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.02855314397751903 | validation: 0.030560601776499816]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022843785739578935		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.022843785739578935 | validation: 0.04641534041650046]
	TIME [epoch: 25.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030279196211847076		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.030279196211847076 | validation: 0.038392683060526595]
	TIME [epoch: 25.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018347070693217064		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.018347070693217064 | validation: 0.035163383251789825]
	TIME [epoch: 25.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026973567022380348		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.026973567022380348 | validation: 0.06596212986151488]
	TIME [epoch: 25.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028595142672088094		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.028595142672088094 | validation: 0.036876387441408254]
	TIME [epoch: 25.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019404340768450008		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.019404340768450008 | validation: 0.05096889662703702]
	TIME [epoch: 25.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029874400009599778		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.029874400009599778 | validation: 0.03513495875533447]
	TIME [epoch: 25.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023429035043928292		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.023429035043928292 | validation: 0.027946331217842523]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019414212980388496		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.019414212980388496 | validation: 0.04225877355982656]
	TIME [epoch: 25.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03441759787015523		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.03441759787015523 | validation: 0.034282003899142835]
	TIME [epoch: 25.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025886036743742565		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.025886036743742565 | validation: 0.02770857504654152]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01906323374405741		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.01906323374405741 | validation: 0.03965465033685287]
	TIME [epoch: 25.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022784489937461166		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.022784489937461166 | validation: 0.03365874522869379]
	TIME [epoch: 25.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020042718905168042		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.020042718905168042 | validation: 0.045769813493889464]
	TIME [epoch: 25.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026107099682815535		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.026107099682815535 | validation: 0.03223801667518922]
	TIME [epoch: 25.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026380531566558625		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.026380531566558625 | validation: 0.05853449171416085]
	TIME [epoch: 25.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028004297931725264		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.028004297931725264 | validation: 0.030738559730605276]
	TIME [epoch: 25.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023741228022264076		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.023741228022264076 | validation: 0.03743078007607336]
	TIME [epoch: 25.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019286197512616145		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.019286197512616145 | validation: 0.034726202012252384]
	TIME [epoch: 25.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023395920075755895		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.023395920075755895 | validation: 0.04080758283988216]
	TIME [epoch: 25.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02356079640797603		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.02356079640797603 | validation: 0.03191665272879668]
	TIME [epoch: 25.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020439257195724574		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.020439257195724574 | validation: 0.047839117869287795]
	TIME [epoch: 25.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022792457347407816		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.022792457347407816 | validation: 0.02845821233646259]
	TIME [epoch: 25.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02558941708801278		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.02558941708801278 | validation: 0.04146761419456313]
	TIME [epoch: 25.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027998738549157506		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.027998738549157506 | validation: 0.03162058991207392]
	TIME [epoch: 25.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021713840050281007		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.021713840050281007 | validation: 0.038815447908284666]
	TIME [epoch: 25.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02059282503969577		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.02059282503969577 | validation: 0.03665883176954994]
	TIME [epoch: 25.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01968173093021996		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.01968173093021996 | validation: 0.036430352404207306]
	TIME [epoch: 25.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020339416308820128		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.020339416308820128 | validation: 0.037691839983613584]
	TIME [epoch: 25.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03173709800951198		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.03173709800951198 | validation: 0.028393705007054085]
	TIME [epoch: 25.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02050548176045587		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.02050548176045587 | validation: 0.033537917225571534]
	TIME [epoch: 25.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02024190877806026		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.02024190877806026 | validation: 0.043127399718516396]
	TIME [epoch: 25.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018430189696593196		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.018430189696593196 | validation: 0.033067313140541836]
	TIME [epoch: 25.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025351309062351977		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.025351309062351977 | validation: 0.05787891500609102]
	TIME [epoch: 25.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030431632435857874		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.030431632435857874 | validation: 0.032625638514592]
	TIME [epoch: 25.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016571555310577423		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.016571555310577423 | validation: 0.032296932008858734]
	TIME [epoch: 25.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02061101204233769		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.02061101204233769 | validation: 0.030394703554918835]
	TIME [epoch: 25.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021309391134779122		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.021309391134779122 | validation: 0.03850998392116205]
	TIME [epoch: 25.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019706641111883215		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.019706641111883215 | validation: 0.03046748592462918]
	TIME [epoch: 25.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028239823354152965		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.028239823354152965 | validation: 0.02924444157534036]
	TIME [epoch: 25.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01624723912684086		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.01624723912684086 | validation: 0.03211733617873154]
	TIME [epoch: 25.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021123109392557862		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.021123109392557862 | validation: 0.03600957033240074]
	TIME [epoch: 25.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017086545081718692		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.017086545081718692 | validation: 0.030061923157519627]
	TIME [epoch: 25.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02796407663358788		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.02796407663358788 | validation: 0.03239575646290871]
	TIME [epoch: 25.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022946663818759067		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.022946663818759067 | validation: 0.03587580477419816]
	TIME [epoch: 25.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01860786994853815		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.01860786994853815 | validation: 0.031340465044455396]
	TIME [epoch: 25.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017499918302674392		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.017499918302674392 | validation: 0.03840923466271001]
	TIME [epoch: 25.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021019728187654983		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.021019728187654983 | validation: 0.04305687066368295]
	TIME [epoch: 25.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024533052772843306		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.024533052772843306 | validation: 0.030302883902877917]
	TIME [epoch: 25.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020775008794063		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.020775008794063 | validation: 0.032663960386796205]
	TIME [epoch: 25.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018938865622720916		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.018938865622720916 | validation: 0.02710112698125364]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017973943228629295		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.017973943228629295 | validation: 0.038261502270966335]
	TIME [epoch: 25.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02366400165790624		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.02366400165790624 | validation: 0.03171976534822538]
	TIME [epoch: 25.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02077120505955743		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.02077120505955743 | validation: 0.04605613383607912]
	TIME [epoch: 25.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021097338553948068		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.021097338553948068 | validation: 0.037570740490292756]
	TIME [epoch: 25.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024673494083358		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.024673494083358 | validation: 0.02776139650391433]
	TIME [epoch: 25.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0175454466338913		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0175454466338913 | validation: 0.031206476430673615]
	TIME [epoch: 25.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02332907520073667		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.02332907520073667 | validation: 0.042443826221541726]
	TIME [epoch: 25.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024973322933292873		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.024973322933292873 | validation: 0.03070457827052138]
	TIME [epoch: 25.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018694653719036423		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.018694653719036423 | validation: 0.03579728549601565]
	TIME [epoch: 25.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017820705620713324		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.017820705620713324 | validation: 0.03026219650495128]
	TIME [epoch: 25.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01859798719884764		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.01859798719884764 | validation: 0.03027188383243059]
	TIME [epoch: 25.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01804154042906702		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.01804154042906702 | validation: 0.032914138639550145]
	TIME [epoch: 25.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017007923826178103		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.017007923826178103 | validation: 0.0314193857177837]
	TIME [epoch: 25.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025445690429929408		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.025445690429929408 | validation: 0.028884580454150505]
	TIME [epoch: 25.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018298717734910576		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.018298717734910576 | validation: 0.027172200405063365]
	TIME [epoch: 25.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018153547161053395		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.018153547161053395 | validation: 0.03863830115097965]
	TIME [epoch: 25.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020142111025198733		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.020142111025198733 | validation: 0.035351305977098714]
	TIME [epoch: 25.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016493921773304485		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.016493921773304485 | validation: 0.02917939249003681]
	TIME [epoch: 25.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019141646245015704		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.019141646245015704 | validation: 0.030514669842291536]
	TIME [epoch: 25.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02129834647410443		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.02129834647410443 | validation: 0.03061214988054998]
	TIME [epoch: 25.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016475259847463713		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.016475259847463713 | validation: 0.046282010892599076]
	TIME [epoch: 25.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019868195427826293		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.019868195427826293 | validation: 0.028806078202668884]
	TIME [epoch: 25.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017271395011671034		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.017271395011671034 | validation: 0.0316321173678862]
	TIME [epoch: 25.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020039759233137392		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.020039759233137392 | validation: 0.03351867472064857]
	TIME [epoch: 25.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01696862644743102		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.01696862644743102 | validation: 0.024477742203115386]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016662203538570834		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.016662203538570834 | validation: 0.034654002048587755]
	TIME [epoch: 25.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017239098117262194		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.017239098117262194 | validation: 0.029878764842292016]
	TIME [epoch: 25.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023522615553795086		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.023522615553795086 | validation: 0.029033225597398418]
	TIME [epoch: 25.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016185509019148843		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.016185509019148843 | validation: 0.026229592448717712]
	TIME [epoch: 25.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015939190156633275		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.015939190156633275 | validation: 0.030242662500006047]
	TIME [epoch: 25.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01978004026011771		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.01978004026011771 | validation: 0.029662931569266877]
	TIME [epoch: 25.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022768439501243173		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.022768439501243173 | validation: 0.029443569464660854]
	TIME [epoch: 25.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019271311003755533		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.019271311003755533 | validation: 0.03456291615774403]
	TIME [epoch: 25.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01784470810134144		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.01784470810134144 | validation: 0.031075695007096914]
	TIME [epoch: 25.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01874780875952376		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.01874780875952376 | validation: 0.03519587655932373]
	TIME [epoch: 25.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019691306261688163		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.019691306261688163 | validation: 0.03496385895692157]
	TIME [epoch: 25.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016421365615867333		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.016421365615867333 | validation: 0.03403504437561573]
	TIME [epoch: 25.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023183516450407858		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.023183516450407858 | validation: 0.02920367414143308]
	TIME [epoch: 25.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01623064020256239		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.01623064020256239 | validation: 0.02814604741820663]
	TIME [epoch: 25.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01572759604814869		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.01572759604814869 | validation: 0.046975632313112914]
	TIME [epoch: 25.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02310358885324282		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.02310358885324282 | validation: 0.0283874831161523]
	TIME [epoch: 25.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014739270966545648		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.014739270966545648 | validation: 0.03311063738092937]
	TIME [epoch: 25.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018392578461736666		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.018392578461736666 | validation: 0.03180550557834682]
	TIME [epoch: 25.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015092889083836394		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.015092889083836394 | validation: 0.0541553975680972]
	TIME [epoch: 25.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025766685291505984		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.025766685291505984 | validation: 0.03303207553025393]
	TIME [epoch: 25.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019459154167644876		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.019459154167644876 | validation: 0.030694576768832922]
	TIME [epoch: 25.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016327189585419694		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.016327189585419694 | validation: 0.029761680467666033]
	TIME [epoch: 25.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01848252704519868		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.01848252704519868 | validation: 0.026230348918669426]
	TIME [epoch: 25.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0162819145306507		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.0162819145306507 | validation: 0.027640147697719403]
	TIME [epoch: 25.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015608045708173446		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.015608045708173446 | validation: 0.029366296290305606]
	TIME [epoch: 25.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016806322866176688		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.016806322866176688 | validation: 0.036010169094988886]
	TIME [epoch: 25.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02010987776971014		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.02010987776971014 | validation: 0.026708188443818396]
	TIME [epoch: 25.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015847448751054833		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.015847448751054833 | validation: 0.02924283323316165]
	TIME [epoch: 25.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014780315025527995		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.014780315025527995 | validation: 0.03317030360049539]
	TIME [epoch: 25.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037627434406413196		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.037627434406413196 | validation: 0.032170366718119604]
	TIME [epoch: 25.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018949812874459522		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.018949812874459522 | validation: 0.026202633351315802]
	TIME [epoch: 25.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013539682658210239		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.013539682658210239 | validation: 0.028200233298572512]
	TIME [epoch: 25.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018098440279669246		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.018098440279669246 | validation: 0.03116781346921541]
	TIME [epoch: 25.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01664941482217453		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.01664941482217453 | validation: 0.028599138863583427]
	TIME [epoch: 25.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015508168827979604		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.015508168827979604 | validation: 0.02562402282510568]
	TIME [epoch: 25.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015472164654431237		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.015472164654431237 | validation: 0.022866141421312006]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01544337247869081		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.01544337247869081 | validation: 0.04189421429609767]
	TIME [epoch: 25.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018986469705596025		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.018986469705596025 | validation: 0.03673949340866132]
	TIME [epoch: 25.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018299161365147164		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.018299161365147164 | validation: 0.03047175447526515]
	TIME [epoch: 25.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015293355413042048		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.015293355413042048 | validation: 0.028339326279213116]
	TIME [epoch: 25.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016056239009530127		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.016056239009530127 | validation: 0.031081396097844705]
	TIME [epoch: 25.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016961248728063424		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.016961248728063424 | validation: 0.025816529508673906]
	TIME [epoch: 25.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01624773956968812		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.01624773956968812 | validation: 0.03262594388386261]
	TIME [epoch: 25.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01858548590558407		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.01858548590558407 | validation: 0.02656498972486855]
	TIME [epoch: 25.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012615361703494461		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.012615361703494461 | validation: 0.026699756233627758]
	TIME [epoch: 25.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02176639951998103		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.02176639951998103 | validation: 0.028527257808854428]
	TIME [epoch: 25.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016968542745822678		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.016968542745822678 | validation: 0.02470922410871528]
	TIME [epoch: 25.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013747839661701646		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.013747839661701646 | validation: 0.03451896636288308]
	TIME [epoch: 25.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019168996573726674		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.019168996573726674 | validation: 0.029183795011751903]
	TIME [epoch: 25.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015072895885547801		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.015072895885547801 | validation: 0.025712881735029815]
	TIME [epoch: 25.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013216772725011355		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.013216772725011355 | validation: 0.022976321052938016]
	TIME [epoch: 25.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014908329476954302		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.014908329476954302 | validation: 0.036744476652505534]
	TIME [epoch: 25.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01653552642128608		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.01653552642128608 | validation: 0.029982605020506134]
	TIME [epoch: 25.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020387880664487695		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.020387880664487695 | validation: 0.030543719218167297]
	TIME [epoch: 25.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01333924235239526		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.01333924235239526 | validation: 0.023638953471405544]
	TIME [epoch: 25.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015545442084648722		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.015545442084648722 | validation: 0.03808519524366008]
	TIME [epoch: 25.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016889823152444774		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.016889823152444774 | validation: 0.024769581978434863]
	TIME [epoch: 25.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014289863980226154		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.014289863980226154 | validation: 0.025869553137806152]
	TIME [epoch: 25.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0171775654742784		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0171775654742784 | validation: 0.030654336808209993]
	TIME [epoch: 25.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018872709390582776		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.018872709390582776 | validation: 0.0239472254466888]
	TIME [epoch: 25.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015051303065675998		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.015051303065675998 | validation: 0.02707776952957278]
	TIME [epoch: 25.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01407740349446612		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.01407740349446612 | validation: 0.030111952529667987]
	TIME [epoch: 25.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017893162547843122		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.017893162547843122 | validation: 0.025527243464187624]
	TIME [epoch: 25.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01591944542514671		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.01591944542514671 | validation: 0.029243521745144822]
	TIME [epoch: 25.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014701076876288074		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.014701076876288074 | validation: 0.02249959625704908]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013931902608676683		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.013931902608676683 | validation: 0.026420033111848342]
	TIME [epoch: 25.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016882125889106387		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.016882125889106387 | validation: 0.02493820949627052]
	TIME [epoch: 25.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012593248372544528		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.012593248372544528 | validation: 0.024464069507782876]
	TIME [epoch: 25.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013960451191789873		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.013960451191789873 | validation: 0.023418904256533965]
	TIME [epoch: 25.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015818120958322072		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.015818120958322072 | validation: 0.03805205191538619]
	TIME [epoch: 25.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01737241765245671		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.01737241765245671 | validation: 0.03138991459736493]
	TIME [epoch: 25.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014222707242158057		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.014222707242158057 | validation: 0.028176236739563244]
	TIME [epoch: 25.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015368081823706905		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.015368081823706905 | validation: 0.028643669896973797]
	TIME [epoch: 25.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01325142817709338		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.01325142817709338 | validation: 0.026892866408595163]
	TIME [epoch: 25.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017363264222237714		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.017363264222237714 | validation: 0.024295953833648826]
	TIME [epoch: 25.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014790843589762855		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.014790843589762855 | validation: 0.028309118580705367]
	TIME [epoch: 25.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014659826593978607		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.014659826593978607 | validation: 0.024582770432432993]
	TIME [epoch: 25.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014394514173655417		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.014394514173655417 | validation: 0.036898219206704996]
	TIME [epoch: 25.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02389699375975847		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.02389699375975847 | validation: 0.026726075526007483]
	TIME [epoch: 25.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014706349634563232		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.014706349634563232 | validation: 0.02933494878511466]
	TIME [epoch: 25.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013522218364379422		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.013522218364379422 | validation: 0.024498980817060964]
	TIME [epoch: 25.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014028349727519554		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.014028349727519554 | validation: 0.03466047471895189]
	TIME [epoch: 25.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017582161198581053		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.017582161198581053 | validation: 0.02675476950696945]
	TIME [epoch: 25.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013738467344162276		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.013738467344162276 | validation: 0.02605842307256369]
	TIME [epoch: 25.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012436544803921137		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.012436544803921137 | validation: 0.027172635534417275]
	TIME [epoch: 25.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014902972628080952		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.014902972628080952 | validation: 0.02694732534944385]
	TIME [epoch: 25.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013730048711524823		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.013730048711524823 | validation: 0.026068327094074736]
	TIME [epoch: 25.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015835415057671495		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.015835415057671495 | validation: 0.03127384409128372]
	TIME [epoch: 25.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01237521293872573		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.01237521293872573 | validation: 0.026486857131410586]
	TIME [epoch: 25.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017464791884714566		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.017464791884714566 | validation: 0.027441438756829355]
	TIME [epoch: 25.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013673352635909106		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.013673352635909106 | validation: 0.024032756876629726]
	TIME [epoch: 25.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013456197875564117		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.013456197875564117 | validation: 0.02636328701882012]
	TIME [epoch: 25.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013146030768387974		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.013146030768387974 | validation: 0.02420990534204485]
	TIME [epoch: 25.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014348101187460464		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.014348101187460464 | validation: 0.030833261757448785]
	TIME [epoch: 25.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015737718506965318		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.015737718506965318 | validation: 0.024963380824090005]
	TIME [epoch: 25.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013406987579004551		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.013406987579004551 | validation: 0.026269328218015156]
	TIME [epoch: 25.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012234229590807924		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.012234229590807924 | validation: 0.025093773340079]
	TIME [epoch: 25.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017757263589211134		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.017757263589211134 | validation: 0.02704456929995453]
	TIME [epoch: 25.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014539257483071569		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.014539257483071569 | validation: 0.0288393557243956]
	TIME [epoch: 25.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013814154775918958		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.013814154775918958 | validation: 0.024404169710997552]
	TIME [epoch: 25.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013352051681903988		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.013352051681903988 | validation: 0.030201008001633667]
	TIME [epoch: 25.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014915582016726357		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.014915582016726357 | validation: 0.02804023496875197]
	TIME [epoch: 25.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012905119442061509		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.012905119442061509 | validation: 0.022762906434098135]
	TIME [epoch: 25.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014707765923485447		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.014707765923485447 | validation: 0.025690064816628347]
	TIME [epoch: 25.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01408239581705565		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.01408239581705565 | validation: 0.023721776169751983]
	TIME [epoch: 25.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012663041867811954		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.012663041867811954 | validation: 0.02730889239781388]
	TIME [epoch: 25.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01248209300583838		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.01248209300583838 | validation: 0.0332019236372831]
	TIME [epoch: 25.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015477862368943057		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.015477862368943057 | validation: 0.029502814283657038]
	TIME [epoch: 25.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013383932062935044		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.013383932062935044 | validation: 0.023099014221215786]
	TIME [epoch: 25.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012535739013730365		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.012535739013730365 | validation: 0.024059802338210232]
	TIME [epoch: 25.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023204197302435534		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.023204197302435534 | validation: 0.04672498141131916]
	TIME [epoch: 25.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02923356058691742		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.02923356058691742 | validation: 0.022533564713790553]
	TIME [epoch: 25.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01331744066018035		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.01331744066018035 | validation: 0.02769801891600751]
	TIME [epoch: 25.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013663802030030052		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.013663802030030052 | validation: 0.022887678450774634]
	TIME [epoch: 25.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011080893227288731		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.011080893227288731 | validation: 0.02211957880428416]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012550807921226198		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.012550807921226198 | validation: 0.02229365903059432]
	TIME [epoch: 25.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013667023974586431		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.013667023974586431 | validation: 0.030632954982448668]
	TIME [epoch: 25.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014401243840790492		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.014401243840790492 | validation: 0.023400628860804983]
	TIME [epoch: 25.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012323945640202604		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.012323945640202604 | validation: 0.023798545705851396]
	TIME [epoch: 25.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012335609092342954		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.012335609092342954 | validation: 0.02373305383827665]
	TIME [epoch: 25.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01316948627740419		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.01316948627740419 | validation: 0.02148408894035751]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01484217837103436		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.01484217837103436 | validation: 0.0222751717160566]
	TIME [epoch: 25.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012759148405244593		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.012759148405244593 | validation: 0.02608628009629777]
	TIME [epoch: 25.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013777507749718776		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.013777507749718776 | validation: 0.023490835688659232]
	TIME [epoch: 25.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012591143931971714		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.012591143931971714 | validation: 0.02274355109329252]
	TIME [epoch: 25.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01562467889627801		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.01562467889627801 | validation: 0.022685665183781813]
	TIME [epoch: 25.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012818252654481194		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.012818252654481194 | validation: 0.024711335443998672]
	TIME [epoch: 25.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011802166075611926		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.011802166075611926 | validation: 0.0229826083406634]
	TIME [epoch: 25.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013310287336162574		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.013310287336162574 | validation: 0.024545737942171277]
	TIME [epoch: 25.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01256434833561951		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.01256434833561951 | validation: 0.022577794788802483]
	TIME [epoch: 25.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014531370836375502		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.014531370836375502 | validation: 0.02220762178988185]
	TIME [epoch: 25.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012557075535472648		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.012557075535472648 | validation: 0.02365097341213901]
	TIME [epoch: 25.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013015424385385976		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.013015424385385976 | validation: 0.023999551120611848]
	TIME [epoch: 25.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012794285682086688		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.012794285682086688 | validation: 0.023065644343915247]
	TIME [epoch: 25.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014649551565664996		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.014649551565664996 | validation: 0.023823631998098806]
	TIME [epoch: 25.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012762154708144147		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.012762154708144147 | validation: 0.02626383794585626]
	TIME [epoch: 25.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01354335244104965		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.01354335244104965 | validation: 0.02143005371460895]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014210330197098685		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.014210330197098685 | validation: 0.029921361603199213]
	TIME [epoch: 25.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014029701883478882		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.014029701883478882 | validation: 0.024041243728707168]
	TIME [epoch: 25.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011983643839652667		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.011983643839652667 | validation: 0.023987363957037287]
	TIME [epoch: 25.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012488172189652556		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.012488172189652556 | validation: 0.024318378664690162]
	TIME [epoch: 25.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011164954929359205		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.011164954929359205 | validation: 0.02290873147801404]
	TIME [epoch: 25.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015296177054311964		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.015296177054311964 | validation: 0.023607044185790498]
	TIME [epoch: 25.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01167439470221587		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.01167439470221587 | validation: 0.024496041507236173]
	TIME [epoch: 25.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012262881300997086		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.012262881300997086 | validation: 0.025184121051789896]
	TIME [epoch: 25.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013213305500381002		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.013213305500381002 | validation: 0.02138873269741423]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01287587389450687		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.01287587389450687 | validation: 0.023242500776281122]
	TIME [epoch: 25.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013360997969640971		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.013360997969640971 | validation: 0.02430828753115978]
	TIME [epoch: 25.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012865351215853301		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.012865351215853301 | validation: 0.024367533434884847]
	TIME [epoch: 25.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011948914555405655		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.011948914555405655 | validation: 0.0220321584147079]
	TIME [epoch: 25.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01287672596735709		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.01287672596735709 | validation: 0.02647433339216352]
	TIME [epoch: 25.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012031317988932776		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.012031317988932776 | validation: 0.024590998762978593]
	TIME [epoch: 25.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013551395403023596		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.013551395403023596 | validation: 0.022042544617053232]
	TIME [epoch: 25.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013217153699610337		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.013217153699610337 | validation: 0.020839451582028275]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011887076084039654		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.011887076084039654 | validation: 0.02354343284302985]
	TIME [epoch: 25.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01125087913416764		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.01125087913416764 | validation: 0.022345023999341268]
	TIME [epoch: 25.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011788541248511898		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.011788541248511898 | validation: 0.023671923493988917]
	TIME [epoch: 25.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013500670507232658		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.013500670507232658 | validation: 0.022801726763754943]
	TIME [epoch: 25.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011420226538779974		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.011420226538779974 | validation: 0.023565302734514612]
	TIME [epoch: 25.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013804117285010947		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.013804117285010947 | validation: 0.02321626796897463]
	TIME [epoch: 25.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013118234804052656		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.013118234804052656 | validation: 0.024799011649212575]
	TIME [epoch: 25.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013576852059713982		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.013576852059713982 | validation: 0.021662627579046276]
	TIME [epoch: 25.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010975039071622523		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.010975039071622523 | validation: 0.04910093107926746]
	TIME [epoch: 25.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03225267853341024		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.03225267853341024 | validation: 0.03182662369390035]
	TIME [epoch: 25.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019667590130922775		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.019667590130922775 | validation: 0.022272226965521804]
	TIME [epoch: 25.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012454708812492488		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.012454708812492488 | validation: 0.019456000173191153]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010913853881785491		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.010913853881785491 | validation: 0.020663759387637693]
	TIME [epoch: 25.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012936594584046043		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.012936594584046043 | validation: 0.022127854110614215]
	TIME [epoch: 25.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01296138746864947		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.01296138746864947 | validation: 0.019335253843133653]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01111786699996756		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.01111786699996756 | validation: 0.019184555017535954]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011082707178938455		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.011082707178938455 | validation: 0.02264120468262039]
	TIME [epoch: 25.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015285027735818737		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.015285027735818737 | validation: 0.02102943247252768]
	TIME [epoch: 25.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011666664312558674		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.011666664312558674 | validation: 0.022449728940324293]
	TIME [epoch: 25.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012122272468593731		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.012122272468593731 | validation: 0.02135090621440395]
	TIME [epoch: 25.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010720612320375626		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.010720612320375626 | validation: 0.02333411713762806]
	TIME [epoch: 25.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012333228046785823		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.012333228046785823 | validation: 0.023684839559729515]
	TIME [epoch: 25.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011988551494142219		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.011988551494142219 | validation: 0.021270115627151916]
	TIME [epoch: 25.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011579869328287338		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.011579869328287338 | validation: 0.025188003146345776]
	TIME [epoch: 25.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012206076346763878		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.012206076346763878 | validation: 0.02292979072630526]
	TIME [epoch: 25.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013118293677505563		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.013118293677505563 | validation: 0.025031456936240044]
	TIME [epoch: 25.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019019144313310726		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.019019144313310726 | validation: 0.026643195380006656]
	TIME [epoch: 25.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014641868906279325		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.014641868906279325 | validation: 0.018538342855851576]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011006183901075764		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.011006183901075764 | validation: 0.02089337775123554]
	TIME [epoch: 25.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010603931791509374		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.010603931791509374 | validation: 0.021450371772350867]
	TIME [epoch: 25.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010937684026603266		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.010937684026603266 | validation: 0.02198473804802817]
	TIME [epoch: 25.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011920195189534431		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.011920195189534431 | validation: 0.019627354957068188]
	TIME [epoch: 25.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011049502068634873		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.011049502068634873 | validation: 0.0207635407360688]
	TIME [epoch: 25.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012050404036493944		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.012050404036493944 | validation: 0.02223406160504885]
	TIME [epoch: 25.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011339630080983622		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.011339630080983622 | validation: 0.022134128005592914]
	TIME [epoch: 25.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010769134743955631		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.010769134743955631 | validation: 0.02216560366641443]
	TIME [epoch: 25.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012661455165886645		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.012661455165886645 | validation: 0.02501133551562072]
	TIME [epoch: 25.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012971423656033662		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.012971423656033662 | validation: 0.02126181871524905]
	TIME [epoch: 25.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011108118673860103		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.011108118673860103 | validation: 0.019751089879461968]
	TIME [epoch: 25.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0124318066027548		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.0124318066027548 | validation: 0.02113493734336607]
	TIME [epoch: 25.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010486363503463655		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.010486363503463655 | validation: 0.022641153971137702]
	TIME [epoch: 25.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012052967769564037		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.012052967769564037 | validation: 0.021915483935372647]
	TIME [epoch: 25.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010693575888102016		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.010693575888102016 | validation: 0.02202228090090761]
	TIME [epoch: 25.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011503260912315547		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.011503260912315547 | validation: 0.022335609937273022]
	TIME [epoch: 25.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013552521926918738		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.013552521926918738 | validation: 0.022048993609871365]
	TIME [epoch: 25.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010338984007533058		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.010338984007533058 | validation: 0.023963158534397393]
	TIME [epoch: 25.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011409631843005412		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.011409631843005412 | validation: 0.025314131839864654]
	TIME [epoch: 25.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011543359395819219		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.011543359395819219 | validation: 0.022527767535673357]
	TIME [epoch: 25.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011464680309497356		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.011464680309497356 | validation: 0.02169087832981048]
	TIME [epoch: 25.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010669795715339301		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.010669795715339301 | validation: 0.024404737827685853]
	TIME [epoch: 25.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013354128026894976		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.013354128026894976 | validation: 0.02337345773659567]
	TIME [epoch: 25.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010540502228063625		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.010540502228063625 | validation: 0.021617790551348142]
	TIME [epoch: 25.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011739562084045455		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.011739562084045455 | validation: 0.022336363868414866]
	TIME [epoch: 25.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01148140828765349		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.01148140828765349 | validation: 0.01940017050235339]
	TIME [epoch: 25.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010281960526134056		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.010281960526134056 | validation: 0.021317779826907052]
	TIME [epoch: 25.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010869759311524185		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.010869759311524185 | validation: 0.02436000408125593]
	TIME [epoch: 25.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012425508286016065		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.012425508286016065 | validation: 0.021098082659651525]
	TIME [epoch: 25.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010960988421822873		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.010960988421822873 | validation: 0.02343493951692855]
	TIME [epoch: 25.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010570846223669488		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.010570846223669488 | validation: 0.02070617939689214]
	TIME [epoch: 25.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01206996337959368		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.01206996337959368 | validation: 0.023678411187432753]
	TIME [epoch: 25.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010799290438300128		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.010799290438300128 | validation: 0.019971162260187005]
	TIME [epoch: 25.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012465496017642507		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.012465496017642507 | validation: 0.020693368877770013]
	TIME [epoch: 25.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010191870105620608		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.010191870105620608 | validation: 0.023034058458866292]
	TIME [epoch: 25.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011721176032534964		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.011721176032534964 | validation: 0.020183713167854556]
	TIME [epoch: 25.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010633165703573318		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.010633165703573318 | validation: 0.022415644048542203]
	TIME [epoch: 25.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012149094748475465		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.012149094748475465 | validation: 0.0247646846844692]
	TIME [epoch: 25.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013182011430039326		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.013182011430039326 | validation: 0.02030804350295237]
	TIME [epoch: 25.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011416015126732744		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.011416015126732744 | validation: 0.019992942559277506]
	TIME [epoch: 25.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009942755880982462		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.009942755880982462 | validation: 0.02343276852712071]
	TIME [epoch: 25.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0106102781250485		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.0106102781250485 | validation: 0.02045041033693799]
	TIME [epoch: 25.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01122930881246707		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.01122930881246707 | validation: 0.023511231243594652]
	TIME [epoch: 25.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01132356215632342		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.01132356215632342 | validation: 0.020366110681803308]
	TIME [epoch: 25.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010004531404664938		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.010004531404664938 | validation: 0.021719490902052857]
	TIME [epoch: 25.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012138151867128081		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.012138151867128081 | validation: 0.02116993386698464]
	TIME [epoch: 25.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010379618816705394		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.010379618816705394 | validation: 0.021561013706236837]
	TIME [epoch: 25.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010199460309501444		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.010199460309501444 | validation: 0.021174993651654254]
	TIME [epoch: 25.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01139623651563744		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.01139623651563744 | validation: 0.02009781836729141]
	TIME [epoch: 25.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009963008412379928		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.009963008412379928 | validation: 0.024264475115725912]
	TIME [epoch: 25.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010522346315780083		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.010522346315780083 | validation: 0.022576815711125027]
	TIME [epoch: 25.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010742455516315782		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.010742455516315782 | validation: 0.021872770494520383]
	TIME [epoch: 25.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011041815479167957		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.011041815479167957 | validation: 0.022815044245818455]
	TIME [epoch: 25.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010737743811828957		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.010737743811828957 | validation: 0.026126538314877504]
	TIME [epoch: 25.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011886552504072345		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.011886552504072345 | validation: 0.02008299870340787]
	TIME [epoch: 25.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010470575380367317		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.010470575380367317 | validation: 0.02092094150428484]
	TIME [epoch: 25.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011309023896341934		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.011309023896341934 | validation: 0.021749389121670598]
	TIME [epoch: 25.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010084167115725674		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.010084167115725674 | validation: 0.023432430534860056]
	TIME [epoch: 25.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00969745568437724		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.00969745568437724 | validation: 0.019579742478538355]
	TIME [epoch: 25.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011089124830069842		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.011089124830069842 | validation: 0.022938643667217783]
	TIME [epoch: 25.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01182823321970216		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.01182823321970216 | validation: 0.022759800179096717]
	TIME [epoch: 25.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010764952039872773		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.010764952039872773 | validation: 0.021427293873301265]
	TIME [epoch: 25.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0117033022604073		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.0117033022604073 | validation: 0.0186208741659932]
	TIME [epoch: 25.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011291232749227002		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.011291232749227002 | validation: 0.020685506891440213]
	TIME [epoch: 25.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009977599022253706		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.009977599022253706 | validation: 0.020411896057012463]
	TIME [epoch: 25.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01064804789039392		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.01064804789039392 | validation: 0.023791424771779392]
	TIME [epoch: 25.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01231289204478267		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.01231289204478267 | validation: 0.02213244749703122]
	TIME [epoch: 25.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011388469170552504		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.011388469170552504 | validation: 0.020774850600853132]
	TIME [epoch: 25.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01101824041996563		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.01101824041996563 | validation: 0.021932796519766984]
	TIME [epoch: 25.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009399517092826158		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.009399517092826158 | validation: 0.02126153465141408]
	TIME [epoch: 25.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012586267913903895		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.012586267913903895 | validation: 0.020956825935493733]
	TIME [epoch: 25.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011553691159112724		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.011553691159112724 | validation: 0.02012537995085221]
	TIME [epoch: 25.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011897017163060944		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.011897017163060944 | validation: 0.019140246305933386]
	TIME [epoch: 25.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01051160987560881		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.01051160987560881 | validation: 0.02191520925302505]
	TIME [epoch: 25.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009953861861766541		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.009953861861766541 | validation: 0.01993452826845668]
	TIME [epoch: 25.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010045497749121123		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.010045497749121123 | validation: 0.018811220255248395]
	TIME [epoch: 25.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0098514600754979		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0098514600754979 | validation: 0.021302866871878157]
	TIME [epoch: 25.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00961952058840495		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.00961952058840495 | validation: 0.01996457249805124]
	TIME [epoch: 25.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010941729842609547		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.010941729842609547 | validation: 0.020632941464208472]
	TIME [epoch: 25.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009418026072734809		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.009418026072734809 | validation: 0.02818196833686923]
	TIME [epoch: 25.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011866052418184603		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.011866052418184603 | validation: 0.022559174506947194]
	TIME [epoch: 25.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010315155171556616		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.010315155171556616 | validation: 0.02043482019486749]
	TIME [epoch: 25.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009740482532743379		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.009740482532743379 | validation: 0.023461085547365593]
	TIME [epoch: 25.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010898855611930708		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.010898855611930708 | validation: 0.021136924811865306]
	TIME [epoch: 25.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011595646487283576		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.011595646487283576 | validation: 0.019213153863396986]
	TIME [epoch: 25.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009608159788950903		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.009608159788950903 | validation: 0.019255437802644815]
	TIME [epoch: 25.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01085433302593098		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.01085433302593098 | validation: 0.018249040161666084]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011062057877304443		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.011062057877304443 | validation: 0.01941992253384514]
	TIME [epoch: 25.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01013642008203801		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.01013642008203801 | validation: 0.019006661826081038]
	TIME [epoch: 25.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010511686224923909		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.010511686224923909 | validation: 0.020596909319325382]
	TIME [epoch: 25.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01015514806112124		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.01015514806112124 | validation: 0.020912264168156357]
	TIME [epoch: 25.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010770285354127565		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.010770285354127565 | validation: 0.018911411343868277]
	TIME [epoch: 25.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010827907422898183		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.010827907422898183 | validation: 0.020309271153613247]
	TIME [epoch: 25.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009850937718181643		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.009850937718181643 | validation: 0.02030034184095025]
	TIME [epoch: 25.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010840485301933185		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.010840485301933185 | validation: 0.019717475188258102]
	TIME [epoch: 25.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010891594238809355		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.010891594238809355 | validation: 0.02112807433022172]
	TIME [epoch: 25.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010879514541286519		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.010879514541286519 | validation: 0.02000453968489794]
	TIME [epoch: 25.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009895723514469336		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.009895723514469336 | validation: 0.019375193048703172]
	TIME [epoch: 25.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01011922817914824		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.01011922817914824 | validation: 0.02015074266235512]
	TIME [epoch: 25.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009410681567601206		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.009410681567601206 | validation: 0.018419029763911975]
	TIME [epoch: 25.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010035803024390784		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.010035803024390784 | validation: 0.01980038074633359]
	TIME [epoch: 25.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009784399344098709		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.009784399344098709 | validation: 0.020465990014398836]
	TIME [epoch: 25.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009762782271889101		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.009762782271889101 | validation: 0.01959835361524683]
	TIME [epoch: 25.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009346216619470054		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.009346216619470054 | validation: 0.019360465378251982]
	TIME [epoch: 25.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010277882490151963		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.010277882490151963 | validation: 0.019987387897205847]
	TIME [epoch: 25.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01053152281075004		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.01053152281075004 | validation: 0.0184481934306948]
	TIME [epoch: 25.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00972422988529068		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.00972422988529068 | validation: 0.019497170537215412]
	TIME [epoch: 25.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009679952672176636		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.009679952672176636 | validation: 0.020078647412210722]
	TIME [epoch: 25.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011507878165823895		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.011507878165823895 | validation: 0.02011491889543828]
	TIME [epoch: 25.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00999839187855098		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.00999839187855098 | validation: 0.017777880513580917]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00933864682289145		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.00933864682289145 | validation: 0.021277083998273997]
	TIME [epoch: 25.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010033194424488627		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.010033194424488627 | validation: 0.020597160957907562]
	TIME [epoch: 25.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00951335686087673		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.00951335686087673 | validation: 0.018906670787348298]
	TIME [epoch: 25.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009645928497789405		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.009645928497789405 | validation: 0.022316767076273572]
	TIME [epoch: 25.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009501650793371357		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.009501650793371357 | validation: 0.019407060675418173]
	TIME [epoch: 25.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010422831555374216		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.010422831555374216 | validation: 0.027687267239867357]
	TIME [epoch: 25.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018807616367602455		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.018807616367602455 | validation: 0.018933434798447453]
	TIME [epoch: 25.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011087247665208848		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.011087247665208848 | validation: 0.017691896238293855]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009996101490250365		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.009996101490250365 | validation: 0.018838959386761497]
	TIME [epoch: 25.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00905471365579018		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.00905471365579018 | validation: 0.01994239809552429]
	TIME [epoch: 25.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00946232521668501		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.00946232521668501 | validation: 0.01762553413232738]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009404414854515729		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.009404414854515729 | validation: 0.019905884906368344]
	TIME [epoch: 25.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010630497422069677		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.010630497422069677 | validation: 0.020136154857415385]
	TIME [epoch: 25.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009465067627360798		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.009465067627360798 | validation: 0.01917403892649066]
	TIME [epoch: 25.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009335006659697886		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.009335006659697886 | validation: 0.02065246858349674]
	TIME [epoch: 25.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009360177957421839		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.009360177957421839 | validation: 0.02162256521585832]
	TIME [epoch: 25.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009782981592750381		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.009782981592750381 | validation: 0.020137616239196375]
	TIME [epoch: 25.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009213981829247042		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.009213981829247042 | validation: 0.01987047372470531]
	TIME [epoch: 25.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009715982614170643		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.009715982614170643 | validation: 0.022351825288388034]
	TIME [epoch: 25.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009888640477260919		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.009888640477260919 | validation: 0.019328786587818475]
	TIME [epoch: 25.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009531653718075139		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.009531653718075139 | validation: 0.020747275088496914]
	TIME [epoch: 25.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010413882898879965		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.010413882898879965 | validation: 0.021853031584934672]
	TIME [epoch: 25.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009773492655711917		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.009773492655711917 | validation: 0.01994617739538468]
	TIME [epoch: 25.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01004673247718475		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.01004673247718475 | validation: 0.019952498874595097]
	TIME [epoch: 25.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009434549217401894		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.009434549217401894 | validation: 0.01911344690557536]
	TIME [epoch: 25.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009235597260092617		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.009235597260092617 | validation: 0.02029115383373631]
	TIME [epoch: 25.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00931359323034441		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.00931359323034441 | validation: 0.01821750974157877]
	TIME [epoch: 25.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01050794051288214		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.01050794051288214 | validation: 0.022485933164178715]
	TIME [epoch: 25.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010425525452086859		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.010425525452086859 | validation: 0.019977661700741504]
	TIME [epoch: 25.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009230512642025356		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.009230512642025356 | validation: 0.02021507121672724]
	TIME [epoch: 25.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009920846685413598		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.009920846685413598 | validation: 0.01916927327865545]
	TIME [epoch: 25.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008913645732740463		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.008913645732740463 | validation: 0.01823676954034653]
	TIME [epoch: 25.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009828085663181373		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.009828085663181373 | validation: 0.021220613681007392]
	TIME [epoch: 25.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008830194389124312		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.008830194389124312 | validation: 0.018503778373644753]
	TIME [epoch: 25.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009675769143431795		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.009675769143431795 | validation: 0.018526120995915486]
	TIME [epoch: 25.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009554500287143957		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.009554500287143957 | validation: 0.021884946271459343]
	TIME [epoch: 25.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0092755266129917		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0092755266129917 | validation: 0.021331569583038442]
	TIME [epoch: 25.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009633107256225598		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.009633107256225598 | validation: 0.0196966941053125]
	TIME [epoch: 25.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009376926089298025		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.009376926089298025 | validation: 0.019191488487962587]
	TIME [epoch: 25.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009653623483034165		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.009653623483034165 | validation: 0.02012695812560941]
	TIME [epoch: 25.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008501080941091827		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.008501080941091827 | validation: 0.018273642680714828]
	TIME [epoch: 25.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009180263234328334		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.009180263234328334 | validation: 0.020942100662180163]
	TIME [epoch: 25.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00956229063645608		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.00956229063645608 | validation: 0.019308976408927595]
	TIME [epoch: 25.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009480132663644322		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.009480132663644322 | validation: 0.018866290302237885]
	TIME [epoch: 25.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009242064591259152		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.009242064591259152 | validation: 0.01861510595038001]
	TIME [epoch: 25.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009328919168482332		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.009328919168482332 | validation: 0.019591608661797294]
	TIME [epoch: 25.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010122461581478668		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.010122461581478668 | validation: 0.020866755284419586]
	TIME [epoch: 25.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009858014441377985		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.009858014441377985 | validation: 0.021975756953948863]
	TIME [epoch: 25.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009009949244550874		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.009009949244550874 | validation: 0.018088139703241288]
	TIME [epoch: 25.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009435296561487012		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.009435296561487012 | validation: 0.019151357276506856]
	TIME [epoch: 25.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009341287445734859		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.009341287445734859 | validation: 0.018153113678544917]
	TIME [epoch: 25.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009506828120897837		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.009506828120897837 | validation: 0.02049749466604565]
	TIME [epoch: 25.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009143766515039908		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.009143766515039908 | validation: 0.020547043715613188]
	TIME [epoch: 25.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00991895962604764		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.00991895962604764 | validation: 0.020907289843461175]
	TIME [epoch: 25.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009266707231914974		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.009266707231914974 | validation: 0.018842013827545186]
	TIME [epoch: 25.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00896486141083683		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.00896486141083683 | validation: 0.018659982535911702]
	TIME [epoch: 25.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011341804232106865		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.011341804232106865 | validation: 0.019578902151882377]
	TIME [epoch: 25.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009376474914536052		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.009376474914536052 | validation: 0.01920775306807471]
	TIME [epoch: 25.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00875585979071836		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.00875585979071836 | validation: 0.020785206189027683]
	TIME [epoch: 25.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009375847225539349		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.009375847225539349 | validation: 0.018244847276470983]
	TIME [epoch: 25.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010284938619587075		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.010284938619587075 | validation: 0.01946130629707899]
	TIME [epoch: 25.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009602942324839237		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.009602942324839237 | validation: 0.01883766702239953]
	TIME [epoch: 25.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008813645546586342		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.008813645546586342 | validation: 0.019306448499388222]
	TIME [epoch: 25.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008865158058614143		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.008865158058614143 | validation: 0.021052160534857672]
	TIME [epoch: 25.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009361217084037605		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.009361217084037605 | validation: 0.019003547080603825]
	TIME [epoch: 25.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010124597544806092		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.010124597544806092 | validation: 0.01881303264633165]
	TIME [epoch: 25.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009100211886112796		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.009100211886112796 | validation: 0.018582778820147167]
	TIME [epoch: 25.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009031195259868742		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.009031195259868742 | validation: 0.01695195962643034]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009406605918042361		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.009406605918042361 | validation: 0.018821398147679403]
	TIME [epoch: 25.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009042006962790065		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.009042006962790065 | validation: 0.02041007115629911]
	TIME [epoch: 25.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008771024065147792		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.008771024065147792 | validation: 0.017195033643007618]
	TIME [epoch: 25.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008944805864036584		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.008944805864036584 | validation: 0.01709422440382437]
	TIME [epoch: 25.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009754447059386707		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.009754447059386707 | validation: 0.018818882939927093]
	TIME [epoch: 25.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008969325493301487		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.008969325493301487 | validation: 0.016343732198914385]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00910921461550794		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.00910921461550794 | validation: 0.020413632571083314]
	TIME [epoch: 25.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00979552558629051		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.00979552558629051 | validation: 0.01692157861470254]
	TIME [epoch: 25.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008514027235689765		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.008514027235689765 | validation: 0.018848024819498363]
	TIME [epoch: 25.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008865117628023455		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.008865117628023455 | validation: 0.018899177231694434]
	TIME [epoch: 25.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008828644521290787		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.008828644521290787 | validation: 0.020071624210371435]
	TIME [epoch: 25.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009230073268630738		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.009230073268630738 | validation: 0.017322447493813224]
	TIME [epoch: 25.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008342863404382757		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.008342863404382757 | validation: 0.01906939291039763]
	TIME [epoch: 25.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009336333490011946		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.009336333490011946 | validation: 0.017381398720569124]
	TIME [epoch: 25.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008890887379485866		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.008890887379485866 | validation: 0.01787390856397522]
	TIME [epoch: 25.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008728819122707284		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.008728819122707284 | validation: 0.020338513388304436]
	TIME [epoch: 25.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009724435426611019		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.009724435426611019 | validation: 0.020858639777681875]
	TIME [epoch: 25.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008914461290467027		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.008914461290467027 | validation: 0.017538133641421462]
	TIME [epoch: 25.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009079124637707988		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.009079124637707988 | validation: 0.018450374451199562]
	TIME [epoch: 25.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008565228813221554		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.008565228813221554 | validation: 0.017750922587895054]
	TIME [epoch: 25.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008365719108154361		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.008365719108154361 | validation: 0.018208342247032508]
	TIME [epoch: 25.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009163315722963403		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.009163315722963403 | validation: 0.021828966963973385]
	TIME [epoch: 25.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009087472644564786		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.009087472644564786 | validation: 0.017361827025355693]
	TIME [epoch: 25.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008853315023129724		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.008853315023129724 | validation: 0.0188826038893979]
	TIME [epoch: 25.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009330066709379277		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.009330066709379277 | validation: 0.01811377679837707]
	TIME [epoch: 25.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008547661457121233		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.008547661457121233 | validation: 0.01813039799897903]
	TIME [epoch: 25.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007867906119380796		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.007867906119380796 | validation: 0.01854754694494879]
	TIME [epoch: 25.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009134247228316685		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.009134247228316685 | validation: 0.01920026334863273]
	TIME [epoch: 25.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008690847415307654		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.008690847415307654 | validation: 0.017472225194417077]
	TIME [epoch: 25.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008878474311576675		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.008878474311576675 | validation: 0.020640656089257622]
	TIME [epoch: 25.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008637114475381862		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.008637114475381862 | validation: 0.021129150514058052]
	TIME [epoch: 25.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008418192413312544		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.008418192413312544 | validation: 0.017072523590975264]
	TIME [epoch: 25.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009102229611936847		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.009102229611936847 | validation: 0.017938614738938694]
	TIME [epoch: 25.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00900934398847338		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.00900934398847338 | validation: 0.01897564418765113]
	TIME [epoch: 25.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008386430368684732		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.008386430368684732 | validation: 0.017396800026758762]
	TIME [epoch: 25.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008858118662717247		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.008858118662717247 | validation: 0.016025047069008244]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008884112571839393		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.008884112571839393 | validation: 0.018261280604648093]
	TIME [epoch: 403 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009197324811531243		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.009197324811531243 | validation: 0.017147680170730453]
	TIME [epoch: 54.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008942118027795204		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.008942118027795204 | validation: 0.01962277710703146]
	TIME [epoch: 54.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008363983009865702		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.008363983009865702 | validation: 0.01665967876781816]
	TIME [epoch: 54.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00842249708632786		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.00842249708632786 | validation: 0.01704185915891811]
	TIME [epoch: 54.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008526896260494556		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.008526896260494556 | validation: 0.019310331236276894]
	TIME [epoch: 54.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008939029862020515		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.008939029862020515 | validation: 0.01769700037378597]
	TIME [epoch: 54.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008141776249178336		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.008141776249178336 | validation: 0.019862724361225676]
	TIME [epoch: 54.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009082977279683433		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.009082977279683433 | validation: 0.01965940349784598]
	TIME [epoch: 54.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008346796899876064		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.008346796899876064 | validation: 0.018499935625200685]
	TIME [epoch: 54.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009316601190376503		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.009316601190376503 | validation: 0.01733370066643605]
	TIME [epoch: 54.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009582279030617554		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.009582279030617554 | validation: 0.017320265801385282]
	TIME [epoch: 54.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010239350655735202		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.010239350655735202 | validation: 0.017410062075806663]
	TIME [epoch: 54.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008791555979420919		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.008791555979420919 | validation: 0.016237330684463168]
	TIME [epoch: 54.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008475252160112988		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.008475252160112988 | validation: 0.019724190625772128]
	TIME [epoch: 54.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008643175769673604		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.008643175769673604 | validation: 0.016146154385217423]
	TIME [epoch: 54.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008543610264781946		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.008543610264781946 | validation: 0.01851962871746456]
	TIME [epoch: 54.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008737235655397631		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.008737235655397631 | validation: 0.019775710880250416]
	TIME [epoch: 54.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008445474282796146		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.008445474282796146 | validation: 0.01849055724824733]
	TIME [epoch: 54.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007740814901423363		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.007740814901423363 | validation: 0.01776535107531759]
	TIME [epoch: 54.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00842647185522702		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.00842647185522702 | validation: 0.018201642880220113]
	TIME [epoch: 54.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008683960095208719		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.008683960095208719 | validation: 0.0180369121850881]
	TIME [epoch: 54.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00835519349463031		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.00835519349463031 | validation: 0.018258908188212988]
	TIME [epoch: 54.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008838704373449172		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.008838704373449172 | validation: 0.016115630149022092]
	TIME [epoch: 54.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008887786134330928		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.008887786134330928 | validation: 0.019196879147764696]
	TIME [epoch: 54.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008393022774020252		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.008393022774020252 | validation: 0.01736272614199409]
	TIME [epoch: 54.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008494066534437135		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.008494066534437135 | validation: 0.01970311760085406]
	TIME [epoch: 54.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008313777094002972		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.008313777094002972 | validation: 0.016507815486821606]
	TIME [epoch: 54.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008069283190301705		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.008069283190301705 | validation: 0.017890654350541935]
	TIME [epoch: 54.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008367427325158551		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.008367427325158551 | validation: 0.01833117688195166]
	TIME [epoch: 54.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008461337465874304		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.008461337465874304 | validation: 0.019860776571689637]
	TIME [epoch: 54.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008454673470689709		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.008454673470689709 | validation: 0.017860418565876286]
	TIME [epoch: 54.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008823274558734204		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.008823274558734204 | validation: 0.018729152749641202]
	TIME [epoch: 54.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009510008409264161		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.009510008409264161 | validation: 0.018288743650308845]
	TIME [epoch: 54.2 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00839730544336213		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.00839730544336213 | validation: 0.017769783739551993]
	TIME [epoch: 54.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008108795134436114		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.008108795134436114 | validation: 0.017115713994941086]
	TIME [epoch: 54.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008002731651227531		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.008002731651227531 | validation: 0.018679851633170434]
	TIME [epoch: 54.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008634263549612635		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.008634263549612635 | validation: 0.018381712643659766]
	TIME [epoch: 54.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00885694323522952		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.00885694323522952 | validation: 0.018152156496626236]
	TIME [epoch: 54.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008292707859118079		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.008292707859118079 | validation: 0.01915655110701058]
	TIME [epoch: 54.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00787055332631463		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.00787055332631463 | validation: 0.01746718256255423]
	TIME [epoch: 54.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008270997476303362		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.008270997476303362 | validation: 0.017749432007306366]
	TIME [epoch: 54.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008384112337906018		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.008384112337906018 | validation: 0.019107300094011768]
	TIME [epoch: 54.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009208324168159624		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.009208324168159624 | validation: 0.018365033078682227]
	TIME [epoch: 54.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008531723118450284		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.008531723118450284 | validation: 0.018857980070689444]
	TIME [epoch: 54.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00870408880982599		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.00870408880982599 | validation: 0.016875731211368785]
	TIME [epoch: 54.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007738719143581954		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.007738719143581954 | validation: 0.01757298590153392]
	TIME [epoch: 54.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008463222705558998		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.008463222705558998 | validation: 0.017062560206843422]
	TIME [epoch: 54.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007956423322966005		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.007956423322966005 | validation: 0.018112109201872315]
	TIME [epoch: 54.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008416102100784247		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.008416102100784247 | validation: 0.01650160758187376]
	TIME [epoch: 54.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008211569115445825		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.008211569115445825 | validation: 0.017665241396141885]
	TIME [epoch: 54.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00806474821213591		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.00806474821213591 | validation: 0.01715061522064553]
	TIME [epoch: 54.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008274985394570587		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.008274985394570587 | validation: 0.016900769695758145]
	TIME [epoch: 54.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008527090613862769		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.008527090613862769 | validation: 0.01735501330291278]
	TIME [epoch: 54.2 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008398762382947327		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.008398762382947327 | validation: 0.018270344920331083]
	TIME [epoch: 54.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008932395932103028		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.008932395932103028 | validation: 0.01718154326006975]
	TIME [epoch: 54.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008137726700924868		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.008137726700924868 | validation: 0.017241897090619688]
	TIME [epoch: 54.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007890764319571019		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.007890764319571019 | validation: 0.017819567490919216]
	TIME [epoch: 54.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00798681938337354		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.00798681938337354 | validation: 0.017632618408686872]
	TIME [epoch: 54.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008987226040021489		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.008987226040021489 | validation: 0.016367911241676806]
	TIME [epoch: 54.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008156999818613052		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.008156999818613052 | validation: 0.016869958170581272]
	TIME [epoch: 54.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008064881613078162		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.008064881613078162 | validation: 0.017757415472856225]
	TIME [epoch: 54.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008799804713139386		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.008799804713139386 | validation: 0.016593156035978354]
	TIME [epoch: 54.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008555115726957944		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.008555115726957944 | validation: 0.017341141371816072]
	TIME [epoch: 54.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008443417898634243		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.008443417898634243 | validation: 0.01734996180292834]
	TIME [epoch: 54.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007943682897100202		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.007943682897100202 | validation: 0.016591694869221105]
	TIME [epoch: 54.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00841442412499119		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.00841442412499119 | validation: 0.019133311333862306]
	TIME [epoch: 54.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008329487134609445		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.008329487134609445 | validation: 0.01743648142410226]
	TIME [epoch: 54.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007996969763619707		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.007996969763619707 | validation: 0.016421150540150413]
	TIME [epoch: 54.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00803751446278523		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.00803751446278523 | validation: 0.017943659981036444]
	TIME [epoch: 54.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00857092518720873		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.00857092518720873 | validation: 0.01878418397090279]
	TIME [epoch: 54.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008281585429594097		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.008281585429594097 | validation: 0.018304419666574725]
	TIME [epoch: 54.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008269615544533251		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.008269615544533251 | validation: 0.01915426726918258]
	TIME [epoch: 54.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008237631291337259		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.008237631291337259 | validation: 0.018493984907774774]
	TIME [epoch: 54.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008331294535513702		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.008331294535513702 | validation: 0.016806654468972206]
	TIME [epoch: 54.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007867600365917797		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.007867600365917797 | validation: 0.017757814774181695]
	TIME [epoch: 54.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00834888127500925		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.00834888127500925 | validation: 0.017638538118828144]
	TIME [epoch: 54.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007773020941431473		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.007773020941431473 | validation: 0.020392662727688818]
	TIME [epoch: 54.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008250412637540831		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.008250412637540831 | validation: 0.016593218031700968]
	TIME [epoch: 54.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008060539747255933		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.008060539747255933 | validation: 0.019004276612496027]
	TIME [epoch: 54.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008250517037080985		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.008250517037080985 | validation: 0.01737468349727489]
	TIME [epoch: 54.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008168112324021485		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.008168112324021485 | validation: 0.016609229978822117]
	TIME [epoch: 54.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007955804676022266		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.007955804676022266 | validation: 0.017205008852067376]
	TIME [epoch: 54.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008049844173184818		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.008049844173184818 | validation: 0.01738098157538623]
	TIME [epoch: 54.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008445513902641786		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.008445513902641786 | validation: 0.017161348794024492]
	TIME [epoch: 54.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008556040353821018		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.008556040353821018 | validation: 0.01876861127832361]
	TIME [epoch: 54.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007651911670425405		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.007651911670425405 | validation: 0.018412198174688962]
	TIME [epoch: 54.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00790379759672145		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.00790379759672145 | validation: 0.01765023190176834]
	TIME [epoch: 54.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008377816598353165		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.008377816598353165 | validation: 0.017381405399576395]
	TIME [epoch: 54.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008083973521160637		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.008083973521160637 | validation: 0.015686852483317665]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_1090.pth
	Model improved!!!
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0078062157467715335		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0078062157467715335 | validation: 0.016047212445813076]
	TIME [epoch: 54.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007623524689324256		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.007623524689324256 | validation: 0.017741809762000675]
	TIME [epoch: 54.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00834429324905016		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.00834429324905016 | validation: 0.017703279163594432]
	TIME [epoch: 54.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007764948103986537		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.007764948103986537 | validation: 0.014974142037002851]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_1094.pth
	Model improved!!!
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008205238457058799		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.008205238457058799 | validation: 0.018178881770529457]
	TIME [epoch: 54.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007969495216203916		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.007969495216203916 | validation: 0.016408921318564005]
	TIME [epoch: 54.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00737489187206815		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.00737489187206815 | validation: 0.017177679443778817]
	TIME [epoch: 54.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007536957553196848		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.007536957553196848 | validation: 0.018522503164879894]
	TIME [epoch: 54.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00751577080599451		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.00751577080599451 | validation: 0.016220496011335357]
	TIME [epoch: 54.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008360512439685636		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.008360512439685636 | validation: 0.017562850551406613]
	TIME [epoch: 54.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008124688976435528		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.008124688976435528 | validation: 0.016738134566097334]
	TIME [epoch: 54.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008168517057933483		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.008168517057933483 | validation: 0.017357291329649582]
	TIME [epoch: 54.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00801255814796969		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.00801255814796969 | validation: 0.017688441303859326]
	TIME [epoch: 54.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007983834085782768		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.007983834085782768 | validation: 0.016349665465936657]
	TIME [epoch: 54.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007830653698281374		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.007830653698281374 | validation: 0.01924095250763023]
	TIME [epoch: 54.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007726060711002226		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.007726060711002226 | validation: 0.01803435891287216]
	TIME [epoch: 54.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007787937766825209		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.007787937766825209 | validation: 0.016899476250664118]
	TIME [epoch: 54.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008423997277959564		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.008423997277959564 | validation: 0.01752596328846701]
	TIME [epoch: 54.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00788068704995573		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.00788068704995573 | validation: 0.01629233962432078]
	TIME [epoch: 54.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007684229878747591		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.007684229878747591 | validation: 0.016543258972778307]
	TIME [epoch: 54.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007665359754117876		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.007665359754117876 | validation: 0.01816147700369697]
	TIME [epoch: 54.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008091660809606976		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.008091660809606976 | validation: 0.017992779739218447]
	TIME [epoch: 54.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007944182891389229		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.007944182891389229 | validation: 0.015943697517690824]
	TIME [epoch: 54.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074912990946228355		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.0074912990946228355 | validation: 0.015424445963162156]
	TIME [epoch: 54.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008746630264695803		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.008746630264695803 | validation: 0.017159928580395733]
	TIME [epoch: 54.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007887392999435338		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.007887392999435338 | validation: 0.016223572086521185]
	TIME [epoch: 54.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007860585092186567		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.007860585092186567 | validation: 0.017763827478494944]
	TIME [epoch: 54.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008038122825606822		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.008038122825606822 | validation: 0.016215743721599396]
	TIME [epoch: 54.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007732225207003091		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.007732225207003091 | validation: 0.016835107235033717]
	TIME [epoch: 54.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008052519921056597		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.008052519921056597 | validation: 0.017709262677111225]
	TIME [epoch: 54.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077182717001353564		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0077182717001353564 | validation: 0.016124812879590972]
	TIME [epoch: 54.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00790055601403521		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.00790055601403521 | validation: 0.016119852623285573]
	TIME [epoch: 54.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007833541838950558		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.007833541838950558 | validation: 0.016651502674392336]
	TIME [epoch: 54.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007513356388966495		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.007513356388966495 | validation: 0.0167128000075086]
	TIME [epoch: 54.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008036056648396642		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.008036056648396642 | validation: 0.0179695264580777]
	TIME [epoch: 54.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008111076215294426		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.008111076215294426 | validation: 0.01859007328548917]
	TIME [epoch: 54.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008318306487668865		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.008318306487668865 | validation: 0.017152842492730237]
	TIME [epoch: 54.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007806658208206693		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.007806658208206693 | validation: 0.02157356438180087]
	TIME [epoch: 54.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007972609032691131		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.007972609032691131 | validation: 0.01695926047770461]
	TIME [epoch: 54.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007595417111670869		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.007595417111670869 | validation: 0.01936733823638931]
	TIME [epoch: 54.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008059165892999907		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.008059165892999907 | validation: 0.015920963548533798]
	TIME [epoch: 54.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00774815716031602		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.00774815716031602 | validation: 0.016375852393962845]
	TIME [epoch: 54.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007406261494049791		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.007406261494049791 | validation: 0.017968537840521243]
	TIME [epoch: 54.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007886960780774319		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.007886960780774319 | validation: 0.017169222903427962]
	TIME [epoch: 54.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075887817136075664		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0075887817136075664 | validation: 0.018570818200100602]
	TIME [epoch: 54.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008106185725697506		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.008106185725697506 | validation: 0.016183889103738386]
	TIME [epoch: 54.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007552231312922491		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.007552231312922491 | validation: 0.017435550694881223]
	TIME [epoch: 54.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007775518067666786		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.007775518067666786 | validation: 0.018052016078244575]
	TIME [epoch: 54.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007706522433787991		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.007706522433787991 | validation: 0.01578111245656966]
	TIME [epoch: 54.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00785267721082183		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.00785267721082183 | validation: 0.019005409545341703]
	TIME [epoch: 54.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007575027990199296		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.007575027990199296 | validation: 0.017401967912857252]
	TIME [epoch: 54.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007921540396736915		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.007921540396736915 | validation: 0.01684568058776004]
	TIME [epoch: 54.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007700013663165238		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.007700013663165238 | validation: 0.017407528856690297]
	TIME [epoch: 54.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007764498123962403		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.007764498123962403 | validation: 0.017529085573938542]
	TIME [epoch: 54.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007704062973989949		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.007704062973989949 | validation: 0.01632982233673932]
	TIME [epoch: 54.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008055091226639895		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.008055091226639895 | validation: 0.016229663464079728]
	TIME [epoch: 54.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007667345271118297		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.007667345271118297 | validation: 0.015923598172485297]
	TIME [epoch: 54.2 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007659779239180524		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.007659779239180524 | validation: 0.018330190383679307]
	TIME [epoch: 54.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076922300335366105		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.0076922300335366105 | validation: 0.016318926636068702]
	TIME [epoch: 54.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007907530338914833		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.007907530338914833 | validation: 0.016120674979811882]
	TIME [epoch: 54.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007380510664349621		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.007380510664349621 | validation: 0.01609753048613095]
	TIME [epoch: 54.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007784520480538775		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.007784520480538775 | validation: 0.017608925444963598]
	TIME [epoch: 54.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007542830704496769		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.007542830704496769 | validation: 0.016986918396132457]
	TIME [epoch: 54.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007775599816863916		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.007775599816863916 | validation: 0.015771776153213686]
	TIME [epoch: 54.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007937413232964336		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.007937413232964336 | validation: 0.01730426520791428]
	TIME [epoch: 54.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007361757734039664		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.007361757734039664 | validation: 0.01951931496977837]
	TIME [epoch: 54.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007409145836698484		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.007409145836698484 | validation: 0.016132233522804944]
	TIME [epoch: 54.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007919697169073061		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.007919697169073061 | validation: 0.01825776833083083]
	TIME [epoch: 54.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007637611603272197		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.007637611603272197 | validation: 0.018573916736520684]
	TIME [epoch: 54.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00784471149972466		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.00784471149972466 | validation: 0.01641235497674535]
	TIME [epoch: 54.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007523685237447017		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.007523685237447017 | validation: 0.018840240992844787]
	TIME [epoch: 54.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007557047926349803		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.007557047926349803 | validation: 0.018191820894488175]
	TIME [epoch: 54.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007440157919587018		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.007440157919587018 | validation: 0.018152932129584118]
	TIME [epoch: 54.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007606646802677679		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.007606646802677679 | validation: 0.016423781942438622]
	TIME [epoch: 54.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007778188160886648		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.007778188160886648 | validation: 0.017937664932210103]
	TIME [epoch: 54.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007317288259050393		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.007317288259050393 | validation: 0.018294131478419417]
	TIME [epoch: 54.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007186396197715664		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.007186396197715664 | validation: 0.015858079902957973]
	TIME [epoch: 54.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00758260329723928		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.00758260329723928 | validation: 0.015441904446288885]
	TIME [epoch: 54.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007873851937925249		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.007873851937925249 | validation: 0.016567806967539955]
	TIME [epoch: 54.2 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00783989700886256		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.00783989700886256 | validation: 0.016149810754652227]
	TIME [epoch: 54.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007223504572942247		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.007223504572942247 | validation: 0.01590580945866426]
	TIME [epoch: 54.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007577360895166484		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.007577360895166484 | validation: 0.017064414775334432]
	TIME [epoch: 54.2 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072348530466706945		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.0072348530466706945 | validation: 0.01665103089633213]
	TIME [epoch: 54.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007594244450773742		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.007594244450773742 | validation: 0.017480325418445]
	TIME [epoch: 54.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00805074484874949		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.00805074484874949 | validation: 0.01678225645818575]
	TIME [epoch: 54.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007632212053672277		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.007632212053672277 | validation: 0.01716245454301793]
	TIME [epoch: 54.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007538744060907021		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.007538744060907021 | validation: 0.016527640577644755]
	TIME [epoch: 54.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007100592362619146		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.007100592362619146 | validation: 0.01497483212745395]
	TIME [epoch: 54.2 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007346895859222129		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.007346895859222129 | validation: 0.016990312820910215]
	TIME [epoch: 54.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007578424380163136		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.007578424380163136 | validation: 0.01524329459291572]
	TIME [epoch: 54.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007787640059624899		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.007787640059624899 | validation: 0.01649971905147888]
	TIME [epoch: 54.2 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007357486620873609		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.007357486620873609 | validation: 0.016103653665054563]
	TIME [epoch: 54.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00735657162406881		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.00735657162406881 | validation: 0.01627761252008584]
	TIME [epoch: 54.2 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007245700554816154		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.007245700554816154 | validation: 0.016434172862175744]
	TIME [epoch: 54.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007487681053173078		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.007487681053173078 | validation: 0.015325014548519166]
	TIME [epoch: 54.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00705540135978183		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.00705540135978183 | validation: 0.016891640206357673]
	TIME [epoch: 54.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0078034116066137095		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0078034116066137095 | validation: 0.015318130150838934]
	TIME [epoch: 54.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007678758538429982		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.007678758538429982 | validation: 0.016308627956778204]
	TIME [epoch: 54.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00785582315218227		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.00785582315218227 | validation: 0.016707498647798306]
	TIME [epoch: 54.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007879540488306965		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.007879540488306965 | validation: 0.017504401178559434]
	TIME [epoch: 54.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007736862853446904		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.007736862853446904 | validation: 0.01734621641263496]
	TIME [epoch: 54.2 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075121262787562985		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.0075121262787562985 | validation: 0.015527968417779373]
	TIME [epoch: 54.2 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00756338722815932		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.00756338722815932 | validation: 0.017020618357557478]
	TIME [epoch: 54.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072384027262371135		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.0072384027262371135 | validation: 0.01596536629794965]
	TIME [epoch: 54.2 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007249194014032139		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.007249194014032139 | validation: 0.01487389508406044]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_0_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_0_v_mmd1_1195.pth
	Model improved!!!
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 29806.686 seconds.
