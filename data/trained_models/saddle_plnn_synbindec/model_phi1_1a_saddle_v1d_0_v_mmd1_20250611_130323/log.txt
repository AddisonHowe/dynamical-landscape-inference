Args:
Namespace(name='model_phi1_1a_saddle_v1d_0_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1d_0/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1d_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.0570029616355896, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1803577933

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.197453641114037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.197453641114037 | validation: 6.054512937510529]
	TIME [epoch: 380 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.630129905444719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.630129905444719 | validation: 5.589627351339621]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.241229492294493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.241229492294493 | validation: 5.38854503471619]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.060876521426039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.060876521426039 | validation: 5.241206201191614]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.873531587254301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.873531587254301 | validation: 5.101370482848919]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.662791350174519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.662791350174519 | validation: 5.080557089080401]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.520653210081975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.520653210081975 | validation: 4.773468935310287]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.355178916047729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.355178916047729 | validation: 4.609932503308469]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.221295713611255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221295713611255 | validation: 4.674441013486149]
	TIME [epoch: 6.1 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.027617908827125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027617908827125 | validation: 4.232278649780072]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8147994421415174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8147994421415174 | validation: 3.9847489879230222]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5724796853709173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5724796853709173 | validation: 3.669645974533877]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.348264839672681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.348264839672681 | validation: 3.523823171674909]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210548273883483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.210548273883483 | validation: 3.3515085612278765]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.003745651694452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.003745651694452 | validation: 3.098407657103408]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8694542061566466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8694542061566466 | validation: 3.639924180901975]
	TIME [epoch: 6.1 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0248096698630915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0248096698630915 | validation: 2.8404275955828795]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.671419254519904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.671419254519904 | validation: 2.785942421680733]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.672486541649155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.672486541649155 | validation: 2.555764518227755]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4514166657556835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4514166657556835 | validation: 2.4176172185304883]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3639142335346883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3639142335346883 | validation: 2.493954745941358]
	TIME [epoch: 6.08 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1741732593233234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1741732593233234 | validation: 2.3195700926876435]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1587705266426944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1587705266426944 | validation: 2.066717716202521]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861048289195279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.861048289195279 | validation: 1.6881562402988524]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7550022286791152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7550022286791152 | validation: 1.621715962756459]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.48052531124093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.48052531124093 | validation: 1.3622272899689545]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7491084171733584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7491084171733584 | validation: 1.891511149466493]
	TIME [epoch: 6.1 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6399190133036579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6399190133036579 | validation: 1.331053382374513]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3143043609770335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3143043609770335 | validation: 1.1804974980163156]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2727116694052156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2727116694052156 | validation: 1.1780241832275045]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2821106259031971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2821106259031971 | validation: 1.0468974890371237]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1460900389004947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1460900389004947 | validation: 1.0257028046560732]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0795886088323376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0795886088323376 | validation: 1.260187444790022]
	TIME [epoch: 6.1 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3055926085077347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3055926085077347 | validation: 1.0382765399817866]
	TIME [epoch: 6.09 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9741479420083504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9741479420083504 | validation: 0.8438065137990001]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7998755377062918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998755377062918 | validation: 1.148898243929581]
	TIME [epoch: 6.09 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.494441971882602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.494441971882602 | validation: 1.3633567794320562]
	TIME [epoch: 6.09 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0138951295635759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0138951295635759 | validation: 0.7843876194108736]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7736445583373619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736445583373619 | validation: 0.7246818896895867]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8186343406802532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186343406802532 | validation: 0.7530008477431446]
	TIME [epoch: 6.1 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0051715703579538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0051715703579538 | validation: 0.9112348731477738]
	TIME [epoch: 6.09 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0118889460511462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0118889460511462 | validation: 0.7491696276442434]
	TIME [epoch: 6.1 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8941251828713991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8941251828713991 | validation: 0.7279375067461253]
	TIME [epoch: 6.1 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8995031160641405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8995031160641405 | validation: 0.8656944002964522]
	TIME [epoch: 6.09 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8364099680942738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364099680942738 | validation: 0.7894473182369097]
	TIME [epoch: 6.09 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7264368990639815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264368990639815 | validation: 1.0865414258419543]
	TIME [epoch: 6.09 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0806070124067921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0806070124067921 | validation: 1.1442737445538902]
	TIME [epoch: 6.09 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9501260725753635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9501260725753635 | validation: 0.7712977670800245]
	TIME [epoch: 6.09 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7684658933912459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684658933912459 | validation: 0.698456085597495]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7702770423596308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7702770423596308 | validation: 0.7793168876794598]
	TIME [epoch: 6.09 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7468074294808158		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7468074294808158 | validation: 0.8185059111109501]
	TIME [epoch: 6.09 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8221466159723652		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.8221466159723652 | validation: 0.815547593360713]
	TIME [epoch: 6.09 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7127726619716019		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.7127726619716019 | validation: 0.6897741864436007]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7828084362219896		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.7828084362219896 | validation: 1.091319015374281]
	TIME [epoch: 6.09 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9120411100947183		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.9120411100947183 | validation: 0.6541870535776295]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6920924774657848		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.6920924774657848 | validation: 0.5923909881736995]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8324535931896392		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.8324535931896392 | validation: 0.7247446314755994]
	TIME [epoch: 6.09 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7317297460181376		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.7317297460181376 | validation: 0.5921789210914886]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7148852650910297		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.7148852650910297 | validation: 0.6859059290688947]
	TIME [epoch: 6.1 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6455602528279863		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.6455602528279863 | validation: 0.9194154495661722]
	TIME [epoch: 6.1 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7824934605493665		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.7824934605493665 | validation: 0.7537117546830079]
	TIME [epoch: 6.1 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6345260166361713		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.6345260166361713 | validation: 0.6315852334545846]
	TIME [epoch: 6.08 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6610585401922331		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.6610585401922331 | validation: 0.6443859434756081]
	TIME [epoch: 6.08 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5894628119636949		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5894628119636949 | validation: 0.5823252238911802]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5033504502370679		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.5033504502370679 | validation: 0.6839697087968332]
	TIME [epoch: 6.1 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7626885759999287		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.7626885759999287 | validation: 0.9859906829711356]
	TIME [epoch: 6.1 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7626218311948192		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.7626218311948192 | validation: 0.639853675593926]
	TIME [epoch: 6.09 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6363891374312703		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.6363891374312703 | validation: 0.625170941001653]
	TIME [epoch: 6.09 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6507351967939151		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.6507351967939151 | validation: 0.4987033457875043]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5704091468136783		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.5704091468136783 | validation: 0.48676510000213546]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5610174930708445		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.5610174930708445 | validation: 0.733035037462793]
	TIME [epoch: 6.1 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6822534144273271		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6822534144273271 | validation: 0.5346590940973154]
	TIME [epoch: 6.09 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5010122195697942		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5010122195697942 | validation: 0.7676835520845235]
	TIME [epoch: 6.09 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7053723439617471		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.7053723439617471 | validation: 0.4624497730311516]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6059065215410874		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6059065215410874 | validation: 0.6350618140873234]
	TIME [epoch: 6.11 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5138807182290249		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5138807182290249 | validation: 0.798513111962516]
	TIME [epoch: 6.09 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6603741033747228		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.6603741033747228 | validation: 0.7717587942504531]
	TIME [epoch: 6.1 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.593967305960086		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.593967305960086 | validation: 0.5354469535680273]
	TIME [epoch: 6.09 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5422140416601213		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5422140416601213 | validation: 0.569355667278473]
	TIME [epoch: 6.09 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6020939059390367		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.6020939059390367 | validation: 0.47290289693259535]
	TIME [epoch: 6.1 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561375886897402		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5561375886897402 | validation: 0.5440512136495438]
	TIME [epoch: 6.09 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.548738561799397		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.548738561799397 | validation: 0.44456614931649563]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5654550182321646		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.5654550182321646 | validation: 0.6338847334427369]
	TIME [epoch: 6.1 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5107773572620428		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.5107773572620428 | validation: 0.4108336986768531]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4404239624979758		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4404239624979758 | validation: 0.5272209102606344]
	TIME [epoch: 6.1 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.570080497105482		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.570080497105482 | validation: 0.5976812107643437]
	TIME [epoch: 6.09 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4600858797047058		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.4600858797047058 | validation: 0.4437436600938751]
	TIME [epoch: 6.09 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5380891029215753		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5380891029215753 | validation: 0.45413903526721516]
	TIME [epoch: 6.09 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4343567865881096		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.4343567865881096 | validation: 0.4056371402338827]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4575274782371279		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4575274782371279 | validation: 0.5206224569519544]
	TIME [epoch: 6.1 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48445241843648656		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.48445241843648656 | validation: 0.5069145117816133]
	TIME [epoch: 6.09 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4592170962011601		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.4592170962011601 | validation: 0.4278128032931673]
	TIME [epoch: 6.09 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4060312244200253		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.4060312244200253 | validation: 0.4156209610207662]
	TIME [epoch: 6.09 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4675338618226735		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.4675338618226735 | validation: 0.8511453823898357]
	TIME [epoch: 6.09 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6459088315000925		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.6459088315000925 | validation: 0.4754781267393265]
	TIME [epoch: 6.09 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4455514752002179		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.4455514752002179 | validation: 0.43260623673856274]
	TIME [epoch: 6.1 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4273657925898423		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.4273657925898423 | validation: 0.3898768451529912]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4356058628873069		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.4356058628873069 | validation: 0.4100074065654783]
	TIME [epoch: 6.11 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4930434993143843		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.4930434993143843 | validation: 0.5326684129911671]
	TIME [epoch: 6.12 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5124326063448106		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.5124326063448106 | validation: 0.49243865917190005]
	TIME [epoch: 6.11 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4101435144315976		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.4101435144315976 | validation: 0.40454570940925527]
	TIME [epoch: 6.11 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40996340509170237		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.40996340509170237 | validation: 0.49850624439004393]
	TIME [epoch: 6.11 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018696820824371		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.6018696820824371 | validation: 0.5615372773617624]
	TIME [epoch: 6.11 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5026469100576902		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5026469100576902 | validation: 0.38702927760640127]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37986093780426444		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.37986093780426444 | validation: 0.4127890541165092]
	TIME [epoch: 6.11 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39988069478094956		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.39988069478094956 | validation: 0.42799588849936676]
	TIME [epoch: 6.1 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42449130815779584		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.42449130815779584 | validation: 0.4914826778785155]
	TIME [epoch: 6.11 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4448619154407488		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.4448619154407488 | validation: 0.49481697178195827]
	TIME [epoch: 6.1 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4780536818281918		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.4780536818281918 | validation: 0.4699005699751476]
	TIME [epoch: 6.11 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4621809410344839		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.4621809410344839 | validation: 0.3994611969968979]
	TIME [epoch: 6.1 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3878045163812468		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3878045163812468 | validation: 0.4091751454966845]
	TIME [epoch: 6.1 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4606061760411165		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.4606061760411165 | validation: 0.4483766221107377]
	TIME [epoch: 6.09 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4632674579375844		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.4632674579375844 | validation: 0.4916056770050553]
	TIME [epoch: 6.09 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39011149770649056		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.39011149770649056 | validation: 0.3442480930559363]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4209247249656387		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.4209247249656387 | validation: 0.3538337363310976]
	TIME [epoch: 6.09 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38301127415977493		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.38301127415977493 | validation: 0.404479503911605]
	TIME [epoch: 6.09 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37452358776987404		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.37452358776987404 | validation: 0.4280756091820253]
	TIME [epoch: 6.1 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41300582100296135		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.41300582100296135 | validation: 0.43360716321545545]
	TIME [epoch: 6.09 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38070023378953616		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.38070023378953616 | validation: 0.3552157004838821]
	TIME [epoch: 6.09 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4242422358944363		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.4242422358944363 | validation: 0.509806011652175]
	TIME [epoch: 6.1 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4368027378776623		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.4368027378776623 | validation: 0.41266102289063683]
	TIME [epoch: 6.1 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3708426202941063		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3708426202941063 | validation: 0.40635510726214746]
	TIME [epoch: 6.09 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3531986576784356		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.3531986576784356 | validation: 0.3987803951596965]
	TIME [epoch: 6.1 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4188535456880683		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.4188535456880683 | validation: 0.37975423828127963]
	TIME [epoch: 6.1 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36635211022712766		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.36635211022712766 | validation: 0.35917474509089253]
	TIME [epoch: 6.1 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4413700330223889		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.4413700330223889 | validation: 0.3450141360596773]
	TIME [epoch: 6.1 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3530073298657257		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3530073298657257 | validation: 0.3476155362855967]
	TIME [epoch: 6.09 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3959530196407841		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3959530196407841 | validation: 0.39808060785549004]
	TIME [epoch: 6.09 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3964439003177934		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.3964439003177934 | validation: 0.41118996912134076]
	TIME [epoch: 6.1 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4337332376237992		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.4337332376237992 | validation: 0.39332472723505063]
	TIME [epoch: 6.09 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36189204638034833		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.36189204638034833 | validation: 0.39151747909438633]
	TIME [epoch: 6.09 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43287222239685447		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.43287222239685447 | validation: 0.33856327998564417]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3592831110814185		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3592831110814185 | validation: 0.3065172157466144]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34058173102467093		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.34058173102467093 | validation: 0.32635252273635534]
	TIME [epoch: 6.1 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3250446189848598		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3250446189848598 | validation: 0.32907570604108827]
	TIME [epoch: 6.09 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100820060438847		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.3100820060438847 | validation: 0.31922433449345367]
	TIME [epoch: 6.09 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3363680566181929		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.3363680566181929 | validation: 0.290704903516932]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3324894445798106		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.3324894445798106 | validation: 0.31280789883241633]
	TIME [epoch: 6.1 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30385189148607544		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.30385189148607544 | validation: 0.30910956492927927]
	TIME [epoch: 6.09 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2962557229151569		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2962557229151569 | validation: 0.27192626950529497]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39659288386795094		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.39659288386795094 | validation: 0.372614464167398]
	TIME [epoch: 6.1 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3624361809281642		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.3624361809281642 | validation: 0.36068955148596615]
	TIME [epoch: 6.1 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3417635907485481		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.3417635907485481 | validation: 0.40115172652468434]
	TIME [epoch: 6.1 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31264275500452615		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.31264275500452615 | validation: 0.25454120515751666]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3382041260452965		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.3382041260452965 | validation: 0.36754702585825205]
	TIME [epoch: 6.09 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27189207499477425		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.27189207499477425 | validation: 0.2433742969447018]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2546966852454051		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2546966852454051 | validation: 0.39660878613846573]
	TIME [epoch: 6.09 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33052423674872977		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.33052423674872977 | validation: 0.2936284477064442]
	TIME [epoch: 6.1 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25970567886751217		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.25970567886751217 | validation: 0.2969293927867986]
	TIME [epoch: 6.09 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30061784948785764		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.30061784948785764 | validation: 0.30999757190346067]
	TIME [epoch: 6.09 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2706381911968788		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2706381911968788 | validation: 0.29447442041593674]
	TIME [epoch: 6.08 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647356740416281		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.2647356740416281 | validation: 0.2972220913358103]
	TIME [epoch: 6.09 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2767683843357157		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.2767683843357157 | validation: 0.32693282273717755]
	TIME [epoch: 6.09 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26731030894541025		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.26731030894541025 | validation: 0.3180063014503051]
	TIME [epoch: 6.09 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34304390245816474		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.34304390245816474 | validation: 0.2259242817250784]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23339882738952927		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.23339882738952927 | validation: 0.3204520917932041]
	TIME [epoch: 6.1 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2580136791389659		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.2580136791389659 | validation: 0.24678119840995913]
	TIME [epoch: 6.09 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.244042434819911		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.244042434819911 | validation: 0.25306102294985416]
	TIME [epoch: 6.11 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22468291063316204		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.22468291063316204 | validation: 0.27975730562919776]
	TIME [epoch: 6.09 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2556727940178709		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2556727940178709 | validation: 0.3546573385703466]
	TIME [epoch: 6.09 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786838625400226		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.2786838625400226 | validation: 0.259655440088769]
	TIME [epoch: 6.09 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20832979634361873		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.20832979634361873 | validation: 0.21497970222332635]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2269258397817667		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2269258397817667 | validation: 0.27080721879873487]
	TIME [epoch: 6.09 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23843519913097544		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.23843519913097544 | validation: 0.17738750207418552]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19514559634911485		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.19514559634911485 | validation: 0.4462433197420227]
	TIME [epoch: 6.09 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26879074544889225		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.26879074544889225 | validation: 0.28132062610223985]
	TIME [epoch: 6.09 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26837100758800236		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.26837100758800236 | validation: 0.24733615006717502]
	TIME [epoch: 6.1 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20807476384132542		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.20807476384132542 | validation: 0.19580968934368725]
	TIME [epoch: 6.09 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20762677476044383		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.20762677476044383 | validation: 0.20322749432736925]
	TIME [epoch: 6.08 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2276516725923137		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.2276516725923137 | validation: 0.21977419428138767]
	TIME [epoch: 6.08 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21200970709101052		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.21200970709101052 | validation: 0.24729454975363097]
	TIME [epoch: 6.09 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19757074582974024		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.19757074582974024 | validation: 0.1965802892784273]
	TIME [epoch: 6.1 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20218905482905491		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.20218905482905491 | validation: 0.2810815968656041]
	TIME [epoch: 6.09 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26211554924217945		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.26211554924217945 | validation: 0.22973670452413297]
	TIME [epoch: 6.09 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20644162175452296		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.20644162175452296 | validation: 0.20436615997999252]
	TIME [epoch: 6.09 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18417032945616826		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.18417032945616826 | validation: 0.2555921579259405]
	TIME [epoch: 6.08 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20424574351519204		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.20424574351519204 | validation: 0.24924352641984468]
	TIME [epoch: 6.09 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31066066377612384		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.31066066377612384 | validation: 0.34433256786838673]
	TIME [epoch: 6.09 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3281250232670082		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.3281250232670082 | validation: 0.3023490987213251]
	TIME [epoch: 6.09 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23788036523121958		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.23788036523121958 | validation: 0.1943451380965383]
	TIME [epoch: 6.09 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17909244010423053		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.17909244010423053 | validation: 0.17816741081527854]
	TIME [epoch: 6.09 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23199928235160394		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.23199928235160394 | validation: 0.2142093417953042]
	TIME [epoch: 6.11 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1842015649404021		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.1842015649404021 | validation: 0.22062547922594178]
	TIME [epoch: 6.09 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1727753023515084		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.1727753023515084 | validation: 0.2212263068948753]
	TIME [epoch: 6.09 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20931484803759443		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.20931484803759443 | validation: 0.32706564245096986]
	TIME [epoch: 6.09 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.204941923754325		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.204941923754325 | validation: 0.22483594356492748]
	TIME [epoch: 6.09 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1623135448520341		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.1623135448520341 | validation: 0.24364285943898512]
	TIME [epoch: 6.09 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2133649793219858		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.2133649793219858 | validation: 0.17776234647398154]
	TIME [epoch: 6.09 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22774017022966414		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.22774017022966414 | validation: 0.2030276254742531]
	TIME [epoch: 6.08 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16058013147484626		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.16058013147484626 | validation: 0.18558517581463002]
	TIME [epoch: 6.1 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15970705828328247		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.15970705828328247 | validation: 0.3120977680936948]
	TIME [epoch: 6.09 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19663751909997876		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.19663751909997876 | validation: 0.23260101008723272]
	TIME [epoch: 6.09 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22752912592671248		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.22752912592671248 | validation: 0.3549174147552622]
	TIME [epoch: 6.09 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24819287157727088		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.24819287157727088 | validation: 0.182756507973076]
	TIME [epoch: 6.09 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1532227599394638		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.1532227599394638 | validation: 0.17354985489697825]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14645542910714485		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.14645542910714485 | validation: 0.3125291971580011]
	TIME [epoch: 6.1 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20866517066429613		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.20866517066429613 | validation: 0.22228964092085884]
	TIME [epoch: 6.1 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2028742623535007		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.2028742623535007 | validation: 0.12597677112148012]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16135601310979497		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.16135601310979497 | validation: 0.2222993231928484]
	TIME [epoch: 6.1 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15486769953042537		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.15486769953042537 | validation: 0.17883839784226996]
	TIME [epoch: 6.1 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16078385145830357		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.16078385145830357 | validation: 0.18241371425973835]
	TIME [epoch: 407 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18186932339232956		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.18186932339232956 | validation: 0.1581750734192331]
	TIME [epoch: 12 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1352774673617105		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.1352774673617105 | validation: 0.1275882975147732]
	TIME [epoch: 12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19081147464526538		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.19081147464526538 | validation: 0.20831708371903312]
	TIME [epoch: 12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18126796679315016		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.18126796679315016 | validation: 0.1703444063824328]
	TIME [epoch: 12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16280989671256177		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.16280989671256177 | validation: 0.18535149609309884]
	TIME [epoch: 12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16288218097783574		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.16288218097783574 | validation: 0.1304113334709096]
	TIME [epoch: 12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11649377527798226		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.11649377527798226 | validation: 0.22337978549608617]
	TIME [epoch: 12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1971412339838404		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.1971412339838404 | validation: 0.16698668942362715]
	TIME [epoch: 12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13500958638204996		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.13500958638204996 | validation: 0.1743497711137315]
	TIME [epoch: 12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23827634628891456		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.23827634628891456 | validation: 0.25042978876322763]
	TIME [epoch: 12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19525632557684464		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.19525632557684464 | validation: 0.18000295094369215]
	TIME [epoch: 12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14129727738883652		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.14129727738883652 | validation: 0.1898277249029139]
	TIME [epoch: 12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1485044352291785		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.1485044352291785 | validation: 0.1906963520191346]
	TIME [epoch: 12.1 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13416266566370041		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.13416266566370041 | validation: 0.19248510395369567]
	TIME [epoch: 12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1895754207345165		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.1895754207345165 | validation: 0.14279244899739707]
	TIME [epoch: 12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10707311041063085		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.10707311041063085 | validation: 0.15474465849916283]
	TIME [epoch: 12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14116641372073777		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.14116641372073777 | validation: 0.2994794556076709]
	TIME [epoch: 12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17916648918159161		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.17916648918159161 | validation: 0.14728964037169145]
	TIME [epoch: 12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13078544581235155		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.13078544581235155 | validation: 0.18025991446006917]
	TIME [epoch: 12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14093329987873862		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.14093329987873862 | validation: 0.13616601225745362]
	TIME [epoch: 12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12429313069354772		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.12429313069354772 | validation: 0.23910806432635537]
	TIME [epoch: 12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20161025199729934		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.20161025199729934 | validation: 0.2734888179938335]
	TIME [epoch: 12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16428227178466215		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.16428227178466215 | validation: 0.1481786458555318]
	TIME [epoch: 12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1499498400701976		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.1499498400701976 | validation: 0.14622053325932063]
	TIME [epoch: 12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1395099657547007		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.1395099657547007 | validation: 0.14291947439744984]
	TIME [epoch: 12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1311768451631437		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.1311768451631437 | validation: 0.14252716805073273]
	TIME [epoch: 12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11089860412831284		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.11089860412831284 | validation: 0.17958457191313978]
	TIME [epoch: 12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15577885711238976		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.15577885711238976 | validation: 0.19443345279443028]
	TIME [epoch: 12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12488474022673582		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.12488474022673582 | validation: 0.11534012082152825]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1233955964334057		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.1233955964334057 | validation: 0.18576572286003307]
	TIME [epoch: 12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13951926181916935		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.13951926181916935 | validation: 0.17073507598909465]
	TIME [epoch: 12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1472416646138958		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1472416646138958 | validation: 0.22474456260404208]
	TIME [epoch: 12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1511519649140859		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.1511519649140859 | validation: 0.1352809207250755]
	TIME [epoch: 12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11043020194046471		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.11043020194046471 | validation: 0.18389293052824152]
	TIME [epoch: 12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11129293097999671		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.11129293097999671 | validation: 0.1132328125242662]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11759415778665712		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.11759415778665712 | validation: 0.19659227087151954]
	TIME [epoch: 12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1576545185515802		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1576545185515802 | validation: 0.17713469143901753]
	TIME [epoch: 12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1387176587007996		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1387176587007996 | validation: 0.0923903963531214]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08804770255060113		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.08804770255060113 | validation: 0.16293078586765375]
	TIME [epoch: 12 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1316250222637447		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1316250222637447 | validation: 0.13281731676027533]
	TIME [epoch: 12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13008045013350364		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.13008045013350364 | validation: 0.09948750733879486]
	TIME [epoch: 12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10763023312800919		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.10763023312800919 | validation: 0.19050560597309185]
	TIME [epoch: 12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1209699056472494		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.1209699056472494 | validation: 0.11490533560183533]
	TIME [epoch: 12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13732526417137625		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.13732526417137625 | validation: 0.1348231660248526]
	TIME [epoch: 12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12257089393325327		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.12257089393325327 | validation: 0.2095790828237103]
	TIME [epoch: 12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14443196705506828		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.14443196705506828 | validation: 0.08793279824734676]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07681231198789296		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.07681231198789296 | validation: 0.13803715118483323]
	TIME [epoch: 12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1290817353187535		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.1290817353187535 | validation: 0.16747205917262875]
	TIME [epoch: 12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10518406859187406		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.10518406859187406 | validation: 0.1347118156136195]
	TIME [epoch: 12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1131089813785388		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.1131089813785388 | validation: 0.1852901188591197]
	TIME [epoch: 12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13326376633539805		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.13326376633539805 | validation: 0.14484686499672578]
	TIME [epoch: 12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11612796804488278		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.11612796804488278 | validation: 0.1081917588746166]
	TIME [epoch: 12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09325687076078791		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.09325687076078791 | validation: 0.17908644573819477]
	TIME [epoch: 12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11433912458138482		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.11433912458138482 | validation: 0.09703224103807165]
	TIME [epoch: 12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13287031337696856		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.13287031337696856 | validation: 0.16643921038595494]
	TIME [epoch: 12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11568176686273551		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.11568176686273551 | validation: 0.17914164129401589]
	TIME [epoch: 12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13305737915774446		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.13305737915774446 | validation: 0.11909145281435737]
	TIME [epoch: 12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10069845433582743		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.10069845433582743 | validation: 0.165432147207729]
	TIME [epoch: 12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1489661338854749		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1489661338854749 | validation: 0.12463016527972436]
	TIME [epoch: 12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09972260358392179		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.09972260358392179 | validation: 0.10281693954850174]
	TIME [epoch: 12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09304158768105547		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.09304158768105547 | validation: 0.10904674512030854]
	TIME [epoch: 12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11545491797335855		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.11545491797335855 | validation: 0.11336787062187266]
	TIME [epoch: 12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09958675580289553		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09958675580289553 | validation: 0.18627760691724973]
	TIME [epoch: 12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1284978449571712		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.1284978449571712 | validation: 0.13432539768768767]
	TIME [epoch: 12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09643370170686072		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.09643370170686072 | validation: 0.12353035662095652]
	TIME [epoch: 12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09412201120306463		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.09412201120306463 | validation: 0.1153008239472036]
	TIME [epoch: 12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11861653866770014		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.11861653866770014 | validation: 0.10888664965841777]
	TIME [epoch: 12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09603453271720346		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.09603453271720346 | validation: 0.08935008863045518]
	TIME [epoch: 12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07663763153738011		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.07663763153738011 | validation: 0.09750113459463783]
	TIME [epoch: 12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10963003292760491		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.10963003292760491 | validation: 0.1631790959292435]
	TIME [epoch: 12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10184698549477512		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.10184698549477512 | validation: 0.09456504301514826]
	TIME [epoch: 12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11533314181795204		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.11533314181795204 | validation: 0.12037288913869124]
	TIME [epoch: 12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08865009089354503		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.08865009089354503 | validation: 0.07601045281077476]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09515067621995482		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.09515067621995482 | validation: 0.08682049824338522]
	TIME [epoch: 12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07260733260254748		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.07260733260254748 | validation: 0.14156319739831688]
	TIME [epoch: 12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09839523853442925		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.09839523853442925 | validation: 0.12225303429368578]
	TIME [epoch: 12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11869914751454932		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.11869914751454932 | validation: 0.09936241572112466]
	TIME [epoch: 12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07697849966885476		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.07697849966885476 | validation: 0.17970109439792337]
	TIME [epoch: 12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1385150575313695		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.1385150575313695 | validation: 0.07650081142362117]
	TIME [epoch: 12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0863157847019956		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.0863157847019956 | validation: 0.10992153005454647]
	TIME [epoch: 12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07295863634144108		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.07295863634144108 | validation: 0.12033965714354655]
	TIME [epoch: 12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10673151163851191		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.10673151163851191 | validation: 0.09994959036989035]
	TIME [epoch: 12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09102114541026067		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.09102114541026067 | validation: 0.1150928380369127]
	TIME [epoch: 12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08951720338314151		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.08951720338314151 | validation: 0.15185284059774523]
	TIME [epoch: 12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12281673426546415		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.12281673426546415 | validation: 0.08901875048791452]
	TIME [epoch: 12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07851560338587435		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.07851560338587435 | validation: 0.16000086542337522]
	TIME [epoch: 12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11179669079135043		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.11179669079135043 | validation: 0.09281235187271039]
	TIME [epoch: 12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0709600452127889		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.0709600452127889 | validation: 0.1344871313754622]
	TIME [epoch: 12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09893516448266194		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.09893516448266194 | validation: 0.08339909909596646]
	TIME [epoch: 12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08687104963495114		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.08687104963495114 | validation: 0.081357959387654]
	TIME [epoch: 12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07641084628731244		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.07641084628731244 | validation: 0.13575581873759254]
	TIME [epoch: 12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0850035243246161		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.0850035243246161 | validation: 0.10364747372575239]
	TIME [epoch: 12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07598382197464065		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.07598382197464065 | validation: 0.10806545163722761]
	TIME [epoch: 12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11882602104324225		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.11882602104324225 | validation: 0.07912393260422404]
	TIME [epoch: 12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09683283851389475		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.09683283851389475 | validation: 0.09603095193743454]
	TIME [epoch: 12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08092507984963708		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.08092507984963708 | validation: 0.0724268629704182]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055938447891551495		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.055938447891551495 | validation: 0.07108292582708148]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11263080473779671		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.11263080473779671 | validation: 0.11990538302584461]
	TIME [epoch: 12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08221681925121624		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.08221681925121624 | validation: 0.10572428416432654]
	TIME [epoch: 12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0658040913652975		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.0658040913652975 | validation: 0.08775912200362745]
	TIME [epoch: 12.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09759315566335162		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.09759315566335162 | validation: 0.09412100817829443]
	TIME [epoch: 12.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0772051508504664		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.0772051508504664 | validation: 0.07356697472366415]
	TIME [epoch: 12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05694238240621786		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.05694238240621786 | validation: 0.0849769476818164]
	TIME [epoch: 12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09818513177369934		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.09818513177369934 | validation: 0.08607558932306358]
	TIME [epoch: 12.1 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07512468168136262		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.07512468168136262 | validation: 0.11725032835445265]
	TIME [epoch: 12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0732015226792697		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.0732015226792697 | validation: 0.06243840535297423]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06583672948036201		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.06583672948036201 | validation: 0.16808131624495073]
	TIME [epoch: 12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09955846427536041		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.09955846427536041 | validation: 0.06400481821103478]
	TIME [epoch: 12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06832348715403488		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.06832348715403488 | validation: 0.11553513969325097]
	TIME [epoch: 12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10385048590262067		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.10385048590262067 | validation: 0.07262810415846925]
	TIME [epoch: 12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051489782404303616		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.051489782404303616 | validation: 0.0684289910214084]
	TIME [epoch: 12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09530832100351391		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.09530832100351391 | validation: 0.07658449670095033]
	TIME [epoch: 12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05995942441294917		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.05995942441294917 | validation: 0.16493374357756688]
	TIME [epoch: 12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08640756478444418		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.08640756478444418 | validation: 0.10145973397061761]
	TIME [epoch: 12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08762814369746469		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.08762814369746469 | validation: 0.08040622590453356]
	TIME [epoch: 12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08510709047305806		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.08510709047305806 | validation: 0.1444743475898661]
	TIME [epoch: 12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11973966594855892		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.11973966594855892 | validation: 0.1445341903027227]
	TIME [epoch: 12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09848359793315263		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.09848359793315263 | validation: 0.07819107100612706]
	TIME [epoch: 12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06326080205264951		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.06326080205264951 | validation: 0.06621894707237576]
	TIME [epoch: 12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05066624876029985		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.05066624876029985 | validation: 0.07632169155613136]
	TIME [epoch: 12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07304759304679039		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07304759304679039 | validation: 0.11880965925436868]
	TIME [epoch: 12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08291580129980282		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08291580129980282 | validation: 0.09762896929098473]
	TIME [epoch: 12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06355010602278276		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.06355010602278276 | validation: 0.07004910900790356]
	TIME [epoch: 12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08023954429168964		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.08023954429168964 | validation: 0.1136949641527249]
	TIME [epoch: 12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08426075214568178		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.08426075214568178 | validation: 0.06024931122213477]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06237442771647626		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.06237442771647626 | validation: 0.07574228163966394]
	TIME [epoch: 12.1 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061568449882310826		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.061568449882310826 | validation: 0.0719546562744797]
	TIME [epoch: 12.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051790955615861486		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.051790955615861486 | validation: 0.112069008330331]
	TIME [epoch: 12.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09254840707601432		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.09254840707601432 | validation: 0.07903195067010234]
	TIME [epoch: 12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07402570329603075		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.07402570329603075 | validation: 0.07816063539245341]
	TIME [epoch: 12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0518911259330634		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.0518911259330634 | validation: 0.07111784125089345]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06042281456452201		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.06042281456452201 | validation: 0.08209064030193128]
	TIME [epoch: 12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05927353441225848		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.05927353441225848 | validation: 0.0684768444033606]
	TIME [epoch: 12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0750860598598398		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.0750860598598398 | validation: 0.08488591998799139]
	TIME [epoch: 12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640943410127145		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.0640943410127145 | validation: 0.05689481399791399]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05144418816137181		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.05144418816137181 | validation: 0.05926176775350078]
	TIME [epoch: 12.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06447517494081507		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.06447517494081507 | validation: 0.15217424288776765]
	TIME [epoch: 12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08493553852818352		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.08493553852818352 | validation: 0.07912556764073059]
	TIME [epoch: 12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06679441986600998		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.06679441986600998 | validation: 0.07155974150490921]
	TIME [epoch: 12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050423390852833136		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.050423390852833136 | validation: 0.06278087110877291]
	TIME [epoch: 12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0712434546884089		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.0712434546884089 | validation: 0.12261768887012525]
	TIME [epoch: 12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08770669501276154		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.08770669501276154 | validation: 0.12609733056079636]
	TIME [epoch: 12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0768494272145377		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.0768494272145377 | validation: 0.06295474021246772]
	TIME [epoch: 12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059565081115557736		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.059565081115557736 | validation: 0.0749962843651127]
	TIME [epoch: 12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05291712166165409		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.05291712166165409 | validation: 0.08959885741123974]
	TIME [epoch: 12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.066905518970543		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.066905518970543 | validation: 0.060984040421650315]
	TIME [epoch: 12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054039469154784955		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.054039469154784955 | validation: 0.06925770030785366]
	TIME [epoch: 12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062190433882259345		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.062190433882259345 | validation: 0.067169918980257]
	TIME [epoch: 12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06479597065975562		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.06479597065975562 | validation: 0.06832397470343626]
	TIME [epoch: 12.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06162442138943054		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.06162442138943054 | validation: 0.06305401582412952]
	TIME [epoch: 12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07374354968962893		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07374354968962893 | validation: 0.08678854282320467]
	TIME [epoch: 12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050963253389665064		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.050963253389665064 | validation: 0.06080021084972842]
	TIME [epoch: 12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0659951346185875		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.0659951346185875 | validation: 0.06201479455367377]
	TIME [epoch: 12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048103452014660625		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.048103452014660625 | validation: 0.07439865174923342]
	TIME [epoch: 12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057160161143454985		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.057160161143454985 | validation: 0.10157859440621622]
	TIME [epoch: 12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08309934900143262		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.08309934900143262 | validation: 0.06867907536511415]
	TIME [epoch: 12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05724452406128766		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.05724452406128766 | validation: 0.06655290336785358]
	TIME [epoch: 12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041927515656845396		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.041927515656845396 | validation: 0.045950479365216235]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059285056977377834		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.059285056977377834 | validation: 0.0960860208639282]
	TIME [epoch: 12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05407554404546881		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.05407554404546881 | validation: 0.05573934938789603]
	TIME [epoch: 12 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04372216096243896		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.04372216096243896 | validation: 0.09741286118593888]
	TIME [epoch: 12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06279657677784138		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.06279657677784138 | validation: 0.06986447482157726]
	TIME [epoch: 12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04779354562031116		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.04779354562031116 | validation: 0.07349091679065806]
	TIME [epoch: 12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06760832369915115		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.06760832369915115 | validation: 0.07587015175933456]
	TIME [epoch: 12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059049475766761596		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.059049475766761596 | validation: 0.05883037529000776]
	TIME [epoch: 12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04140384846571535		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.04140384846571535 | validation: 0.0559695392262278]
	TIME [epoch: 12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0544609651029269		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.0544609651029269 | validation: 0.06152106319434626]
	TIME [epoch: 12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04093514416124673		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.04093514416124673 | validation: 0.08690403894408734]
	TIME [epoch: 12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07020479796643055		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.07020479796643055 | validation: 0.0701054187812159]
	TIME [epoch: 12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05183115875601979		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.05183115875601979 | validation: 0.0542858983219362]
	TIME [epoch: 12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040450855267771854		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.040450855267771854 | validation: 0.0459306867642775]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06132966180191604		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.06132966180191604 | validation: 0.13892594409995784]
	TIME [epoch: 12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1014986573474278		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.1014986573474278 | validation: 0.05248068600476946]
	TIME [epoch: 12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04634050234791425		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.04634050234791425 | validation: 0.07627473997286435]
	TIME [epoch: 12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05568308092077551		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.05568308092077551 | validation: 0.05704389002573318]
	TIME [epoch: 12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03654287711847314		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.03654287711847314 | validation: 0.05756271840179887]
	TIME [epoch: 12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049704779725009404		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.049704779725009404 | validation: 0.10234753799441572]
	TIME [epoch: 12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054683419501211285		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.054683419501211285 | validation: 0.047715153595103484]
	TIME [epoch: 12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039816901188001436		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.039816901188001436 | validation: 0.05687695820909999]
	TIME [epoch: 12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06665773193891572		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.06665773193891572 | validation: 0.045606128134364696]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04108359056685021		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.04108359056685021 | validation: 0.06852686741133297]
	TIME [epoch: 12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640491325549538		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.06640491325549538 | validation: 0.08143324542006318]
	TIME [epoch: 12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051444913890919944		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.051444913890919944 | validation: 0.0459052825867065]
	TIME [epoch: 12.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033532920577431194		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.033532920577431194 | validation: 0.04916310450687404]
	TIME [epoch: 12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06139476069586365		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.06139476069586365 | validation: 0.10949684358535108]
	TIME [epoch: 12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04904116356083224		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.04904116356083224 | validation: 0.05020879889834627]
	TIME [epoch: 12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05639982920224796		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.05639982920224796 | validation: 0.07260605526507524]
	TIME [epoch: 12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047380057630134254		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.047380057630134254 | validation: 0.05556263240198933]
	TIME [epoch: 12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045479222939702295		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.045479222939702295 | validation: 0.07428541251807715]
	TIME [epoch: 12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04113541996157685		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.04113541996157685 | validation: 0.05005955325387142]
	TIME [epoch: 12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04569599785650594		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.04569599785650594 | validation: 0.07645677720710295]
	TIME [epoch: 12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04415775325555896		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.04415775325555896 | validation: 0.05253697982092241]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0505091708493922		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.0505091708493922 | validation: 0.06747878466242518]
	TIME [epoch: 12.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05616105222797984		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.05616105222797984 | validation: 0.04946248666425915]
	TIME [epoch: 12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04491250724589359		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.04491250724589359 | validation: 0.06417186572682913]
	TIME [epoch: 12.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04458606764433885		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04458606764433885 | validation: 0.05683276530247834]
	TIME [epoch: 12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035184235289908536		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.035184235289908536 | validation: 0.05406483985658857]
	TIME [epoch: 12.1 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05133973401010246		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.05133973401010246 | validation: 0.04235078962847513]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03363440960567697		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.03363440960567697 | validation: 0.08889665822228246]
	TIME [epoch: 12.1 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060571643059979446		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.060571643059979446 | validation: 0.045172983395344236]
	TIME [epoch: 12.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03518017002792179		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.03518017002792179 | validation: 0.051568005245696526]
	TIME [epoch: 12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04683571094818095		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.04683571094818095 | validation: 0.038721758529154854]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04437711909532729		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.04437711909532729 | validation: 0.04581725972434188]
	TIME [epoch: 12.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03628097622095369		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.03628097622095369 | validation: 0.052038782909948664]
	TIME [epoch: 12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04092680672289384		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.04092680672289384 | validation: 0.05291853620119203]
	TIME [epoch: 12.1 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03777407539458678		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.03777407539458678 | validation: 0.0651761978773286]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04255686961881678		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.04255686961881678 | validation: 0.051645790790174395]
	TIME [epoch: 12.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03895115071412306		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.03895115071412306 | validation: 0.07631644030553945]
	TIME [epoch: 12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04529825403433543		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.04529825403433543 | validation: 0.03516452296467628]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030596426038891074		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.030596426038891074 | validation: 0.05313885994535313]
	TIME [epoch: 12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05540550284450032		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.05540550284450032 | validation: 0.05937481566816161]
	TIME [epoch: 12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041264637541169324		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.041264637541169324 | validation: 0.06867601164527573]
	TIME [epoch: 12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038117276189760414		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.038117276189760414 | validation: 0.039153490366398785]
	TIME [epoch: 12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03434129617167368		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.03434129617167368 | validation: 0.06942146802792876]
	TIME [epoch: 12 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04441917387115958		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.04441917387115958 | validation: 0.03652429655534869]
	TIME [epoch: 12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04376623043473285		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.04376623043473285 | validation: 0.04969233245354189]
	TIME [epoch: 12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039539964329492416		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.039539964329492416 | validation: 0.07159629712223042]
	TIME [epoch: 12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039193720986813146		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.039193720986813146 | validation: 0.03701569318372894]
	TIME [epoch: 12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03754928008768719		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.03754928008768719 | validation: 0.04915382739040578]
	TIME [epoch: 12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03823855220980804		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.03823855220980804 | validation: 0.05458236578029996]
	TIME [epoch: 12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03909460874997331		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.03909460874997331 | validation: 0.043349059464772785]
	TIME [epoch: 12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037883966635658614		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.037883966635658614 | validation: 0.0363395894692449]
	TIME [epoch: 12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03202244653942095		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.03202244653942095 | validation: 0.06465290902958196]
	TIME [epoch: 12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044972040542186176		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.044972040542186176 | validation: 0.03237339651157225]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03100205308379228		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.03100205308379228 | validation: 0.0549830403876468]
	TIME [epoch: 12.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03651911220086894		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.03651911220086894 | validation: 0.0551538854642199]
	TIME [epoch: 12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04210585212055608		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.04210585212055608 | validation: 0.07391530471368832]
	TIME [epoch: 12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0457637043536418		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.0457637043536418 | validation: 0.03835051607127975]
	TIME [epoch: 12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04176559676155672		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.04176559676155672 | validation: 0.04394310341962511]
	TIME [epoch: 12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03296001959735721		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.03296001959735721 | validation: 0.059087976718188565]
	TIME [epoch: 12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0327504998148362		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.0327504998148362 | validation: 0.03667702810823612]
	TIME [epoch: 12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03270770357109077		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.03270770357109077 | validation: 0.06402722290288439]
	TIME [epoch: 12.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04294380507656122		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.04294380507656122 | validation: 0.03941755606559445]
	TIME [epoch: 12.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03043328969655177		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.03043328969655177 | validation: 0.07167590672505891]
	TIME [epoch: 12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04069075972309117		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.04069075972309117 | validation: 0.03757049344464905]
	TIME [epoch: 12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041785684356638056		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.041785684356638056 | validation: 0.06790253962617326]
	TIME [epoch: 12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04431465650732056		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.04431465650732056 | validation: 0.048139943894507986]
	TIME [epoch: 12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0358860338023529		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.0358860338023529 | validation: 0.044192294509599786]
	TIME [epoch: 12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0360656813103728		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.0360656813103728 | validation: 0.043684791297706235]
	TIME [epoch: 12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03994272179622339		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.03994272179622339 | validation: 0.041243583015247304]
	TIME [epoch: 12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032168199439927374		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.032168199439927374 | validation: 0.036100738347960244]
	TIME [epoch: 12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03146007151938529		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.03146007151938529 | validation: 0.039737571688567855]
	TIME [epoch: 12.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02637613583414754		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.02637613583414754 | validation: 0.027603187682096755]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03702883412815264		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.03702883412815264 | validation: 0.07949420999272755]
	TIME [epoch: 12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049994573920376636		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.049994573920376636 | validation: 0.035654099306421584]
	TIME [epoch: 12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028272478303580918		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.028272478303580918 | validation: 0.03943374525147242]
	TIME [epoch: 12.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0314544328551548		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0314544328551548 | validation: 0.03374067028907583]
	TIME [epoch: 12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038603789597695795		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.038603789597695795 | validation: 0.03651002756902845]
	TIME [epoch: 12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03812359866945532		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.03812359866945532 | validation: 0.042359333570707064]
	TIME [epoch: 12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0323084856496243		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.0323084856496243 | validation: 0.03237832936175036]
	TIME [epoch: 12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032794922286655676		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.032794922286655676 | validation: 0.0437590213436837]
	TIME [epoch: 12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03665822129230753		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03665822129230753 | validation: 0.0431367311453662]
	TIME [epoch: 12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03252666779352844		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.03252666779352844 | validation: 0.03378867817932019]
	TIME [epoch: 12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022190120703364843		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.022190120703364843 | validation: 0.03766210980670549]
	TIME [epoch: 12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04508636585761663		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.04508636585761663 | validation: 0.0305796477683421]
	TIME [epoch: 12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03156400831367452		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.03156400831367452 | validation: 0.047898914090390626]
	TIME [epoch: 12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030774932047842155		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.030774932047842155 | validation: 0.02506328576753814]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023963580098408866		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.023963580098408866 | validation: 0.058691161323268445]
	TIME [epoch: 12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03324474573099854		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.03324474573099854 | validation: 0.029350844381281063]
	TIME [epoch: 12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028504471593630343		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.028504471593630343 | validation: 0.050339047128166114]
	TIME [epoch: 12.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035592303134628765		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.035592303134628765 | validation: 0.03672723698020764]
	TIME [epoch: 12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031121492834143578		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.031121492834143578 | validation: 0.047815119921112766]
	TIME [epoch: 12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032512413966992476		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.032512413966992476 | validation: 0.02986907379212316]
	TIME [epoch: 12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029477100672575904		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.029477100672575904 | validation: 0.04397811417929666]
	TIME [epoch: 12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03401119108932282		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.03401119108932282 | validation: 0.040245518270034863]
	TIME [epoch: 12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0279168028802957		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.0279168028802957 | validation: 0.03950356162748017]
	TIME [epoch: 12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03546613861712336		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.03546613861712336 | validation: 0.0388411919305732]
	TIME [epoch: 12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03618162161789018		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.03618162161789018 | validation: 0.04352910560358174]
	TIME [epoch: 12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023780082454845402		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.023780082454845402 | validation: 0.03001255667077088]
	TIME [epoch: 12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028672637533684903		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.028672637533684903 | validation: 0.04414473223488315]
	TIME [epoch: 12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029381729717226787		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.029381729717226787 | validation: 0.028182399784084648]
	TIME [epoch: 12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028074452217696298		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.028074452217696298 | validation: 0.05433608039612475]
	TIME [epoch: 12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03532702086560066		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.03532702086560066 | validation: 0.029095078637661812]
	TIME [epoch: 12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03658752712842013		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.03658752712842013 | validation: 0.03461161257432152]
	TIME [epoch: 12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024857905238245232		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.024857905238245232 | validation: 0.025118556550284603]
	TIME [epoch: 12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019662086789964006		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.019662086789964006 | validation: 0.03268911636579808]
	TIME [epoch: 12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03813882199234187		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.03813882199234187 | validation: 0.03472427094688797]
	TIME [epoch: 12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02844532440262243		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.02844532440262243 | validation: 0.040420495816767044]
	TIME [epoch: 12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03274314209509459		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.03274314209509459 | validation: 0.04563116091192526]
	TIME [epoch: 12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030155880383033403		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.030155880383033403 | validation: 0.027693935083841267]
	TIME [epoch: 12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025885957673093006		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.025885957673093006 | validation: 0.042528527545596315]
	TIME [epoch: 12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03582691500210914		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.03582691500210914 | validation: 0.037713272580940455]
	TIME [epoch: 12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027750076077237		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.027750076077237 | validation: 0.030888642193627913]
	TIME [epoch: 12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023013559702135387		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.023013559702135387 | validation: 0.0385753088143604]
	TIME [epoch: 12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036112675594001796		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.036112675594001796 | validation: 0.03421524551681987]
	TIME [epoch: 12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033221793849936225		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.033221793849936225 | validation: 0.04153635161877358]
	TIME [epoch: 12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02633926502496218		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.02633926502496218 | validation: 0.027763692433551236]
	TIME [epoch: 12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02016374584595124		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.02016374584595124 | validation: 0.04638489641719387]
	TIME [epoch: 12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04003228341279193		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.04003228341279193 | validation: 0.03923903387089562]
	TIME [epoch: 12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02376436903468181		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.02376436903468181 | validation: 0.024833262513981318]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024375844142241726		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.024375844142241726 | validation: 0.051806019854966384]
	TIME [epoch: 12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03805232749303682		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.03805232749303682 | validation: 0.03341417984426649]
	TIME [epoch: 12 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024495681308246014		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.024495681308246014 | validation: 0.037887742988077205]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025416805527670585		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.025416805527670585 | validation: 0.03354121828569598]
	TIME [epoch: 12.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03716591065377506		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.03716591065377506 | validation: 0.03895023265279364]
	TIME [epoch: 12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02334801061707929		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.02334801061707929 | validation: 0.02520951177321673]
	TIME [epoch: 12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02165011491545328		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.02165011491545328 | validation: 0.038085730058559183]
	TIME [epoch: 12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02297914169368039		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.02297914169368039 | validation: 0.03178083659485618]
	TIME [epoch: 12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03601304801797039		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.03601304801797039 | validation: 0.023407683164405425]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025648366033097263		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.025648366033097263 | validation: 0.031363788507711404]
	TIME [epoch: 439 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026751143709187738		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.026751143709187738 | validation: 0.02857151429312059]
	TIME [epoch: 25.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021373828901707		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.02021373828901707 | validation: 0.03299800313387284]
	TIME [epoch: 25.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031280278906406486		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.031280278906406486 | validation: 0.0450514991759242]
	TIME [epoch: 25.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029123711341286225		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.029123711341286225 | validation: 0.039064900450394055]
	TIME [epoch: 25.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022027346419239462		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.022027346419239462 | validation: 0.029328050910732865]
	TIME [epoch: 25.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02007101324189738		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.02007101324189738 | validation: 0.036143015839859446]
	TIME [epoch: 25.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03257940779974931		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.03257940779974931 | validation: 0.05215869544112981]
	TIME [epoch: 25.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030866663175001127		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.030866663175001127 | validation: 0.025205893341311153]
	TIME [epoch: 25.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01812999403653819		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.01812999403653819 | validation: 0.024779374066363408]
	TIME [epoch: 25.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024067940232724447		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.024067940232724447 | validation: 0.04234368900410707]
	TIME [epoch: 25.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03243175073097321		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.03243175073097321 | validation: 0.02871336958130473]
	TIME [epoch: 25.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022034928546733334		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.022034928546733334 | validation: 0.022551871416732067]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017538344559120844		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.017538344559120844 | validation: 0.030339655889698378]
	TIME [epoch: 25.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032725335539759544		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.032725335539759544 | validation: 0.02731946109726304]
	TIME [epoch: 25.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020551177556437636		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.020551177556437636 | validation: 0.024524440278435547]
	TIME [epoch: 25.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02446417751819032		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.02446417751819032 | validation: 0.028708438827533538]
	TIME [epoch: 25.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024605543546292397		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.024605543546292397 | validation: 0.03187152661511136]
	TIME [epoch: 25.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02266459752735216		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.02266459752735216 | validation: 0.043477452652793416]
	TIME [epoch: 25.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02536907037288437		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.02536907037288437 | validation: 0.03304063797885623]
	TIME [epoch: 25.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025919726777428077		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.025919726777428077 | validation: 0.026473692913267014]
	TIME [epoch: 25.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020254683576088693		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.020254683576088693 | validation: 0.026047013703851915]
	TIME [epoch: 25.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026458280055293742		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.026458280055293742 | validation: 0.04213319297527346]
	TIME [epoch: 25.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02525981192099754		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.02525981192099754 | validation: 0.03601494035211887]
	TIME [epoch: 25.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025738548193698953		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.025738548193698953 | validation: 0.02434891622554396]
	TIME [epoch: 25.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027490868119565626		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.027490868119565626 | validation: 0.04091374657501326]
	TIME [epoch: 25.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030460295380129346		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.030460295380129346 | validation: 0.03051468464460068]
	TIME [epoch: 25.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02576894701092925		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.02576894701092925 | validation: 0.028111332106221902]
	TIME [epoch: 25.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02412504364122189		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.02412504364122189 | validation: 0.031225951147696468]
	TIME [epoch: 25.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019614145785083192		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.019614145785083192 | validation: 0.025375712762699794]
	TIME [epoch: 25.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02626823386432293		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.02626823386432293 | validation: 0.02521749094810538]
	TIME [epoch: 25.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017155181562862963		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.017155181562862963 | validation: 0.02899858579370064]
	TIME [epoch: 25.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026442684337765438		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.026442684337765438 | validation: 0.03268282509407969]
	TIME [epoch: 25.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01926211595705346		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.01926211595705346 | validation: 0.02761417833353693]
	TIME [epoch: 25.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019460646233941115		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.019460646233941115 | validation: 0.027850069724335808]
	TIME [epoch: 25.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02475147726222411		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.02475147726222411 | validation: 0.0273660388413487]
	TIME [epoch: 25.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02134881445714898		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02134881445714898 | validation: 0.028072725760470596]
	TIME [epoch: 25.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03248624272972466		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.03248624272972466 | validation: 0.030721742989768448]
	TIME [epoch: 25.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02100006500393791		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.02100006500393791 | validation: 0.020850844655377336]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019958050988804148		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.019958050988804148 | validation: 0.025567659840454532]
	TIME [epoch: 25.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02218039692191485		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.02218039692191485 | validation: 0.027636466919464267]
	TIME [epoch: 25.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022030310093543982		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.022030310093543982 | validation: 0.03108656856500473]
	TIME [epoch: 25.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024408429368640655		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.024408429368640655 | validation: 0.025619197917067002]
	TIME [epoch: 25.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016825094835507204		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.016825094835507204 | validation: 0.02437454019730353]
	TIME [epoch: 25.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018796826423605625		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.018796826423605625 | validation: 0.03157029837950266]
	TIME [epoch: 25.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020986199408845517		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.020986199408845517 | validation: 0.02800888767181162]
	TIME [epoch: 25.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020414159830632224		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.020414159830632224 | validation: 0.027205321343577386]
	TIME [epoch: 25.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02657447543777028		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.02657447543777028 | validation: 0.02176461216436916]
	TIME [epoch: 25.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021622253219638517		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.021622253219638517 | validation: 0.03822717679692948]
	TIME [epoch: 25.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02478908278843165		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.02478908278843165 | validation: 0.02154280217603354]
	TIME [epoch: 25.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015110182605952824		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.015110182605952824 | validation: 0.03179456632336339]
	TIME [epoch: 25.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026848885103895825		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.026848885103895825 | validation: 0.02625075485252542]
	TIME [epoch: 25.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018875090227842575		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.018875090227842575 | validation: 0.031867963994651756]
	TIME [epoch: 25.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025779824485992107		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.025779824485992107 | validation: 0.023209827777349923]
	TIME [epoch: 25.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016333495099412912		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.016333495099412912 | validation: 0.02717067825976527]
	TIME [epoch: 25.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02458712034016119		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.02458712034016119 | validation: 0.02656602299330882]
	TIME [epoch: 25.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017964966604792743		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.017964966604792743 | validation: 0.021828792755928625]
	TIME [epoch: 25.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017240692601034533		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.017240692601034533 | validation: 0.025318945198834916]
	TIME [epoch: 25.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022374162650791236		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.022374162650791236 | validation: 0.025502639826391005]
	TIME [epoch: 25.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02839630090203507		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.02839630090203507 | validation: 0.049600833647551246]
	TIME [epoch: 25.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02397962372229854		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.02397962372229854 | validation: 0.025114579551581702]
	TIME [epoch: 25.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02470385678107214		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.02470385678107214 | validation: 0.026382855469551485]
	TIME [epoch: 25.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020771275294543957		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.020771275294543957 | validation: 0.021590577010630746]
	TIME [epoch: 25.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0195459087686045		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.0195459087686045 | validation: 0.03146592856161651]
	TIME [epoch: 25.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021412800855848253		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.021412800855848253 | validation: 0.02201915679224328]
	TIME [epoch: 25.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014346905710209724		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.014346905710209724 | validation: 0.024270511656375112]
	TIME [epoch: 25.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02589533162758884		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.02589533162758884 | validation: 0.031318229038596046]
	TIME [epoch: 25.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01958959098249946		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.01958959098249946 | validation: 0.02791070063878988]
	TIME [epoch: 25.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01935822863141697		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.01935822863141697 | validation: 0.023871100258429596]
	TIME [epoch: 25.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016638024314471818		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.016638024314471818 | validation: 0.024078887538122498]
	TIME [epoch: 25.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022131848953560643		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.022131848953560643 | validation: 0.019884328140243476]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02727901291625178		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.02727901291625178 | validation: 0.033319808835814306]
	TIME [epoch: 25.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022946345633695263		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.022946345633695263 | validation: 0.018814544693439135]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015341208285136196		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.015341208285136196 | validation: 0.022357565308903392]
	TIME [epoch: 25.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02128193559967951		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.02128193559967951 | validation: 0.03235471565456411]
	TIME [epoch: 25.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02154063695345165		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.02154063695345165 | validation: 0.028059392274842467]
	TIME [epoch: 25.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01757382017122529		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.01757382017122529 | validation: 0.02535933763498784]
	TIME [epoch: 25.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02003760332302742		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.02003760332302742 | validation: 0.02859926323736105]
	TIME [epoch: 25.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01777488426451227		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.01777488426451227 | validation: 0.021023897912428334]
	TIME [epoch: 25.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01866011812211095		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.01866011812211095 | validation: 0.02782640461583743]
	TIME [epoch: 25.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019429778433180812		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.019429778433180812 | validation: 0.02425835723622423]
	TIME [epoch: 25.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015996757920977274		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.015996757920977274 | validation: 0.02325103242018616]
	TIME [epoch: 25.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02459309905374028		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.02459309905374028 | validation: 0.056360153760515744]
	TIME [epoch: 25.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03376664439463917		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.03376664439463917 | validation: 0.03638100539434471]
	TIME [epoch: 25.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02007435583720201		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.02007435583720201 | validation: 0.030716575516069357]
	TIME [epoch: 25.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019577679113654264		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.019577679113654264 | validation: 0.03066957025900853]
	TIME [epoch: 25.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019968202816497478		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.019968202816497478 | validation: 0.02437619961420025]
	TIME [epoch: 25.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01555743960087242		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.01555743960087242 | validation: 0.024683551696416332]
	TIME [epoch: 25.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02019627957507119		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.02019627957507119 | validation: 0.0314363874981843]
	TIME [epoch: 25.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020048478676184728		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.020048478676184728 | validation: 0.0217125961958829]
	TIME [epoch: 25.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01760616531088888		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.01760616531088888 | validation: 0.023691856644718103]
	TIME [epoch: 25.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02068912267003721		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.02068912267003721 | validation: 0.02373585191109824]
	TIME [epoch: 25.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01883372611692717		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.01883372611692717 | validation: 0.022662914040184057]
	TIME [epoch: 25.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015631813297206733		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.015631813297206733 | validation: 0.02231345716172322]
	TIME [epoch: 25.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019554725532256973		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.019554725532256973 | validation: 0.028492152169593228]
	TIME [epoch: 25.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018145464126891424		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.018145464126891424 | validation: 0.02742500448074926]
	TIME [epoch: 25.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017409668941005946		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.017409668941005946 | validation: 0.026876699515050284]
	TIME [epoch: 25.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017945113859168544		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.017945113859168544 | validation: 0.025605165372736022]
	TIME [epoch: 25.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017496485772971637		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.017496485772971637 | validation: 0.02990214320670337]
	TIME [epoch: 25.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02151247965003458		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.02151247965003458 | validation: 0.019045598853660625]
	TIME [epoch: 25.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013594002282112046		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.013594002282112046 | validation: 0.03459989966653618]
	TIME [epoch: 25.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0262077184161637		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0262077184161637 | validation: 0.025738313401809543]
	TIME [epoch: 25.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018439438837446094		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.018439438837446094 | validation: 0.03544859341654052]
	TIME [epoch: 25.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02228648858424724		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.02228648858424724 | validation: 0.021224338421260237]
	TIME [epoch: 25.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016707551907953387		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.016707551907953387 | validation: 0.02365862438654142]
	TIME [epoch: 25.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017526527489529892		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.017526527489529892 | validation: 0.020924123743320092]
	TIME [epoch: 25.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013847071925640705		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.013847071925640705 | validation: 0.019359931366383143]
	TIME [epoch: 25.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019254150998766775		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.019254150998766775 | validation: 0.030636888287311658]
	TIME [epoch: 25.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01741946545221674		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.01741946545221674 | validation: 0.023227814577163466]
	TIME [epoch: 25.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014628524086278929		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.014628524086278929 | validation: 0.029072790551986548]
	TIME [epoch: 25.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019895431213790497		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.019895431213790497 | validation: 0.02416884181376221]
	TIME [epoch: 25.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01745645717193989		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.01745645717193989 | validation: 0.022318518235920175]
	TIME [epoch: 25.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014146242464887306		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.014146242464887306 | validation: 0.023236199194443453]
	TIME [epoch: 25.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015172187755396449		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.015172187755396449 | validation: 0.025270892435845037]
	TIME [epoch: 25.7 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017768473655632964		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.017768473655632964 | validation: 0.019057156186657508]
	TIME [epoch: 25.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019278055693098057		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.019278055693098057 | validation: 0.023978170731707344]
	TIME [epoch: 25.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01447493703677962		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.01447493703677962 | validation: 0.02111013922270615]
	TIME [epoch: 25.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019741716198126532		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.019741716198126532 | validation: 0.019649477462059604]
	TIME [epoch: 25.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015978725747924607		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.015978725747924607 | validation: 0.021106819243299095]
	TIME [epoch: 25.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013640507324468802		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.013640507324468802 | validation: 0.019073872874716763]
	TIME [epoch: 25.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017664217044342144		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.017664217044342144 | validation: 0.026729219909002774]
	TIME [epoch: 25.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016604320766379892		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.016604320766379892 | validation: 0.02061614971647356]
	TIME [epoch: 25.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015316934449153352		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.015316934449153352 | validation: 0.022728800301661425]
	TIME [epoch: 25.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017685861663414133		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.017685861663414133 | validation: 0.020909883683683038]
	TIME [epoch: 25.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013000993777322268		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.013000993777322268 | validation: 0.019256042856681244]
	TIME [epoch: 25.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018658557069892487		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.018658557069892487 | validation: 0.024131474830251028]
	TIME [epoch: 25.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01683248667225038		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.01683248667225038 | validation: 0.02063454103804658]
	TIME [epoch: 25.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014429349181382879		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.014429349181382879 | validation: 0.020040233670166708]
	TIME [epoch: 25.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015803863300631465		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.015803863300631465 | validation: 0.03420571639323169]
	TIME [epoch: 25.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01804857671922409		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.01804857671922409 | validation: 0.019002862665800736]
	TIME [epoch: 25.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015035287349333971		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.015035287349333971 | validation: 0.025509637749045345]
	TIME [epoch: 25.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01751383820892335		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.01751383820892335 | validation: 0.017120209039896434]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016197298897352146		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.016197298897352146 | validation: 0.0197394563754796]
	TIME [epoch: 25.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016122745628182555		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.016122745628182555 | validation: 0.028692599972183423]
	TIME [epoch: 25.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017544262427412763		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.017544262427412763 | validation: 0.019095111745261142]
	TIME [epoch: 25.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012867558594566033		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.012867558594566033 | validation: 0.021840874143790358]
	TIME [epoch: 25.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013605885422937763		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.013605885422937763 | validation: 0.022133510723502482]
	TIME [epoch: 25.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015183929325196065		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.015183929325196065 | validation: 0.027868216498378208]
	TIME [epoch: 25.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016789650591307315		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.016789650591307315 | validation: 0.02565759079579897]
	TIME [epoch: 25.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01670024518423574		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.01670024518423574 | validation: 0.02443329726410064]
	TIME [epoch: 25.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015717423115012414		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.015717423115012414 | validation: 0.023470403923867456]
	TIME [epoch: 25.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01729270493896519		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.01729270493896519 | validation: 0.019798432845738307]
	TIME [epoch: 25.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01343784603030037		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.01343784603030037 | validation: 0.017836891269850153]
	TIME [epoch: 25.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012292508441555822		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.012292508441555822 | validation: 0.020157298832788162]
	TIME [epoch: 25.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021728955843340367		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.021728955843340367 | validation: 0.01907724639813021]
	TIME [epoch: 25.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012593440570142127		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.012593440570142127 | validation: 0.022493060879165142]
	TIME [epoch: 25.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01470288839777243		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.01470288839777243 | validation: 0.017517063093216483]
	TIME [epoch: 25.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0119151427713558		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0119151427713558 | validation: 0.02044028261340797]
	TIME [epoch: 25.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01881274545820946		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.01881274545820946 | validation: 0.01907414165847177]
	TIME [epoch: 25.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014735791700594861		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.014735791700594861 | validation: 0.019687425714946503]
	TIME [epoch: 25.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014278216017292079		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.014278216017292079 | validation: 0.02018807197797858]
	TIME [epoch: 25.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01557278247689655		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.01557278247689655 | validation: 0.021475451692151308]
	TIME [epoch: 25.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013413076449700955		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.013413076449700955 | validation: 0.01900543279761105]
	TIME [epoch: 25.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01522092317748259		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.01522092317748259 | validation: 0.02192230682594129]
	TIME [epoch: 25.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014201509069648698		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.014201509069648698 | validation: 0.02501448151178557]
	TIME [epoch: 25.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014950862635216501		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.014950862635216501 | validation: 0.019129524899075722]
	TIME [epoch: 25.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019210199560578126		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.019210199560578126 | validation: 0.029102061274975173]
	TIME [epoch: 25.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01763339024430878		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.01763339024430878 | validation: 0.01821739062714325]
	TIME [epoch: 25.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013327791382990166		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.013327791382990166 | validation: 0.01990648633314828]
	TIME [epoch: 25.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0136368497292533		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.0136368497292533 | validation: 0.01622356352575971]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012376229146776822		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.012376229146776822 | validation: 0.01856357348672019]
	TIME [epoch: 25.7 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01558053394402363		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.01558053394402363 | validation: 0.02563699783516224]
	TIME [epoch: 25.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016948764132475532		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.016948764132475532 | validation: 0.017619966797209285]
	TIME [epoch: 25.7 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012922925836199084		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.012922925836199084 | validation: 0.022428565660576494]
	TIME [epoch: 25.7 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01302226612920242		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.01302226612920242 | validation: 0.020142233135620736]
	TIME [epoch: 25.7 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013180258998892799		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.013180258998892799 | validation: 0.022909240124054823]
	TIME [epoch: 25.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02005704147991981		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.02005704147991981 | validation: 0.018377477485546013]
	TIME [epoch: 25.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01212925932617272		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.01212925932617272 | validation: 0.018480177337553486]
	TIME [epoch: 25.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014745617333141847		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.014745617333141847 | validation: 0.0261259812106568]
	TIME [epoch: 25.7 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023645734683734924		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.023645734683734924 | validation: 0.025020460502039238]
	TIME [epoch: 25.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016083185411950404		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.016083185411950404 | validation: 0.021452279454689693]
	TIME [epoch: 25.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015551561191885955		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.015551561191885955 | validation: 0.018186040225421227]
	TIME [epoch: 25.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013455997529883726		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.013455997529883726 | validation: 0.020151070231320834]
	TIME [epoch: 25.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012251700288910681		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.012251700288910681 | validation: 0.017439680579301443]
	TIME [epoch: 25.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013658708475595346		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.013658708475595346 | validation: 0.02828286558362777]
	TIME [epoch: 25.7 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01459633607814682		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.01459633607814682 | validation: 0.017642426787729017]
	TIME [epoch: 25.7 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012556708430296094		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.012556708430296094 | validation: 0.018742920236067242]
	TIME [epoch: 25.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014890349203196755		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.014890349203196755 | validation: 0.018513951432658188]
	TIME [epoch: 25.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012807673054669707		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.012807673054669707 | validation: 0.021068064264009572]
	TIME [epoch: 25.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01734610857142467		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.01734610857142467 | validation: 0.020496436008181244]
	TIME [epoch: 25.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012437258436571612		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.012437258436571612 | validation: 0.01703165739197523]
	TIME [epoch: 25.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01388648950348301		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.01388648950348301 | validation: 0.018807725933124127]
	TIME [epoch: 25.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013418843339486715		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.013418843339486715 | validation: 0.01849247942824407]
	TIME [epoch: 25.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014308743515967165		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.014308743515967165 | validation: 0.015846287996223625]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014656525417521236		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.014656525417521236 | validation: 0.016404017008412305]
	TIME [epoch: 25.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012789206857871013		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.012789206857871013 | validation: 0.016300957572896332]
	TIME [epoch: 25.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01542614192559405		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.01542614192559405 | validation: 0.017603275860281972]
	TIME [epoch: 25.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012048651376628142		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.012048651376628142 | validation: 0.019797132978751096]
	TIME [epoch: 25.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012217837890473336		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.012217837890473336 | validation: 0.019025598748701605]
	TIME [epoch: 25.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017397079479779344		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.017397079479779344 | validation: 0.015848723011840548]
	TIME [epoch: 25.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012349609596720042		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.012349609596720042 | validation: 0.020873403217614446]
	TIME [epoch: 25.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012973473265218761		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.012973473265218761 | validation: 0.017791237608482623]
	TIME [epoch: 25.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01167007570888357		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.01167007570888357 | validation: 0.02211971707124298]
	TIME [epoch: 25.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014640888090930502		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.014640888090930502 | validation: 0.01522448330960546]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011327775143472902		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.011327775143472902 | validation: 0.02406187779349511]
	TIME [epoch: 25.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020918473012704237		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.020918473012704237 | validation: 0.019993535530963797]
	TIME [epoch: 25.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013457511448661344		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.013457511448661344 | validation: 0.019547654020411]
	TIME [epoch: 25.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01341687498828079		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.01341687498828079 | validation: 0.016746552092338655]
	TIME [epoch: 25.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011066382832751041		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.011066382832751041 | validation: 0.015100562474899555]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012117208352359405		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.012117208352359405 | validation: 0.0176781751904928]
	TIME [epoch: 25.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010862124125334428		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.010862124125334428 | validation: 0.017748568334582947]
	TIME [epoch: 25.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015046903909925675		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.015046903909925675 | validation: 0.016041408370748415]
	TIME [epoch: 25.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01255266258800182		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.01255266258800182 | validation: 0.01753249718055563]
	TIME [epoch: 25.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010968794408178486		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.010968794408178486 | validation: 0.017200877206657746]
	TIME [epoch: 25.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014418476248444575		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.014418476248444575 | validation: 0.01718898969548649]
	TIME [epoch: 25.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0105360109394798		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0105360109394798 | validation: 0.02254358538410068]
	TIME [epoch: 25.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019385558487500253		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.019385558487500253 | validation: 0.017132249397939933]
	TIME [epoch: 25.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012563233065259782		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.012563233065259782 | validation: 0.018108955943145537]
	TIME [epoch: 25.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010816067909747285		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.010816067909747285 | validation: 0.016943740517805513]
	TIME [epoch: 25.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012555615160771425		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.012555615160771425 | validation: 0.023377669751685454]
	TIME [epoch: 25.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014611083555636431		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.014611083555636431 | validation: 0.021234163471630152]
	TIME [epoch: 25.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012303732430212329		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.012303732430212329 | validation: 0.0203448058573975]
	TIME [epoch: 25.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013313197619131167		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.013313197619131167 | validation: 0.02103692711200179]
	TIME [epoch: 25.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013283103879832817		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.013283103879832817 | validation: 0.017913991865323407]
	TIME [epoch: 25.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01223443105823021		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.01223443105823021 | validation: 0.024760937804069814]
	TIME [epoch: 25.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014416357938661542		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.014416357938661542 | validation: 0.016676152939838082]
	TIME [epoch: 25.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010511075908432655		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.010511075908432655 | validation: 0.014070139205244477]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011416438343196817		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.011416438343196817 | validation: 0.017647540101434683]
	TIME [epoch: 25.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014160823954024841		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.014160823954024841 | validation: 0.016636373244871536]
	TIME [epoch: 25.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012108551831799303		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.012108551831799303 | validation: 0.01703035212396058]
	TIME [epoch: 25.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011652443427191982		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.011652443427191982 | validation: 0.01868856039930994]
	TIME [epoch: 25.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013417971701169614		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.013417971701169614 | validation: 0.019305870788079705]
	TIME [epoch: 25.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01272238383388526		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.01272238383388526 | validation: 0.02189457379361267]
	TIME [epoch: 25.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012421590308450539		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.012421590308450539 | validation: 0.014450593087210233]
	TIME [epoch: 25.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010232552265522988		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.010232552265522988 | validation: 0.018066544132943955]
	TIME [epoch: 25.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016276866820625846		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.016276866820625846 | validation: 0.021272020066862407]
	TIME [epoch: 25.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01158630451133892		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.01158630451133892 | validation: 0.016841079281693778]
	TIME [epoch: 25.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012992155765936268		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.012992155765936268 | validation: 0.0209792115895086]
	TIME [epoch: 25.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012388042626703493		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.012388042626703493 | validation: 0.018235669207011827]
	TIME [epoch: 25.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01188327590472857		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.01188327590472857 | validation: 0.016042157113593077]
	TIME [epoch: 25.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011693697788006382		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.011693697788006382 | validation: 0.021904955233390387]
	TIME [epoch: 25.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012233771018183726		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.012233771018183726 | validation: 0.01699004762197139]
	TIME [epoch: 25.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011028153692263174		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.011028153692263174 | validation: 0.016096601552195756]
	TIME [epoch: 25.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014385350457593982		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.014385350457593982 | validation: 0.016652884448674465]
	TIME [epoch: 25.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011361720608595641		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.011361720608595641 | validation: 0.01667221185073931]
	TIME [epoch: 25.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011552638107898673		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.011552638107898673 | validation: 0.020903236233804065]
	TIME [epoch: 25.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012486096685132744		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.012486096685132744 | validation: 0.016345446247844852]
	TIME [epoch: 25.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011236545046917306		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.011236545046917306 | validation: 0.018222771457427504]
	TIME [epoch: 25.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01128974889246046		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.01128974889246046 | validation: 0.01783373108620287]
	TIME [epoch: 25.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013188571978169958		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.013188571978169958 | validation: 0.016105602141585518]
	TIME [epoch: 25.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010952477736187776		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.010952477736187776 | validation: 0.019056204263935134]
	TIME [epoch: 25.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011469876177798747		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.011469876177798747 | validation: 0.018784384075704236]
	TIME [epoch: 25.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012000073095595783		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.012000073095595783 | validation: 0.016320262596523492]
	TIME [epoch: 25.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017755213070834663		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.017755213070834663 | validation: 0.017155240853773346]
	TIME [epoch: 25.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01106703480871455		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.01106703480871455 | validation: 0.02175733315243397]
	TIME [epoch: 25.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011608189049283927		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.011608189049283927 | validation: 0.015242011942861046]
	TIME [epoch: 25.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01021454514600945		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.01021454514600945 | validation: 0.01549444167106836]
	TIME [epoch: 25.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009857244720347963		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.009857244720347963 | validation: 0.01513586578328348]
	TIME [epoch: 25.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009366095977971469		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.009366095977971469 | validation: 0.016840411586050923]
	TIME [epoch: 25.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013783387486739285		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.013783387486739285 | validation: 0.01648920282068644]
	TIME [epoch: 25.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01285575134530828		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.01285575134530828 | validation: 0.017629877928110035]
	TIME [epoch: 25.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010649066659559746		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.010649066659559746 | validation: 0.01587918989197191]
	TIME [epoch: 25.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01098874588295471		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.01098874588295471 | validation: 0.015478537393519443]
	TIME [epoch: 25.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011870208379078269		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.011870208379078269 | validation: 0.01971970372068272]
	TIME [epoch: 25.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012608219285210002		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.012608219285210002 | validation: 0.016167511293284333]
	TIME [epoch: 25.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011748590020739847		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.011748590020739847 | validation: 0.01505886187168546]
	TIME [epoch: 25.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011136716045787358		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.011136716045787358 | validation: 0.019534926851003682]
	TIME [epoch: 25.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01024542809377991		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.01024542809377991 | validation: 0.01565983424513586]
	TIME [epoch: 25.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009941834004313293		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.009941834004313293 | validation: 0.017171758564251884]
	TIME [epoch: 25.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011426957586658223		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.011426957586658223 | validation: 0.015191492993135651]
	TIME [epoch: 25.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011877080653753062		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.011877080653753062 | validation: 0.015576794925576638]
	TIME [epoch: 25.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010873879249879842		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.010873879249879842 | validation: 0.015078360921467762]
	TIME [epoch: 25.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010333848773830099		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.010333848773830099 | validation: 0.014654742855075661]
	TIME [epoch: 25.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011433440107147941		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.011433440107147941 | validation: 0.014667195609750602]
	TIME [epoch: 25.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009828746649291871		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.009828746649291871 | validation: 0.014453518295208412]
	TIME [epoch: 25.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009778156832572002		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.009778156832572002 | validation: 0.01725681678353788]
	TIME [epoch: 25.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014418207185377258		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.014418207185377258 | validation: 0.021746320531121924]
	TIME [epoch: 25.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012401606628664262		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.012401606628664262 | validation: 0.013891282510803907]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009299221947193237		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.009299221947193237 | validation: 0.014652595195613065]
	TIME [epoch: 25.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010084255515678629		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.010084255515678629 | validation: 0.016841755147593097]
	TIME [epoch: 25.7 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011749050163893073		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.011749050163893073 | validation: 0.015455558643272463]
	TIME [epoch: 25.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010598534484888143		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.010598534484888143 | validation: 0.01639682074258543]
	TIME [epoch: 25.7 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009554990236251653		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.009554990236251653 | validation: 0.013354233556945237]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010066943118272938		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.010066943118272938 | validation: 0.01487428048366083]
	TIME [epoch: 25.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010972443713238023		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.010972443713238023 | validation: 0.019154289346157274]
	TIME [epoch: 25.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013523263629221663		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.013523263629221663 | validation: 0.014810416787593997]
	TIME [epoch: 25.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010432230059935398		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.010432230059935398 | validation: 0.014813024551442996]
	TIME [epoch: 25.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010158803274842785		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.010158803274842785 | validation: 0.014460558233416103]
	TIME [epoch: 25.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010654594711468303		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.010654594711468303 | validation: 0.015637780299590558]
	TIME [epoch: 25.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009017374203968111		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.009017374203968111 | validation: 0.014926677395925273]
	TIME [epoch: 25.7 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010668841296130608		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.010668841296130608 | validation: 0.01743849001732295]
	TIME [epoch: 25.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010056080018943252		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.010056080018943252 | validation: 0.014934334029075274]
	TIME [epoch: 25.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011063529069053047		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.011063529069053047 | validation: 0.019556132289426625]
	TIME [epoch: 25.7 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010545385083985613		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.010545385083985613 | validation: 0.01587135880208204]
	TIME [epoch: 25.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011003298029254066		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.011003298029254066 | validation: 0.015548317804035849]
	TIME [epoch: 25.7 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010450585758359526		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.010450585758359526 | validation: 0.014432532743030666]
	TIME [epoch: 25.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01059546661770634		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.01059546661770634 | validation: 0.014715010401394599]
	TIME [epoch: 25.7 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010107514095925816		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.010107514095925816 | validation: 0.012804904807928157]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009310908688434713		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.009310908688434713 | validation: 0.013787667564082612]
	TIME [epoch: 25.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008866215016586078		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.008866215016586078 | validation: 0.015799183270517513]
	TIME [epoch: 25.7 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011166380128499898		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.011166380128499898 | validation: 0.01345592438354204]
	TIME [epoch: 25.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009931130064321747		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.009931130064321747 | validation: 0.017360897839848222]
	TIME [epoch: 25.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010831595194946293		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.010831595194946293 | validation: 0.017943737192888387]
	TIME [epoch: 25.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016416234090167126		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.016416234090167126 | validation: 0.0157310393820145]
	TIME [epoch: 25.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011381314897050816		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.011381314897050816 | validation: 0.018614981287445216]
	TIME [epoch: 25.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011359382113563181		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.011359382113563181 | validation: 0.015169656136878148]
	TIME [epoch: 25.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009192388987895745		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.009192388987895745 | validation: 0.014431345310362532]
	TIME [epoch: 25.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00954467571884892		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.00954467571884892 | validation: 0.014527258430866611]
	TIME [epoch: 25.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009529221658607768		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.009529221658607768 | validation: 0.015107536821551658]
	TIME [epoch: 25.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011159844196966254		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.011159844196966254 | validation: 0.014728569123298542]
	TIME [epoch: 25.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009155992033062946		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.009155992033062946 | validation: 0.015574873433727578]
	TIME [epoch: 25.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009152232860575218		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.009152232860575218 | validation: 0.013692169610859707]
	TIME [epoch: 25.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010043168517056522		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.010043168517056522 | validation: 0.013432106555165291]
	TIME [epoch: 25.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011528108308011417		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.011528108308011417 | validation: 0.014069030836814007]
	TIME [epoch: 25.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009113194334160937		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.009113194334160937 | validation: 0.01721170464753187]
	TIME [epoch: 25.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009375071918122613		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.009375071918122613 | validation: 0.014388478350199942]
	TIME [epoch: 25.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011243901713844635		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.011243901713844635 | validation: 0.01472455446296483]
	TIME [epoch: 25.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009927955191893678		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.009927955191893678 | validation: 0.014470584852586753]
	TIME [epoch: 25.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009143190051052207		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.009143190051052207 | validation: 0.014597144336796725]
	TIME [epoch: 25.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010200413769366564		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.010200413769366564 | validation: 0.013766671837989884]
	TIME [epoch: 25.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01112011167139292		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.01112011167139292 | validation: 0.015632754495455217]
	TIME [epoch: 25.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009091616313635548		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.009091616313635548 | validation: 0.014482319295672681]
	TIME [epoch: 25.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00947135263996746		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.00947135263996746 | validation: 0.013885557133291183]
	TIME [epoch: 25.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010391182679014588		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.010391182679014588 | validation: 0.015032616909093662]
	TIME [epoch: 25.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008994620603564905		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.008994620603564905 | validation: 0.014344186516837084]
	TIME [epoch: 25.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009875927945001438		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.009875927945001438 | validation: 0.01611816984158915]
	TIME [epoch: 25.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011126084763500538		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.011126084763500538 | validation: 0.013924038330934634]
	TIME [epoch: 25.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009895243865352015		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.009895243865352015 | validation: 0.013558812327064199]
	TIME [epoch: 25.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009039331463455848		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.009039331463455848 | validation: 0.018027429875198408]
	TIME [epoch: 25.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010445711644207111		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.010445711644207111 | validation: 0.012971313228522413]
	TIME [epoch: 25.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008945430254381454		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.008945430254381454 | validation: 0.01781307510178902]
	TIME [epoch: 25.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010151814501456632		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.010151814501456632 | validation: 0.014264205374526525]
	TIME [epoch: 25.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010035301386281677		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.010035301386281677 | validation: 0.014792947157811515]
	TIME [epoch: 25.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009596649147852342		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.009596649147852342 | validation: 0.013266831406383291]
	TIME [epoch: 25.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009336832097219774		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.009336832097219774 | validation: 0.01579044379139721]
	TIME [epoch: 25.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010260991129408011		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.010260991129408011 | validation: 0.01301233951017276]
	TIME [epoch: 25.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00983618430886021		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.00983618430886021 | validation: 0.014183899594980967]
	TIME [epoch: 25.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009193181116648696		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.009193181116648696 | validation: 0.01430724657755085]
	TIME [epoch: 25.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010114730999776684		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.010114730999776684 | validation: 0.014373510552228977]
	TIME [epoch: 25.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009878712594048811		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.009878712594048811 | validation: 0.013559173965881757]
	TIME [epoch: 25.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008560713288477205		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.008560713288477205 | validation: 0.01311093247190397]
	TIME [epoch: 25.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009153928302320685		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.009153928302320685 | validation: 0.014573859194331146]
	TIME [epoch: 25.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010360929668336592		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.010360929668336592 | validation: 0.013076441667979652]
	TIME [epoch: 25.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00869503736821164		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.00869503736821164 | validation: 0.01221978738497824]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00912821024712621		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.00912821024712621 | validation: 0.014333150599352897]
	TIME [epoch: 25.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009162997330587824		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.009162997330587824 | validation: 0.015203685271864963]
	TIME [epoch: 25.8 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00881507611569847		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.00881507611569847 | validation: 0.013566059838750661]
	TIME [epoch: 25.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009392482703711716		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.009392482703711716 | validation: 0.013043126299429483]
	TIME [epoch: 25.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010619349634359287		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.010619349634359287 | validation: 0.015233261615147927]
	TIME [epoch: 25.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009635359388521605		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.009635359388521605 | validation: 0.013604101347543316]
	TIME [epoch: 25.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009116721338226908		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.009116721338226908 | validation: 0.014020179359239245]
	TIME [epoch: 25.7 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00861970428416868		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.00861970428416868 | validation: 0.013127579441864467]
	TIME [epoch: 25.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008397127993451008		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.008397127993451008 | validation: 0.013485237796384553]
	TIME [epoch: 25.7 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009639035031162004		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.009639035031162004 | validation: 0.014154899686093297]
	TIME [epoch: 25.7 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008269272796575371		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.008269272796575371 | validation: 0.013802660627709566]
	TIME [epoch: 25.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009029011803680655		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.009029011803680655 | validation: 0.014220359215724584]
	TIME [epoch: 25.7 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00992393396181247		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.00992393396181247 | validation: 0.014818613358132962]
	TIME [epoch: 25.7 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009065866160108309		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.009065866160108309 | validation: 0.013787262178016218]
	TIME [epoch: 25.7 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009522960258911643		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.009522960258911643 | validation: 0.013129251070458004]
	TIME [epoch: 25.7 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00874814505414762		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.00874814505414762 | validation: 0.012919024444181675]
	TIME [epoch: 25.7 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008201300539539767		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.008201300539539767 | validation: 0.013124034817667107]
	TIME [epoch: 25.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010872636723263187		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.010872636723263187 | validation: 0.012988003330307321]
	TIME [epoch: 25.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00992108308753843		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.00992108308753843 | validation: 0.013743346993110899]
	TIME [epoch: 25.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008735239682344968		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.008735239682344968 | validation: 0.014117346121893946]
	TIME [epoch: 25.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008745874997133711		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.008745874997133711 | validation: 0.013220312656837114]
	TIME [epoch: 25.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008702546491097538		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.008702546491097538 | validation: 0.013878210836595762]
	TIME [epoch: 25.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008748869174664116		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.008748869174664116 | validation: 0.012380828226264188]
	TIME [epoch: 25.7 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008276492215531651		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.008276492215531651 | validation: 0.013366736466440363]
	TIME [epoch: 25.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008339748848990325		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.008339748848990325 | validation: 0.01807770364888936]
	TIME [epoch: 25.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010088977654922043		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.010088977654922043 | validation: 0.013939829586157488]
	TIME [epoch: 25.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008629810031957976		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.008629810031957976 | validation: 0.01301545426170937]
	TIME [epoch: 25.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00915461840827253		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.00915461840827253 | validation: 0.012613236807165884]
	TIME [epoch: 25.7 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00847045180194871		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.00847045180194871 | validation: 0.012703050581144973]
	TIME [epoch: 25.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008691295768194576		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.008691295768194576 | validation: 0.013962145799242683]
	TIME [epoch: 25.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008742561240981233		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.008742561240981233 | validation: 0.013339701093745871]
	TIME [epoch: 25.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008757307634652595		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.008757307634652595 | validation: 0.015190208171573112]
	TIME [epoch: 25.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009234904515209624		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.009234904515209624 | validation: 0.011751883580592029]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00916344529213348		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.00916344529213348 | validation: 0.013456066879280713]
	TIME [epoch: 25.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008843321197811142		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.008843321197811142 | validation: 0.014133536243140659]
	TIME [epoch: 25.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008662641471982807		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.008662641471982807 | validation: 0.012907805915568176]
	TIME [epoch: 25.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00818693543546376		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.00818693543546376 | validation: 0.01669689514680462]
	TIME [epoch: 25.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010410985615695567		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.010410985615695567 | validation: 0.019401250543411665]
	TIME [epoch: 25.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01050059524302156		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.01050059524302156 | validation: 0.014554015511761404]
	TIME [epoch: 25.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00800570083556858		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.00800570083556858 | validation: 0.012595969644250821]
	TIME [epoch: 25.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008565026511702236		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.008565026511702236 | validation: 0.014382177304756816]
	TIME [epoch: 25.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009794895840379313		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.009794895840379313 | validation: 0.013125320192974584]
	TIME [epoch: 25.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008510006184502246		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.008510006184502246 | validation: 0.014264756886514459]
	TIME [epoch: 25.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007914573733153954		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.007914573733153954 | validation: 0.013499872744772629]
	TIME [epoch: 25.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009160097663952042		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.009160097663952042 | validation: 0.012945926474619568]
	TIME [epoch: 25.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008182427179137997		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.008182427179137997 | validation: 0.01297823097104947]
	TIME [epoch: 25.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087347819404779		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.0087347819404779 | validation: 0.013672405671203695]
	TIME [epoch: 25.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009079292631327573		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.009079292631327573 | validation: 0.012273322701268432]
	TIME [epoch: 25.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008646679596085245		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.008646679596085245 | validation: 0.01241469126685674]
	TIME [epoch: 25.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009253706068549201		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.009253706068549201 | validation: 0.012702554163472164]
	TIME [epoch: 25.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008402833994234051		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.008402833994234051 | validation: 0.012291485929435374]
	TIME [epoch: 25.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008035327924327335		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.008035327924327335 | validation: 0.012922061622764734]
	TIME [epoch: 25.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008629539096180546		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.008629539096180546 | validation: 0.014904043294727356]
	TIME [epoch: 25.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009556361626056392		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.009556361626056392 | validation: 0.013503090901889957]
	TIME [epoch: 25.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008374124588266706		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.008374124588266706 | validation: 0.014774336193646532]
	TIME [epoch: 25.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008762886361398563		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.008762886361398563 | validation: 0.014126739885397796]
	TIME [epoch: 25.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008749810998815223		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.008749810998815223 | validation: 0.012280297678329624]
	TIME [epoch: 25.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008711320239926768		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.008711320239926768 | validation: 0.013764217083625584]
	TIME [epoch: 25.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00788572200226358		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.00788572200226358 | validation: 0.012153299278532956]
	TIME [epoch: 25.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009335963402197708		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.009335963402197708 | validation: 0.01332571691315551]
	TIME [epoch: 25.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008584689253645058		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.008584689253645058 | validation: 0.01589325396174561]
	TIME [epoch: 25.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009315070129768463		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.009315070129768463 | validation: 0.013050440905412505]
	TIME [epoch: 25.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007549764525433933		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.007549764525433933 | validation: 0.014498293188734926]
	TIME [epoch: 25.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007534114234443503		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.007534114234443503 | validation: 0.01203994887469174]
	TIME [epoch: 25.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008742338042674191		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.008742338042674191 | validation: 0.012601489549993317]
	TIME [epoch: 25.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007739739437598536		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.007739739437598536 | validation: 0.012195827805691445]
	TIME [epoch: 25.7 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007601145272258907		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.007601145272258907 | validation: 0.012307827038142037]
	TIME [epoch: 25.7 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007849615417330338		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.007849615417330338 | validation: 0.013355366177294402]
	TIME [epoch: 25.7 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009135667618265909		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.009135667618265909 | validation: 0.013989111469899184]
	TIME [epoch: 25.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008087118959271082		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.008087118959271082 | validation: 0.011484774223585308]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00767778827974036		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.00767778827974036 | validation: 0.014228100821724484]
	TIME [epoch: 25.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0078697088802012		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0078697088802012 | validation: 0.012078671068067726]
	TIME [epoch: 25.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008943695450652038		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.008943695450652038 | validation: 0.013170946141496581]
	TIME [epoch: 25.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008444756638810445		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.008444756638810445 | validation: 0.011099400343883715]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008237312665750887		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.008237312665750887 | validation: 0.01152340943538943]
	TIME [epoch: 25.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007653132779353907		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.007653132779353907 | validation: 0.01203270178483241]
	TIME [epoch: 25.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007277887137377079		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.007277887137377079 | validation: 0.011466646229774703]
	TIME [epoch: 25.7 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007804619017215176		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.007804619017215176 | validation: 0.013790567079769769]
	TIME [epoch: 25.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007963178342202993		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.007963178342202993 | validation: 0.012739673289827192]
	TIME [epoch: 25.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00852870481097634		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.00852870481097634 | validation: 0.011928775755934304]
	TIME [epoch: 25.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007663202813236833		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.007663202813236833 | validation: 0.011836677769172898]
	TIME [epoch: 25.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008129139918201026		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.008129139918201026 | validation: 0.01070777674627904]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008039222467847216		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.008039222467847216 | validation: 0.011045940218939815]
	TIME [epoch: 25.7 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008042737258856271		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.008042737258856271 | validation: 0.012921743682572873]
	TIME [epoch: 25.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076251751060264356		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.0076251751060264356 | validation: 0.014386891963131629]
	TIME [epoch: 25.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007954256760606467		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.007954256760606467 | validation: 0.01207766563846091]
	TIME [epoch: 25.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007763669528744285		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.007763669528744285 | validation: 0.01063459387515971]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007689160106398972		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.007689160106398972 | validation: 0.013776676389985025]
	TIME [epoch: 25.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007996744487135925		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.007996744487135925 | validation: 0.011453895993247226]
	TIME [epoch: 25.7 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007397369248261714		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.007397369248261714 | validation: 0.012358651312737437]
	TIME [epoch: 25.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007684733367381217		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.007684733367381217 | validation: 0.011840300973712243]
	TIME [epoch: 25.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008636033356557249		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.008636033356557249 | validation: 0.010888028802781695]
	TIME [epoch: 25.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007214912956581086		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.007214912956581086 | validation: 0.011643729080883684]
	TIME [epoch: 25.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007806390572663999		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.007806390572663999 | validation: 0.01125547784072694]
	TIME [epoch: 25.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008022611654558625		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.008022611654558625 | validation: 0.012277392313995477]
	TIME [epoch: 25.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007783557586337421		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.007783557586337421 | validation: 0.011351976168980217]
	TIME [epoch: 25.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007233655000466901		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.007233655000466901 | validation: 0.012839991571222637]
	TIME [epoch: 25.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008002708122349827		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.008002708122349827 | validation: 0.012400854807983206]
	TIME [epoch: 25.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00826993036811554		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.00826993036811554 | validation: 0.012527023096314902]
	TIME [epoch: 25.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077726468989413365		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.0077726468989413365 | validation: 0.010948509719680293]
	TIME [epoch: 25.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007393838256750024		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.007393838256750024 | validation: 0.012069072956198612]
	TIME [epoch: 25.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008105465511223429		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.008105465511223429 | validation: 0.011023027944640306]
	TIME [epoch: 25.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007703248317956778		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.007703248317956778 | validation: 0.013018206070768559]
	TIME [epoch: 25.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010034586757132926		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.010034586757132926 | validation: 0.012750163398683075]
	TIME [epoch: 25.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007504209686992596		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.007504209686992596 | validation: 0.01188134445651521]
	TIME [epoch: 25.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007147749632227825		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.007147749632227825 | validation: 0.011323006821226451]
	TIME [epoch: 25.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007527030300649235		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.007527030300649235 | validation: 0.011412774305342162]
	TIME [epoch: 25.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007107508292008555		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.007107508292008555 | validation: 0.011649349050630345]
	TIME [epoch: 25.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007328162095149047		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.007328162095149047 | validation: 0.011116787876613464]
	TIME [epoch: 25.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008363887227862363		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.008363887227862363 | validation: 0.01453448445949818]
	TIME [epoch: 25.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008368679519658273		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.008368679519658273 | validation: 0.011478966845324309]
	TIME [epoch: 25.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007607404869968693		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.007607404869968693 | validation: 0.012994770892686608]
	TIME [epoch: 25.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007566791456291548		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.007566791456291548 | validation: 0.012251960136222085]
	TIME [epoch: 25.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007296026255060932		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.007296026255060932 | validation: 0.012135286150509213]
	TIME [epoch: 25.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006843745188119229		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.006843745188119229 | validation: 0.012655624131035312]
	TIME [epoch: 25.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008591777452789325		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.008591777452789325 | validation: 0.01365172103827025]
	TIME [epoch: 25.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007436721300785073		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.007436721300785073 | validation: 0.01114898091193259]
	TIME [epoch: 25.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071615471201423985		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0071615471201423985 | validation: 0.011368441872623447]
	TIME [epoch: 25.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007202275362082971		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.007202275362082971 | validation: 0.012075194300292365]
	TIME [epoch: 25.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007114117769656192		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.007114117769656192 | validation: 0.011832979142234554]
	TIME [epoch: 25.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007081807254616677		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.007081807254616677 | validation: 0.010828431853398223]
	TIME [epoch: 25.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007973229650382592		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.007973229650382592 | validation: 0.011409403708877186]
	TIME [epoch: 25.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00973917642717714		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.00973917642717714 | validation: 0.011939199424377947]
	TIME [epoch: 25.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072019458182728795		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.0072019458182728795 | validation: 0.01118328673377051]
	TIME [epoch: 25.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007055281937322102		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.007055281937322102 | validation: 0.011622705362276825]
	TIME [epoch: 25.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007536262889659116		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.007536262889659116 | validation: 0.011383739974623266]
	TIME [epoch: 25.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007372202406658252		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.007372202406658252 | validation: 0.011435902645171005]
	TIME [epoch: 25.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007808995354347747		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.007808995354347747 | validation: 0.01092069849855704]
	TIME [epoch: 25.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007595088728329619		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.007595088728329619 | validation: 0.010829535956195568]
	TIME [epoch: 25.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006804891551208666		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.006804891551208666 | validation: 0.010481256429638798]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068838863572673785		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0068838863572673785 | validation: 0.012558467166553521]
	TIME [epoch: 25.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007264463136042531		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.007264463136042531 | validation: 0.009879934384350452]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_966.pth
	Model improved!!!
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007241014068411307		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.007241014068411307 | validation: 0.011335517506849855]
	TIME [epoch: 25.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00724929520383561		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.00724929520383561 | validation: 0.010634902663822287]
	TIME [epoch: 25.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008182567192681208		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.008182567192681208 | validation: 0.01236008489762394]
	TIME [epoch: 25.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00784441582910663		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.00784441582910663 | validation: 0.010247989485774635]
	TIME [epoch: 25.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007117970334034213		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.007117970334034213 | validation: 0.011703001575703871]
	TIME [epoch: 25.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007400906795127052		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.007400906795127052 | validation: 0.01045194656877639]
	TIME [epoch: 25.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007289475157739608		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.007289475157739608 | validation: 0.011787697103362062]
	TIME [epoch: 25.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007341815847793419		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.007341815847793419 | validation: 0.01122998627179485]
	TIME [epoch: 25.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006475463692458828		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.006475463692458828 | validation: 0.009835917245105269]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074357950540308556		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0074357950540308556 | validation: 0.011776389902612164]
	TIME [epoch: 25.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006647625649305384		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.006647625649305384 | validation: 0.01293308630621237]
	TIME [epoch: 25.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007589441829176845		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.007589441829176845 | validation: 0.010972612556497742]
	TIME [epoch: 25.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007755958840892895		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.007755958840892895 | validation: 0.01348991764028119]
	TIME [epoch: 25.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00784486885964441		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.00784486885964441 | validation: 0.011536354086424137]
	TIME [epoch: 25.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006928068210601481		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.006928068210601481 | validation: 0.011516036732432176]
	TIME [epoch: 25.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007483094092101456		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.007483094092101456 | validation: 0.009428819390127232]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007356367453040181		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.007356367453040181 | validation: 0.011330637177116843]
	TIME [epoch: 25.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007854849521872832		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.007854849521872832 | validation: 0.013200565776588206]
	TIME [epoch: 25.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065903403213450965		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0065903403213450965 | validation: 0.010906244055397075]
	TIME [epoch: 25.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067677237397035135		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.0067677237397035135 | validation: 0.00989383679676933]
	TIME [epoch: 25.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00663778257512399		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.00663778257512399 | validation: 0.012006349629449726]
	TIME [epoch: 25.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007262365189415832		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.007262365189415832 | validation: 0.009050317000576236]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007325015001237656		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.007325015001237656 | validation: 0.010497065811540208]
	TIME [epoch: 25.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069891836561048245		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0069891836561048245 | validation: 0.013060144954157315]
	TIME [epoch: 25.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008773308822984365		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.008773308822984365 | validation: 0.01043876675433004]
	TIME [epoch: 25.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072106268848201125		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0072106268848201125 | validation: 0.00987075890345119]
	TIME [epoch: 25.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069297478867540955		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0069297478867540955 | validation: 0.01142832432496863]
	TIME [epoch: 25.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007204335594993406		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.007204335594993406 | validation: 0.011903190646878125]
	TIME [epoch: 25.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072800332077099066		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0072800332077099066 | validation: 0.013195847666242513]
	TIME [epoch: 25.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007303757062930574		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.007303757062930574 | validation: 0.011288940658393207]
	TIME [epoch: 25.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006607518355905921		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.006607518355905921 | validation: 0.012665903515783956]
	TIME [epoch: 25.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007007193882994577		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.007007193882994577 | validation: 0.010338313629096391]
	TIME [epoch: 25.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006884273336759572		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.006884273336759572 | validation: 0.009355058260046564]
	TIME [epoch: 25.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007326402950468365		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.007326402950468365 | validation: 0.015115565816602238]
	TIME [epoch: 25.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00868691864951511		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.00868691864951511 | validation: 0.010919704466053596]
	TIME [epoch: 407 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007028322509635234		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.007028322509635234 | validation: 0.012613566006055777]
	TIME [epoch: 54.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007361542323224681		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.007361542323224681 | validation: 0.010812865655570431]
	TIME [epoch: 54.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006871007654234104		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.006871007654234104 | validation: 0.010543090433501892]
	TIME [epoch: 54.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007238428774676455		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.007238428774676455 | validation: 0.012159469822811676]
	TIME [epoch: 54.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007697964287228519		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.007697964287228519 | validation: 0.012406826430646622]
	TIME [epoch: 54.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074973109686081755		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0074973109686081755 | validation: 0.011579089590146571]
	TIME [epoch: 54.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00711678851383691		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.00711678851383691 | validation: 0.012156605777122766]
	TIME [epoch: 54.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006552876089508801		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.006552876089508801 | validation: 0.009549885681271842]
	TIME [epoch: 54.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006784466645799918		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.006784466645799918 | validation: 0.010846185970804232]
	TIME [epoch: 54.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006477269898683224		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.006477269898683224 | validation: 0.010843357516540995]
	TIME [epoch: 54.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007144455329769168		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.007144455329769168 | validation: 0.011588260470805451]
	TIME [epoch: 54.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068078526750396		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0068078526750396 | validation: 0.009383127643228401]
	TIME [epoch: 54.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006621836579335321		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.006621836579335321 | validation: 0.011059085115475304]
	TIME [epoch: 54.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00663856193426995		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.00663856193426995 | validation: 0.01029768546716038]
	TIME [epoch: 54.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007364821926027863		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.007364821926027863 | validation: 0.009480479933665551]
	TIME [epoch: 54.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006295753766849568		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.006295753766849568 | validation: 0.00953669791864597]
	TIME [epoch: 54.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066130360787524005		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0066130360787524005 | validation: 0.01101419796235963]
	TIME [epoch: 54.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007142996945292637		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.007142996945292637 | validation: 0.010545118988298184]
	TIME [epoch: 54.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007582360457003348		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.007582360457003348 | validation: 0.010561679625620306]
	TIME [epoch: 54.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006690699767797794		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.006690699767797794 | validation: 0.010482468224046857]
	TIME [epoch: 54.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006753768019106427		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.006753768019106427 | validation: 0.01146018265954589]
	TIME [epoch: 54.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006325795880244264		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.006325795880244264 | validation: 0.01062340732411966]
	TIME [epoch: 54.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067499754796473055		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0067499754796473055 | validation: 0.008608958311981108]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1024.pth
	Model improved!!!
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006589660423937196		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.006589660423937196 | validation: 0.010557317329144642]
	TIME [epoch: 54.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006570941981240188		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.006570941981240188 | validation: 0.0102091701609631]
	TIME [epoch: 54.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006601773220214161		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.006601773220214161 | validation: 0.0105590724653898]
	TIME [epoch: 54.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006517790475540107		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.006517790475540107 | validation: 0.009128311207631825]
	TIME [epoch: 54.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006795249691013737		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.006795249691013737 | validation: 0.011436251068893063]
	TIME [epoch: 54.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006829064924334567		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.006829064924334567 | validation: 0.009097477543601436]
	TIME [epoch: 54.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007285651027249063		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.007285651027249063 | validation: 0.010322968318389353]
	TIME [epoch: 54.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007132216426604639		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.007132216426604639 | validation: 0.011325710250933946]
	TIME [epoch: 54.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006613868155231242		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.006613868155231242 | validation: 0.009472485174048249]
	TIME [epoch: 54.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006376161678084883		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.006376161678084883 | validation: 0.009373895841230143]
	TIME [epoch: 54.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007286400048645078		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.007286400048645078 | validation: 0.009437361522588275]
	TIME [epoch: 54.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006797071006169093		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.006797071006169093 | validation: 0.010911382195098481]
	TIME [epoch: 54.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006029447300065401		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.006029447300065401 | validation: 0.00964369765612012]
	TIME [epoch: 54.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005731585744067357		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.005731585744067357 | validation: 0.009059400436331664]
	TIME [epoch: 54.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005980479816255303		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.005980479816255303 | validation: 0.010040891077816343]
	TIME [epoch: 54.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006799167553276532		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.006799167553276532 | validation: 0.009093027342898847]
	TIME [epoch: 54.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00677313545606316		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.00677313545606316 | validation: 0.01013851729935167]
	TIME [epoch: 54.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006370954322631969		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.006370954322631969 | validation: 0.01001523010844707]
	TIME [epoch: 54.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006682772455583893		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.006682772455583893 | validation: 0.010044209092110068]
	TIME [epoch: 54.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006250532996611933		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.006250532996611933 | validation: 0.009467299383309487]
	TIME [epoch: 54.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069843244883995344		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0069843244883995344 | validation: 0.010301793848264474]
	TIME [epoch: 54.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006489460135362328		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.006489460135362328 | validation: 0.00995946298150709]
	TIME [epoch: 54.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006119054240629152		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.006119054240629152 | validation: 0.009265369538686025]
	TIME [epoch: 54.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005925177736549298		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.005925177736549298 | validation: 0.010989907331616528]
	TIME [epoch: 54.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006565653744848315		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.006565653744848315 | validation: 0.008599348331168776]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1049.pth
	Model improved!!!
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006737286424574831		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.006737286424574831 | validation: 0.009654002293673908]
	TIME [epoch: 54.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006214026222020314		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.006214026222020314 | validation: 0.011014625258853338]
	TIME [epoch: 54.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007261109288196745		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.007261109288196745 | validation: 0.009883596366776444]
	TIME [epoch: 54.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006391797165304991		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.006391797165304991 | validation: 0.010046863791431292]
	TIME [epoch: 54.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006792691742412983		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.006792691742412983 | validation: 0.009953668147433944]
	TIME [epoch: 54.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006261464131263314		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.006261464131263314 | validation: 0.009008159832923626]
	TIME [epoch: 54.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006224366960106828		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.006224366960106828 | validation: 0.009949369914763612]
	TIME [epoch: 54.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006862463155400904		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.006862463155400904 | validation: 0.010200523418337817]
	TIME [epoch: 54.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061570704679479995		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0061570704679479995 | validation: 0.012197329079203617]
	TIME [epoch: 54.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005983281286665241		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.005983281286665241 | validation: 0.008581246224179526]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057851410333593185		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0057851410333593185 | validation: 0.00947985925428259]
	TIME [epoch: 54.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006377356314888014		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.006377356314888014 | validation: 0.008798253033917958]
	TIME [epoch: 54.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006565978026605682		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.006565978026605682 | validation: 0.01076067350679705]
	TIME [epoch: 54.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006463530951871023		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.006463530951871023 | validation: 0.008496383446026937]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005875517731366237		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.005875517731366237 | validation: 0.009983686332358747]
	TIME [epoch: 54.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066701592547841115		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0066701592547841115 | validation: 0.008847290395917078]
	TIME [epoch: 54.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006387141199278359		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.006387141199278359 | validation: 0.010183040524622261]
	TIME [epoch: 54.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006213630876226884		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.006213630876226884 | validation: 0.009560145150231413]
	TIME [epoch: 54.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006079560948449938		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.006079560948449938 | validation: 0.00936121378624933]
	TIME [epoch: 54.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006191926811094576		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.006191926811094576 | validation: 0.009364358781726253]
	TIME [epoch: 54.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006001043499404908		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.006001043499404908 | validation: 0.008978918255060938]
	TIME [epoch: 54.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00661855371149441		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.00661855371149441 | validation: 0.010562493353088443]
	TIME [epoch: 54.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006091934746660866		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.006091934746660866 | validation: 0.009681096301201966]
	TIME [epoch: 54.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006045961242351594		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.006045961242351594 | validation: 0.009192827910972975]
	TIME [epoch: 54.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006232124923834738		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.006232124923834738 | validation: 0.010485308396432707]
	TIME [epoch: 54.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006374096159461573		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.006374096159461573 | validation: 0.009154996831048713]
	TIME [epoch: 54.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00578682083492954		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.00578682083492954 | validation: 0.009254426213564678]
	TIME [epoch: 54.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060811862689985514		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0060811862689985514 | validation: 0.009982531554491306]
	TIME [epoch: 54.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007523724211377137		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.007523724211377137 | validation: 0.010299410635698402]
	TIME [epoch: 54.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006275014481340793		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.006275014481340793 | validation: 0.008849467177004265]
	TIME [epoch: 54.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006102098989524407		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.006102098989524407 | validation: 0.00983794717813591]
	TIME [epoch: 54.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005856772386460707		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.005856772386460707 | validation: 0.009986430453359691]
	TIME [epoch: 54.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006772600937091316		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.006772600937091316 | validation: 0.008713152497275476]
	TIME [epoch: 54.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006313392091186525		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.006313392091186525 | validation: 0.008432014944758676]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1083.pth
	Model improved!!!
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005870216773845594		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.005870216773845594 | validation: 0.009571386623965478]
	TIME [epoch: 54.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005983728151148268		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.005983728151148268 | validation: 0.008894408586084474]
	TIME [epoch: 54.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061688404983530265		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.0061688404983530265 | validation: 0.009838312500504178]
	TIME [epoch: 54.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006386664351778704		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.006386664351778704 | validation: 0.008959580106102902]
	TIME [epoch: 54.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006228648302141936		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.006228648302141936 | validation: 0.010127832036461542]
	TIME [epoch: 54.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006759395335798031		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.006759395335798031 | validation: 0.009850625007775825]
	TIME [epoch: 54.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006426255327301971		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.006426255327301971 | validation: 0.008843592672485145]
	TIME [epoch: 54.6 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006121022349790592		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.006121022349790592 | validation: 0.008889640386748113]
	TIME [epoch: 54.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006224284169108963		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.006224284169108963 | validation: 0.009131950304161396]
	TIME [epoch: 54.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006044150835552288		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.006044150835552288 | validation: 0.010006902770891155]
	TIME [epoch: 54.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005906217109414564		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.005906217109414564 | validation: 0.008855435927147645]
	TIME [epoch: 54.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005819825459762833		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.005819825459762833 | validation: 0.008770055871648166]
	TIME [epoch: 54.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006179192430082535		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.006179192430082535 | validation: 0.010022467590540308]
	TIME [epoch: 54.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006553429285745118		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.006553429285745118 | validation: 0.008278004479638971]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1097.pth
	Model improved!!!
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005973415486497359		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.005973415486497359 | validation: 0.008978162220183196]
	TIME [epoch: 54.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006162201887469805		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.006162201887469805 | validation: 0.009182643968753536]
	TIME [epoch: 54.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006060555337489474		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.006060555337489474 | validation: 0.008614441359242763]
	TIME [epoch: 54.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061311654301074246		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0061311654301074246 | validation: 0.010923694979381803]
	TIME [epoch: 54.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006291005971304454		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.006291005971304454 | validation: 0.009855919949161013]
	TIME [epoch: 54.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006021327281410938		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.006021327281410938 | validation: 0.010331596761958108]
	TIME [epoch: 54.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006545999830640876		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.006545999830640876 | validation: 0.009181698852071082]
	TIME [epoch: 54.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005929283196686246		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.005929283196686246 | validation: 0.008315080167011478]
	TIME [epoch: 54.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005801249466627309		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.005801249466627309 | validation: 0.009160941920497113]
	TIME [epoch: 54.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005480276806469785		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.005480276806469785 | validation: 0.008472163798723939]
	TIME [epoch: 54.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005689973487467217		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.005689973487467217 | validation: 0.008116503048078016]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1108.pth
	Model improved!!!
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006261284122340916		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.006261284122340916 | validation: 0.009016306425710194]
	TIME [epoch: 54.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063717902390042404		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0063717902390042404 | validation: 0.009027509909612805]
	TIME [epoch: 54.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005901695277984509		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.005901695277984509 | validation: 0.008599230085113794]
	TIME [epoch: 54.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005735558946103824		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.005735558946103824 | validation: 0.008403608899747084]
	TIME [epoch: 54.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005919744398564459		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.005919744398564459 | validation: 0.008672209030722494]
	TIME [epoch: 54.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005601009561566945		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.005601009561566945 | validation: 0.009208017968647625]
	TIME [epoch: 54.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005284689445196488		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.005284689445196488 | validation: 0.008904400491694615]
	TIME [epoch: 54.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005774660812037936		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.005774660812037936 | validation: 0.008924403634060316]
	TIME [epoch: 54.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006028981234369034		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.006028981234369034 | validation: 0.008869473714469472]
	TIME [epoch: 54.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005771962355505885		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.005771962355505885 | validation: 0.008492339191762115]
	TIME [epoch: 54.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00603849445537784		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.00603849445537784 | validation: 0.009945303965937473]
	TIME [epoch: 54.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006520995210916409		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.006520995210916409 | validation: 0.008351756470973925]
	TIME [epoch: 54.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005496532379902386		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.005496532379902386 | validation: 0.007748602812628783]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057827015902364685		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0057827015902364685 | validation: 0.00913602360887149]
	TIME [epoch: 54.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005453185551483123		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.005453185551483123 | validation: 0.009088354583099499]
	TIME [epoch: 54.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006053562314413046		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.006053562314413046 | validation: 0.009956174748529674]
	TIME [epoch: 54.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006167189684476047		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.006167189684476047 | validation: 0.008733001332661838]
	TIME [epoch: 54.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006063819888287919		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.006063819888287919 | validation: 0.008354610808910036]
	TIME [epoch: 54.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005278264348768285		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.005278264348768285 | validation: 0.009563348757733372]
	TIME [epoch: 54.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005837898336022711		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.005837898336022711 | validation: 0.007611391731465367]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005728751832796211		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.005728751832796211 | validation: 0.008575514229882925]
	TIME [epoch: 54.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005953028515492187		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.005953028515492187 | validation: 0.00778182256520151]
	TIME [epoch: 54.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005571993609363345		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.005571993609363345 | validation: 0.00909618083419451]
	TIME [epoch: 54.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056717585099833046		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.0056717585099833046 | validation: 0.008078367426393427]
	TIME [epoch: 54.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005744297250086423		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.005744297250086423 | validation: 0.007641213356080069]
	TIME [epoch: 54.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00576940526273927		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.00576940526273927 | validation: 0.0084009441916255]
	TIME [epoch: 54.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005541914307751541		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.005541914307751541 | validation: 0.00884550137288187]
	TIME [epoch: 54.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005360434956654831		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.005360434956654831 | validation: 0.008207956354964314]
	TIME [epoch: 54.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059269844993093705		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0059269844993093705 | validation: 0.009241006486589352]
	TIME [epoch: 54.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005770203009397486		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.005770203009397486 | validation: 0.008785887172530434]
	TIME [epoch: 54.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005611983828035691		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.005611983828035691 | validation: 0.008386671817618289]
	TIME [epoch: 54.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005647477224369113		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.005647477224369113 | validation: 0.008124091816108051]
	TIME [epoch: 54.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005550208710012144		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.005550208710012144 | validation: 0.008050223805467379]
	TIME [epoch: 54.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005536766284928753		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.005536766284928753 | validation: 0.00789827090561147]
	TIME [epoch: 54.6 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005674776268681929		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.005674776268681929 | validation: 0.00846029514662782]
	TIME [epoch: 54.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00507361324163002		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.00507361324163002 | validation: 0.007971109783412852]
	TIME [epoch: 54.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005606649683620049		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.005606649683620049 | validation: 0.00885455570111469]
	TIME [epoch: 54.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005626710127592256		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.005626710127592256 | validation: 0.008918114193137365]
	TIME [epoch: 54.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005727264492150291		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.005727264492150291 | validation: 0.008298451954867824]
	TIME [epoch: 54.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005755518495777993		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.005755518495777993 | validation: 0.00821514032566696]
	TIME [epoch: 54.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005234979559640612		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.005234979559640612 | validation: 0.007696385717244071]
	TIME [epoch: 54.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005734431524997665		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.005734431524997665 | validation: 0.009912903907135878]
	TIME [epoch: 54.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005293701839525078		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.005293701839525078 | validation: 0.00837802796651211]
	TIME [epoch: 54.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005580460937180086		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.005580460937180086 | validation: 0.009391197061909978]
	TIME [epoch: 54.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005503426045059096		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.005503426045059096 | validation: 0.008797153888047804]
	TIME [epoch: 54.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005969312395154546		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.005969312395154546 | validation: 0.009135595896879265]
	TIME [epoch: 54.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005601780733875825		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.005601780733875825 | validation: 0.008005218209639235]
	TIME [epoch: 54.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051577727275454776		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.0051577727275454776 | validation: 0.008091256841473342]
	TIME [epoch: 54.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005281832290218753		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.005281832290218753 | validation: 0.008174038758618132]
	TIME [epoch: 54.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005422854849209437		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.005422854849209437 | validation: 0.008022836681991889]
	TIME [epoch: 54.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005253298730300177		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.005253298730300177 | validation: 0.008267201103124311]
	TIME [epoch: 54.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005661817367323333		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.005661817367323333 | validation: 0.00807292860234714]
	TIME [epoch: 54.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005179076058162595		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.005179076058162595 | validation: 0.008429553955861082]
	TIME [epoch: 54.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005533149514945736		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.005533149514945736 | validation: 0.007273235751633595]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1162.pth
	Model improved!!!
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005438734555153904		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.005438734555153904 | validation: 0.009091268900475727]
	TIME [epoch: 54.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056219897647601606		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0056219897647601606 | validation: 0.007820583020623745]
	TIME [epoch: 54.2 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005384330348732496		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.005384330348732496 | validation: 0.007925553468799494]
	TIME [epoch: 54.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005472212647419339		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.005472212647419339 | validation: 0.00947327519978583]
	TIME [epoch: 54.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005449109747609189		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.005449109747609189 | validation: 0.008145003193845842]
	TIME [epoch: 54.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004858599024014971		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.004858599024014971 | validation: 0.007755419148745269]
	TIME [epoch: 54.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005407328288825556		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.005407328288825556 | validation: 0.008859043447986447]
	TIME [epoch: 54.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005531721102879467		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.005531721102879467 | validation: 0.008072607183043621]
	TIME [epoch: 54.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005775869227034565		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.005775869227034565 | validation: 0.00806564925346125]
	TIME [epoch: 54.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005478194262925649		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.005478194262925649 | validation: 0.0076921932429625025]
	TIME [epoch: 54.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005619233124999446		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.005619233124999446 | validation: 0.007979539182342217]
	TIME [epoch: 54.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005230555407607399		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.005230555407607399 | validation: 0.008190739739715302]
	TIME [epoch: 54.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005424031366620144		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.005424031366620144 | validation: 0.008500091384839734]
	TIME [epoch: 54.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054378597361784		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.0054378597361784 | validation: 0.009361719551583398]
	TIME [epoch: 54.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055021590349616884		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.0055021590349616884 | validation: 0.007838598557666636]
	TIME [epoch: 54.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005687763224340957		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.005687763224340957 | validation: 0.007433209179929767]
	TIME [epoch: 54.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005833252169476319		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.005833252169476319 | validation: 0.00648167851274806]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1179.pth
	Model improved!!!
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055700622534281905		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.0055700622534281905 | validation: 0.008219287337723825]
	TIME [epoch: 54.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005217676158743817		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.005217676158743817 | validation: 0.0074292296690494945]
	TIME [epoch: 54.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005427331881025052		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.005427331881025052 | validation: 0.008625783733209373]
	TIME [epoch: 54.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005446469920929304		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.005446469920929304 | validation: 0.007448852103153942]
	TIME [epoch: 54.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00560497224776234		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.00560497224776234 | validation: 0.009211031386606951]
	TIME [epoch: 54.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005402455300041295		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.005402455300041295 | validation: 0.008369646686487174]
	TIME [epoch: 54.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005383612722753934		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.005383612722753934 | validation: 0.007769388816385368]
	TIME [epoch: 54.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005231824502878108		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.005231824502878108 | validation: 0.00766469283059837]
	TIME [epoch: 54.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005296537881424926		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.005296537881424926 | validation: 0.007283313607022595]
	TIME [epoch: 54.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005737926499749294		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.005737926499749294 | validation: 0.006952839783921009]
	TIME [epoch: 54.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005421129394821533		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.005421129394821533 | validation: 0.008267753276485691]
	TIME [epoch: 54.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048021874547347675		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.0048021874547347675 | validation: 0.007689777597444993]
	TIME [epoch: 54.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005078714255101675		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.005078714255101675 | validation: 0.008125946695399837]
	TIME [epoch: 54.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004973567535900602		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.004973567535900602 | validation: 0.007079487437825985]
	TIME [epoch: 54.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004992841543803336		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.004992841543803336 | validation: 0.007744567080177272]
	TIME [epoch: 54.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005224775972670015		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.005224775972670015 | validation: 0.007557245515747682]
	TIME [epoch: 54.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005443751641622649		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.005443751641622649 | validation: 0.008069178112025026]
	TIME [epoch: 54.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004919226007396399		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.004919226007396399 | validation: 0.007478025681325844]
	TIME [epoch: 54.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005450847054371526		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.005450847054371526 | validation: 0.006710323583609184]
	TIME [epoch: 54.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004788602501797603		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.004788602501797603 | validation: 0.007760174028197713]
	TIME [epoch: 54.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005233267113341412		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.005233267113341412 | validation: 0.008198057943794215]
	TIME [epoch: 54.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004972278273490178		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.004972278273490178 | validation: 0.007545667784520452]
	TIME [epoch: 54.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005293793240972907		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.005293793240972907 | validation: 0.007118121810335229]
	TIME [epoch: 54.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004843279514479028		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.004843279514479028 | validation: 0.007894427911941186]
	TIME [epoch: 54.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005085553299759252		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.005085553299759252 | validation: 0.007657014774043062]
	TIME [epoch: 54.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005272272607593108		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.005272272607593108 | validation: 0.008062228961699187]
	TIME [epoch: 54.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005049508292219368		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.005049508292219368 | validation: 0.007603267016792566]
	TIME [epoch: 54.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004868183893561636		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.004868183893561636 | validation: 0.007475452348475811]
	TIME [epoch: 54.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005289093441914524		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.005289093441914524 | validation: 0.008212470025818212]
	TIME [epoch: 54.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005111551470350045		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.005111551470350045 | validation: 0.008770219094678159]
	TIME [epoch: 54.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051504122002861506		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.0051504122002861506 | validation: 0.00810509011241691]
	TIME [epoch: 54.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005238966553883656		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.005238966553883656 | validation: 0.00732024962849596]
	TIME [epoch: 54.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005167575331047116		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.005167575331047116 | validation: 0.007427604472664544]
	TIME [epoch: 54.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005577113337874183		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.005577113337874183 | validation: 0.007894847296610675]
	TIME [epoch: 54.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005110325845084161		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.005110325845084161 | validation: 0.007695930415670043]
	TIME [epoch: 54.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052002077734441075		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.0052002077734441075 | validation: 0.007468764874966064]
	TIME [epoch: 54.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00502801730516598		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.00502801730516598 | validation: 0.007406364922245237]
	TIME [epoch: 54.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005406213746474169		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.005406213746474169 | validation: 0.007150136165128269]
	TIME [epoch: 54.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005026619135787194		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.005026619135787194 | validation: 0.008285068699457714]
	TIME [epoch: 54.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051987892315598725		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.0051987892315598725 | validation: 0.007949935040147408]
	TIME [epoch: 54.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005367065090652081		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.005367065090652081 | validation: 0.007122612862589445]
	TIME [epoch: 54.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004804839689283069		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.004804839689283069 | validation: 0.007242754519861505]
	TIME [epoch: 54.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005807573441255766		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.005807573441255766 | validation: 0.007312165599687761]
	TIME [epoch: 54.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052606383830138695		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0052606383830138695 | validation: 0.006583775439280764]
	TIME [epoch: 54.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00499335125642053		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.00499335125642053 | validation: 0.007216030599834799]
	TIME [epoch: 54.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005265767104541566		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.005265767104541566 | validation: 0.007124485765746413]
	TIME [epoch: 54.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049312197764596315		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0049312197764596315 | validation: 0.007690999227961693]
	TIME [epoch: 54.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051571842959161045		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.0051571842959161045 | validation: 0.006977335715558088]
	TIME [epoch: 54.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050626536080107245		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.0050626536080107245 | validation: 0.00757762287342929]
	TIME [epoch: 54.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005085436957931859		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.005085436957931859 | validation: 0.007821521310137972]
	TIME [epoch: 54.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004718647881944109		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.004718647881944109 | validation: 0.007616943036347679]
	TIME [epoch: 54.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052977688889268605		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.0052977688889268605 | validation: 0.008547259460686218]
	TIME [epoch: 54.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004989746489968051		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.004989746489968051 | validation: 0.007752943278579736]
	TIME [epoch: 54.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005021614580610082		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.005021614580610082 | validation: 0.006920752994337864]
	TIME [epoch: 54.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005342565128778081		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.005342565128778081 | validation: 0.007693567084518294]
	TIME [epoch: 54.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005299319802375621		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.005299319802375621 | validation: 0.00670080433449496]
	TIME [epoch: 54.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004653114098589634		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.004653114098589634 | validation: 0.0072937692250429585]
	TIME [epoch: 54.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048050038282436884		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.0048050038282436884 | validation: 0.00757810069619828]
	TIME [epoch: 54.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004902877859626863		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.004902877859626863 | validation: 0.006411819204954619]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048027166086384784		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.0048027166086384784 | validation: 0.0071738203879124045]
	TIME [epoch: 54.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047493545280900265		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0047493545280900265 | validation: 0.006904808514116853]
	TIME [epoch: 54.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00493613999380133		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.00493613999380133 | validation: 0.007095404476079475]
	TIME [epoch: 54.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048012452313668155		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.0048012452313668155 | validation: 0.007550444303600238]
	TIME [epoch: 54.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004655945577925864		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.004655945577925864 | validation: 0.007321850019533881]
	TIME [epoch: 54.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004998694104502299		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.004998694104502299 | validation: 0.007725853747400597]
	TIME [epoch: 54.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004752981678711417		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.004752981678711417 | validation: 0.007783153202509316]
	TIME [epoch: 54.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005448715470079256		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.005448715470079256 | validation: 0.0065739844934796345]
	TIME [epoch: 54.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004826849445082838		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.004826849445082838 | validation: 0.00863748170998141]
	TIME [epoch: 54.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00498654780227952		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.00498654780227952 | validation: 0.006863976237242918]
	TIME [epoch: 54.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005021546554518704		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.005021546554518704 | validation: 0.007730281376674503]
	TIME [epoch: 54.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005876715600456124		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.005876715600456124 | validation: 0.007140193597039014]
	TIME [epoch: 54.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004965023782584415		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.004965023782584415 | validation: 0.007216277573494721]
	TIME [epoch: 54.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005358367833245511		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.005358367833245511 | validation: 0.008023928442110416]
	TIME [epoch: 54.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005051135968943748		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.005051135968943748 | validation: 0.006937923109880966]
	TIME [epoch: 54.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004793001229975504		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.004793001229975504 | validation: 0.006763037253438777]
	TIME [epoch: 54.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005005616199186422		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.005005616199186422 | validation: 0.007006733653624473]
	TIME [epoch: 54.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004986062374621779		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.004986062374621779 | validation: 0.007687362934487256]
	TIME [epoch: 54.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005052486557025573		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.005052486557025573 | validation: 0.007227455758791626]
	TIME [epoch: 54.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004499294409573423		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.004499294409573423 | validation: 0.006844506956048916]
	TIME [epoch: 54.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004851071667727997		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.004851071667727997 | validation: 0.006375333210481191]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1259.pth
	Model improved!!!
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004725424578138827		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.004725424578138827 | validation: 0.007654212667958906]
	TIME [epoch: 54.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004387403102786963		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.004387403102786963 | validation: 0.006951041518190002]
	TIME [epoch: 54.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004806013901936674		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.004806013901936674 | validation: 0.006637675032732338]
	TIME [epoch: 54.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004679564572984044		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.004679564572984044 | validation: 0.007649617449374686]
	TIME [epoch: 54.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004560382719537574		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.004560382719537574 | validation: 0.006975339282005986]
	TIME [epoch: 54.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004892382063692755		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.004892382063692755 | validation: 0.0074074129868667]
	TIME [epoch: 54.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004556736050381869		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.004556736050381869 | validation: 0.006590562107413321]
	TIME [epoch: 54.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00493133802996786		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.00493133802996786 | validation: 0.007034600235732425]
	TIME [epoch: 54.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004865935611383735		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.004865935611383735 | validation: 0.007992084487165083]
	TIME [epoch: 54.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005113160469527007		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.005113160469527007 | validation: 0.007712855911431428]
	TIME [epoch: 54.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004889032952417561		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.004889032952417561 | validation: 0.006891594905750056]
	TIME [epoch: 54.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004736546819349582		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.004736546819349582 | validation: 0.0070566189614238595]
	TIME [epoch: 54.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005050651041432366		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.005050651041432366 | validation: 0.00714519960302788]
	TIME [epoch: 54.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043673829131827815		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.0043673829131827815 | validation: 0.00735002503149637]
	TIME [epoch: 54.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004590317352611727		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.004590317352611727 | validation: 0.006222445817461821]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1274.pth
	Model improved!!!
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004573610532872693		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.004573610532872693 | validation: 0.008005503675708386]
	TIME [epoch: 54.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004874417878979567		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.004874417878979567 | validation: 0.008731074479687715]
	TIME [epoch: 54.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005174904187426657		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.005174904187426657 | validation: 0.006406432898485817]
	TIME [epoch: 54.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004471819129953703		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.004471819129953703 | validation: 0.0070833577874542305]
	TIME [epoch: 54.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005013649415182071		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.005013649415182071 | validation: 0.007987344128259516]
	TIME [epoch: 54.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004640194941211744		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.004640194941211744 | validation: 0.00665759926353174]
	TIME [epoch: 54.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046397255278117795		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.0046397255278117795 | validation: 0.007129456131582003]
	TIME [epoch: 54.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004725879098483418		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.004725879098483418 | validation: 0.006550678716132875]
	TIME [epoch: 54.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005109654964196		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.005109654964196 | validation: 0.0070821486611580275]
	TIME [epoch: 54.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004858240042547474		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.004858240042547474 | validation: 0.00703239989743366]
	TIME [epoch: 54.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045271339381862934		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.0045271339381862934 | validation: 0.007170919070626676]
	TIME [epoch: 54.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004913435299469689		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.004913435299469689 | validation: 0.007626120331457601]
	TIME [epoch: 54.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00471636791537253		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.00471636791537253 | validation: 0.006625745914830252]
	TIME [epoch: 54.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00464218012461667		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.00464218012461667 | validation: 0.007277866436402723]
	TIME [epoch: 54.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004703243906529581		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.004703243906529581 | validation: 0.006972386961486632]
	TIME [epoch: 54.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005078003070260527		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.005078003070260527 | validation: 0.007278847901100607]
	TIME [epoch: 54.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00470834728614445		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.00470834728614445 | validation: 0.007153001968262359]
	TIME [epoch: 54.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00474217714769431		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.00474217714769431 | validation: 0.007991159849802774]
	TIME [epoch: 54.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005081534206256417		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.005081534206256417 | validation: 0.006589419798224843]
	TIME [epoch: 54.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004988650178811753		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.004988650178811753 | validation: 0.00741843663292756]
	TIME [epoch: 54.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004468021338398556		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.004468021338398556 | validation: 0.00618194374550151]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1295.pth
	Model improved!!!
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004974568781067155		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.004974568781067155 | validation: 0.007256724432745446]
	TIME [epoch: 54.4 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004505227815780761		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.004505227815780761 | validation: 0.00588828914807466]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1297.pth
	Model improved!!!
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004418476489914987		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.004418476489914987 | validation: 0.006913964667143142]
	TIME [epoch: 54.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005103243402469679		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.005103243402469679 | validation: 0.0075670672707267884]
	TIME [epoch: 54.4 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602090812848945		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.004602090812848945 | validation: 0.007151600214685122]
	TIME [epoch: 54.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050104578528449364		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.0050104578528449364 | validation: 0.006741672516041285]
	TIME [epoch: 54.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004582821481999998		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.004582821481999998 | validation: 0.00690721755786273]
	TIME [epoch: 54.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004564219909326807		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.004564219909326807 | validation: 0.008046123914579864]
	TIME [epoch: 54.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053173263710525245		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.0053173263710525245 | validation: 0.006379531064044748]
	TIME [epoch: 54.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004525594644574436		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.004525594644574436 | validation: 0.00700151055755556]
	TIME [epoch: 54.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046792637879760025		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.0046792637879760025 | validation: 0.006526155047834247]
	TIME [epoch: 54.4 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004672797899039196		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.004672797899039196 | validation: 0.0067565598723419275]
	TIME [epoch: 54.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004595869161947446		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.004595869161947446 | validation: 0.006906096663305786]
	TIME [epoch: 54.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004594514643241043		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.004594514643241043 | validation: 0.006969293433098872]
	TIME [epoch: 54.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004278695710267056		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.004278695710267056 | validation: 0.0065052490642917085]
	TIME [epoch: 54.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005108572180877319		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.005108572180877319 | validation: 0.00741399576912002]
	TIME [epoch: 54.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004432899813072604		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.004432899813072604 | validation: 0.007049309816719345]
	TIME [epoch: 54.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004501181626385476		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.004501181626385476 | validation: 0.007004434014077775]
	TIME [epoch: 54.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004903647765416094		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.004903647765416094 | validation: 0.00676533432757265]
	TIME [epoch: 54.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004658357602371072		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.004658357602371072 | validation: 0.006456473762373027]
	TIME [epoch: 54.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004619245543252489		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.004619245543252489 | validation: 0.006872401362589899]
	TIME [epoch: 54.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004618761845960305		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.004618761845960305 | validation: 0.006700398778840648]
	TIME [epoch: 54.4 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004516126040120674		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.004516126040120674 | validation: 0.007008085797060349]
	TIME [epoch: 54.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005024962942449373		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.005024962942449373 | validation: 0.006305322857759614]
	TIME [epoch: 54.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00469122156065439		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.00469122156065439 | validation: 0.0075108806272061205]
	TIME [epoch: 54.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004443090175644855		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.004443090175644855 | validation: 0.006180582902474885]
	TIME [epoch: 54.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004766029315545429		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.004766029315545429 | validation: 0.006850146697313028]
	TIME [epoch: 54.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004775617093339838		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.004775617093339838 | validation: 0.005783672031525564]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1323.pth
	Model improved!!!
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00468948065483886		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.00468948065483886 | validation: 0.007009721503760405]
	TIME [epoch: 54.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004201907784351908		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.004201907784351908 | validation: 0.007307997865203376]
	TIME [epoch: 54.4 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004507022336713367		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.004507022336713367 | validation: 0.006677762080119905]
	TIME [epoch: 54.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004497976086920991		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.004497976086920991 | validation: 0.005770575208941443]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1327.pth
	Model improved!!!
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004731560414518733		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.004731560414518733 | validation: 0.0062428322655874325]
	TIME [epoch: 54.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004747129447402174		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.004747129447402174 | validation: 0.006593025877818436]
	TIME [epoch: 54.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004606969878567449		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.004606969878567449 | validation: 0.006973299203958328]
	TIME [epoch: 54.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004759048336279572		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.004759048336279572 | validation: 0.006673243157741277]
	TIME [epoch: 54.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004736741512416279		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.004736741512416279 | validation: 0.006906586090711357]
	TIME [epoch: 54.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004255218339912043		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.004255218339912043 | validation: 0.006138634042493346]
	TIME [epoch: 54.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004665679376957779		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.004665679376957779 | validation: 0.006332028343985815]
	TIME [epoch: 54.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004286859821376755		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.004286859821376755 | validation: 0.007518109613317376]
	TIME [epoch: 54.2 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004588004291300101		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.004588004291300101 | validation: 0.006952245556523305]
	TIME [epoch: 54.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004671056934380599		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.004671056934380599 | validation: 0.007167461819825895]
	TIME [epoch: 54.2 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004650618079113683		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.004650618079113683 | validation: 0.0062220770969317295]
	TIME [epoch: 54.2 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004234775121661979		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.004234775121661979 | validation: 0.006703902905771181]
	TIME [epoch: 54.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004528883600470128		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.004528883600470128 | validation: 0.006825059753648997]
	TIME [epoch: 54.2 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004325482188120568		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.004325482188120568 | validation: 0.006184851756578374]
	TIME [epoch: 54.2 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045162511920932786		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.0045162511920932786 | validation: 0.006179772219673214]
	TIME [epoch: 54.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004431898071751872		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.004431898071751872 | validation: 0.007072470306338275]
	TIME [epoch: 54.3 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004468123221199759		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.004468123221199759 | validation: 0.006894412417150703]
	TIME [epoch: 54.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004470647294806642		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.004470647294806642 | validation: 0.0067866968589855]
	TIME [epoch: 54.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004501468191795322		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.004501468191795322 | validation: 0.006820903366324852]
	TIME [epoch: 54.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004408787573645011		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.004408787573645011 | validation: 0.0073575982897263114]
	TIME [epoch: 54.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00432281546024243		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.00432281546024243 | validation: 0.006339066762266579]
	TIME [epoch: 54.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004653596346924976		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.004653596346924976 | validation: 0.0063330971955074785]
	TIME [epoch: 54.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004438778161594958		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.004438778161594958 | validation: 0.005971166959445485]
	TIME [epoch: 54.4 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004435838029507902		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.004435838029507902 | validation: 0.006300348911746986]
	TIME [epoch: 54.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004865299036249624		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.004865299036249624 | validation: 0.005991361655501091]
	TIME [epoch: 54.3 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004457010643068716		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.004457010643068716 | validation: 0.006927640230766367]
	TIME [epoch: 54.4 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004409198168434071		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.004409198168434071 | validation: 0.005630984061001578]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1354.pth
	Model improved!!!
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004617827493119318		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.004617827493119318 | validation: 0.005865156430233718]
	TIME [epoch: 54.3 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004544414628617473		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.004544414628617473 | validation: 0.005553167151269812]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1356.pth
	Model improved!!!
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004954079649286975		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.004954079649286975 | validation: 0.006171246775356999]
	TIME [epoch: 54.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004255414966177158		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.004255414966177158 | validation: 0.005842614882583621]
	TIME [epoch: 54.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004457688262097764		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.004457688262097764 | validation: 0.006163072004427321]
	TIME [epoch: 54.2 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004303392376126295		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.004303392376126295 | validation: 0.005711765520747338]
	TIME [epoch: 54.2 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004546092730067641		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.004546092730067641 | validation: 0.00517145548335274]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1361.pth
	Model improved!!!
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004263657781957804		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.004263657781957804 | validation: 0.006928210679857313]
	TIME [epoch: 54.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00416656287304168		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.00416656287304168 | validation: 0.007021538480855654]
	TIME [epoch: 54.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039893723512756125		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.0039893723512756125 | validation: 0.007069938398751239]
	TIME [epoch: 54.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004541400705616812		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.004541400705616812 | validation: 0.005610308043271887]
	TIME [epoch: 54.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004435576732863117		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.004435576732863117 | validation: 0.0059318239243401644]
	TIME [epoch: 54.3 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004183990006459266		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.004183990006459266 | validation: 0.00744943139096051]
	TIME [epoch: 54.4 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004352317622193246		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.004352317622193246 | validation: 0.005605815578616338]
	TIME [epoch: 54.4 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004342590006300298		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.004342590006300298 | validation: 0.006348386657578276]
	TIME [epoch: 54.4 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004367449861871173		[learning rate: 9.3243e-05]
	Learning Rate: 9.32429e-05
	LOSS [training: 0.004367449861871173 | validation: 0.006626799183712799]
	TIME [epoch: 54.4 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004155630374140117		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.004155630374140117 | validation: 0.006205357372490892]
	TIME [epoch: 54.4 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004307454284055571		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.004307454284055571 | validation: 0.00631527312166877]
	TIME [epoch: 54.4 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004558997987506067		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.004558997987506067 | validation: 0.006525935790074023]
	TIME [epoch: 54.4 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004383006062075052		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.004383006062075052 | validation: 0.005874885078643054]
	TIME [epoch: 54.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00419706633041358		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.00419706633041358 | validation: 0.006774421312660095]
	TIME [epoch: 54.4 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004267221794043681		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.004267221794043681 | validation: 0.006036286821246609]
	TIME [epoch: 54.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004124326803554654		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.004124326803554654 | validation: 0.006162560152226507]
	TIME [epoch: 54.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046959361574370456		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.0046959361574370456 | validation: 0.006292522326457229]
	TIME [epoch: 54.4 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004468830276529067		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.004468830276529067 | validation: 0.0063140942467490736]
	TIME [epoch: 54.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00427550579682992		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.00427550579682992 | validation: 0.006932228407300434]
	TIME [epoch: 54.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004350210890835323		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.004350210890835323 | validation: 0.005699967283494872]
	TIME [epoch: 54.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004783555243837603		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.004783555243837603 | validation: 0.005113754764707324]
	TIME [epoch: 54.4 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1382.pth
	Model improved!!!
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042577826642136015		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.0042577826642136015 | validation: 0.006288257143954027]
	TIME [epoch: 54.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00410541496956373		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.00410541496956373 | validation: 0.005574365706880432]
	TIME [epoch: 54.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004393538868911702		[learning rate: 8.8418e-05]
	Learning Rate: 8.84176e-05
	LOSS [training: 0.004393538868911702 | validation: 0.005429626536051862]
	TIME [epoch: 54.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042678490483486825		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.0042678490483486825 | validation: 0.006199637977124737]
	TIME [epoch: 54.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004396882530134693		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.004396882530134693 | validation: 0.0060084729118222895]
	TIME [epoch: 54.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004248264261504035		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.004248264261504035 | validation: 0.005451141674077883]
	TIME [epoch: 54.2 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004239067646617897		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.004239067646617897 | validation: 0.006773309416568208]
	TIME [epoch: 54.2 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004083232786974349		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.004083232786974349 | validation: 0.006558373985736428]
	TIME [epoch: 54.2 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004211420902475608		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.004211420902475608 | validation: 0.0057838281104093114]
	TIME [epoch: 54.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004397813463765128		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.004397813463765128 | validation: 0.006551982701200803]
	TIME [epoch: 54.2 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00433507715649379		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.00433507715649379 | validation: 0.005815988963031999]
	TIME [epoch: 54.3 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004474092043471186		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.004474092043471186 | validation: 0.005906524481596301]
	TIME [epoch: 54.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040653269385992185		[learning rate: 8.534e-05]
	Learning Rate: 8.53403e-05
	LOSS [training: 0.0040653269385992185 | validation: 0.006277123043557679]
	TIME [epoch: 54.4 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004442275527251709		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.004442275527251709 | validation: 0.00664615397600019]
	TIME [epoch: 54.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004393533139352161		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.004393533139352161 | validation: 0.005421887124498366]
	TIME [epoch: 54.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004453612553199582		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.004453612553199582 | validation: 0.006268964241980213]
	TIME [epoch: 54.3 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004275256904473573		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.004275256904473573 | validation: 0.0056953464303321835]
	TIME [epoch: 54.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00431814347057148		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.00431814347057148 | validation: 0.006437614290805004]
	TIME [epoch: 54.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00439686370879082		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.00439686370879082 | validation: 0.005652329889676067]
	TIME [epoch: 54.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004152154261078612		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.004152154261078612 | validation: 0.005692551240777053]
	TIME [epoch: 54.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004273454954508996		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.004273454954508996 | validation: 0.005485223326927522]
	TIME [epoch: 54.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004293289852435094		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.004293289852435094 | validation: 0.006052641983693813]
	TIME [epoch: 54.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004125447589167533		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.004125447589167533 | validation: 0.005877264073031239]
	TIME [epoch: 54.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004352358522780854		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.004352358522780854 | validation: 0.005518387322595339]
	TIME [epoch: 54.3 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004126114300204582		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.004126114300204582 | validation: 0.0061266218211394495]
	TIME [epoch: 54.3 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003907965371186191		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.003907965371186191 | validation: 0.005621432769331464]
	TIME [epoch: 54.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004170668636987547		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.004170668636987547 | validation: 0.0057227934805836374]
	TIME [epoch: 54.3 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039673360477321265		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.0039673360477321265 | validation: 0.006586422511047126]
	TIME [epoch: 54.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004110692021002315		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.004110692021002315 | validation: 0.006250965395751048]
	TIME [epoch: 54.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004254691558976141		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.004254691558976141 | validation: 0.005547086741479378]
	TIME [epoch: 54.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004362604416997649		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.004362604416997649 | validation: 0.005480550870812659]
	TIME [epoch: 54.3 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004612270075963595		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.004612270075963595 | validation: 0.006999916396272309]
	TIME [epoch: 54.3 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00431362294593179		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.00431362294593179 | validation: 0.006089685503558934]
	TIME [epoch: 54.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043108922958367815		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.0043108922958367815 | validation: 0.006234397940312658]
	TIME [epoch: 54.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004448844025321246		[learning rate: 7.8942e-05]
	Learning Rate: 7.89419e-05
	LOSS [training: 0.004448844025321246 | validation: 0.0051095207124563025]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_0_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_0_v_mmd1_1417.pth
	Model improved!!!
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004020224358713218		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.004020224358713218 | validation: 0.005974161539963814]
	TIME [epoch: 54.3 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039008735857870066		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.0039008735857870066 | validation: 0.005895921368188211]
	TIME [epoch: 54.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004238339441017737		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.004238339441017737 | validation: 0.006453480752990167]
	TIME [epoch: 54.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004068767336646494		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.004068767336646494 | validation: 0.005579444375204826]
	TIME [epoch: 54.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004434034817845525		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.004434034817845525 | validation: 0.005817106591332691]
	TIME [epoch: 54.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004519305620378111		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.004519305620378111 | validation: 0.005544535962905857]
	TIME [epoch: 54.3 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045203474162191296		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.0045203474162191296 | validation: 0.006394153870109286]
	TIME [epoch: 54.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004355258441838385		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.004355258441838385 | validation: 0.00526173615932229]
	TIME [epoch: 54.3 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043302390548921755		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.0043302390548921755 | validation: 0.005419395638553751]
	TIME [epoch: 54.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004158444034887207		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.004158444034887207 | validation: 0.005381542495002758]
	TIME [epoch: 54.2 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004275202725595438		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.004275202725595438 | validation: 0.006620016848524073]
	TIME [epoch: 54.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004374081218615498		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.004374081218615498 | validation: 0.0053330659790731426]
	TIME [epoch: 54.3 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004060339663405648		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.004060339663405648 | validation: 0.006154435272794279]
	TIME [epoch: 54.2 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004255479371445831		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.004255479371445831 | validation: 0.0056241596042461844]
	TIME [epoch: 54.2 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004011418343425561		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.004011418343425561 | validation: 0.006395013584798917]
	TIME [epoch: 54.2 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004009861155732229		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 0.004009861155732229 | validation: 0.005905487855118345]
	TIME [epoch: 54.4 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004159798347730087		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 0.004159798347730087 | validation: 0.005262904873741941]
	TIME [epoch: 54.4 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004032907081739685		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 0.004032907081739685 | validation: 0.006502160237103583]
	TIME [epoch: 54.4 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004612891714719114		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 0.004612891714719114 | validation: 0.005457199201972398]
	TIME [epoch: 54.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004396599516810715		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 0.004396599516810715 | validation: 0.00568028976255369]
	TIME [epoch: 54.4 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004227249755358867		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 0.004227249755358867 | validation: 0.006083091533690358]
	TIME [epoch: 54.3 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004440259003471924		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 0.004440259003471924 | validation: 0.006814866738894093]
	TIME [epoch: 54.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003854780281687182		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 0.003854780281687182 | validation: 0.005986111833923413]
	TIME [epoch: 54.3 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003986406573348528		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 0.003986406573348528 | validation: 0.005747440176253992]
	TIME [epoch: 54.3 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004121536782857049		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 0.004121536782857049 | validation: 0.005936284318883618]
	TIME [epoch: 54.4 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004269616969446841		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 0.004269616969446841 | validation: 0.005585058069009746]
	TIME [epoch: 54.3 sec]
EPOCH 1444/2000:
	Training over batches...
