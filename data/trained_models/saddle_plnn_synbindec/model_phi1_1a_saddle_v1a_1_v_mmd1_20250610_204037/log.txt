Args:
Namespace(name='model_phi1_1a_saddle_v1a_1_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_1/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05044207721948624, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 393215884

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.891509116108172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.891509116108172 | validation: 6.548149997105211]
	TIME [epoch: 375 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.399369892593516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.399369892593516 | validation: 5.429017327743862]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.762671782179543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.762671782179543 | validation: 5.08036119152597]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.359357100193176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.359357100193176 | validation: 4.455480335322189]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.243771137266646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243771137266646 | validation: 3.844698489504676]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.679634200873651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.679634200873651 | validation: 3.591941526143473]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.312559987484275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.312559987484275 | validation: 3.5542805013265184]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.152236608829832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.152236608829832 | validation: 3.4324701847237717]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.117856818762288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.117856818762288 | validation: 3.5122019118681913]
	TIME [epoch: 6.1 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1822947572302445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1822947572302445 | validation: 3.4115898632203963]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0853361693258874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0853361693258874 | validation: 3.4697497053419957]
	TIME [epoch: 6.09 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.062958607703882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.062958607703882 | validation: 3.4086690553215746]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.951239278458095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.951239278458095 | validation: 3.2939119202052076]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9735406458498037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9735406458498037 | validation: 3.3009362960532656]
	TIME [epoch: 6.08 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9489908942422987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9489908942422987 | validation: 3.3768977450415076]
	TIME [epoch: 6.08 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5880477546598843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5880477546598843 | validation: 3.2167996771994787]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.322565608791813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.322565608791813 | validation: 3.1972416885772854]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4160284308463638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4160284308463638 | validation: 3.3872770049497785]
	TIME [epoch: 6.09 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3422671753735083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3422671753735083 | validation: 2.9069882336553667]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1301995820057527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1301995820057527 | validation: 2.8650986287529285]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.132648683854109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.132648683854109 | validation: 2.8696390043452884]
	TIME [epoch: 6.09 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1292499583411555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1292499583411555 | validation: 2.9336456532439357]
	TIME [epoch: 6.08 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1524734789619493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1524734789619493 | validation: 2.926863741512042]
	TIME [epoch: 6.09 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1345247876451223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1345247876451223 | validation: 2.829725456001027]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.017855172088091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.017855172088091 | validation: 2.9145853800873707]
	TIME [epoch: 6.08 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.156461406921257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.156461406921257 | validation: 3.002665763907081]
	TIME [epoch: 6.08 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1774231880393096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1774231880393096 | validation: 2.8160898487377235]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0059346433627483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0059346433627483 | validation: 2.821779337254628]
	TIME [epoch: 6.09 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.03740906948691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.03740906948691 | validation: 2.857991083920968]
	TIME [epoch: 6.08 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1028141986957807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1028141986957807 | validation: 2.912434062069336]
	TIME [epoch: 6.08 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0191920502540737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0191920502540737 | validation: 2.8830380006176]
	TIME [epoch: 6.08 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0498233555584946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0498233555584946 | validation: 2.788144309382882]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0492725627607604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0492725627607604 | validation: 2.8463848888013086]
	TIME [epoch: 6.09 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.092855482947475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.092855482947475 | validation: 2.7902659768152205]
	TIME [epoch: 6.07 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.998746739671077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.998746739671077 | validation: 2.7583712229885986]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.939392260005392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.939392260005392 | validation: 2.8537585976246738]
	TIME [epoch: 6.09 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0265051944067887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0265051944067887 | validation: 2.7806529595232092]
	TIME [epoch: 6.09 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9950182219966355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9950182219966355 | validation: 2.915587458423267]
	TIME [epoch: 6.08 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9623163665075314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9623163665075314 | validation: 2.7290464113912014]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9348905001376877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9348905001376877 | validation: 2.7761661342401545]
	TIME [epoch: 6.09 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9623319978058396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9623319978058396 | validation: 2.6969556232948335]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8724549715600616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8724549715600616 | validation: 2.846261386480608]
	TIME [epoch: 6.09 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9156060497793104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9156060497793104 | validation: 2.7143563481564588]
	TIME [epoch: 6.08 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0253967557172063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0253967557172063 | validation: 2.7165568878324784]
	TIME [epoch: 6.09 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.90401741126348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.90401741126348 | validation: 2.8318444934365603]
	TIME [epoch: 6.08 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.017908003921495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.017908003921495 | validation: 2.682256486999011]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.847939927830497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.847939927830497 | validation: 2.6486631029836367]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8288892705416586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8288892705416586 | validation: 2.786861044581972]
	TIME [epoch: 6.09 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9687746498595524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9687746498595524 | validation: 2.791249023519589]
	TIME [epoch: 6.08 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9033469387705102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9033469387705102 | validation: 2.656296087677868]
	TIME [epoch: 6.07 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8055929067692462		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.8055929067692462 | validation: 2.6630376175873165]
	TIME [epoch: 6.08 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857207123743485		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.857207123743485 | validation: 2.6707435716940116]
	TIME [epoch: 6.09 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9368898902039358		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.9368898902039358 | validation: 2.6374730146023557]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9207578044540936		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.9207578044540936 | validation: 2.7460033944697235]
	TIME [epoch: 6.08 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849527038266443		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.849527038266443 | validation: 2.602918772360999]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519777799439234		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.8519777799439234 | validation: 2.6324326760998646]
	TIME [epoch: 6.1 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654799277024847		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.8654799277024847 | validation: 2.68128945138824]
	TIME [epoch: 6.08 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.826098919256291		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.826098919256291 | validation: 2.612476259747134]
	TIME [epoch: 6.08 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.791667741262831		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.791667741262831 | validation: 2.6633010040467813]
	TIME [epoch: 6.08 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539813666793417		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.8539813666793417 | validation: 2.60109254543783]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7863577228074405		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.7863577228074405 | validation: 2.6602850126050837]
	TIME [epoch: 6.09 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.910838959543605		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.910838959543605 | validation: 2.6881797606048687]
	TIME [epoch: 6.07 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8247808918633823		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.8247808918633823 | validation: 2.6206970499353295]
	TIME [epoch: 6.07 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7590120619493066		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.7590120619493066 | validation: 2.61503507456954]
	TIME [epoch: 6.08 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7606700517133003		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.7606700517133003 | validation: 2.6129779690617623]
	TIME [epoch: 6.08 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.890808010015398		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 1.890808010015398 | validation: 2.593291916667427]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8114560756340303		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.8114560756340303 | validation: 2.619299182840309]
	TIME [epoch: 6.09 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.782139883335113		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.782139883335113 | validation: 2.6036951799486125]
	TIME [epoch: 6.08 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7335725921330363		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.7335725921330363 | validation: 2.6971365397475853]
	TIME [epoch: 6.07 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7838218141129674		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 1.7838218141129674 | validation: 2.611310175883621]
	TIME [epoch: 6.07 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8961589444251008		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 1.8961589444251008 | validation: 2.6146233802087884]
	TIME [epoch: 6.08 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7337096778614238		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 1.7337096778614238 | validation: 2.5881666642123085]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7753828121688597		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.7753828121688597 | validation: 2.6878796897977044]
	TIME [epoch: 6.08 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7757151168345453		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 1.7757151168345453 | validation: 2.5729136515973363]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7934016113108153		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 1.7934016113108153 | validation: 2.5794410068017886]
	TIME [epoch: 6.08 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7684738986008846		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 1.7684738986008846 | validation: 2.571043619006811]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7139422965350308		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 1.7139422965350308 | validation: 2.5661404349698684]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.837253517806185		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 1.837253517806185 | validation: 2.5556470359661496]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7530845935643011		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.7530845935643011 | validation: 2.570969661418349]
	TIME [epoch: 6.08 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7130802084904926		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 1.7130802084904926 | validation: 2.617957763079896]
	TIME [epoch: 6.08 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7536826225329147		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 1.7536826225329147 | validation: 2.571862607389942]
	TIME [epoch: 6.08 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7204614842680428		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 1.7204614842680428 | validation: 2.7974185416220614]
	TIME [epoch: 6.07 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8134801655176565		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 1.8134801655176565 | validation: 2.5992052839193307]
	TIME [epoch: 6.08 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7548191044556296		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 1.7548191044556296 | validation: 2.566814553661695]
	TIME [epoch: 6.07 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7568321757399705		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 1.7568321757399705 | validation: 2.613327691526877]
	TIME [epoch: 6.08 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.739758940718652		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 1.739758940718652 | validation: 2.59003423505263]
	TIME [epoch: 6.08 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7452547341024394		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 1.7452547341024394 | validation: 2.578880196395934]
	TIME [epoch: 6.07 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6853812674191175		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 1.6853812674191175 | validation: 2.5699971433447284]
	TIME [epoch: 6.07 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7379520896217038		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 1.7379520896217038 | validation: 2.552672330781582]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.742089460605599		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 1.742089460605599 | validation: 2.6363205153436606]
	TIME [epoch: 6.09 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.706870751712263		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 1.706870751712263 | validation: 2.555685972803582]
	TIME [epoch: 6.08 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7669718237155754		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 1.7669718237155754 | validation: 2.566768798742918]
	TIME [epoch: 6.07 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7494217875721978		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 1.7494217875721978 | validation: 2.524318958148497]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6701703319292702		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 1.6701703319292702 | validation: 2.5968521111223195]
	TIME [epoch: 6.08 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.707292608616676		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 1.707292608616676 | validation: 2.5641720931918215]
	TIME [epoch: 6.08 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7215307567736682		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 1.7215307567736682 | validation: 2.572755241889375]
	TIME [epoch: 6.08 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6966195090185499		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 1.6966195090185499 | validation: 2.548413841820879]
	TIME [epoch: 6.08 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6580363696809568		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 1.6580363696809568 | validation: 2.6567654640081133]
	TIME [epoch: 6.08 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7307737118981028		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.7307737118981028 | validation: 2.508135674785283]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6934413289303034		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 1.6934413289303034 | validation: 2.5333857013721968]
	TIME [epoch: 6.08 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6785907376113682		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 1.6785907376113682 | validation: 2.560348294775802]
	TIME [epoch: 6.08 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6830653026368274		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 1.6830653026368274 | validation: 2.561179301867525]
	TIME [epoch: 6.08 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6804991759562777		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 1.6804991759562777 | validation: 2.5936165783903355]
	TIME [epoch: 6.07 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6919516341440173		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 1.6919516341440173 | validation: 2.5850374318187646]
	TIME [epoch: 6.07 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6950935535657292		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 1.6950935535657292 | validation: 2.5731160516417715]
	TIME [epoch: 6.08 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6555739347786504		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 1.6555739347786504 | validation: 2.522805563205688]
	TIME [epoch: 6.08 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7008315506164582		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 1.7008315506164582 | validation: 2.496427270458279]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6285293065324145		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 1.6285293065324145 | validation: 2.5148448787801008]
	TIME [epoch: 6.08 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6267684249475924		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 1.6267684249475924 | validation: 2.5991227017563623]
	TIME [epoch: 6.08 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6963457823846824		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 1.6963457823846824 | validation: 2.591938292584644]
	TIME [epoch: 6.08 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6430903608147824		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 1.6430903608147824 | validation: 2.4981669574535195]
	TIME [epoch: 6.08 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6268368968993365		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 1.6268368968993365 | validation: 2.593747807066963]
	TIME [epoch: 6.08 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6388483790014885		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 1.6388483790014885 | validation: 2.5098217620570944]
	TIME [epoch: 6.08 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.591961840495496		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 1.591961840495496 | validation: 2.552303137854418]
	TIME [epoch: 6.09 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6266719597243		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 1.6266719597243 | validation: 2.5265964758465933]
	TIME [epoch: 6.08 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.590595992025495		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 1.590595992025495 | validation: 2.431603694408802]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5631463968655535		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 1.5631463968655535 | validation: 2.50418014903741]
	TIME [epoch: 6.09 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.595288357259658		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 1.595288357259658 | validation: 2.41216822216315]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5611604104375552		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 1.5611604104375552 | validation: 2.433189231284988]
	TIME [epoch: 6.1 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5648174944268707		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 1.5648174944268707 | validation: 2.4080857644911964]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5135419155113188		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 1.5135419155113188 | validation: 2.4917226401528785]
	TIME [epoch: 6.1 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5494125299100279		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 1.5494125299100279 | validation: 2.406848359461941]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5277968350319986		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 1.5277968350319986 | validation: 2.3592980386772053]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.462758893199807		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 1.462758893199807 | validation: 2.414936454381404]
	TIME [epoch: 6.1 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6162321064491567		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 1.6162321064491567 | validation: 2.933188485380401]
	TIME [epoch: 6.08 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.984661201084167		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 3.984661201084167 | validation: 6.823092374246622]
	TIME [epoch: 6.08 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8580795609249368		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 3.8580795609249368 | validation: 2.943544322097398]
	TIME [epoch: 6.08 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7672978710522438		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 1.7672978710522438 | validation: 2.447239243566611]
	TIME [epoch: 6.08 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5482969587281528		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 1.5482969587281528 | validation: 2.3617810943921964]
	TIME [epoch: 6.09 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4907994710920252		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 1.4907994710920252 | validation: 2.2904844891525604]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4182425073496807		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 1.4182425073496807 | validation: 2.190301215002366]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3560935344792264		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 1.3560935344792264 | validation: 2.1405759699827938]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3737777216779814		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 1.3737777216779814 | validation: 2.0746539055686775]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2797005766957346		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 1.2797005766957346 | validation: 1.9816090282587255]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2238930976446722		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 1.2238930976446722 | validation: 1.9669385314921395]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.193918948189805		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 1.193918948189805 | validation: 1.91745066031008]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1758398827156942		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 1.1758398827156942 | validation: 1.9599560106256697]
	TIME [epoch: 6.09 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1784062667993445		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 1.1784062667993445 | validation: 1.9202194343471648]
	TIME [epoch: 6.09 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1875153224290969		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 1.1875153224290969 | validation: 1.8773072775969595]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.127586131252567		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 1.127586131252567 | validation: 1.7987757845241785]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1358024118734344		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 1.1358024118734344 | validation: 1.7663775370895352]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0897244679983247		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 1.0897244679983247 | validation: 1.7495770432011248]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0442243419198332		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 1.0442243419198332 | validation: 1.6020090273535872]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9850319501309509		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.9850319501309509 | validation: 1.5866087422855788]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9455071784388374		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.9455071784388374 | validation: 1.1649631102481102]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7693638393621752		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.7693638393621752 | validation: 0.9247831998822069]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7374280190909749		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.7374280190909749 | validation: 1.011362637134226]
	TIME [epoch: 6.09 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6759039909666813		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.6759039909666813 | validation: 0.9029051413792635]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7360552356929302		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.7360552356929302 | validation: 0.8608930542733162]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6746801941031499		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.6746801941031499 | validation: 0.9083860420222232]
	TIME [epoch: 6.08 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6794054453330558		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.6794054453330558 | validation: 0.7408082708685344]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6631517147836543		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.6631517147836543 | validation: 0.7753052822185776]
	TIME [epoch: 6.09 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6383986827160855		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.6383986827160855 | validation: 0.7328370307545116]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6553496316794181		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.6553496316794181 | validation: 0.6245515564780811]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5712344810466881		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.5712344810466881 | validation: 0.7904871368240709]
	TIME [epoch: 6.09 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7036833629494574		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.7036833629494574 | validation: 0.739890418973436]
	TIME [epoch: 6.08 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5915704592575532		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.5915704592575532 | validation: 0.7412137560218193]
	TIME [epoch: 6.07 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5919709914718408		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.5919709914718408 | validation: 0.6830263939730034]
	TIME [epoch: 6.07 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5753654824873278		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.5753654824873278 | validation: 0.6780037769221507]
	TIME [epoch: 6.08 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303946758781163		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.5303946758781163 | validation: 0.6993824644104611]
	TIME [epoch: 6.07 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5432569950889369		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.5432569950889369 | validation: 0.6780997944711584]
	TIME [epoch: 6.08 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5608646139602739		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.5608646139602739 | validation: 0.6519617736021237]
	TIME [epoch: 6.08 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5429903470925332		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.5429903470925332 | validation: 0.773156027943451]
	TIME [epoch: 6.08 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.522059562579145		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.522059562579145 | validation: 0.6209440941571347]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5578551995345628		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.5578551995345628 | validation: 0.6606789299811798]
	TIME [epoch: 6.08 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5874013505328188		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.5874013505328188 | validation: 0.7267242952019151]
	TIME [epoch: 6.08 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5212209305518741		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.5212209305518741 | validation: 0.7794761891742916]
	TIME [epoch: 6.07 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465717316099737		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.5465717316099737 | validation: 0.5879425212378881]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5291531158651909		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.5291531158651909 | validation: 0.8923272436116989]
	TIME [epoch: 6.08 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6470275844369298		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.6470275844369298 | validation: 0.5311066083678497]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4994948596710779		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.4994948596710779 | validation: 0.6350540022231699]
	TIME [epoch: 6.09 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4994967311704236		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.4994967311704236 | validation: 0.582066070316891]
	TIME [epoch: 6.09 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5214873186829285		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.5214873186829285 | validation: 0.6018356645607608]
	TIME [epoch: 6.09 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49681743155150554		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.49681743155150554 | validation: 0.636369414740918]
	TIME [epoch: 6.09 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5222756728069593		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.5222756728069593 | validation: 0.6774350700193631]
	TIME [epoch: 6.08 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5316661820729135		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.5316661820729135 | validation: 0.5695550792912045]
	TIME [epoch: 6.09 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4670656257275587		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.4670656257275587 | validation: 0.5679353784790246]
	TIME [epoch: 6.09 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4842304060571293		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.4842304060571293 | validation: 0.6228213359956005]
	TIME [epoch: 6.09 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4567514896961501		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.4567514896961501 | validation: 0.6438524224634199]
	TIME [epoch: 6.09 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45844770859700523		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.45844770859700523 | validation: 0.6499312707433988]
	TIME [epoch: 6.1 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.554134591964644		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.554134591964644 | validation: 0.6743204487799015]
	TIME [epoch: 6.08 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4822047181015038		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.4822047181015038 | validation: 0.7730795528434165]
	TIME [epoch: 6.08 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45455638593444264		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.45455638593444264 | validation: 0.6240889358237052]
	TIME [epoch: 6.08 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5005433922843893		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.5005433922843893 | validation: 0.5643280676214082]
	TIME [epoch: 6.08 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5103873306709793		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.5103873306709793 | validation: 0.48308114141001407]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40777706260846847		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.40777706260846847 | validation: 0.4917538210253022]
	TIME [epoch: 6.08 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4677201347336975		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.4677201347336975 | validation: 0.5635642683496589]
	TIME [epoch: 6.08 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4952832949878549		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.4952832949878549 | validation: 0.7504443487986664]
	TIME [epoch: 6.08 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5205399443994118		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.5205399443994118 | validation: 0.5142949327518549]
	TIME [epoch: 6.08 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4236808874993845		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.4236808874993845 | validation: 0.4960499775132968]
	TIME [epoch: 6.08 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4290935969597913		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.4290935969597913 | validation: 0.503171213480364]
	TIME [epoch: 6.08 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49441190034032034		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.49441190034032034 | validation: 0.4577922445087942]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39881960621677603		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.39881960621677603 | validation: 0.5298472565321196]
	TIME [epoch: 6.09 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42794820497673935		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.42794820497673935 | validation: 0.5090837169702724]
	TIME [epoch: 6.08 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4490869237933717		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.4490869237933717 | validation: 0.5217233466863915]
	TIME [epoch: 6.09 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4466605663660158		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.4466605663660158 | validation: 0.55207459565663]
	TIME [epoch: 6.09 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4162342989594403		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.4162342989594403 | validation: 0.5904853720418008]
	TIME [epoch: 6.09 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5058226565783049		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.5058226565783049 | validation: 0.5272892171568482]
	TIME [epoch: 6.09 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3968257840132514		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.3968257840132514 | validation: 0.5168522390078187]
	TIME [epoch: 6.09 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4343547976158856		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.4343547976158856 | validation: 0.6287234790666076]
	TIME [epoch: 6.1 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5985784494374348		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.5985784494374348 | validation: 0.5826642127353578]
	TIME [epoch: 399 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40513349420004346		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.40513349420004346 | validation: 0.5734856040453418]
	TIME [epoch: 12 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43113348179768135		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.43113348179768135 | validation: 0.499394086449956]
	TIME [epoch: 12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39969119795555114		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.39969119795555114 | validation: 0.47248215529278004]
	TIME [epoch: 12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4406216363108212		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.4406216363108212 | validation: 0.4316642470107913]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3802418210016213		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.3802418210016213 | validation: 0.6340191017145953]
	TIME [epoch: 12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4835078018731412		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.4835078018731412 | validation: 0.49350826438584505]
	TIME [epoch: 12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39406346188445635		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.39406346188445635 | validation: 0.5800001643721189]
	TIME [epoch: 12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4293264225650429		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.4293264225650429 | validation: 0.48071042110954515]
	TIME [epoch: 12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3728420425829113		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.3728420425829113 | validation: 0.4884112306323354]
	TIME [epoch: 12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40778277851092903		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.40778277851092903 | validation: 0.49401132738941433]
	TIME [epoch: 12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42214317730791784		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.42214317730791784 | validation: 0.4800833675706179]
	TIME [epoch: 12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40685137951970196		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.40685137951970196 | validation: 0.5088305621869531]
	TIME [epoch: 12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3935458817042152		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.3935458817042152 | validation: 0.5160649278753033]
	TIME [epoch: 12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41742842850402734		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.41742842850402734 | validation: 0.47142347156477765]
	TIME [epoch: 12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4077207739334774		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.4077207739334774 | validation: 0.45538764226673223]
	TIME [epoch: 12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3720786208153277		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.3720786208153277 | validation: 0.547408790665398]
	TIME [epoch: 12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41411281179398635		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.41411281179398635 | validation: 0.44926974589365015]
	TIME [epoch: 12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3833551660848171		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.3833551660848171 | validation: 0.4359808419195032]
	TIME [epoch: 12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38522883875020675		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.38522883875020675 | validation: 0.39376666676189415]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4718611888055771		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.4718611888055771 | validation: 0.44014165056365306]
	TIME [epoch: 12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39166900002138866		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.39166900002138866 | validation: 0.40314981188408655]
	TIME [epoch: 12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38504573493362015		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.38504573493362015 | validation: 0.44406052789953543]
	TIME [epoch: 12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4465408906992566		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.4465408906992566 | validation: 0.7088969199999091]
	TIME [epoch: 12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37907255789162764		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.37907255789162764 | validation: 0.41901442726174115]
	TIME [epoch: 12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3992984289439		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.3992984289439 | validation: 0.40202614765647815]
	TIME [epoch: 12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31861443241933396		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.31861443241933396 | validation: 0.3907259725310198]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4230372875163397		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.4230372875163397 | validation: 0.4782182436263085]
	TIME [epoch: 12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34943332113856446		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.34943332113856446 | validation: 0.4654644744975468]
	TIME [epoch: 12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3644761904355839		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.3644761904355839 | validation: 0.43090221150182767]
	TIME [epoch: 12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37281893521338727		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.37281893521338727 | validation: 0.4222758849952119]
	TIME [epoch: 12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36294316598564735		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.36294316598564735 | validation: 0.48044880692354197]
	TIME [epoch: 12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3859312907790017		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.3859312907790017 | validation: 0.5829078929147786]
	TIME [epoch: 12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40964840455227797		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.40964840455227797 | validation: 0.45337537032320674]
	TIME [epoch: 12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33146949508693424		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.33146949508693424 | validation: 0.4963442206504205]
	TIME [epoch: 12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3682598266212825		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.3682598266212825 | validation: 0.40529166208085265]
	TIME [epoch: 12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36026156409081306		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.36026156409081306 | validation: 0.6180035864993922]
	TIME [epoch: 12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37723674318768474		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.37723674318768474 | validation: 0.5038435017795369]
	TIME [epoch: 12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36925292801281645		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.36925292801281645 | validation: 0.5010474887871512]
	TIME [epoch: 12 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3542829214646235		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.3542829214646235 | validation: 0.41741965294752625]
	TIME [epoch: 12 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3417973898549422		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.3417973898549422 | validation: 0.4630190682104164]
	TIME [epoch: 12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3237662207021065		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.3237662207021065 | validation: 0.3932289441615909]
	TIME [epoch: 12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36740260099936706		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.36740260099936706 | validation: 0.7319185936222363]
	TIME [epoch: 12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4278030717622685		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.4278030717622685 | validation: 0.5428689655010527]
	TIME [epoch: 12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3630807672867678		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.3630807672867678 | validation: 0.40560242600608076]
	TIME [epoch: 12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31614123675803973		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.31614123675803973 | validation: 0.47249110453639986]
	TIME [epoch: 12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3643692001851405		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.3643692001851405 | validation: 0.4398842463509093]
	TIME [epoch: 12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36675873091984273		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.36675873091984273 | validation: 0.42304273080517535]
	TIME [epoch: 12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3678452066143432		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.3678452066143432 | validation: 0.43766409968273196]
	TIME [epoch: 12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37927110392657654		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.37927110392657654 | validation: 0.42978824045758457]
	TIME [epoch: 12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3487040667391686		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.3487040667391686 | validation: 0.4254404161424391]
	TIME [epoch: 12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.349500489012481		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.349500489012481 | validation: 0.4084756111875262]
	TIME [epoch: 12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3221007658314461		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.3221007658314461 | validation: 0.5022057920367219]
	TIME [epoch: 12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3468540996573814		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.3468540996573814 | validation: 0.3965729669180702]
	TIME [epoch: 12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31304949945720567		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.31304949945720567 | validation: 0.49886531660145944]
	TIME [epoch: 12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3633855730237971		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.3633855730237971 | validation: 0.5013162332165331]
	TIME [epoch: 12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4038261770209467		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.4038261770209467 | validation: 0.4148963846446995]
	TIME [epoch: 12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31432908926053604		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.31432908926053604 | validation: 0.4069116912455315]
	TIME [epoch: 12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3140595671888473		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.3140595671888473 | validation: 0.38454800970501435]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.357339196627452		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.357339196627452 | validation: 0.4457382716257574]
	TIME [epoch: 12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34524901903540156		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.34524901903540156 | validation: 0.5711299152534022]
	TIME [epoch: 12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159453848887852		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.3159453848887852 | validation: 0.37063847235340924]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34546953519047485		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.34546953519047485 | validation: 0.4424212944194115]
	TIME [epoch: 12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33379709972332655		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.33379709972332655 | validation: 0.39097252524558523]
	TIME [epoch: 12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31861246551489963		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.31861246551489963 | validation: 0.5324634799191569]
	TIME [epoch: 12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.353317996752308		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.353317996752308 | validation: 0.40041976268130064]
	TIME [epoch: 12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4075549151042159		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.4075549151042159 | validation: 0.5891515375258995]
	TIME [epoch: 12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48289046776826733		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.48289046776826733 | validation: 0.4762321707450684]
	TIME [epoch: 12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31087903433507913		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.31087903433507913 | validation: 0.4292215900998486]
	TIME [epoch: 12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31955249212925024		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.31955249212925024 | validation: 0.3834377254323339]
	TIME [epoch: 12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3174200218349468		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.3174200218349468 | validation: 0.3737944371729094]
	TIME [epoch: 12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876996613097814		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.2876996613097814 | validation: 0.3766870821268955]
	TIME [epoch: 12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159115283386372		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.3159115283386372 | validation: 0.48940457253661174]
	TIME [epoch: 12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4020384426996396		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.4020384426996396 | validation: 0.4643331488828544]
	TIME [epoch: 12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.313810758064638		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.313810758064638 | validation: 0.37703892933156313]
	TIME [epoch: 12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29253427286831		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.29253427286831 | validation: 0.401899950842197]
	TIME [epoch: 12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32688758216511715		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.32688758216511715 | validation: 0.3995453460064362]
	TIME [epoch: 12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2993132035862887		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.2993132035862887 | validation: 0.4652100319093576]
	TIME [epoch: 12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31360938108107744		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.31360938108107744 | validation: 0.37545683097870675]
	TIME [epoch: 12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34704113958575245		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.34704113958575245 | validation: 0.41659049678907445]
	TIME [epoch: 12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30610031128107024		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.30610031128107024 | validation: 0.3656505435913422]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30549042490102085		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.30549042490102085 | validation: 0.3711868573537275]
	TIME [epoch: 12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30168481176825906		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.30168481176825906 | validation: 0.4581808041053357]
	TIME [epoch: 12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29794771585198315		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.29794771585198315 | validation: 0.36947736973048706]
	TIME [epoch: 12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3581587445529731		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.3581587445529731 | validation: 0.48228711678622777]
	TIME [epoch: 12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38826913715234745		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.38826913715234745 | validation: 0.3987089740148809]
	TIME [epoch: 12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3002615524806996		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.3002615524806996 | validation: 0.3691043788732662]
	TIME [epoch: 12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2831851502510521		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.2831851502510521 | validation: 0.4591550744793209]
	TIME [epoch: 12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3206921190149247		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.3206921190149247 | validation: 0.47782514342711535]
	TIME [epoch: 12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35417560929255776		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.35417560929255776 | validation: 0.4134594959790596]
	TIME [epoch: 12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3042704450876661		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.3042704450876661 | validation: 0.4748168764230993]
	TIME [epoch: 12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3010337245901127		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.3010337245901127 | validation: 0.39076751859100156]
	TIME [epoch: 12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32717148463400764		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.32717148463400764 | validation: 0.3666339384261432]
	TIME [epoch: 12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627691399037892		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.2627691399037892 | validation: 0.3901284382936626]
	TIME [epoch: 12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3226272944771959		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.3226272944771959 | validation: 0.40864303481677744]
	TIME [epoch: 12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3023637165254616		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.3023637165254616 | validation: 0.4332927357106885]
	TIME [epoch: 12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34339385785148424		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.34339385785148424 | validation: 0.3751151345814384]
	TIME [epoch: 12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27802851801448686		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.27802851801448686 | validation: 0.37878346284943715]
	TIME [epoch: 12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086217776513571		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.3086217776513571 | validation: 0.36217047932401203]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2940504369905129		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.2940504369905129 | validation: 0.3552311499475938]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2918002543514045		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.2918002543514045 | validation: 0.4439769985582484]
	TIME [epoch: 12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32001614738939227		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.32001614738939227 | validation: 0.4239886340965152]
	TIME [epoch: 12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3055529208924194		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.3055529208924194 | validation: 0.36554690320428573]
	TIME [epoch: 12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2809839029529028		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.2809839029529028 | validation: 0.3751112170561434]
	TIME [epoch: 12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3159061718580451		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.3159061718580451 | validation: 0.37283763640690426]
	TIME [epoch: 12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3171825814761108		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.3171825814761108 | validation: 0.5162072878444732]
	TIME [epoch: 12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3487745613910801		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.3487745613910801 | validation: 0.38038730649070707]
	TIME [epoch: 12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2753987543894455		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.2753987543894455 | validation: 0.35661195106474053]
	TIME [epoch: 12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2966715519070482		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.2966715519070482 | validation: 0.4107657370881801]
	TIME [epoch: 12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3389317736833636		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.3389317736833636 | validation: 0.35506355076093765]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2931355471858819		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.2931355471858819 | validation: 0.38586699273498754]
	TIME [epoch: 12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25710665320858594		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.25710665320858594 | validation: 0.3934841574618224]
	TIME [epoch: 12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3206708983878389		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.3206708983878389 | validation: 0.3406569073252848]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803413805180195		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.2803413805180195 | validation: 0.4065836747168147]
	TIME [epoch: 12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3199935727021696		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.3199935727021696 | validation: 0.3795187574829444]
	TIME [epoch: 12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298000391578093		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.298000391578093 | validation: 0.34969776096144545]
	TIME [epoch: 12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2708073871264387		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.2708073871264387 | validation: 0.3401380511673777]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26619640493800945		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.26619640493800945 | validation: 0.4529764885829978]
	TIME [epoch: 12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969837294904031		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.2969837294904031 | validation: 0.35644982273995285]
	TIME [epoch: 12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30019472223583216		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.30019472223583216 | validation: 0.44603934711552545]
	TIME [epoch: 12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2889267359272361		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.2889267359272361 | validation: 0.4857225569935086]
	TIME [epoch: 12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31062075861799476		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.31062075861799476 | validation: 0.3388556556971163]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25907327764128885		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.25907327764128885 | validation: 0.5224512589424877]
	TIME [epoch: 12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3352359392970633		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.3352359392970633 | validation: 0.39089114772782874]
	TIME [epoch: 12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2711764196416113		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.2711764196416113 | validation: 0.47105410895040845]
	TIME [epoch: 12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32368425752170377		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.32368425752170377 | validation: 0.3605799232914792]
	TIME [epoch: 12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.261638748295123		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.261638748295123 | validation: 0.38020612269992093]
	TIME [epoch: 12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2948384697682601		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.2948384697682601 | validation: 0.3914910462054101]
	TIME [epoch: 12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2733955159635307		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.2733955159635307 | validation: 0.42698785666247785]
	TIME [epoch: 12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853355801391243		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.2853355801391243 | validation: 0.36536267145536977]
	TIME [epoch: 12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31366446266736364		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.31366446266736364 | validation: 0.4290565188962735]
	TIME [epoch: 12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.296205101585902		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.296205101585902 | validation: 0.3720575435962683]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869013297459206		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.2869013297459206 | validation: 0.3550032101525601]
	TIME [epoch: 12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2819068342399591		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.2819068342399591 | validation: 0.3744643274480596]
	TIME [epoch: 12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773869662502836		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.2773869662502836 | validation: 0.36558515110806467]
	TIME [epoch: 12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26561787441641327		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.26561787441641327 | validation: 0.379188533619032]
	TIME [epoch: 12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3015472471740071		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.3015472471740071 | validation: 0.3924846664249051]
	TIME [epoch: 12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3067293321887278		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.3067293321887278 | validation: 0.3656455944352744]
	TIME [epoch: 12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26860204026818524		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.26860204026818524 | validation: 0.4543609141354187]
	TIME [epoch: 12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28009867290353724		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.28009867290353724 | validation: 0.38491253207636295]
	TIME [epoch: 12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2966571217442525		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.2966571217442525 | validation: 0.37244571066621]
	TIME [epoch: 12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2864030707641174		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.2864030707641174 | validation: 0.3709556188532449]
	TIME [epoch: 12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28392118613777423		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.28392118613777423 | validation: 0.35269333825079874]
	TIME [epoch: 12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2500941472696314		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.2500941472696314 | validation: 0.3342014970068361]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844148053612114		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.2844148053612114 | validation: 0.3614596283714925]
	TIME [epoch: 12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3021123221143301		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.3021123221143301 | validation: 0.38018444109432226]
	TIME [epoch: 12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2905142723825508		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.2905142723825508 | validation: 0.4098101378140366]
	TIME [epoch: 12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2857080386019472		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.2857080386019472 | validation: 0.4078889876885258]
	TIME [epoch: 12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2694388153476206		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.2694388153476206 | validation: 0.36734695097158165]
	TIME [epoch: 12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26023818921962555		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.26023818921962555 | validation: 0.40993007410753785]
	TIME [epoch: 12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2747517112604679		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.2747517112604679 | validation: 0.33710828999363635]
	TIME [epoch: 12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29520845819295904		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.29520845819295904 | validation: 0.4724142808981152]
	TIME [epoch: 12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30716895808265227		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.30716895808265227 | validation: 0.41480765859414837]
	TIME [epoch: 12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2923760189619725		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.2923760189619725 | validation: 0.359496103010272]
	TIME [epoch: 12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26650296245756055		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.26650296245756055 | validation: 0.38451847260116123]
	TIME [epoch: 12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25858130472859087		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.25858130472859087 | validation: 0.33988946236086653]
	TIME [epoch: 12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2816066755971434		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.2816066755971434 | validation: 0.43170829350264717]
	TIME [epoch: 12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2800112846010452		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.2800112846010452 | validation: 0.3505199313474447]
	TIME [epoch: 12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26290968529412573		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.26290968529412573 | validation: 0.36040240950564206]
	TIME [epoch: 12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736626477785788		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.2736626477785788 | validation: 0.3487405242175661]
	TIME [epoch: 12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28158123705172927		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.28158123705172927 | validation: 0.3247181097383199]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23076756688269282		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.23076756688269282 | validation: 0.3725765612226964]
	TIME [epoch: 12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2865436183258163		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.2865436183258163 | validation: 0.3721416439383908]
	TIME [epoch: 12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921724238561261		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.2921724238561261 | validation: 0.34739268716672767]
	TIME [epoch: 12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28885350691696304		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.28885350691696304 | validation: 0.3272234142872905]
	TIME [epoch: 12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2376219699448085		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.2376219699448085 | validation: 0.4080766038150274]
	TIME [epoch: 12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27378689309443915		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.27378689309443915 | validation: 0.3502475743597897]
	TIME [epoch: 12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28007148729759335		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.28007148729759335 | validation: 0.34909811993112816]
	TIME [epoch: 12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945814532669139		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.2945814532669139 | validation: 0.3314035706317704]
	TIME [epoch: 12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23937504863106002		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.23937504863106002 | validation: 0.33844156593507435]
	TIME [epoch: 12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26261006202302334		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.26261006202302334 | validation: 0.3615716727761472]
	TIME [epoch: 12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801647027279406		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.2801647027279406 | validation: 0.34907368507269354]
	TIME [epoch: 12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2467291736912296		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.2467291736912296 | validation: 0.38043333282917413]
	TIME [epoch: 12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610947643845567		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.2610947643845567 | validation: 0.3298367977611144]
	TIME [epoch: 12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28716771074495623		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.28716771074495623 | validation: 0.3437523718957823]
	TIME [epoch: 12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640733710815743		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.2640733710815743 | validation: 0.32499986525709085]
	TIME [epoch: 12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2299660535402991		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.2299660535402991 | validation: 0.3269966814829026]
	TIME [epoch: 12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567687308673937		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.2567687308673937 | validation: 0.36404594815194524]
	TIME [epoch: 12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643843383637844		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.2643843383637844 | validation: 0.4356332638052353]
	TIME [epoch: 12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31658324232377044		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.31658324232377044 | validation: 0.3411702688962027]
	TIME [epoch: 12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3128888610360181		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.3128888610360181 | validation: 0.47760898344146674]
	TIME [epoch: 12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2973869827678257		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.2973869827678257 | validation: 0.366717747442698]
	TIME [epoch: 12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27216951276003054		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.27216951276003054 | validation: 0.3625267690780253]
	TIME [epoch: 12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25465739834158896		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.25465739834158896 | validation: 0.4634827621641057]
	TIME [epoch: 12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2567825238413494		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.2567825238413494 | validation: 0.3498321763343133]
	TIME [epoch: 12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2605338678581726		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.2605338678581726 | validation: 0.31007062866864793]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2772540422933503		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.2772540422933503 | validation: 0.37146955318988406]
	TIME [epoch: 12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2440108998926417		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.2440108998926417 | validation: 0.3332685872261905]
	TIME [epoch: 12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2733998359745167		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.2733998359745167 | validation: 0.3612260334101415]
	TIME [epoch: 12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2556599059935725		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.2556599059935725 | validation: 0.36931669697607195]
	TIME [epoch: 12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3578217665736332		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.3578217665736332 | validation: 0.44002021634697486]
	TIME [epoch: 12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27093544077887993		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.27093544077887993 | validation: 0.3252169494592472]
	TIME [epoch: 12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2381737690566495		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.2381737690566495 | validation: 0.3542844270518468]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23854412571288267		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.23854412571288267 | validation: 0.33566402897322023]
	TIME [epoch: 12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2498862150637436		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.2498862150637436 | validation: 0.3449189794977551]
	TIME [epoch: 12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593217547424097		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.2593217547424097 | validation: 0.3064742651205973]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25506734233483686		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.25506734233483686 | validation: 0.3730072179149607]
	TIME [epoch: 12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2805001713058186		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.2805001713058186 | validation: 0.3393753148060861]
	TIME [epoch: 12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2316692952375402		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.2316692952375402 | validation: 0.3300600110575282]
	TIME [epoch: 12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463334504773388		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.2463334504773388 | validation: 0.30808326001500674]
	TIME [epoch: 12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2603100170463234		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.2603100170463234 | validation: 0.3106412548337565]
	TIME [epoch: 12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2234599733513557		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.2234599733513557 | validation: 0.3456268951295274]
	TIME [epoch: 12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.251122864972839		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.251122864972839 | validation: 0.31869634541896624]
	TIME [epoch: 12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2697859557051141		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.2697859557051141 | validation: 0.3157889689462766]
	TIME [epoch: 12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2327418112880353		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.2327418112880353 | validation: 0.3370735881022947]
	TIME [epoch: 12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677732245688865		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.2677732245688865 | validation: 0.35129067605928005]
	TIME [epoch: 12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2410701171140428		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.2410701171140428 | validation: 0.3151218534740072]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25285834414315905		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.25285834414315905 | validation: 0.31803280705654957]
	TIME [epoch: 12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2423831055618221		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.2423831055618221 | validation: 0.3476333651084942]
	TIME [epoch: 12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24449285579167648		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.24449285579167648 | validation: 0.33441976232365034]
	TIME [epoch: 12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538449715662024		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.2538449715662024 | validation: 0.3265138514596506]
	TIME [epoch: 12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24004042272810328		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.24004042272810328 | validation: 0.366694094169421]
	TIME [epoch: 12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23584797787454115		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.23584797787454115 | validation: 0.3151849172541582]
	TIME [epoch: 12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24452912072760677		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.24452912072760677 | validation: 0.34944794364401555]
	TIME [epoch: 12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2409399191973421		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.2409399191973421 | validation: 0.29820410710683587]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22305950343449132		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.22305950343449132 | validation: 0.34533843984497126]
	TIME [epoch: 12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23861559009946604		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.23861559009946604 | validation: 0.3483486940723695]
	TIME [epoch: 12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25365244499705547		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.25365244499705547 | validation: 0.3067027555600907]
	TIME [epoch: 12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2170797527740907		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.2170797527740907 | validation: 0.31061792019953444]
	TIME [epoch: 12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2419632097609736		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.2419632097609736 | validation: 0.3556504519207812]
	TIME [epoch: 12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23050697070250747		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.23050697070250747 | validation: 0.3618399975029107]
	TIME [epoch: 12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22298184648834593		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.22298184648834593 | validation: 0.3153852778470392]
	TIME [epoch: 12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23863678510486241		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.23863678510486241 | validation: 0.28409632649283556]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21987109332849902		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.21987109332849902 | validation: 0.21574356759748822]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17776623923542273		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.17776623923542273 | validation: 0.14032666574527503]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13242270722766414		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.13242270722766414 | validation: 0.10802956801849024]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13582973862915132		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.13582973862915132 | validation: 0.14969069525643058]
	TIME [epoch: 12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08747113448807443		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.08747113448807443 | validation: 0.12177653826997042]
	TIME [epoch: 12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10280594627517532		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.10280594627517532 | validation: 0.12190753989970797]
	TIME [epoch: 12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08377019233739792		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.08377019233739792 | validation: 0.07554129219218016]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06652884606480919		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.06652884606480919 | validation: 0.17435966494012484]
	TIME [epoch: 12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10467301607726164		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.10467301607726164 | validation: 0.16850159346760013]
	TIME [epoch: 12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11131985217376489		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.11131985217376489 | validation: 0.08150117199296787]
	TIME [epoch: 12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05391081476337419		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.05391081476337419 | validation: 0.0703870866159744]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17610888099909916		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.17610888099909916 | validation: 0.11682629673095771]
	TIME [epoch: 12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08766090171264854		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.08766090171264854 | validation: 0.07760686839659196]
	TIME [epoch: 12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06121153010629352		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.06121153010629352 | validation: 0.06970772228958433]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054099358219841145		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.054099358219841145 | validation: 0.13239076342460773]
	TIME [epoch: 12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09557518774094959		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.09557518774094959 | validation: 0.05571687489322814]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06357583732256825		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.06357583732256825 | validation: 0.09975605396129192]
	TIME [epoch: 12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08584785573538592		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.08584785573538592 | validation: 0.1080668690874271]
	TIME [epoch: 12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07775427328226		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.07775427328226 | validation: 0.09257249807856278]
	TIME [epoch: 12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06853132093708589		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.06853132093708589 | validation: 0.0922502773621296]
	TIME [epoch: 12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04410397340926999		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.04410397340926999 | validation: 0.05107952536745873]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048857287853141536		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.048857287853141536 | validation: 0.08359470042063434]
	TIME [epoch: 12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05832647359706891		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.05832647359706891 | validation: 0.21313459862892564]
	TIME [epoch: 12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10161828510179775		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.10161828510179775 | validation: 0.07231001406602858]
	TIME [epoch: 12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0870510205294008		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0870510205294008 | validation: 0.1187175558191743]
	TIME [epoch: 12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07869594217624046		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.07869594217624046 | validation: 0.05748417859139006]
	TIME [epoch: 12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05380359908003843		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.05380359908003843 | validation: 0.1620147437697589]
	TIME [epoch: 12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06522655858194769		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.06522655858194769 | validation: 0.08857134883316706]
	TIME [epoch: 12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060016016520711465		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.060016016520711465 | validation: 0.04634747912576527]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08848447146633223		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.08848447146633223 | validation: 0.07672359255954045]
	TIME [epoch: 12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05576452772405748		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.05576452772405748 | validation: 0.0885651453440427]
	TIME [epoch: 12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05665267362512587		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.05665267362512587 | validation: 0.04459677955877388]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03482432088987516		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.03482432088987516 | validation: 0.1433585593389456]
	TIME [epoch: 12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08473884795979783		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.08473884795979783 | validation: 0.05327032355077711]
	TIME [epoch: 12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05373361805016811		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.05373361805016811 | validation: 0.06848207180382199]
	TIME [epoch: 12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08005600704019586		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.08005600704019586 | validation: 0.058755338752376624]
	TIME [epoch: 12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04736527319740983		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.04736527319740983 | validation: 0.04496385807891993]
	TIME [epoch: 12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0419548218762868		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.0419548218762868 | validation: 0.09882081302657239]
	TIME [epoch: 12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0847034595134082		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.0847034595134082 | validation: 0.05235480286497886]
	TIME [epoch: 12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05155252091208238		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.05155252091208238 | validation: 0.06251369297621956]
	TIME [epoch: 12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04849099823306017		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.04849099823306017 | validation: 0.059028766018576435]
	TIME [epoch: 12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06302722900055277		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.06302722900055277 | validation: 0.07426426348085863]
	TIME [epoch: 12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06199060457303024		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.06199060457303024 | validation: 0.04974613760495568]
	TIME [epoch: 12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043862962264214725		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.043862962264214725 | validation: 0.07399093502296902]
	TIME [epoch: 12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04219694030943104		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.04219694030943104 | validation: 0.05158682754971681]
	TIME [epoch: 12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06597166688069053		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.06597166688069053 | validation: 0.07745151096294471]
	TIME [epoch: 12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05665757790988851		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.05665757790988851 | validation: 0.042138112090569904]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04078265883551378		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.04078265883551378 | validation: 0.07809803325560276]
	TIME [epoch: 12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05872732144893853		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.05872732144893853 | validation: 0.12987327086901185]
	TIME [epoch: 12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06124906889259458		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.06124906889259458 | validation: 0.04841781929803536]
	TIME [epoch: 12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05279924897212901		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.05279924897212901 | validation: 0.13198087981941142]
	TIME [epoch: 12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05929929237773576		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.05929929237773576 | validation: 0.03304223819068232]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035324359719529484		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.035324359719529484 | validation: 0.16494578653047287]
	TIME [epoch: 12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08183170676335552		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.08183170676335552 | validation: 0.07591863421035594]
	TIME [epoch: 12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04809174429422644		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.04809174429422644 | validation: 0.058569332828692165]
	TIME [epoch: 12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06247104426038243		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.06247104426038243 | validation: 0.08255352091653283]
	TIME [epoch: 12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04371731772005742		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.04371731772005742 | validation: 0.05638845911223527]
	TIME [epoch: 12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037082022002981066		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.037082022002981066 | validation: 0.03854133114438931]
	TIME [epoch: 12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04263653029493808		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.04263653029493808 | validation: 0.07442189826357418]
	TIME [epoch: 12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05040756131540966		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.05040756131540966 | validation: 0.08299347721922723]
	TIME [epoch: 12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07260960187711661		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.07260960187711661 | validation: 0.0731340727977024]
	TIME [epoch: 12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04852988596212239		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.04852988596212239 | validation: 0.05214137996368812]
	TIME [epoch: 12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03563590972186421		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.03563590972186421 | validation: 0.07115662315088356]
	TIME [epoch: 12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05079375691718485		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.05079375691718485 | validation: 0.08741398688225538]
	TIME [epoch: 12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05021106238153755		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.05021106238153755 | validation: 0.04129851389537792]
	TIME [epoch: 12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03570313774362273		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03570313774362273 | validation: 0.05264491190087533]
	TIME [epoch: 12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047496839125535116		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.047496839125535116 | validation: 0.06957071040854976]
	TIME [epoch: 12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0554669270765179		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.0554669270765179 | validation: 0.042399885090341115]
	TIME [epoch: 12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052976664937354215		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.052976664937354215 | validation: 0.04563058516692446]
	TIME [epoch: 12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045219348789699174		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.045219348789699174 | validation: 0.05116341498373904]
	TIME [epoch: 12 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038113431668031994		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.038113431668031994 | validation: 0.05042839290482454]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05716171840163728		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.05716171840163728 | validation: 0.0423767758292192]
	TIME [epoch: 12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04384479889673855		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.04384479889673855 | validation: 0.10793494312294746]
	TIME [epoch: 12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0486334466648525		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.0486334466648525 | validation: 0.058400730050872054]
	TIME [epoch: 12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035634133182798754		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.035634133182798754 | validation: 0.0451811932386878]
	TIME [epoch: 12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0456001762680766		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0456001762680766 | validation: 0.09783234325117822]
	TIME [epoch: 12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04519167454686691		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.04519167454686691 | validation: 0.03585827073376256]
	TIME [epoch: 12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03729402366444021		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.03729402366444021 | validation: 0.03989491374115973]
	TIME [epoch: 395 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03319745304641321		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.03319745304641321 | validation: 0.040638398781941296]
	TIME [epoch: 25.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0517872306120906		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.0517872306120906 | validation: 0.04643020303548906]
	TIME [epoch: 25.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03903499655177948		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.03903499655177948 | validation: 0.09020165984688633]
	TIME [epoch: 25.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05257699907861723		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.05257699907861723 | validation: 0.03269711719759706]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031387731799080776		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.031387731799080776 | validation: 0.033244890105061764]
	TIME [epoch: 25.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047160109287766254		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.047160109287766254 | validation: 0.03872635578320926]
	TIME [epoch: 25.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02329816206291751		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.02329816206291751 | validation: 0.0337132809706604]
	TIME [epoch: 25.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039035603504624955		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.039035603504624955 | validation: 0.05280219195546341]
	TIME [epoch: 25.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05852023675776622		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.05852023675776622 | validation: 0.037050857145051345]
	TIME [epoch: 25.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041608865687310564		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.041608865687310564 | validation: 0.053073857344918304]
	TIME [epoch: 25.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029042545170183613		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.029042545170183613 | validation: 0.0696897341459139]
	TIME [epoch: 25.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03062368193785322		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.03062368193785322 | validation: 0.026319784493615415]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05511410448128498		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.05511410448128498 | validation: 0.0443563939956715]
	TIME [epoch: 25.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041121642747464236		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.041121642747464236 | validation: 0.031572151913959595]
	TIME [epoch: 25.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025793128590804563		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.025793128590804563 | validation: 0.061169918808013674]
	TIME [epoch: 25.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04997596693434306		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.04997596693434306 | validation: 0.051921067068789725]
	TIME [epoch: 25.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029917302806173995		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.029917302806173995 | validation: 0.05620965486866458]
	TIME [epoch: 25.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04105727309329608		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.04105727309329608 | validation: 0.036423300230553166]
	TIME [epoch: 25.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039768727807139644		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.039768727807139644 | validation: 0.07520321074731375]
	TIME [epoch: 25.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03694219663536238		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.03694219663536238 | validation: 0.024749523907913677]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03125733711170964		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.03125733711170964 | validation: 0.06753751330807593]
	TIME [epoch: 25.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04397610648254066		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.04397610648254066 | validation: 0.04223357616111291]
	TIME [epoch: 25.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0380280163456903		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.0380280163456903 | validation: 0.04218973856615242]
	TIME [epoch: 25.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03135399986274599		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.03135399986274599 | validation: 0.029746306731407598]
	TIME [epoch: 25.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02908817519681204		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.02908817519681204 | validation: 0.04262180242514069]
	TIME [epoch: 25.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037588500879795025		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.037588500879795025 | validation: 0.0395101246469271]
	TIME [epoch: 25.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029871137428110477		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.029871137428110477 | validation: 0.05259332567088616]
	TIME [epoch: 25.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04890856589047117		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.04890856589047117 | validation: 0.05423327795335273]
	TIME [epoch: 25.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02927551701666944		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.02927551701666944 | validation: 0.03587151581904558]
	TIME [epoch: 25.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03517045381090679		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.03517045381090679 | validation: 0.03188531226428246]
	TIME [epoch: 25.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04426007113623775		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.04426007113623775 | validation: 0.026147419722391565]
	TIME [epoch: 25.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03638732597992404		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.03638732597992404 | validation: 0.06276398122254352]
	TIME [epoch: 25.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03115273726677369		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.03115273726677369 | validation: 0.07213517301904196]
	TIME [epoch: 25.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03742001867717826		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.03742001867717826 | validation: 0.026148341000672586]
	TIME [epoch: 25.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024343272425841334		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.024343272425841334 | validation: 0.032657113268504734]
	TIME [epoch: 25.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02103282279379203		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02103282279379203 | validation: 0.03721737211911163]
	TIME [epoch: 25.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053022384009681546		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.053022384009681546 | validation: 0.04007985452817686]
	TIME [epoch: 25.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034615704230014935		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.034615704230014935 | validation: 0.03414066454185133]
	TIME [epoch: 25.7 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017865934458133736		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.017865934458133736 | validation: 0.0626432416835933]
	TIME [epoch: 25.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04846182833220411		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.04846182833220411 | validation: 0.03813314534150798]
	TIME [epoch: 25.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031071631150821087		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.031071631150821087 | validation: 0.04493478007460365]
	TIME [epoch: 25.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02509675814195271		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.02509675814195271 | validation: 0.04563909533798571]
	TIME [epoch: 25.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037767349909800414		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.037767349909800414 | validation: 0.06176955359165033]
	TIME [epoch: 25.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03529370332588567		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.03529370332588567 | validation: 0.03879906470303256]
	TIME [epoch: 25.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026333224519840714		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.026333224519840714 | validation: 0.033723721637015175]
	TIME [epoch: 25.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03353178895753505		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.03353178895753505 | validation: 0.029700252369268425]
	TIME [epoch: 25.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025370785570113553		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.025370785570113553 | validation: 0.06347779830243444]
	TIME [epoch: 25.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04150714855564132		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.04150714855564132 | validation: 0.047624806463153826]
	TIME [epoch: 25.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03133387239658985		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.03133387239658985 | validation: 0.02827945279082255]
	TIME [epoch: 25.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017323705312272255		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.017323705312272255 | validation: 0.0461800891217261]
	TIME [epoch: 25.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029069309229946064		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.029069309229946064 | validation: 0.0356136070571707]
	TIME [epoch: 25.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04677350735263491		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.04677350735263491 | validation: 0.09508638399071515]
	TIME [epoch: 25.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05229340721273677		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.05229340721273677 | validation: 0.0293005378749814]
	TIME [epoch: 25.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02355265884694318		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.02355265884694318 | validation: 0.025779469189354837]
	TIME [epoch: 25.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02242355526888238		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.02242355526888238 | validation: 0.05401235759427432]
	TIME [epoch: 25.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029369013434031775		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.029369013434031775 | validation: 0.043373013496491725]
	TIME [epoch: 25.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03388619922566367		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.03388619922566367 | validation: 0.02613138203904119]
	TIME [epoch: 25.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01991020102705287		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.01991020102705287 | validation: 0.026975175343634323]
	TIME [epoch: 25.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025727848784548896		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.025727848784548896 | validation: 0.0395695901124073]
	TIME [epoch: 25.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04305000642249676		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.04305000642249676 | validation: 0.05492026975143832]
	TIME [epoch: 25.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0217878328845913		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0217878328845913 | validation: 0.02459536312683123]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020451779310174854		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.020451779310174854 | validation: 0.047625173162896335]
	TIME [epoch: 25.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028211039778209504		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.028211039778209504 | validation: 0.020749951141988902]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03648961258480605		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.03648961258480605 | validation: 0.06726953383600065]
	TIME [epoch: 25.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024227046287717463		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.024227046287717463 | validation: 0.0197036779870784]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03136412058191952		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.03136412058191952 | validation: 0.061767663615698645]
	TIME [epoch: 25.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029033066847589835		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.029033066847589835 | validation: 0.023925168186971952]
	TIME [epoch: 25.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017533252598690298		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.017533252598690298 | validation: 0.0738364764137948]
	TIME [epoch: 25.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03595733497721837		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.03595733497721837 | validation: 0.02612500304534642]
	TIME [epoch: 25.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035582815671110324		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.035582815671110324 | validation: 0.0452588365781593]
	TIME [epoch: 25.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02246064949298765		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.02246064949298765 | validation: 0.02431626094398455]
	TIME [epoch: 25.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020440454270336066		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.020440454270336066 | validation: 0.07307132412366119]
	TIME [epoch: 25.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028567445007498027		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.028567445007498027 | validation: 0.039917744137842597]
	TIME [epoch: 25.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026937045984657623		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.026937045984657623 | validation: 0.020886394570834188]
	TIME [epoch: 25.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028088447438775206		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.028088447438775206 | validation: 0.04359070405753523]
	TIME [epoch: 25.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030524078130425777		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.030524078130425777 | validation: 0.046386168871466425]
	TIME [epoch: 25.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026462277981637368		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.026462277981637368 | validation: 0.026107966401231628]
	TIME [epoch: 25.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021549638502658244		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.021549638502658244 | validation: 0.025753734847072135]
	TIME [epoch: 25.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027649646445590745		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.027649646445590745 | validation: 0.03142951520595333]
	TIME [epoch: 25.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02636916353542114		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.02636916353542114 | validation: 0.04727623123532655]
	TIME [epoch: 25.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02540815988431438		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.02540815988431438 | validation: 0.03490408927570319]
	TIME [epoch: 25.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027136409536545666		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.027136409536545666 | validation: 0.024304381300911716]
	TIME [epoch: 25.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021576414591914374		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.021576414591914374 | validation: 0.04061428585669001]
	TIME [epoch: 25.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0260967111025816		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0260967111025816 | validation: 0.019492320608426525]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027142116775356533		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.027142116775356533 | validation: 0.04580932348825597]
	TIME [epoch: 25.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019955071462151024		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.019955071462151024 | validation: 0.022051319178813818]
	TIME [epoch: 25.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024899407824173138		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.024899407824173138 | validation: 0.03313121581186719]
	TIME [epoch: 25.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023966670649977657		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.023966670649977657 | validation: 0.023104304274579362]
	TIME [epoch: 25.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027663755158476876		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.027663755158476876 | validation: 0.04156230655249796]
	TIME [epoch: 25.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019329192339912395		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.019329192339912395 | validation: 0.03446550968172085]
	TIME [epoch: 25.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025581524613918428		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.025581524613918428 | validation: 0.02335329594886868]
	TIME [epoch: 25.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019455777341611316		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.019455777341611316 | validation: 0.05524264763603062]
	TIME [epoch: 25.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03185522311216725		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.03185522311216725 | validation: 0.02121070763398955]
	TIME [epoch: 25.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025484641678464752		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.025484641678464752 | validation: 0.021933225685455467]
	TIME [epoch: 25.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019998623971292453		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.019998623971292453 | validation: 0.029014612505416745]
	TIME [epoch: 25.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027731970155962724		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.027731970155962724 | validation: 0.03453281216778337]
	TIME [epoch: 25.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021643380582273598		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.021643380582273598 | validation: 0.027652333992635968]
	TIME [epoch: 25.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02049524105406084		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.02049524105406084 | validation: 0.030339054033221773]
	TIME [epoch: 25.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020608641176431346		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.020608641176431346 | validation: 0.029573407699156114]
	TIME [epoch: 25.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021755966379088696		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.021755966379088696 | validation: 0.023436068763616424]
	TIME [epoch: 25.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025105680896371125		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.025105680896371125 | validation: 0.03744659579104715]
	TIME [epoch: 25.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02527463822218617		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.02527463822218617 | validation: 0.021658506493493865]
	TIME [epoch: 25.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016959933175187823		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.016959933175187823 | validation: 0.033101788813524685]
	TIME [epoch: 25.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02050633737382844		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.02050633737382844 | validation: 0.03294436558141786]
	TIME [epoch: 25.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027369773063693262		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.027369773063693262 | validation: 0.02270316055473655]
	TIME [epoch: 25.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02275951807449661		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.02275951807449661 | validation: 0.03928452843833653]
	TIME [epoch: 25.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016155576278944376		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.016155576278944376 | validation: 0.023883349212505817]
	TIME [epoch: 25.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018353733368944254		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.018353733368944254 | validation: 0.031368001008454695]
	TIME [epoch: 25.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025371477965939977		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.025371477965939977 | validation: 0.02955307997143309]
	TIME [epoch: 25.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021110635102533096		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.021110635102533096 | validation: 0.02274699155194492]
	TIME [epoch: 25.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021012761083868736		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.021012761083868736 | validation: 0.029812366433425767]
	TIME [epoch: 25.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02611161525672523		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.02611161525672523 | validation: 0.05935968194959264]
	TIME [epoch: 25.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0286081774306352		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.0286081774306352 | validation: 0.02615898565117738]
	TIME [epoch: 25.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017799236760063064		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.017799236760063064 | validation: 0.029047846057631492]
	TIME [epoch: 25.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016974389799773608		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.016974389799773608 | validation: 0.021335922270423684]
	TIME [epoch: 25.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018423959181886992		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.018423959181886992 | validation: 0.024833556993392906]
	TIME [epoch: 25.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025932653546705944		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.025932653546705944 | validation: 0.03340778406476951]
	TIME [epoch: 25.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02174156271629325		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.02174156271629325 | validation: 0.024673560659150637]
	TIME [epoch: 25.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016306094463800422		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.016306094463800422 | validation: 0.03528728956617238]
	TIME [epoch: 25.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021977965406076827		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.021977965406076827 | validation: 0.02160248312117328]
	TIME [epoch: 25.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027670833918851754		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.027670833918851754 | validation: 0.02180293435120481]
	TIME [epoch: 25.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01670437597412566		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.01670437597412566 | validation: 0.030598191362884143]
	TIME [epoch: 25.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020764985008401925		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.020764985008401925 | validation: 0.03047038223134154]
	TIME [epoch: 25.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024638033025416034		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.024638033025416034 | validation: 0.03859847816549071]
	TIME [epoch: 25.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021099985064361126		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.021099985064361126 | validation: 0.021843050298677835]
	TIME [epoch: 25.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017552198984989522		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.017552198984989522 | validation: 0.021993751693595433]
	TIME [epoch: 25.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021698175818242238		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.021698175818242238 | validation: 0.020960058089663315]
	TIME [epoch: 25.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017810008791962463		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.017810008791962463 | validation: 0.037509932969263714]
	TIME [epoch: 25.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029475529278862975		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.029475529278862975 | validation: 0.023820967629391093]
	TIME [epoch: 25.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016439565233274375		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.016439565233274375 | validation: 0.02379157310603265]
	TIME [epoch: 25.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016547627740564005		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.016547627740564005 | validation: 0.020194773184141868]
	TIME [epoch: 25.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013949815650242378		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.013949815650242378 | validation: 0.0276407469231415]
	TIME [epoch: 25.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02529894290309361		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.02529894290309361 | validation: 0.030229658664273087]
	TIME [epoch: 25.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020552858695605254		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.020552858695605254 | validation: 0.02751910505696692]
	TIME [epoch: 25.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015526705003963344		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.015526705003963344 | validation: 0.019682026937905876]
	TIME [epoch: 25.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01923102576111626		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.01923102576111626 | validation: 0.014403675245938469]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01524855207281834		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.01524855207281834 | validation: 0.027989588314380483]
	TIME [epoch: 25.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024429841592090884		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.024429841592090884 | validation: 0.030504314585983246]
	TIME [epoch: 25.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016134284020076037		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.016134284020076037 | validation: 0.015865780741836116]
	TIME [epoch: 25.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012242250894762581		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.012242250894762581 | validation: 0.03521873186494795]
	TIME [epoch: 25.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02506257861442407		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.02506257861442407 | validation: 0.018174455572963596]
	TIME [epoch: 25.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015808871520098192		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.015808871520098192 | validation: 0.030482446870556894]
	TIME [epoch: 25.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021129787424821377		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.021129787424821377 | validation: 0.014356434730683282]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013322644866812108		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.013322644866812108 | validation: 0.02500626821129757]
	TIME [epoch: 25.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016122375116341042		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.016122375116341042 | validation: 0.015965323112254123]
	TIME [epoch: 25.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025062623358024567		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.025062623358024567 | validation: 0.019192804539437617]
	TIME [epoch: 25.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015511367880231237		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.015511367880231237 | validation: 0.028473372535781097]
	TIME [epoch: 25.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022125980609149925		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.022125980609149925 | validation: 0.022625990775685223]
	TIME [epoch: 25.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014001327036254915		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.014001327036254915 | validation: 0.015204703848742828]
	TIME [epoch: 25.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018651285997295295		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.018651285997295295 | validation: 0.017086520373401675]
	TIME [epoch: 25.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016238440233261917		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.016238440233261917 | validation: 0.019188822517914324]
	TIME [epoch: 25.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02100138372543275		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.02100138372543275 | validation: 0.02474650780875577]
	TIME [epoch: 25.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01598472575561675		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.01598472575561675 | validation: 0.0245164165664534]
	TIME [epoch: 25.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018752983740694673		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.018752983740694673 | validation: 0.022299980678141512]
	TIME [epoch: 25.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01669327141036535		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.01669327141036535 | validation: 0.023513627871729968]
	TIME [epoch: 25.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01869892259123505		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.01869892259123505 | validation: 0.021236954732100386]
	TIME [epoch: 25.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013948609733918652		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.013948609733918652 | validation: 0.022509995988116874]
	TIME [epoch: 25.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018209570056656275		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.018209570056656275 | validation: 0.015322791718801063]
	TIME [epoch: 25.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016849291446598313		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.016849291446598313 | validation: 0.023933335767097583]
	TIME [epoch: 25.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014718209513539587		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.014718209513539587 | validation: 0.026299039342391077]
	TIME [epoch: 25.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025208889525972923		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.025208889525972923 | validation: 0.03470872912142614]
	TIME [epoch: 25.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014989986909570333		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.014989986909570333 | validation: 0.018355741978218446]
	TIME [epoch: 25.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01917161108168897		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.01917161108168897 | validation: 0.018131563029226146]
	TIME [epoch: 25.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012746634627721768		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.012746634627721768 | validation: 0.02417200074368487]
	TIME [epoch: 25.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014830747652973979		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.014830747652973979 | validation: 0.02816876611106031]
	TIME [epoch: 25.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019691187554917007		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.019691187554917007 | validation: 0.019201887792656677]
	TIME [epoch: 25.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014242551717873714		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.014242551717873714 | validation: 0.03969737842135172]
	TIME [epoch: 25.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018433459737344123		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.018433459737344123 | validation: 0.016893066530241377]
	TIME [epoch: 25.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014433426924631915		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.014433426924631915 | validation: 0.016855430637144567]
	TIME [epoch: 25.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017204960349160022		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.017204960349160022 | validation: 0.024930822573715622]
	TIME [epoch: 25.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013220355554063274		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.013220355554063274 | validation: 0.02264142013166457]
	TIME [epoch: 25.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016154261807616213		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.016154261807616213 | validation: 0.021686808493521142]
	TIME [epoch: 25.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01867295183442552		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.01867295183442552 | validation: 0.026004068659514233]
	TIME [epoch: 25.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019102012531309304		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.019102012531309304 | validation: 0.015858037814185145]
	TIME [epoch: 25.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011104046284880955		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.011104046284880955 | validation: 0.016904504553451254]
	TIME [epoch: 25.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015479845881603662		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.015479845881603662 | validation: 0.025862772923352054]
	TIME [epoch: 25.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02165586910462329		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.02165586910462329 | validation: 0.028124238022658662]
	TIME [epoch: 25.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014762580775212687		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.014762580775212687 | validation: 0.015233476833618153]
	TIME [epoch: 25.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012851424083401307		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.012851424083401307 | validation: 0.018050703276696656]
	TIME [epoch: 25.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016787399656082287		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.016787399656082287 | validation: 0.02210757827746913]
	TIME [epoch: 25.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02568659019778435		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.02568659019778435 | validation: 0.026310425738773662]
	TIME [epoch: 25.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01796606810953292		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.01796606810953292 | validation: 0.016624874540104168]
	TIME [epoch: 25.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010560353807358551		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.010560353807358551 | validation: 0.014955327711587264]
	TIME [epoch: 25.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009702534514439032		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.009702534514439032 | validation: 0.012074569729153283]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01209420075387251		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.01209420075387251 | validation: 0.029857037387347504]
	TIME [epoch: 25.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021140152281872753		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.021140152281872753 | validation: 0.017964039439665402]
	TIME [epoch: 25.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011269815221806877		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.011269815221806877 | validation: 0.017431951833073535]
	TIME [epoch: 25.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012056624054872213		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.012056624054872213 | validation: 0.01884027826500112]
	TIME [epoch: 25.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02017228514094356		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.02017228514094356 | validation: 0.0297629319449843]
	TIME [epoch: 25.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01623320989475685		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.01623320989475685 | validation: 0.026575940073204164]
	TIME [epoch: 25.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013140894794313896		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.013140894794313896 | validation: 0.02225719750288053]
	TIME [epoch: 25.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01410348577350266		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.01410348577350266 | validation: 0.01556251498253055]
	TIME [epoch: 25.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012722379875441926		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.012722379875441926 | validation: 0.01677587412735791]
	TIME [epoch: 25.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013796075425381797		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.013796075425381797 | validation: 0.015736126453512172]
	TIME [epoch: 25.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017854537797043504		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.017854537797043504 | validation: 0.013015128158020046]
	TIME [epoch: 25.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01050097629997104		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.01050097629997104 | validation: 0.020156157880671016]
	TIME [epoch: 25.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016810637863336655		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.016810637863336655 | validation: 0.012366954696848493]
	TIME [epoch: 25.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013250760646833167		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.013250760646833167 | validation: 0.017025537842421022]
	TIME [epoch: 25.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014285723085694431		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.014285723085694431 | validation: 0.0212683127839407]
	TIME [epoch: 25.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013606147827703972		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.013606147827703972 | validation: 0.014347692884904284]
	TIME [epoch: 25.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013804091597390641		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.013804091597390641 | validation: 0.02639352489033587]
	TIME [epoch: 25.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015547737950624292		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.015547737950624292 | validation: 0.01577343861111259]
	TIME [epoch: 25.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017687499037103142		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.017687499037103142 | validation: 0.022067603835933827]
	TIME [epoch: 25.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014407738467146679		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.014407738467146679 | validation: 0.017146609968231537]
	TIME [epoch: 25.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010781837775453804		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.010781837775453804 | validation: 0.014943359019201852]
	TIME [epoch: 25.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013823533103727229		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.013823533103727229 | validation: 0.012349309653135614]
	TIME [epoch: 25.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016858759355161557		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.016858759355161557 | validation: 0.02306601142678926]
	TIME [epoch: 25.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014726947378349104		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.014726947378349104 | validation: 0.013514346637671005]
	TIME [epoch: 25.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011955460168456718		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.011955460168456718 | validation: 0.020218270136269735]
	TIME [epoch: 25.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01365503255829045		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.01365503255829045 | validation: 0.017988881761358638]
	TIME [epoch: 25.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014559104229747051		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.014559104229747051 | validation: 0.019719038302713793]
	TIME [epoch: 25.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013441275736530072		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.013441275736530072 | validation: 0.019889580075559715]
	TIME [epoch: 25.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011159857832155848		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.011159857832155848 | validation: 0.01994166539306607]
	TIME [epoch: 25.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014030273532600715		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.014030273532600715 | validation: 0.022538564725699345]
	TIME [epoch: 25.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018413040203164453		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.018413040203164453 | validation: 0.01561190456010451]
	TIME [epoch: 25.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01169245622990818		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.01169245622990818 | validation: 0.01372717144456959]
	TIME [epoch: 25.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009388939996340832		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.009388939996340832 | validation: 0.012420032062338474]
	TIME [epoch: 25.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01535798421777716		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.01535798421777716 | validation: 0.025652024410895622]
	TIME [epoch: 25.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01341557242964172		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.01341557242964172 | validation: 0.013475671318674508]
	TIME [epoch: 25.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01158653170136355		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.01158653170136355 | validation: 0.029044413791128494]
	TIME [epoch: 25.6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01445813535822531		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.01445813535822531 | validation: 0.012438380035890977]
	TIME [epoch: 25.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011016607493913318		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.011016607493913318 | validation: 0.01765946326595516]
	TIME [epoch: 25.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01370006296501309		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.01370006296501309 | validation: 0.015585810292498045]
	TIME [epoch: 25.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012317295546027547		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.012317295546027547 | validation: 0.0194094085641612]
	TIME [epoch: 25.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013174347619992541		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.013174347619992541 | validation: 0.017270709335630696]
	TIME [epoch: 25.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021023313054777726		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.021023313054777726 | validation: 0.014314304180115664]
	TIME [epoch: 25.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010980610845284327		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.010980610845284327 | validation: 0.01268652953062414]
	TIME [epoch: 25.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010943073153086515		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.010943073153086515 | validation: 0.01800839663101901]
	TIME [epoch: 25.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014403857634419408		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.014403857634419408 | validation: 0.01424783640005569]
	TIME [epoch: 25.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008829882700861677		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.008829882700861677 | validation: 0.018740626029011866]
	TIME [epoch: 25.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012408572515575604		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.012408572515575604 | validation: 0.022955981804375733]
	TIME [epoch: 25.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013141260594563414		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.013141260594563414 | validation: 0.026074118631801403]
	TIME [epoch: 25.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015469202361827424		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.015469202361827424 | validation: 0.01457937495477743]
	TIME [epoch: 25.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00945704512991129		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.00945704512991129 | validation: 0.012808172107325213]
	TIME [epoch: 25.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012432493129899054		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.012432493129899054 | validation: 0.019137285754260363]
	TIME [epoch: 25.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0125260128954129		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0125260128954129 | validation: 0.019277153791485684]
	TIME [epoch: 25.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015064491608514263		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.015064491608514263 | validation: 0.013523860337235252]
	TIME [epoch: 25.6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010290096580070553		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.010290096580070553 | validation: 0.013502378553312978]
	TIME [epoch: 25.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012819742146582505		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.012819742146582505 | validation: 0.012972166965617753]
	TIME [epoch: 25.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00963164525827806		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.00963164525827806 | validation: 0.012090780771076405]
	TIME [epoch: 25.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011880088192577399		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.011880088192577399 | validation: 0.01475240031236618]
	TIME [epoch: 25.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010183725622009459		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.010183725622009459 | validation: 0.02205368687098521]
	TIME [epoch: 25.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015184884712328027		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.015184884712328027 | validation: 0.014553767805726982]
	TIME [epoch: 25.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011906219646084158		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.011906219646084158 | validation: 0.012550943808149372]
	TIME [epoch: 25.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008858627676131384		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.008858627676131384 | validation: 0.012899985165032348]
	TIME [epoch: 25.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011593830589515195		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.011593830589515195 | validation: 0.01693349505175451]
	TIME [epoch: 25.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012723439342398356		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.012723439342398356 | validation: 0.012705727589847253]
	TIME [epoch: 25.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009463870917039707		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.009463870917039707 | validation: 0.0222496686868168]
	TIME [epoch: 25.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012347253227171479		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.012347253227171479 | validation: 0.01411983460933891]
	TIME [epoch: 25.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011323942771846701		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.011323942771846701 | validation: 0.014536540159547025]
	TIME [epoch: 25.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009410662155635331		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.009410662155635331 | validation: 0.013281372379773868]
	TIME [epoch: 25.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01120685906192502		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.01120685906192502 | validation: 0.026999739663523148]
	TIME [epoch: 25.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012527349070488603		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.012527349070488603 | validation: 0.016567812851108657]
	TIME [epoch: 25.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012534498940987022		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.012534498940987022 | validation: 0.024973680699448365]
	TIME [epoch: 25.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013371451254057507		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.013371451254057507 | validation: 0.012433045603201902]
	TIME [epoch: 25.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009225283017553512		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.009225283017553512 | validation: 0.012216324986012852]
	TIME [epoch: 25.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009739967957944113		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.009739967957944113 | validation: 0.015125025597774946]
	TIME [epoch: 25.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012925696458251201		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.012925696458251201 | validation: 0.011573759748139407]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009662138614064154		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.009662138614064154 | validation: 0.012925234503800023]
	TIME [epoch: 25.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013746338210668315		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.013746338210668315 | validation: 0.014662414472553897]
	TIME [epoch: 25.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011409454654475633		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.011409454654475633 | validation: 0.012668923186951352]
	TIME [epoch: 25.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009144228306988368		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.009144228306988368 | validation: 0.012028759092396587]
	TIME [epoch: 25.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010504025540160389		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.010504025540160389 | validation: 0.012130244274850426]
	TIME [epoch: 25.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010184421965753249		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.010184421965753249 | validation: 0.019895230877181737]
	TIME [epoch: 25.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014664876549605986		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.014664876549605986 | validation: 0.01456567694965527]
	TIME [epoch: 25.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011120612087636983		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.011120612087636983 | validation: 0.01599479511901981]
	TIME [epoch: 25.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01052282841785105		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.01052282841785105 | validation: 0.013233512144114314]
	TIME [epoch: 25.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009025848539267732		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.009025848539267732 | validation: 0.016089990747417897]
	TIME [epoch: 25.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013151294918731642		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.013151294918731642 | validation: 0.011325770541586722]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010595183771182771		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.010595183771182771 | validation: 0.010037863066173845]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007614867621739919		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.007614867621739919 | validation: 0.009588393521273962]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011392207909435445		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.011392207909435445 | validation: 0.012475266449057776]
	TIME [epoch: 25.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01235606762699007		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.01235606762699007 | validation: 0.013958496453201144]
	TIME [epoch: 25.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011250790329811865		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.011250790329811865 | validation: 0.01247655294553779]
	TIME [epoch: 25.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009667469839289886		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.009667469839289886 | validation: 0.010335954215108117]
	TIME [epoch: 25.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00886835076179318		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.00886835076179318 | validation: 0.012090327163557537]
	TIME [epoch: 25.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009351248689919139		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.009351248689919139 | validation: 0.01154126314898892]
	TIME [epoch: 25.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010914267203091084		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.010914267203091084 | validation: 0.008913020295014952]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01019327390839395		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.01019327390839395 | validation: 0.013535762212533584]
	TIME [epoch: 25.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012153075296477703		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.012153075296477703 | validation: 0.01281055194399366]
	TIME [epoch: 25.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009803827642913304		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.009803827642913304 | validation: 0.013885858680995641]
	TIME [epoch: 25.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009836403705157275		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.009836403705157275 | validation: 0.012723069340269923]
	TIME [epoch: 25.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009849689144083454		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.009849689144083454 | validation: 0.015359441450017167]
	TIME [epoch: 25.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01018712266422366		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.01018712266422366 | validation: 0.01492682502427033]
	TIME [epoch: 25.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009191145428547601		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.009191145428547601 | validation: 0.020244196332494004]
	TIME [epoch: 25.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012323613744426185		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.012323613744426185 | validation: 0.01743695346776817]
	TIME [epoch: 25.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011083519803830906		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.011083519803830906 | validation: 0.009782429720286477]
	TIME [epoch: 25.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006407515419351314		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.006407515419351314 | validation: 0.008157574750554529]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008999308802145778		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.008999308802145778 | validation: 0.02616439125021265]
	TIME [epoch: 25.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01750318007868322		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.01750318007868322 | validation: 0.0097644857679721]
	TIME [epoch: 25.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069228153104208875		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.0069228153104208875 | validation: 0.011673373730722355]
	TIME [epoch: 25.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006470494757901678		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.006470494757901678 | validation: 0.00819788940693729]
	TIME [epoch: 25.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009256911518092183		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.009256911518092183 | validation: 0.014468201521239392]
	TIME [epoch: 25.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011465153244102485		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.011465153244102485 | validation: 0.011058104240349424]
	TIME [epoch: 25.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009218138835473837		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.009218138835473837 | validation: 0.01193112325902818]
	TIME [epoch: 25.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008614196900682176		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.008614196900682176 | validation: 0.011137998861949041]
	TIME [epoch: 25.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010588650210484425		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.010588650210484425 | validation: 0.008482358282754985]
	TIME [epoch: 25.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007554935317837706		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.007554935317837706 | validation: 0.008335942965430061]
	TIME [epoch: 25.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008800957937803939		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.008800957937803939 | validation: 0.010995723437395579]
	TIME [epoch: 25.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00966134992656884		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.00966134992656884 | validation: 0.010805666169828384]
	TIME [epoch: 25.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007063057344814556		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.007063057344814556 | validation: 0.015645839350597372]
	TIME [epoch: 25.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009241633327118057		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.009241633327118057 | validation: 0.012269496763113787]
	TIME [epoch: 25.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008236624878553821		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.008236624878553821 | validation: 0.00854490517091556]
	TIME [epoch: 25.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008724719166973215		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.008724719166973215 | validation: 0.013711083214221447]
	TIME [epoch: 25.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010483880038097564		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.010483880038097564 | validation: 0.007702135582243011]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010139427640116043		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.010139427640116043 | validation: 0.010380399497590505]
	TIME [epoch: 25.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010852821072598905		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.010852821072598905 | validation: 0.021325110696783747]
	TIME [epoch: 25.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01219215103524598		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.01219215103524598 | validation: 0.011878224536304147]
	TIME [epoch: 25.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007726234789592811		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.007726234789592811 | validation: 0.014503936423424794]
	TIME [epoch: 25.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009630736715039022		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.009630736715039022 | validation: 0.007232669302331193]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009102997332904277		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.009102997332904277 | validation: 0.008131566840892912]
	TIME [epoch: 25.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007204237456493265		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.007204237456493265 | validation: 0.010634183758716203]
	TIME [epoch: 25.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00854652404823185		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.00854652404823185 | validation: 0.00763311212629382]
	TIME [epoch: 25.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00972668041635645		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.00972668041635645 | validation: 0.013736594263017946]
	TIME [epoch: 25.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010396126025280174		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.010396126025280174 | validation: 0.008039147856871176]
	TIME [epoch: 25.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006453670450263087		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.006453670450263087 | validation: 0.0069083140800773055]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006688166075965553		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.006688166075965553 | validation: 0.012124160921828397]
	TIME [epoch: 25.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009173409381162135		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.009173409381162135 | validation: 0.008759165851025184]
	TIME [epoch: 25.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00640555364423805		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.00640555364423805 | validation: 0.010566953118572593]
	TIME [epoch: 25.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009435628019556823		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.009435628019556823 | validation: 0.009940557693173386]
	TIME [epoch: 25.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007427720353514428		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.007427720353514428 | validation: 0.010173094652971032]
	TIME [epoch: 25.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011977214994228637		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.011977214994228637 | validation: 0.008285195975136652]
	TIME [epoch: 25.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008198887095184976		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.008198887095184976 | validation: 0.008109389208010583]
	TIME [epoch: 25.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007272074604830006		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.007272074604830006 | validation: 0.011121727491781582]
	TIME [epoch: 25.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071780964169963594		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.0071780964169963594 | validation: 0.009036003417791596]
	TIME [epoch: 25.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008547744626063045		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.008547744626063045 | validation: 0.006711199602562049]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008422410873256583		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.008422410873256583 | validation: 0.012114178964231358]
	TIME [epoch: 25.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008483451395332233		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.008483451395332233 | validation: 0.00946523101540297]
	TIME [epoch: 25.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007884662324307526		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.007884662324307526 | validation: 0.006728708519274157]
	TIME [epoch: 25.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007109051262424175		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.007109051262424175 | validation: 0.011345394996524984]
	TIME [epoch: 25.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007235829605015219		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.007235829605015219 | validation: 0.01057732164119816]
	TIME [epoch: 25.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008662106944206119		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.008662106944206119 | validation: 0.008440305805194323]
	TIME [epoch: 25.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00884939468713073		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.00884939468713073 | validation: 0.005852151689222841]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006435065948284124		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.006435065948284124 | validation: 0.012589497542439916]
	TIME [epoch: 25.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008886056569564914		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.008886056569564914 | validation: 0.007035854367285271]
	TIME [epoch: 25.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006609369236755698		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.006609369236755698 | validation: 0.012201769039494277]
	TIME [epoch: 25.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009065060282344866		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.009065060282344866 | validation: 0.006883356276518426]
	TIME [epoch: 25.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006936879760069202		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.006936879760069202 | validation: 0.00865866488110625]
	TIME [epoch: 25.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00802752544498246		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.00802752544498246 | validation: 0.013383916675160736]
	TIME [epoch: 25.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007723365365403174		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.007723365365403174 | validation: 0.006571489438119488]
	TIME [epoch: 25.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007208352475351303		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.007208352475351303 | validation: 0.008325118323341416]
	TIME [epoch: 25.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007544184417955719		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.007544184417955719 | validation: 0.00878628820764641]
	TIME [epoch: 25.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007034839403442751		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.007034839403442751 | validation: 0.010738272089664163]
	TIME [epoch: 25.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008658201169535007		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.008658201169535007 | validation: 0.010056028939022016]
	TIME [epoch: 25.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059067142476005256		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0059067142476005256 | validation: 0.005850386683337356]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071492457286538285		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0071492457286538285 | validation: 0.0068937443266457255]
	TIME [epoch: 25.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007005044740362338		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.007005044740362338 | validation: 0.007776284352864159]
	TIME [epoch: 25.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008068956462572831		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.008068956462572831 | validation: 0.007597423640505177]
	TIME [epoch: 25.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006192821880324579		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.006192821880324579 | validation: 0.008136999252270563]
	TIME [epoch: 25.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00953191657676714		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.00953191657676714 | validation: 0.011964794144962574]
	TIME [epoch: 25.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006519372408309606		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.006519372408309606 | validation: 0.0058474394054380445]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007277776691635722		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.007277776691635722 | validation: 0.005436363819822494]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058377136063592315		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0058377136063592315 | validation: 0.007842941376606634]
	TIME [epoch: 25.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007817343681424718		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.007817343681424718 | validation: 0.006065643955606289]
	TIME [epoch: 25.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006147566472041756		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.006147566472041756 | validation: 0.006134539753596315]
	TIME [epoch: 25.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076102066629030836		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.0076102066629030836 | validation: 0.008219663447543723]
	TIME [epoch: 25.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006956601014498196		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.006956601014498196 | validation: 0.007333989668440979]
	TIME [epoch: 25.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006045538598332154		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.006045538598332154 | validation: 0.008978642517000902]
	TIME [epoch: 25.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006970276605045119		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.006970276605045119 | validation: 0.0074537144228941895]
	TIME [epoch: 25.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00609989564639329		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.00609989564639329 | validation: 0.005837781025301058]
	TIME [epoch: 25.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005781359809228544		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.005781359809228544 | validation: 0.007532692376827478]
	TIME [epoch: 25.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009369417771490422		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.009369417771490422 | validation: 0.01029726071687223]
	TIME [epoch: 25.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006961679362539511		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.006961679362539511 | validation: 0.00755618963530068]
	TIME [epoch: 25.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071691510137261005		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.0071691510137261005 | validation: 0.006769235850437596]
	TIME [epoch: 25.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006886475676561266		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.006886475676561266 | validation: 0.009273953430414903]
	TIME [epoch: 25.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008325127268062109		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.008325127268062109 | validation: 0.005348247640107001]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006430352674798639		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.006430352674798639 | validation: 0.0053564971743207245]
	TIME [epoch: 25.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005905496917454929		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.005905496917454929 | validation: 0.011792120308312951]
	TIME [epoch: 25.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069844781942119015		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.0069844781942119015 | validation: 0.004953216897518722]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005036519329631819		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.005036519329631819 | validation: 0.005428583390797969]
	TIME [epoch: 25.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007186022244094857		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.007186022244094857 | validation: 0.0067487497737718765]
	TIME [epoch: 25.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074293506754784955		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.0074293506754784955 | validation: 0.005338555950453472]
	TIME [epoch: 25.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007238506073939622		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.007238506073939622 | validation: 0.006399406150859245]
	TIME [epoch: 25.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005132106412612315		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.005132106412612315 | validation: 0.00595055521998222]
	TIME [epoch: 25.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005690706709900891		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.005690706709900891 | validation: 0.009063340930097766]
	TIME [epoch: 25.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008049218514365386		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.008049218514365386 | validation: 0.004392123657797517]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006281144730744297		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.006281144730744297 | validation: 0.005172183793801292]
	TIME [epoch: 25.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005041253405140838		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.005041253405140838 | validation: 0.004779909841273881]
	TIME [epoch: 25.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061139620773422824		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0061139620773422824 | validation: 0.010383556492664262]
	TIME [epoch: 25.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00674165091702607		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.00674165091702607 | validation: 0.005653706446990821]
	TIME [epoch: 25.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005760265570384272		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.005760265570384272 | validation: 0.006691116438137124]
	TIME [epoch: 25.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006828017763598204		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.006828017763598204 | validation: 0.006283234094255361]
	TIME [epoch: 25.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006086465682944811		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.006086465682944811 | validation: 0.007408750916991477]
	TIME [epoch: 25.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005301881304304239		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.005301881304304239 | validation: 0.0072036942551445635]
	TIME [epoch: 25.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061783703044581365		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0061783703044581365 | validation: 0.004503388532130686]
	TIME [epoch: 25.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006495454084152952		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.006495454084152952 | validation: 0.009040348673718687]
	TIME [epoch: 25.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061316012663775775		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0061316012663775775 | validation: 0.01709458520002601]
	TIME [epoch: 25.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007617255214649684		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.007617255214649684 | validation: 0.004923597241533594]
	TIME [epoch: 25.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005256981922761884		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.005256981922761884 | validation: 0.004201504167915314]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005998823055579521		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.005998823055579521 | validation: 0.006966872047546262]
	TIME [epoch: 25.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006206930016553556		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.006206930016553556 | validation: 0.005205673668680015]
	TIME [epoch: 25.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005147722524297827		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.005147722524297827 | validation: 0.006984079696518394]
	TIME [epoch: 25.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006158651692033869		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.006158651692033869 | validation: 0.008621562015918363]
	TIME [epoch: 25.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006947949687184979		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.006947949687184979 | validation: 0.0050860975948309505]
	TIME [epoch: 25.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006473130361769393		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.006473130361769393 | validation: 0.00561361493116916]
	TIME [epoch: 25.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005724611387492617		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.005724611387492617 | validation: 0.006987270582935944]
	TIME [epoch: 25.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004257512493772659		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.004257512493772659 | validation: 0.005684972578598945]
	TIME [epoch: 25.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006588351978286717		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.006588351978286717 | validation: 0.014284097237160039]
	TIME [epoch: 25.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066427926879029245		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.0066427926879029245 | validation: 0.005569246861713497]
	TIME [epoch: 25.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005558167268969284		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.005558167268969284 | validation: 0.006703608298524252]
	TIME [epoch: 25.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006427433300109039		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.006427433300109039 | validation: 0.006069163284115731]
	TIME [epoch: 25.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005911003743942055		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.005911003743942055 | validation: 0.005150161275525825]
	TIME [epoch: 25.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006335134360322542		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.006335134360322542 | validation: 0.004254865768846176]
	TIME [epoch: 25.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005165539728746293		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.005165539728746293 | validation: 0.0040774986369727095]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060415643988830465		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.0060415643988830465 | validation: 0.006345718184520827]
	TIME [epoch: 25.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005184527581862724		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.005184527581862724 | validation: 0.004789120032893635]
	TIME [epoch: 25.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004689223164669524		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.004689223164669524 | validation: 0.004664495917024197]
	TIME [epoch: 25.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004912838077646375		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.004912838077646375 | validation: 0.005732884553825391]
	TIME [epoch: 25.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006377525882987954		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.006377525882987954 | validation: 0.007812377794935307]
	TIME [epoch: 25.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006902746774648849		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.006902746774648849 | validation: 0.0068980284482771485]
	TIME [epoch: 25.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004306842640836332		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.004306842640836332 | validation: 0.00467205309246275]
	TIME [epoch: 25.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004407609050817052		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.004407609050817052 | validation: 0.005721106660741604]
	TIME [epoch: 25.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073530471917842375		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0073530471917842375 | validation: 0.013326988794810438]
	TIME [epoch: 25.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005809941981452356		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.005809941981452356 | validation: 0.0044768706617034845]
	TIME [epoch: 25.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005502924443538441		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.005502924443538441 | validation: 0.005239992293065165]
	TIME [epoch: 25.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005693127134786629		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.005693127134786629 | validation: 0.005603521191973048]
	TIME [epoch: 25.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004994191712131111		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.004994191712131111 | validation: 0.005127333285928671]
	TIME [epoch: 25.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005259499264583975		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.005259499264583975 | validation: 0.005923231748971416]
	TIME [epoch: 25.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005278578080180339		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.005278578080180339 | validation: 0.007141370881146779]
	TIME [epoch: 25.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00571155657782026		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.00571155657782026 | validation: 0.006215080884102817]
	TIME [epoch: 25.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005318645012945548		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.005318645012945548 | validation: 0.006131312386554823]
	TIME [epoch: 25.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00508200598947329		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.00508200598947329 | validation: 0.0051167670436653035]
	TIME [epoch: 25.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006126885922749577		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.006126885922749577 | validation: 0.010154609064679502]
	TIME [epoch: 25.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006003696929888096		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.006003696929888096 | validation: 0.004880073046341708]
	TIME [epoch: 25.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003943561947725732		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.003943561947725732 | validation: 0.0045895356690438364]
	TIME [epoch: 25.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048596581950011725		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0048596581950011725 | validation: 0.006316503196738038]
	TIME [epoch: 25.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006003438268696242		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.006003438268696242 | validation: 0.005821782210185875]
	TIME [epoch: 25.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005466981594046348		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.005466981594046348 | validation: 0.00597317818399218]
	TIME [epoch: 25.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004365920917544191		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.004365920917544191 | validation: 0.005777718530813667]
	TIME [epoch: 25.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00587033819333171		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.00587033819333171 | validation: 0.004750357629415938]
	TIME [epoch: 25.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004633738436113217		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.004633738436113217 | validation: 0.004662901403568759]
	TIME [epoch: 25.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059797313832492685		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.0059797313832492685 | validation: 0.017545939514472447]
	TIME [epoch: 25.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007934607324391036		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.007934607324391036 | validation: 0.00712636091038241]
	TIME [epoch: 25.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005195257139787437		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.005195257139787437 | validation: 0.004977456969893938]
	TIME [epoch: 25.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004855925571947631		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.004855925571947631 | validation: 0.005454747361140801]
	TIME [epoch: 25.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004460474837970897		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.004460474837970897 | validation: 0.006909723342322323]
	TIME [epoch: 25.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00685254807081291		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.00685254807081291 | validation: 0.006062466658889943]
	TIME [epoch: 25.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045112249552804575		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0045112249552804575 | validation: 0.005346958747827575]
	TIME [epoch: 25.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051609366540646615		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0051609366540646615 | validation: 0.0038732373319505155]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_940.pth
	Model improved!!!
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00572182305532461		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.00572182305532461 | validation: 0.005398412161621279]
	TIME [epoch: 25.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004582229774968815		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.004582229774968815 | validation: 0.00751891637473512]
	TIME [epoch: 25.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005134626113993873		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.005134626113993873 | validation: 0.008434550515078972]
	TIME [epoch: 25.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004954449750803788		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.004954449750803788 | validation: 0.005869828637667215]
	TIME [epoch: 25.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005068254776325921		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.005068254776325921 | validation: 0.011442218757669041]
	TIME [epoch: 25.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008094816808027858		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.008094816808027858 | validation: 0.00920381843593793]
	TIME [epoch: 25.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005391694097465002		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.005391694097465002 | validation: 0.003959428189594178]
	TIME [epoch: 25.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003977830197463191		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.003977830197463191 | validation: 0.004501712126192916]
	TIME [epoch: 25.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043578828654154845		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0043578828654154845 | validation: 0.006191734601773078]
	TIME [epoch: 25.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005724561895847268		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.005724561895847268 | validation: 0.006939072736549007]
	TIME [epoch: 25.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004662454556375461		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.004662454556375461 | validation: 0.004441292673450876]
	TIME [epoch: 25.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00518773137790448		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.00518773137790448 | validation: 0.004811034391935139]
	TIME [epoch: 25.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004095548201008803		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.004095548201008803 | validation: 0.0049542742161890335]
	TIME [epoch: 25.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004038132530029626		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.004038132530029626 | validation: 0.006929596028524564]
	TIME [epoch: 25.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005121698402187117		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.005121698402187117 | validation: 0.0068268852490864194]
	TIME [epoch: 25.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006458545178441009		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.006458545178441009 | validation: 0.0058763721813720644]
	TIME [epoch: 25.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004830971696675689		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.004830971696675689 | validation: 0.004388055907025923]
	TIME [epoch: 25.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004304700334016947		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.004304700334016947 | validation: 0.006039872612628806]
	TIME [epoch: 25.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00436139257378538		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.00436139257378538 | validation: 0.004278889068090407]
	TIME [epoch: 25.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004703284806521825		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.004703284806521825 | validation: 0.005845317862364367]
	TIME [epoch: 25.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004095015088638434		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.004095015088638434 | validation: 0.004770567471812906]
	TIME [epoch: 25.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056302937617288855		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.0056302937617288855 | validation: 0.0053326019861569636]
	TIME [epoch: 25.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004173237149025294		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.004173237149025294 | validation: 0.00521023639703286]
	TIME [epoch: 25.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003971930979642169		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.003971930979642169 | validation: 0.004656468520240265]
	TIME [epoch: 25.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071574376498754625		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.0071574376498754625 | validation: 0.00787960519295752]
	TIME [epoch: 25.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054732769545063325		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.0054732769545063325 | validation: 0.004771868314172988]
	TIME [epoch: 25.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00482097042599238		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.00482097042599238 | validation: 0.003306846384352853]
	TIME [epoch: 25.6 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043313685992796185		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0043313685992796185 | validation: 0.004811277433901917]
	TIME [epoch: 25.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042604929205855985		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0042604929205855985 | validation: 0.0044022670036280645]
	TIME [epoch: 25.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005793666543213969		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.005793666543213969 | validation: 0.0064536328264197945]
	TIME [epoch: 25.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004691259443982677		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.004691259443982677 | validation: 0.005301905530362359]
	TIME [epoch: 25.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047450046367920225		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.0047450046367920225 | validation: 0.004486022561152706]
	TIME [epoch: 25.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004853146142484866		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.004853146142484866 | validation: 0.005160575499523945]
	TIME [epoch: 25.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004251336411079814		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.004251336411079814 | validation: 0.004210669204343714]
	TIME [epoch: 25.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041939701844740255		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.0041939701844740255 | validation: 0.004470573133131725]
	TIME [epoch: 25.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005617265033067214		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.005617265033067214 | validation: 0.005418995553794369]
	TIME [epoch: 25.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005461353534572315		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.005461353534572315 | validation: 0.005158285597102023]
	TIME [epoch: 25.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004702112499856091		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.004702112499856091 | validation: 0.004357649717702311]
	TIME [epoch: 25.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004620955905186262		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.004620955905186262 | validation: 0.005350452444181114]
	TIME [epoch: 25.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004929515154472761		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.004929515154472761 | validation: 0.004608961824123297]
	TIME [epoch: 25.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004394705840402876		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.004394705840402876 | validation: 0.005284786000261707]
	TIME [epoch: 25.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004223528309432024		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.004223528309432024 | validation: 0.005127654831798641]
	TIME [epoch: 25.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005684466167388499		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.005684466167388499 | validation: 0.005485354512923568]
	TIME [epoch: 25.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051733669222927435		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.0051733669222927435 | validation: 0.005331223416396948]
	TIME [epoch: 25.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043352471331467015		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0043352471331467015 | validation: 0.007044472647807567]
	TIME [epoch: 25.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005102597290981335		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.005102597290981335 | validation: 0.004266199155070648]
	TIME [epoch: 25.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004051141983660783		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.004051141983660783 | validation: 0.004069782192279094]
	TIME [epoch: 25.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003887046198442546		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.003887046198442546 | validation: 0.003991081280472728]
	TIME [epoch: 25.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005291001048021484		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.005291001048021484 | validation: 0.004876269182319116]
	TIME [epoch: 25.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004709765437897344		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.004709765437897344 | validation: 0.004502849774903719]
	TIME [epoch: 25.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040767141962863036		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0040767141962863036 | validation: 0.004592264506919149]
	TIME [epoch: 25.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004424449460632475		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.004424449460632475 | validation: 0.006177026051740976]
	TIME [epoch: 25.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043374478759899025		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0043374478759899025 | validation: 0.005166576734852496]
	TIME [epoch: 25.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00427961691340072		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.00427961691340072 | validation: 0.004425048454607551]
	TIME [epoch: 25.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038910256807873967		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.0038910256807873967 | validation: 0.005005641212162624]
	TIME [epoch: 25.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038883565838516675		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.0038883565838516675 | validation: 0.005033841255291269]
	TIME [epoch: 25.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004399579759804045		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.004399579759804045 | validation: 0.006797089397845773]
	TIME [epoch: 25.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602043863852608		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.004602043863852608 | validation: 0.005675087996617329]
	TIME [epoch: 25.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004021996570007678		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.004021996570007678 | validation: 0.005033891248354668]
	TIME [epoch: 25.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044703614640926904		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.0044703614640926904 | validation: 0.004932952331490615]
	TIME [epoch: 25.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004541515472364748		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.004541515472364748 | validation: 0.0057222375138199204]
	TIME [epoch: 403 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036671523109503223		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.0036671523109503223 | validation: 0.003987579702523466]
	TIME [epoch: 54.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004838996940404663		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.004838996940404663 | validation: 0.0054497141927737445]
	TIME [epoch: 54.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004559119205982352		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.004559119205982352 | validation: 0.00606587034177543]
	TIME [epoch: 54.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004852441130371956		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.004852441130371956 | validation: 0.005031352672282696]
	TIME [epoch: 54.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004732148291138353		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.004732148291138353 | validation: 0.006219036439046198]
	TIME [epoch: 54.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004259494621717476		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.004259494621717476 | validation: 0.0040422993805300615]
	TIME [epoch: 54.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004365471656114224		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.004365471656114224 | validation: 0.005868164406282135]
	TIME [epoch: 54.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003888168200058692		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.003888168200058692 | validation: 0.004048839743789923]
	TIME [epoch: 54.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004236133202451778		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.004236133202451778 | validation: 0.006596287020292356]
	TIME [epoch: 54.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004229696367349371		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.004229696367349371 | validation: 0.004197327979186204]
	TIME [epoch: 54.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004120427534490727		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.004120427534490727 | validation: 0.006065847181163491]
	TIME [epoch: 54.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004671620999018569		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.004671620999018569 | validation: 0.005642276327610137]
	TIME [epoch: 54.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037934333541580607		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0037934333541580607 | validation: 0.005250751577155308]
	TIME [epoch: 54.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045244179922273725		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.0045244179922273725 | validation: 0.004928042403120193]
	TIME [epoch: 54.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004275275392585341		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.004275275392585341 | validation: 0.004041199743641964]
	TIME [epoch: 54.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004071780335102812		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.004071780335102812 | validation: 0.003442700790231833]
	TIME [epoch: 54.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038227309594804915		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.0038227309594804915 | validation: 0.006246408211783441]
	TIME [epoch: 54.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004433228968152861		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.004433228968152861 | validation: 0.0054319894870029]
	TIME [epoch: 54.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00438301308170545		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.00438301308170545 | validation: 0.0050856498768550515]
	TIME [epoch: 54.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004146866347678101		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.004146866347678101 | validation: 0.004725762316265489]
	TIME [epoch: 54.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004082731501467976		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.004082731501467976 | validation: 0.0034926646100625017]
	TIME [epoch: 54.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042964258175761634		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.0042964258175761634 | validation: 0.004208522408981013]
	TIME [epoch: 54.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037013311299023905		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.0037013311299023905 | validation: 0.005734210932812837]
	TIME [epoch: 54.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004126625007766578		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.004126625007766578 | validation: 0.005014311882212356]
	TIME [epoch: 54.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004078888453516103		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.004078888453516103 | validation: 0.006186468293949811]
	TIME [epoch: 54.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004344673622696305		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.004344673622696305 | validation: 0.005595000889770139]
	TIME [epoch: 54.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004265044774019048		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.004265044774019048 | validation: 0.006688369855959355]
	TIME [epoch: 54.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004598681759682072		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.004598681759682072 | validation: 0.004460989657193332]
	TIME [epoch: 54.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004208128490804217		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.004208128490804217 | validation: 0.005679283254071028]
	TIME [epoch: 54.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003342505499987758		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.003342505499987758 | validation: 0.004673515293369791]
	TIME [epoch: 54.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004170206926762045		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.004170206926762045 | validation: 0.006149658837781595]
	TIME [epoch: 54.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045660836758890985		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.0045660836758890985 | validation: 0.006917446631375498]
	TIME [epoch: 54.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00384538215825197		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.00384538215825197 | validation: 0.004919369083062734]
	TIME [epoch: 54.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003872669437462877		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.003872669437462877 | validation: 0.004304868209041057]
	TIME [epoch: 54.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003515072951417853		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.003515072951417853 | validation: 0.005408003895313653]
	TIME [epoch: 54.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048917608141615925		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.0048917608141615925 | validation: 0.003993703735557928]
	TIME [epoch: 54.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004417772856065727		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.004417772856065727 | validation: 0.007022289210294288]
	TIME [epoch: 54.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004202968914290472		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.004202968914290472 | validation: 0.0044606757772166425]
	TIME [epoch: 54.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039996703393840585		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0039996703393840585 | validation: 0.005340764182293304]
	TIME [epoch: 54.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004207167758518325		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.004207167758518325 | validation: 0.005263138505137476]
	TIME [epoch: 54.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004338471748076599		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.004338471748076599 | validation: 0.005404937375868455]
	TIME [epoch: 54.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004539397660571125		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.004539397660571125 | validation: 0.005149633015056431]
	TIME [epoch: 54.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003776481904946419		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.003776481904946419 | validation: 0.0042710937157401235]
	TIME [epoch: 54.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037590266212804307		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0037590266212804307 | validation: 0.00583426610745654]
	TIME [epoch: 54.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004925407388787946		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.004925407388787946 | validation: 0.00576671693340464]
	TIME [epoch: 54.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004137416244299332		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.004137416244299332 | validation: 0.004781677919284166]
	TIME [epoch: 54.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004334397012437335		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.004334397012437335 | validation: 0.00422464440583117]
	TIME [epoch: 54.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003520459257660857		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.003520459257660857 | validation: 0.0043214197406151724]
	TIME [epoch: 54.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003665716126151153		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.003665716126151153 | validation: 0.0052672055039442835]
	TIME [epoch: 54.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004529553539909324		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.004529553539909324 | validation: 0.004167212530000481]
	TIME [epoch: 54.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004361166733335841		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.004361166733335841 | validation: 0.004518888477622919]
	TIME [epoch: 54.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004088092981192758		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.004088092981192758 | validation: 0.004239665367213729]
	TIME [epoch: 54.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036917807497355345		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0036917807497355345 | validation: 0.00532308626744756]
	TIME [epoch: 54.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004357530838114582		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.004357530838114582 | validation: 0.006751487741071433]
	TIME [epoch: 54.2 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004165249512294593		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.004165249512294593 | validation: 0.004109972735660678]
	TIME [epoch: 54.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003856639771703854		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.003856639771703854 | validation: 0.0050366662688625]
	TIME [epoch: 54.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003606074339320618		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.003606074339320618 | validation: 0.004064103798652538]
	TIME [epoch: 54.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038032741294159646		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0038032741294159646 | validation: 0.003702150629727624]
	TIME [epoch: 54.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00398475923585221		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.00398475923585221 | validation: 0.0039195889235590755]
	TIME [epoch: 54.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004287879682783521		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.004287879682783521 | validation: 0.005662389297619513]
	TIME [epoch: 54.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004017566009189495		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.004017566009189495 | validation: 0.002949551262725644]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_1062.pth
	Model improved!!!
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003967130427621879		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.003967130427621879 | validation: 0.00626398472277007]
	TIME [epoch: 54.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003925520097698934		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.003925520097698934 | validation: 0.0033948453999775295]
	TIME [epoch: 54.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003750250108108675		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.003750250108108675 | validation: 0.0042612140114577784]
	TIME [epoch: 54.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004171560956439008		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.004171560956439008 | validation: 0.0056808689203459405]
	TIME [epoch: 54.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044991139968125204		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0044991139968125204 | validation: 0.003453298472516823]
	TIME [epoch: 54.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036762744889392565		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0036762744889392565 | validation: 0.004078134414982794]
	TIME [epoch: 54.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004644089867996399		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.004644089867996399 | validation: 0.004538293626387111]
	TIME [epoch: 54.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039663623708689555		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0039663623708689555 | validation: 0.004324045524034383]
	TIME [epoch: 54.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033999420847320118		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0033999420847320118 | validation: 0.004272004360574865]
	TIME [epoch: 54.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004386116816500866		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.004386116816500866 | validation: 0.0032384014920065884]
	TIME [epoch: 54.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003635372888127629		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.003635372888127629 | validation: 0.0038857836135808535]
	TIME [epoch: 54.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003843515212259386		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.003843515212259386 | validation: 0.004127679947050806]
	TIME [epoch: 54.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035876321054476956		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0035876321054476956 | validation: 0.005600274352217883]
	TIME [epoch: 54.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003680679590578403		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.003680679590578403 | validation: 0.007626300212171108]
	TIME [epoch: 54.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003977429808405866		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.003977429808405866 | validation: 0.004230515593416713]
	TIME [epoch: 54.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004050465533130084		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.004050465533130084 | validation: 0.004069395828562044]
	TIME [epoch: 54.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038945914680169214		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0038945914680169214 | validation: 0.006077060467455175]
	TIME [epoch: 54.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004292449376079413		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.004292449376079413 | validation: 0.004190417646291249]
	TIME [epoch: 54.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003514331955088622		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.003514331955088622 | validation: 0.005166448114487403]
	TIME [epoch: 54.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036143111023618432		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.0036143111023618432 | validation: 0.004388722892389961]
	TIME [epoch: 54.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003283370913050725		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.003283370913050725 | validation: 0.004211629583222997]
	TIME [epoch: 54.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035567606526703457		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.0035567606526703457 | validation: 0.006990021047392223]
	TIME [epoch: 54.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005053709251829803		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.005053709251829803 | validation: 0.0048662186503419195]
	TIME [epoch: 54.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003754504620482415		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.003754504620482415 | validation: 0.005395602555163158]
	TIME [epoch: 54.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034105051501154273		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0034105051501154273 | validation: 0.003445487384484686]
	TIME [epoch: 54.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032845679097286175		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0032845679097286175 | validation: 0.0048804364892639646]
	TIME [epoch: 54.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003855570387141776		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.003855570387141776 | validation: 0.0038832925226680998]
	TIME [epoch: 54.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035352974060570063		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.0035352974060570063 | validation: 0.0038209369621962958]
	TIME [epoch: 54.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003846103910751394		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.003846103910751394 | validation: 0.0052597832059435665]
	TIME [epoch: 54.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003822558870757187		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.003822558870757187 | validation: 0.00468233579863541]
	TIME [epoch: 54.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036061015174276984		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0036061015174276984 | validation: 0.005685461465478787]
	TIME [epoch: 54.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003652808664719915		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.003652808664719915 | validation: 0.005193325101680903]
	TIME [epoch: 54.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003847857758145063		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.003847857758145063 | validation: 0.004067045881886864]
	TIME [epoch: 54.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003518660060987636		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.003518660060987636 | validation: 0.005572075082175715]
	TIME [epoch: 54.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039036103161988966		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0039036103161988966 | validation: 0.004683877285290368]
	TIME [epoch: 54.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003536190261944099		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.003536190261944099 | validation: 0.003846070475458446]
	TIME [epoch: 54.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036617746927464787		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0036617746927464787 | validation: 0.004500041464712813]
	TIME [epoch: 54.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038410574570528645		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0038410574570528645 | validation: 0.004646760074228057]
	TIME [epoch: 54.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00394013559538813		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.00394013559538813 | validation: 0.004411058054073137]
	TIME [epoch: 54.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004078016394793181		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.004078016394793181 | validation: 0.004058729895601305]
	TIME [epoch: 54.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004196187834203055		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.004196187834203055 | validation: 0.004013246073898908]
	TIME [epoch: 54.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036282661451311274		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0036282661451311274 | validation: 0.0039010123157943105]
	TIME [epoch: 54.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003327644977638809		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.003327644977638809 | validation: 0.004134145222828189]
	TIME [epoch: 54.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003687687665906449		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.003687687665906449 | validation: 0.005186400893471646]
	TIME [epoch: 54.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003606962120287648		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.003606962120287648 | validation: 0.0030353533411823755]
	TIME [epoch: 54.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031530501968866954		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0031530501968866954 | validation: 0.003740149740547213]
	TIME [epoch: 54.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042312912965348625		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0042312912965348625 | validation: 0.004158919471712734]
	TIME [epoch: 54.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033433085584077577		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.0033433085584077577 | validation: 0.00398073650466659]
	TIME [epoch: 54.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003978563171262241		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.003978563171262241 | validation: 0.004597094138345369]
	TIME [epoch: 54.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036106889411522		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0036106889411522 | validation: 0.004162896155698322]
	TIME [epoch: 54.2 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040925264195107195		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.0040925264195107195 | validation: 0.004752379006328264]
	TIME [epoch: 54.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004493359710067024		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.004493359710067024 | validation: 0.004524057514647644]
	TIME [epoch: 54.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003522619527142052		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.003522619527142052 | validation: 0.004005179918371885]
	TIME [epoch: 54.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004047171643024511		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.004047171643024511 | validation: 0.0037726850253816113]
	TIME [epoch: 54.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003909273277522483		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.003909273277522483 | validation: 0.0049521320146383154]
	TIME [epoch: 54.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036761675004652874		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.0036761675004652874 | validation: 0.0037374091480991975]
	TIME [epoch: 54.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003608477298091499		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.003608477298091499 | validation: 0.00807790170749658]
	TIME [epoch: 54.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006547127757709837		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.006547127757709837 | validation: 0.005015927932419899]
	TIME [epoch: 54.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033696524988072		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0033696524988072 | validation: 0.003776317607958631]
	TIME [epoch: 54.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003567931781328525		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.003567931781328525 | validation: 0.004273139513677854]
	TIME [epoch: 54.2 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035034317446704477		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0035034317446704477 | validation: 0.0031437420355088807]
	TIME [epoch: 54.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003584223119268971		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.003584223119268971 | validation: 0.00465037777358383]
	TIME [epoch: 54.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003225763123306113		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.003225763123306113 | validation: 0.005673745008771378]
	TIME [epoch: 54.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003682794866429278		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.003682794866429278 | validation: 0.004649990131872809]
	TIME [epoch: 54.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036979486315922563		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.0036979486315922563 | validation: 0.0042281945105241804]
	TIME [epoch: 54.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033708351119560546		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0033708351119560546 | validation: 0.004629281574184704]
	TIME [epoch: 54.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004156910511331812		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.004156910511331812 | validation: 0.0038833574724666587]
	TIME [epoch: 54.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003952347894252783		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.003952347894252783 | validation: 0.003757802714666334]
	TIME [epoch: 54.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039145493302883885		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.0039145493302883885 | validation: 0.00403818137807945]
	TIME [epoch: 54.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003497626825830911		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.003497626825830911 | validation: 0.005984434470112822]
	TIME [epoch: 54.2 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030908242286322		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0030908242286322 | validation: 0.00418269710050889]
	TIME [epoch: 54.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035340469510933507		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0035340469510933507 | validation: 0.004164532764623518]
	TIME [epoch: 54.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003544592134133523		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.003544592134133523 | validation: 0.00539429916369592]
	TIME [epoch: 54.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003465394065626208		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.003465394065626208 | validation: 0.004149677032608078]
	TIME [epoch: 54.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038664935337709267		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0038664935337709267 | validation: 0.004127740485309802]
	TIME [epoch: 54.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003887667048720834		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.003887667048720834 | validation: 0.0053785752148351774]
	TIME [epoch: 54.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004023810513806131		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.004023810513806131 | validation: 0.003696975251035649]
	TIME [epoch: 54.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003675371363807939		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.003675371363807939 | validation: 0.004643972516802866]
	TIME [epoch: 54.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003401128213921059		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.003401128213921059 | validation: 0.003964710134314909]
	TIME [epoch: 54.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004071381330498893		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.004071381330498893 | validation: 0.0036377982082295033]
	TIME [epoch: 54.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031562823882090637		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0031562823882090637 | validation: 0.00448445020194818]
	TIME [epoch: 54.2 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003592998825425059		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.003592998825425059 | validation: 0.004215870686560298]
	TIME [epoch: 54.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003342706191602738		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.003342706191602738 | validation: 0.0035793091023606474]
	TIME [epoch: 54.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003391324121959279		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.003391324121959279 | validation: 0.005262017235746158]
	TIME [epoch: 54.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003685543975246585		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.003685543975246585 | validation: 0.004535444972298968]
	TIME [epoch: 54.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036460295758810202		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0036460295758810202 | validation: 0.004126651587148228]
	TIME [epoch: 54.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00377380008824692		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.00377380008824692 | validation: 0.004994665182435054]
	TIME [epoch: 54.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003859036220336822		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.003859036220336822 | validation: 0.004526766315354713]
	TIME [epoch: 54.2 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003437619400443322		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.003437619400443322 | validation: 0.004150553969356019]
	TIME [epoch: 54.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036671437331556927		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.0036671437331556927 | validation: 0.00422509140184931]
	TIME [epoch: 54.2 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003239556952531336		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.003239556952531336 | validation: 0.003670551623693014]
	TIME [epoch: 54.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032162932808099745		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.0032162932808099745 | validation: 0.005008918161897101]
	TIME [epoch: 54.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035638633979285118		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.0035638633979285118 | validation: 0.003380802130585064]
	TIME [epoch: 54.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003243370353594192		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.003243370353594192 | validation: 0.004002061850508243]
	TIME [epoch: 54.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031903885536060735		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.0031903885536060735 | validation: 0.00561301787532131]
	TIME [epoch: 54.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00336040539024959		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.00336040539024959 | validation: 0.004045261425499876]
	TIME [epoch: 54.2 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002840158775705292		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.002840158775705292 | validation: 0.005040565831343335]
	TIME [epoch: 54.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038969309570784393		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.0038969309570784393 | validation: 0.0036979553099065667]
	TIME [epoch: 54.2 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003506348113888807		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.003506348113888807 | validation: 0.004477290221901899]
	TIME [epoch: 54.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003189503393029008		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.003189503393029008 | validation: 0.003765916077933383]
	TIME [epoch: 54.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033330013023780284		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.0033330013023780284 | validation: 0.00423709880651351]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_1_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_1_v_mmd1_1163.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 28060.290 seconds.
