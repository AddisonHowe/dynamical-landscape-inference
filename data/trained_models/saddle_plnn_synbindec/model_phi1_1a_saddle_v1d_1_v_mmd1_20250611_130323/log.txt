Args:
Namespace(name='model_phi1_1a_saddle_v1d_1_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1d_1/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1d_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.053917355835437775, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3640069146

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.601764370184296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.601764370184296 | validation: 5.697851429826546]
	TIME [epoch: 368 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.770000344190919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.770000344190919 | validation: 5.784227894993493]
	TIME [epoch: 6.12 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.418315809193685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.418315809193685 | validation: 5.40138899690065]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.973235370883998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.973235370883998 | validation: 4.310409031834761]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.298128694303006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.298128694303006 | validation: 3.5941425935400355]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.865886082224312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.865886082224312 | validation: 3.697409614793216]
	TIME [epoch: 6.09 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.647667783314643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.647667783314643 | validation: 3.3501396589386436]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4700484045334288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4700484045334288 | validation: 3.049857869292838]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2966715417400034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2966715417400034 | validation: 3.1188913312054742]
	TIME [epoch: 6.1 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257040410463577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.257040410463577 | validation: 3.0401707208925215]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210438673880922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.210438673880922 | validation: 2.9216022948055187]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.104288974289253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.104288974289253 | validation: 2.9133271296112158]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1631737915077514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1631737915077514 | validation: 2.8069241530063938]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0506302532091523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0506302532091523 | validation: 2.7984001943393477]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.053877423219524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.053877423219524 | validation: 2.679270771095795]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.973415748567994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.973415748567994 | validation: 2.7047400345341828]
	TIME [epoch: 6.1 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9407321122617396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9407321122617396 | validation: 2.630704298428938]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9139589703854476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9139589703854476 | validation: 2.668507302062504]
	TIME [epoch: 6.09 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9108891719111263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9108891719111263 | validation: 2.6331703028617897]
	TIME [epoch: 6.08 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8364671112402973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8364671112402973 | validation: 2.6292541300124883]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8579018492920762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8579018492920762 | validation: 2.5828076313293504]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7768137558489547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7768137558489547 | validation: 2.524970167207954]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.814986288124153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.814986288124153 | validation: 2.4596921290367617]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.672727439877461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.672727439877461 | validation: 2.413561401313233]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.507920386996502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.507920386996502 | validation: 2.2355561447561265]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3445999154940047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3445999154940047 | validation: 1.9434708602268795]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.027667115054366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.027667115054366 | validation: 1.8267558803600399]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.981159108849071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.981159108849071 | validation: 2.3354290861542]
	TIME [epoch: 6.09 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.274227478815114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.274227478815114 | validation: 1.9164614181966182]
	TIME [epoch: 6.08 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9964878974646179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9964878974646179 | validation: 1.790053192939017]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9145721046815292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9145721046815292 | validation: 1.770806437736557]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9449713946192029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9449713946192029 | validation: 1.7888628339560584]
	TIME [epoch: 6.09 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0231356313355127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0231356313355127 | validation: 1.8349195325192178]
	TIME [epoch: 6.08 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.954986969562496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.954986969562496 | validation: 2.302652386825487]
	TIME [epoch: 6.08 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1863102262918472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1863102262918472 | validation: 1.627151585053698]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.932765039759662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.932765039759662 | validation: 1.7916105767595543]
	TIME [epoch: 6.1 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7617338109821592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7617338109821592 | validation: 1.6727676556350928]
	TIME [epoch: 6.07 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8498371633572834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8498371633572834 | validation: 1.5549242774747252]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6541948397777386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6541948397777386 | validation: 1.4849662669674446]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6161256129689132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6161256129689132 | validation: 1.4194686737846043]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4324949434432137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4324949434432137 | validation: 1.3062881901830685]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.440080066371694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.440080066371694 | validation: 1.5459334892932657]
	TIME [epoch: 6.09 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6302594159236823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6302594159236823 | validation: 1.8609966474163675]
	TIME [epoch: 6.08 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.542564948815078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.542564948815078 | validation: 0.9754657544461955]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2655803557681278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2655803557681278 | validation: 0.9263723611069536]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1034723162535398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1034723162535398 | validation: 1.2991891925899144]
	TIME [epoch: 6.1 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1360655334193155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1360655334193155 | validation: 1.1761356736954034]
	TIME [epoch: 6.08 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9710096215334351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9710096215334351 | validation: 1.0503432171414422]
	TIME [epoch: 6.08 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6341047360937404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6341047360937404 | validation: 0.9000034374770308]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8435525659239725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8435525659239725 | validation: 0.8142419650176232]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1125128486300142		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.1125128486300142 | validation: 0.8886441719935417]
	TIME [epoch: 6.09 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8900034161613846		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.8900034161613846 | validation: 0.605816061539957]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8280559827188441		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.8280559827188441 | validation: 1.235899642130952]
	TIME [epoch: 6.09 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8144281222931419		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.8144281222931419 | validation: 0.6472573482539457]
	TIME [epoch: 6.08 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7957151275775445		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.7957151275775445 | validation: 0.7104390026440912]
	TIME [epoch: 6.09 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7447993489869779		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.7447993489869779 | validation: 0.9279757148728449]
	TIME [epoch: 6.08 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6890107357281858		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.6890107357281858 | validation: 0.8334925424376787]
	TIME [epoch: 6.09 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6735247970187385		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.6735247970187385 | validation: 0.5774959173032357]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7824734972713288		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.7824734972713288 | validation: 0.6872415066041616]
	TIME [epoch: 6.09 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8262008672479725		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.8262008672479725 | validation: 0.6197723911261582]
	TIME [epoch: 6.09 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5985168439191649		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5985168439191649 | validation: 0.5073859130689082]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6977097987729965		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.6977097987729965 | validation: 0.5054634726944124]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5784213334354746		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.5784213334354746 | validation: 0.7510965857180698]
	TIME [epoch: 6.09 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5532359973817834		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5532359973817834 | validation: 0.40303729795737686]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7211105053440086		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.7211105053440086 | validation: 0.635521732080297]
	TIME [epoch: 6.09 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1142147928607595		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 1.1142147928607595 | validation: 0.8553840452470718]
	TIME [epoch: 6.07 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9495111443866165		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.9495111443866165 | validation: 0.5277591488069293]
	TIME [epoch: 6.08 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5670238409846042		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5670238409846042 | validation: 0.5328280858112798]
	TIME [epoch: 6.08 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6618590824596464		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.6618590824596464 | validation: 0.8372626670759596]
	TIME [epoch: 6.09 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6822722483195988		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.6822722483195988 | validation: 0.47752153180867785]
	TIME [epoch: 6.08 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5013516025025065		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.5013516025025065 | validation: 0.5216947539640491]
	TIME [epoch: 6.08 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6700196262505665		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.6700196262505665 | validation: 0.6041060495855717]
	TIME [epoch: 6.09 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5618971443355273		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5618971443355273 | validation: 0.5021160451489645]
	TIME [epoch: 6.08 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6055940030847987		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.6055940030847987 | validation: 0.5612274511017006]
	TIME [epoch: 6.08 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47891521278304994		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.47891521278304994 | validation: 0.5427082560618353]
	TIME [epoch: 6.08 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5356536639285963		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5356536639285963 | validation: 0.6699641094305095]
	TIME [epoch: 6.07 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6323333275512244		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.6323333275512244 | validation: 0.5914685390722138]
	TIME [epoch: 6.08 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5479371812421386		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5479371812421386 | validation: 0.6579756485441408]
	TIME [epoch: 6.08 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5761682562234065		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5761682562234065 | validation: 0.6329081385522135]
	TIME [epoch: 6.09 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5607237860004046		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.5607237860004046 | validation: 0.4276805087450378]
	TIME [epoch: 6.08 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5314523962661901		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5314523962661901 | validation: 0.8464889177019328]
	TIME [epoch: 6.09 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7506154878265869		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.7506154878265869 | validation: 0.5480840715441567]
	TIME [epoch: 6.08 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.450731475894843		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.450731475894843 | validation: 0.4663755993080827]
	TIME [epoch: 6.08 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.519334056647373		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.519334056647373 | validation: 0.4967179641710765]
	TIME [epoch: 6.08 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49053191337197244		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.49053191337197244 | validation: 0.49353505259440744]
	TIME [epoch: 6.07 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4302613222119713		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.4302613222119713 | validation: 0.44081441711109715]
	TIME [epoch: 6.08 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5904542648866687		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.5904542648866687 | validation: 0.5983183777930163]
	TIME [epoch: 6.08 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5999349874476269		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5999349874476269 | validation: 0.42585249234122724]
	TIME [epoch: 6.08 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4505332566294794		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.4505332566294794 | validation: 0.5913901618796402]
	TIME [epoch: 6.08 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4677947372732685		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.4677947372732685 | validation: 0.3010064409928278]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5372832998184571		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5372832998184571 | validation: 0.5255708430929794]
	TIME [epoch: 6.08 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4728525433818103		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.4728525433818103 | validation: 0.3777220831720556]
	TIME [epoch: 6.08 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41829161282672095		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.41829161282672095 | validation: 0.3824291368377929]
	TIME [epoch: 6.08 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6464961185777605		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.6464961185777605 | validation: 0.42012634039201]
	TIME [epoch: 6.09 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3545480934454878		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.3545480934454878 | validation: 0.31922359216250396]
	TIME [epoch: 6.08 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5046256285340289		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5046256285340289 | validation: 0.3966962360991432]
	TIME [epoch: 6.09 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42052889514501646		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.42052889514501646 | validation: 0.42147806661163495]
	TIME [epoch: 6.08 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3232716479282849		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.3232716479282849 | validation: 0.4028628884466689]
	TIME [epoch: 6.08 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3769324165829716		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.3769324165829716 | validation: 0.5244313253039843]
	TIME [epoch: 6.09 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42372865436319024		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.42372865436319024 | validation: 0.5363537765661812]
	TIME [epoch: 6.08 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4800432156279385		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.4800432156279385 | validation: 0.5156109090010231]
	TIME [epoch: 6.08 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4433420511671798		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4433420511671798 | validation: 0.38414369697906803]
	TIME [epoch: 6.08 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42632011597097924		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.42632011597097924 | validation: 0.5322056661218773]
	TIME [epoch: 6.07 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338324577741945		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5338324577741945 | validation: 0.4004626039974446]
	TIME [epoch: 6.09 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32592798427774017		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.32592798427774017 | validation: 0.35190983701805645]
	TIME [epoch: 6.08 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42426489830373415		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.42426489830373415 | validation: 0.31033101038892963]
	TIME [epoch: 6.08 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42906438256391694		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.42906438256391694 | validation: 0.630965735350598]
	TIME [epoch: 6.08 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5557122957839686		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.5557122957839686 | validation: 0.41472044695646315]
	TIME [epoch: 6.08 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3225132319379555		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3225132319379555 | validation: 0.3563547527659533]
	TIME [epoch: 6.09 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3209298927126566		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.3209298927126566 | validation: 0.765766375133451]
	TIME [epoch: 6.08 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5868111591798437		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5868111591798437 | validation: 0.26604784626987854]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37328465334916455		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.37328465334916455 | validation: 0.32145691794376785]
	TIME [epoch: 6.09 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25574601599261715		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.25574601599261715 | validation: 0.2749173967381261]
	TIME [epoch: 6.09 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35774133603660974		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.35774133603660974 | validation: 0.4549787677148118]
	TIME [epoch: 6.1 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5112574549717042		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.5112574549717042 | validation: 0.43906158877267953]
	TIME [epoch: 6.08 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5444987057799113		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.5444987057799113 | validation: 0.48412061286473684]
	TIME [epoch: 6.09 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4249287756004695		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.4249287756004695 | validation: 0.2559660525095855]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24410446681347991		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.24410446681347991 | validation: 0.30118186281096754]
	TIME [epoch: 6.1 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45445627727900373		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.45445627727900373 | validation: 0.43011564688860754]
	TIME [epoch: 6.09 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40819680298351224		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.40819680298351224 | validation: 0.25107631476106823]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3109676011882829		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.3109676011882829 | validation: 0.3801662920566869]
	TIME [epoch: 6.09 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39178336915495016		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.39178336915495016 | validation: 0.29204214409940366]
	TIME [epoch: 6.09 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34819890444308016		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.34819890444308016 | validation: 0.3874905294575858]
	TIME [epoch: 6.09 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3164751691868972		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.3164751691868972 | validation: 0.26650111392296294]
	TIME [epoch: 6.08 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32360934999388524		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.32360934999388524 | validation: 0.30317416330514124]
	TIME [epoch: 6.09 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42138347938536924		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.42138347938536924 | validation: 0.5699742392032972]
	TIME [epoch: 6.08 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3453160111041411		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3453160111041411 | validation: 0.3585803860836101]
	TIME [epoch: 6.08 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2632812205952539		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2632812205952539 | validation: 0.2899513709185455]
	TIME [epoch: 6.1 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942871199756888		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2942871199756888 | validation: 0.23714309943625628]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24459584071357632		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.24459584071357632 | validation: 0.2817903771757965]
	TIME [epoch: 6.09 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25758001570700856		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.25758001570700856 | validation: 0.2842867611638007]
	TIME [epoch: 6.08 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39022435543261813		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.39022435543261813 | validation: 0.4564019578034252]
	TIME [epoch: 6.08 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4368905737560516		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.4368905737560516 | validation: 0.32560037698635924]
	TIME [epoch: 6.09 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540229562616096		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.2540229562616096 | validation: 0.25723392069858586]
	TIME [epoch: 6.08 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21071257485539352		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.21071257485539352 | validation: 0.2040284500167316]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2743839921025867		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2743839921025867 | validation: 0.5512327664079664]
	TIME [epoch: 6.09 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292942994792737		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.292942994792737 | validation: 0.26451841712108726]
	TIME [epoch: 6.08 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23877932102208954		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.23877932102208954 | validation: 0.1808974533583853]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19722004104584184		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.19722004104584184 | validation: 0.5506467025012625]
	TIME [epoch: 6.09 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3680973719695592		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.3680973719695592 | validation: 0.40078513856205256]
	TIME [epoch: 6.08 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.262695363163991		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.262695363163991 | validation: 0.19666369179498158]
	TIME [epoch: 6.08 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24836724828612605		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.24836724828612605 | validation: 0.25243341908068806]
	TIME [epoch: 6.09 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25727847444462304		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.25727847444462304 | validation: 0.25123288189770676]
	TIME [epoch: 6.09 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2092402682644277		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2092402682644277 | validation: 0.27370433507427433]
	TIME [epoch: 6.08 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27634495516540014		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.27634495516540014 | validation: 0.16775788074358045]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2014669858832055		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.2014669858832055 | validation: 0.3198659689665253]
	TIME [epoch: 6.08 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26372326207438446		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.26372326207438446 | validation: 0.3685596985949964]
	TIME [epoch: 6.09 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.299531792863058		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.299531792863058 | validation: 0.24721752601820984]
	TIME [epoch: 6.07 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2276009290820735		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2276009290820735 | validation: 0.25064752857355543]
	TIME [epoch: 6.08 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17412300099831843		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.17412300099831843 | validation: 0.23511070422345468]
	TIME [epoch: 6.08 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25389076062303956		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.25389076062303956 | validation: 0.20717712727913223]
	TIME [epoch: 6.07 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24682346004948452		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.24682346004948452 | validation: 0.45078044572427417]
	TIME [epoch: 6.09 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25935294646619117		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.25935294646619117 | validation: 0.25151436935803034]
	TIME [epoch: 6.08 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25455339912800273		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.25455339912800273 | validation: 0.3736447095576765]
	TIME [epoch: 6.08 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25144042442760534		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.25144042442760534 | validation: 0.14593523814109022]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16904275173263378		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.16904275173263378 | validation: 0.18842819535614946]
	TIME [epoch: 6.08 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26236179563091183		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.26236179563091183 | validation: 0.3123568703539654]
	TIME [epoch: 6.09 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1836441636068038		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.1836441636068038 | validation: 0.18348023924762985]
	TIME [epoch: 6.08 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26111575803123366		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.26111575803123366 | validation: 0.23205998236414388]
	TIME [epoch: 6.08 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.189954794214722		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.189954794214722 | validation: 0.1460169162370134]
	TIME [epoch: 6.08 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18302296690121178		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.18302296690121178 | validation: 0.3599501346821127]
	TIME [epoch: 6.08 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24928496344064927		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.24928496344064927 | validation: 0.2328619442314055]
	TIME [epoch: 6.09 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.161086987469166		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.161086987469166 | validation: 0.2466769255773004]
	TIME [epoch: 6.08 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17698849415729687		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.17698849415729687 | validation: 0.23680281269463177]
	TIME [epoch: 6.08 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17734637571126818		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.17734637571126818 | validation: 0.23340805790259456]
	TIME [epoch: 6.08 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3204258642922222		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.3204258642922222 | validation: 0.23645024145422794]
	TIME [epoch: 6.08 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18216765384071756		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.18216765384071756 | validation: 0.19608922544583607]
	TIME [epoch: 6.09 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1549738468586876		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.1549738468586876 | validation: 0.15812844383124586]
	TIME [epoch: 6.08 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26085991693000116		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.26085991693000116 | validation: 0.21159066082912548]
	TIME [epoch: 6.08 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1491138460292163		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1491138460292163 | validation: 0.20515658834628653]
	TIME [epoch: 6.08 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20612295638644995		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.20612295638644995 | validation: 0.1547855945476212]
	TIME [epoch: 6.08 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12313980418407706		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.12313980418407706 | validation: 0.26473273032658007]
	TIME [epoch: 6.09 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4687211034426202		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.4687211034426202 | validation: 1.0754968577010566]
	TIME [epoch: 6.08 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7242617825350433		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.7242617825350433 | validation: 0.2948735370798983]
	TIME [epoch: 6.09 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25310012840228685		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.25310012840228685 | validation: 0.1469607654532774]
	TIME [epoch: 6.08 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12530870285451912		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.12530870285451912 | validation: 0.11583348279995645]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.157425081634069		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.157425081634069 | validation: 0.1535708038955843]
	TIME [epoch: 6.09 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3531767578534692		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.3531767578534692 | validation: 0.12135844298298984]
	TIME [epoch: 6.08 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12066445927891295		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.12066445927891295 | validation: 0.10474259722886281]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1547197082961303		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1547197082961303 | validation: 0.2229794320866852]
	TIME [epoch: 6.08 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17550778795711125		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.17550778795711125 | validation: 0.12364598994027381]
	TIME [epoch: 6.09 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12374257725820825		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.12374257725820825 | validation: 0.17034814907580925]
	TIME [epoch: 6.09 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17297403010165668		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.17297403010165668 | validation: 0.20353379782501752]
	TIME [epoch: 6.08 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20187072927032376		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.20187072927032376 | validation: 0.11842094340422278]
	TIME [epoch: 6.09 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13553051512089767		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.13553051512089767 | validation: 0.1570525296901194]
	TIME [epoch: 6.09 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18711551825189576		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.18711551825189576 | validation: 0.15598466297137054]
	TIME [epoch: 6.09 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1467337128403672		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.1467337128403672 | validation: 0.1621656260715831]
	TIME [epoch: 6.08 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15530063428575017		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.15530063428575017 | validation: 0.37494956324921913]
	TIME [epoch: 6.08 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3243326977130273		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.3243326977130273 | validation: 0.2630634374240067]
	TIME [epoch: 6.08 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16606727405051797		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.16606727405051797 | validation: 0.17601827686106714]
	TIME [epoch: 6.08 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1249229588547984		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.1249229588547984 | validation: 0.1532929366910456]
	TIME [epoch: 6.09 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15320898042079212		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.15320898042079212 | validation: 0.1925704231863088]
	TIME [epoch: 6.08 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19215078421845133		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.19215078421845133 | validation: 0.1490346345773694]
	TIME [epoch: 6.08 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17722262176549516		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.17722262176549516 | validation: 0.13373538286070366]
	TIME [epoch: 6.09 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12028303200235257		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.12028303200235257 | validation: 0.1588930337348663]
	TIME [epoch: 6.08 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1740264728356805		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1740264728356805 | validation: 0.1438818649679438]
	TIME [epoch: 6.09 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10409937676572564		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.10409937676572564 | validation: 0.1934134866902698]
	TIME [epoch: 6.08 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1388667958338837		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.1388667958338837 | validation: 0.20302997083001018]
	TIME [epoch: 6.08 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16988750448146878		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.16988750448146878 | validation: 0.24233638158986853]
	TIME [epoch: 6.09 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14717480446259684		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.14717480446259684 | validation: 0.1381483437543592]
	TIME [epoch: 6.08 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11003324002979606		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.11003324002979606 | validation: 0.16637196310483843]
	TIME [epoch: 391 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1593910629270966		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.1593910629270966 | validation: 0.15184279868250497]
	TIME [epoch: 12 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13691565356700652		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.13691565356700652 | validation: 0.15559024149131856]
	TIME [epoch: 12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1699571300844896		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.1699571300844896 | validation: 0.22231774801412366]
	TIME [epoch: 12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11905693649683793		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.11905693649683793 | validation: 0.09989464443781759]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09909331817089717		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.09909331817089717 | validation: 0.12423212998884028]
	TIME [epoch: 12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12722301858603652		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.12722301858603652 | validation: 0.18260811213510553]
	TIME [epoch: 12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17954171531992857		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.17954171531992857 | validation: 0.11575877374024861]
	TIME [epoch: 12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0999634145147562		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.0999634145147562 | validation: 0.1563776262775433]
	TIME [epoch: 12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14552858686018197		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.14552858686018197 | validation: 0.16429195722106033]
	TIME [epoch: 12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17465858883218752		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.17465858883218752 | validation: 0.33793592890743296]
	TIME [epoch: 12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1669514013898409		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1669514013898409 | validation: 0.09761464941596566]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10033103169847807		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.10033103169847807 | validation: 0.12279854812038478]
	TIME [epoch: 12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1538610715605438		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.1538610715605438 | validation: 0.12688018364697617]
	TIME [epoch: 12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09615955160258718		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.09615955160258718 | validation: 0.08799459300444401]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10703456064990857		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.10703456064990857 | validation: 0.19134485035932697]
	TIME [epoch: 12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14354032626299945		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.14354032626299945 | validation: 0.10735734873455216]
	TIME [epoch: 12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09385354576139338		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.09385354576139338 | validation: 0.20440756673495275]
	TIME [epoch: 12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1567676979853426		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.1567676979853426 | validation: 0.14181426649769627]
	TIME [epoch: 12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10918435145687681		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.10918435145687681 | validation: 0.10378890271990997]
	TIME [epoch: 12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08592812744034392		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.08592812744034392 | validation: 0.11599897993214911]
	TIME [epoch: 12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1354102320242565		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.1354102320242565 | validation: 0.21109570370092828]
	TIME [epoch: 12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15407058110759247		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.15407058110759247 | validation: 0.11009055884791152]
	TIME [epoch: 12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17335677153399204		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.17335677153399204 | validation: 0.14657938585993852]
	TIME [epoch: 12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11673527638261434		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.11673527638261434 | validation: 0.0868503805475902]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10357371841110147		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.10357371841110147 | validation: 0.1601896423427004]
	TIME [epoch: 12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12226374792634694		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.12226374792634694 | validation: 0.09222142840744788]
	TIME [epoch: 12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11170347773557548		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.11170347773557548 | validation: 0.12399624148094582]
	TIME [epoch: 12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1391601791536886		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.1391601791536886 | validation: 0.13868286435623545]
	TIME [epoch: 12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13321030381747423		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.13321030381747423 | validation: 0.140143822294036]
	TIME [epoch: 12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09700459431922992		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.09700459431922992 | validation: 0.1442437538732005]
	TIME [epoch: 12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13995682114016064		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.13995682114016064 | validation: 0.0927649314537197]
	TIME [epoch: 12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10585198912473505		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.10585198912473505 | validation: 0.08709572250177706]
	TIME [epoch: 12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09430536532903372		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.09430536532903372 | validation: 0.12542620533189178]
	TIME [epoch: 12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08196135617644637		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.08196135617644637 | validation: 0.12284144285810633]
	TIME [epoch: 12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1588461986255285		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.1588461986255285 | validation: 0.20551547593507988]
	TIME [epoch: 12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14825651817704863		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.14825651817704863 | validation: 0.12906020562065512]
	TIME [epoch: 12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10649702416638364		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.10649702416638364 | validation: 0.12246331310317166]
	TIME [epoch: 12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1127610290318321		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1127610290318321 | validation: 0.15068740260274266]
	TIME [epoch: 12.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08683886966502202		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.08683886966502202 | validation: 0.11384353304746475]
	TIME [epoch: 12 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1311549678035383		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1311549678035383 | validation: 0.1067712007112831]
	TIME [epoch: 12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09431353574412794		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.09431353574412794 | validation: 0.12338744633967937]
	TIME [epoch: 12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11143938286560223		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11143938286560223 | validation: 0.1315456616502125]
	TIME [epoch: 12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08009980237712575		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.08009980237712575 | validation: 0.11131864133333948]
	TIME [epoch: 12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11023532900509397		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.11023532900509397 | validation: 0.11321513395427635]
	TIME [epoch: 12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12694412447001166		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.12694412447001166 | validation: 0.07385353546442892]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06973602049305169		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.06973602049305169 | validation: 0.12351057021180546]
	TIME [epoch: 12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12194430133050055		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.12194430133050055 | validation: 0.16573972516050647]
	TIME [epoch: 12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12474550196617637		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.12474550196617637 | validation: 0.07714712730404759]
	TIME [epoch: 12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07436230293837928		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.07436230293837928 | validation: 0.15270669031721654]
	TIME [epoch: 12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11058474145616308		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.11058474145616308 | validation: 0.14065142678953851]
	TIME [epoch: 12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11111879057899159		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.11111879057899159 | validation: 0.11088336172959531]
	TIME [epoch: 12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08603786027751284		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.08603786027751284 | validation: 0.07920243726089882]
	TIME [epoch: 12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.108329264240234		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.108329264240234 | validation: 0.09587785367287048]
	TIME [epoch: 12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11327511411092996		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.11327511411092996 | validation: 0.09733245508999153]
	TIME [epoch: 12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07522721586682887		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.07522721586682887 | validation: 0.10691957538678629]
	TIME [epoch: 12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10532801198340577		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.10532801198340577 | validation: 0.06920176532811714]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07946441558021175		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.07946441558021175 | validation: 0.09713911345870803]
	TIME [epoch: 12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09273231269639368		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.09273231269639368 | validation: 0.11238998816866363]
	TIME [epoch: 12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11228534211757757		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.11228534211757757 | validation: 0.09683380938568528]
	TIME [epoch: 12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07957374556368517		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.07957374556368517 | validation: 0.10394752900901909]
	TIME [epoch: 12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10710657605964133		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.10710657605964133 | validation: 0.09353062581153869]
	TIME [epoch: 12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06492547937595436		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.06492547937595436 | validation: 0.1393162553215423]
	TIME [epoch: 12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11116026501208365		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.11116026501208365 | validation: 0.12050638796209098]
	TIME [epoch: 12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10743573756962974		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.10743573756962974 | validation: 0.1185017172179825]
	TIME [epoch: 12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08658646784220964		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.08658646784220964 | validation: 0.0787917123050055]
	TIME [epoch: 12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0767024082233892		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.0767024082233892 | validation: 0.09911622310318946]
	TIME [epoch: 12 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1006629376746554		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.1006629376746554 | validation: 0.13676399537665693]
	TIME [epoch: 12 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08196887212103247		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.08196887212103247 | validation: 0.07716384336292595]
	TIME [epoch: 12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08904723455414237		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.08904723455414237 | validation: 0.10545246816618895]
	TIME [epoch: 12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09738267049746437		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.09738267049746437 | validation: 0.07182985300968688]
	TIME [epoch: 12 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06336649792026876		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.06336649792026876 | validation: 0.11574725251958211]
	TIME [epoch: 12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10491709389209573		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.10491709389209573 | validation: 0.0904823015678459]
	TIME [epoch: 12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06971068657522725		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.06971068657522725 | validation: 0.07322001041286713]
	TIME [epoch: 12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08510553306911983		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.08510553306911983 | validation: 0.0735630487760157]
	TIME [epoch: 12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09567980958570643		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.09567980958570643 | validation: 0.07327116062288438]
	TIME [epoch: 12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07312248390122429		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.07312248390122429 | validation: 0.08418894685157874]
	TIME [epoch: 12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06169455548485575		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.06169455548485575 | validation: 0.14474204498516735]
	TIME [epoch: 12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11345211841164213		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11345211841164213 | validation: 0.09940522874476324]
	TIME [epoch: 12 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0773159441954811		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.0773159441954811 | validation: 0.10156912458386949]
	TIME [epoch: 12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09334531054281847		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.09334531054281847 | validation: 0.10142586953156502]
	TIME [epoch: 12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07235273348637841		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.07235273348637841 | validation: 0.10761071433786956]
	TIME [epoch: 12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07675643661759438		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.07675643661759438 | validation: 0.08375853314102011]
	TIME [epoch: 12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0803710103082004		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.0803710103082004 | validation: 0.09625835410526562]
	TIME [epoch: 12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0807853045857102		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.0807853045857102 | validation: 0.08233294163112878]
	TIME [epoch: 12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0745443971735157		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.0745443971735157 | validation: 0.11435265266819367]
	TIME [epoch: 12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10111887854502324		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.10111887854502324 | validation: 0.0852111649595851]
	TIME [epoch: 12 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06644512606118033		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.06644512606118033 | validation: 0.08034077814453644]
	TIME [epoch: 12 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06962605782118153		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.06962605782118153 | validation: 0.1821785158465254]
	TIME [epoch: 12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1342901206073798		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.1342901206073798 | validation: 0.12044360400531684]
	TIME [epoch: 12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08849754544097843		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.08849754544097843 | validation: 0.057702371062582136]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06486453990331736		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.06486453990331736 | validation: 0.1320972555740258]
	TIME [epoch: 12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08218023377065048		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.08218023377065048 | validation: 0.10682591610923789]
	TIME [epoch: 12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0954742541823692		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.0954742541823692 | validation: 0.07443585561235666]
	TIME [epoch: 12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06632667760102587		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.06632667760102587 | validation: 0.09185428884215213]
	TIME [epoch: 12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08092483352705349		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.08092483352705349 | validation: 0.09876616694323913]
	TIME [epoch: 12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08173413826224235		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.08173413826224235 | validation: 0.0645488634691452]
	TIME [epoch: 12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0628913242309059		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.0628913242309059 | validation: 0.07392043747414076]
	TIME [epoch: 12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0713805242599521		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.0713805242599521 | validation: 0.13203179125328834]
	TIME [epoch: 12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1238723756426518		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.1238723756426518 | validation: 0.06015603502382094]
	TIME [epoch: 12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05521740056202315		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.05521740056202315 | validation: 0.060576647712221765]
	TIME [epoch: 12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05611528915538507		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.05611528915538507 | validation: 0.07863850085114445]
	TIME [epoch: 12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06786892112963024		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.06786892112963024 | validation: 0.07405520112456096]
	TIME [epoch: 12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09474597124155575		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.09474597124155575 | validation: 0.35095310251805495]
	TIME [epoch: 12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23561195878646488		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.23561195878646488 | validation: 0.10175582689632603]
	TIME [epoch: 12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08394487766647137		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.08394487766647137 | validation: 0.0688290538919823]
	TIME [epoch: 12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0633301376681295		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.0633301376681295 | validation: 0.06325408256717059]
	TIME [epoch: 12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07224989303792637		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.07224989303792637 | validation: 0.07632697917702798]
	TIME [epoch: 12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07468261587907461		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.07468261587907461 | validation: 0.08326540714093658]
	TIME [epoch: 12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08059865754519996		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.08059865754519996 | validation: 0.08573882976965615]
	TIME [epoch: 12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061182059777908734		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.061182059777908734 | validation: 0.07201981168604527]
	TIME [epoch: 12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0684277847123943		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.0684277847123943 | validation: 0.047601855941679164]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048194301343287026		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.048194301343287026 | validation: 0.1313625328971041]
	TIME [epoch: 12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0990777794422694		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.0990777794422694 | validation: 0.08132516440331868]
	TIME [epoch: 12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06081170048018473		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.06081170048018473 | validation: 0.059416597902023346]
	TIME [epoch: 12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04930133672373391		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.04930133672373391 | validation: 0.08304918826110896]
	TIME [epoch: 12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08961330389616874		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.08961330389616874 | validation: 0.05621802554127943]
	TIME [epoch: 12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06686044452317585		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.06686044452317585 | validation: 0.06776362446622285]
	TIME [epoch: 12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07381027541383817		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.07381027541383817 | validation: 0.07337380305049421]
	TIME [epoch: 12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060954908022684655		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.060954908022684655 | validation: 0.07174983160255186]
	TIME [epoch: 12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06861034186365834		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.06861034186365834 | validation: 0.07890448112712811]
	TIME [epoch: 12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06887772627409618		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.06887772627409618 | validation: 0.058697494129854415]
	TIME [epoch: 12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06516195641710185		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.06516195641710185 | validation: 0.07857853636443626]
	TIME [epoch: 12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06379470038733312		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.06379470038733312 | validation: 0.07440469976965135]
	TIME [epoch: 12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06765561558001147		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.06765561558001147 | validation: 0.08010347585048452]
	TIME [epoch: 12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07837732164327968		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.07837732164327968 | validation: 0.08281055605705752]
	TIME [epoch: 12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052054066232406035		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.052054066232406035 | validation: 0.05510015689850635]
	TIME [epoch: 12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09185811324326372		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.09185811324326372 | validation: 0.13678735714984624]
	TIME [epoch: 12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07209951282316898		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.07209951282316898 | validation: 0.05049033084667061]
	TIME [epoch: 12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045314356456628055		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.045314356456628055 | validation: 0.0721489693807692]
	TIME [epoch: 12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08562390937970078		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.08562390937970078 | validation: 0.0747722582107245]
	TIME [epoch: 12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05339316094140842		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.05339316094140842 | validation: 0.09230341596833488]
	TIME [epoch: 12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06619294202393125		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.06619294202393125 | validation: 0.057913372742128835]
	TIME [epoch: 12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05964870462441152		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.05964870462441152 | validation: 0.06935814112055512]
	TIME [epoch: 12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06750493504964145		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.06750493504964145 | validation: 0.0589923078582986]
	TIME [epoch: 12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05413334351280383		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.05413334351280383 | validation: 0.08174659633466155]
	TIME [epoch: 12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05113997838567086		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.05113997838567086 | validation: 0.06671295904047148]
	TIME [epoch: 12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08468053409278653		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.08468053409278653 | validation: 0.08942325815210822]
	TIME [epoch: 12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05835653082434941		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.05835653082434941 | validation: 0.07387560078426733]
	TIME [epoch: 12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05979821703181287		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.05979821703181287 | validation: 0.05385468769278137]
	TIME [epoch: 12.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05245350421201432		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.05245350421201432 | validation: 0.0821119514612684]
	TIME [epoch: 12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0641118503249942		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.0641118503249942 | validation: 0.06492840855920358]
	TIME [epoch: 12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0653604470062948		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.0653604470062948 | validation: 0.05988704103133154]
	TIME [epoch: 12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05561973104975222		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.05561973104975222 | validation: 0.08396285587202965]
	TIME [epoch: 12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05589383564308564		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.05589383564308564 | validation: 0.054417613216014814]
	TIME [epoch: 12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05006691914505591		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.05006691914505591 | validation: 0.08193796769619549]
	TIME [epoch: 12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06684769506616497		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.06684769506616497 | validation: 0.06775678528505331]
	TIME [epoch: 12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06549280092922362		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.06549280092922362 | validation: 0.05566840061086449]
	TIME [epoch: 12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03935594582444534		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.03935594582444534 | validation: 0.05938742270786236]
	TIME [epoch: 12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07383518596106547		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.07383518596106547 | validation: 0.07043513768078222]
	TIME [epoch: 12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05783046925342135		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.05783046925342135 | validation: 0.05251690408493172]
	TIME [epoch: 12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05797252280872728		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.05797252280872728 | validation: 0.07139743000283442]
	TIME [epoch: 12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0700998893626116		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.0700998893626116 | validation: 0.07021209550632716]
	TIME [epoch: 12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04827855765852235		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.04827855765852235 | validation: 0.04643042738308]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037861203305458245		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.037861203305458245 | validation: 0.07737353791427559]
	TIME [epoch: 12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07772522532997446		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.07772522532997446 | validation: 0.06373919326367097]
	TIME [epoch: 12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05839734861973654		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.05839734861973654 | validation: 0.05750248730144955]
	TIME [epoch: 12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04504693119262162		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.04504693119262162 | validation: 0.060900765231521284]
	TIME [epoch: 12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07577712567258714		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.07577712567258714 | validation: 0.07103182363792497]
	TIME [epoch: 12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05393896919100415		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.05393896919100415 | validation: 0.07053006074278292]
	TIME [epoch: 12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046549259620354305		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.046549259620354305 | validation: 0.05418200588778837]
	TIME [epoch: 12 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05812753425066828		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.05812753425066828 | validation: 0.10255475076045568]
	TIME [epoch: 12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05155486240883812		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.05155486240883812 | validation: 0.059335175366432555]
	TIME [epoch: 12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04782323567061691		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.04782323567061691 | validation: 0.054538068733970504]
	TIME [epoch: 12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05216330253084646		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.05216330253084646 | validation: 0.07235717558516787]
	TIME [epoch: 12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06482092546815522		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.06482092546815522 | validation: 0.06861474606620863]
	TIME [epoch: 12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05869909917753974		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.05869909917753974 | validation: 0.05396803946773879]
	TIME [epoch: 12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04234384533428308		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.04234384533428308 | validation: 0.05376911284556685]
	TIME [epoch: 12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05426134201391551		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.05426134201391551 | validation: 0.04801437116862651]
	TIME [epoch: 12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04565272509537325		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.04565272509537325 | validation: 0.08295991757055898]
	TIME [epoch: 12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07310249304424488		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.07310249304424488 | validation: 0.05175256280770825]
	TIME [epoch: 12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04022516214713752		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.04022516214713752 | validation: 0.07956114664679853]
	TIME [epoch: 12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057321069154662604		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.057321069154662604 | validation: 0.06255677159530451]
	TIME [epoch: 12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04831642684571327		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.04831642684571327 | validation: 0.06392022437834541]
	TIME [epoch: 12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056718671799273555		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.056718671799273555 | validation: 0.0696387545263336]
	TIME [epoch: 12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054163190565271385		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.054163190565271385 | validation: 0.057485363569769146]
	TIME [epoch: 12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05127773311175324		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.05127773311175324 | validation: 0.056745402907052545]
	TIME [epoch: 12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04831985869741644		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.04831985869741644 | validation: 0.06922324582574635]
	TIME [epoch: 12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05022754678092771		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.05022754678092771 | validation: 0.05542681442872574]
	TIME [epoch: 12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042078862600380726		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.042078862600380726 | validation: 0.06076444503080129]
	TIME [epoch: 12 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06829245619165278		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.06829245619165278 | validation: 0.0774101175309054]
	TIME [epoch: 12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04375757892911211		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.04375757892911211 | validation: 0.06304785369621789]
	TIME [epoch: 12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04883559248119814		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.04883559248119814 | validation: 0.0748051946398127]
	TIME [epoch: 12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0513871474388217		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.0513871474388217 | validation: 0.06912090730643974]
	TIME [epoch: 12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045715385741611964		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.045715385741611964 | validation: 0.06316886045375573]
	TIME [epoch: 12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05382647351847544		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.05382647351847544 | validation: 0.0695189457109822]
	TIME [epoch: 12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05074837770916818		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.05074837770916818 | validation: 0.07014129807655316]
	TIME [epoch: 12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047031119762423315		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.047031119762423315 | validation: 0.049675491566097255]
	TIME [epoch: 12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042764895225927746		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.042764895225927746 | validation: 0.062690831475048]
	TIME [epoch: 12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0482867049882992		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.0482867049882992 | validation: 0.09841970155005883]
	TIME [epoch: 12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0642585155237479		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.0642585155237479 | validation: 0.06504425890965562]
	TIME [epoch: 12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040657729576346543		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.040657729576346543 | validation: 0.0448732195963671]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04073315433779234		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.04073315433779234 | validation: 0.08405810798408885]
	TIME [epoch: 12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06418394049222771		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.06418394049222771 | validation: 0.055046463135773493]
	TIME [epoch: 12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043962019493315026		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.043962019493315026 | validation: 0.05285430437703481]
	TIME [epoch: 12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0565235505126572		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.0565235505126572 | validation: 0.06242772981823315]
	TIME [epoch: 12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03973334544289111		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.03973334544289111 | validation: 0.045357450622392326]
	TIME [epoch: 12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04348754447313756		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.04348754447313756 | validation: 0.103977499757844]
	TIME [epoch: 12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07206902527952481		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.07206902527952481 | validation: 0.04587024175397229]
	TIME [epoch: 12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03322930500074198		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.03322930500074198 | validation: 0.059393682226982006]
	TIME [epoch: 12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0524219456549452		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.0524219456549452 | validation: 0.04472441907263029]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038197224901033236		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.038197224901033236 | validation: 0.04865092649223854]
	TIME [epoch: 12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046470941153644134		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.046470941153644134 | validation: 0.048021176591703796]
	TIME [epoch: 12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046545327423553176		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.046545327423553176 | validation: 0.08269916573927513]
	TIME [epoch: 12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0543842815548704		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.0543842815548704 | validation: 0.04291290821643376]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03459169868283423		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.03459169868283423 | validation: 0.05282521724216187]
	TIME [epoch: 12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04755839659075909		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.04755839659075909 | validation: 0.047299895128113376]
	TIME [epoch: 12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049761897970160174		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.049761897970160174 | validation: 0.06076862791445596]
	TIME [epoch: 12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04795628316804189		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.04795628316804189 | validation: 0.04199790580604544]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03577715101191604		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.03577715101191604 | validation: 0.10521757584595667]
	TIME [epoch: 12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07785983793217803		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.07785983793217803 | validation: 0.060622265469052576]
	TIME [epoch: 12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045430511369875436		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.045430511369875436 | validation: 0.05464469464309331]
	TIME [epoch: 12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04408985295611137		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04408985295611137 | validation: 0.06279452361589705]
	TIME [epoch: 12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03987501548737463		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.03987501548737463 | validation: 0.055417742478519855]
	TIME [epoch: 12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04403058085608344		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.04403058085608344 | validation: 0.05256217193169919]
	TIME [epoch: 12 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04756931673641983		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.04756931673641983 | validation: 0.07376836785906488]
	TIME [epoch: 12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047023898660783554		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.047023898660783554 | validation: 0.052211059111297246]
	TIME [epoch: 12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04289114193674972		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.04289114193674972 | validation: 0.04375638462010434]
	TIME [epoch: 12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03138765527066759		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.03138765527066759 | validation: 0.05565356214472271]
	TIME [epoch: 12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04935778667687497		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04935778667687497 | validation: 0.05496146817508249]
	TIME [epoch: 12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045075861703121625		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.045075861703121625 | validation: 0.05989937319392509]
	TIME [epoch: 12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046452326391533016		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.046452326391533016 | validation: 0.055177131667918905]
	TIME [epoch: 12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04945182507473901		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.04945182507473901 | validation: 0.05360567635539991]
	TIME [epoch: 12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03299905471964931		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.03299905471964931 | validation: 0.04446667406677978]
	TIME [epoch: 12.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04194722796790779		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.04194722796790779 | validation: 0.07061077714449962]
	TIME [epoch: 12 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0803996371203421		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.0803996371203421 | validation: 0.05766078411989223]
	TIME [epoch: 12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0391302915490686		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.0391302915490686 | validation: 0.048382498512978184]
	TIME [epoch: 12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03568195207615613		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.03568195207615613 | validation: 0.07058174495773353]
	TIME [epoch: 12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04463508725201636		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.04463508725201636 | validation: 0.04778206788062004]
	TIME [epoch: 12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032312597801823426		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.032312597801823426 | validation: 0.048671070049383945]
	TIME [epoch: 12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03841937095130143		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.03841937095130143 | validation: 0.04637830545754711]
	TIME [epoch: 12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04979224969444918		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.04979224969444918 | validation: 0.05602349350010792]
	TIME [epoch: 12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03755166063206179		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.03755166063206179 | validation: 0.05958485089307443]
	TIME [epoch: 12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037642550395861306		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.037642550395861306 | validation: 0.048602772831009985]
	TIME [epoch: 12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04318697642700562		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.04318697642700562 | validation: 0.08888122080691187]
	TIME [epoch: 12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057852488282615196		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.057852488282615196 | validation: 0.04325423569872491]
	TIME [epoch: 12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034915612085346506		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.034915612085346506 | validation: 0.05012063694815336]
	TIME [epoch: 12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04074001209499689		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.04074001209499689 | validation: 0.0760229912863965]
	TIME [epoch: 12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04127926106211		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.04127926106211 | validation: 0.048736238172342156]
	TIME [epoch: 12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04295409687403933		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.04295409687403933 | validation: 0.05679594677349459]
	TIME [epoch: 12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03886219249072408		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.03886219249072408 | validation: 0.05562089530822407]
	TIME [epoch: 12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053984648525251595		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.053984648525251595 | validation: 0.055229570512287024]
	TIME [epoch: 12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035906910519284296		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.035906910519284296 | validation: 0.05003679742504426]
	TIME [epoch: 12 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040958260777438166		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.040958260777438166 | validation: 0.04160367054428206]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034227175253077446		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.034227175253077446 | validation: 0.07292511983280456]
	TIME [epoch: 12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04719415049881926		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04719415049881926 | validation: 0.04952951315274963]
	TIME [epoch: 12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03679183363441874		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.03679183363441874 | validation: 0.051128622992829706]
	TIME [epoch: 12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037779653997068376		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.037779653997068376 | validation: 0.0476393525474481]
	TIME [epoch: 12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07527556888685083		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.07527556888685083 | validation: 0.19492241806784172]
	TIME [epoch: 12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12332569863018097		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.12332569863018097 | validation: 0.0749483560311987]
	TIME [epoch: 12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05083709047348815		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.05083709047348815 | validation: 0.0484197293880442]
	TIME [epoch: 12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036629137994123984		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.036629137994123984 | validation: 0.037162073695957504]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03315466007191973		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03315466007191973 | validation: 0.04747777322313612]
	TIME [epoch: 12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03246113277095952		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.03246113277095952 | validation: 0.040428462656688154]
	TIME [epoch: 12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04120717423629693		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.04120717423629693 | validation: 0.05307692449364759]
	TIME [epoch: 12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038090989148814404		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.038090989148814404 | validation: 0.03722164395718913]
	TIME [epoch: 12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030024916045123775		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.030024916045123775 | validation: 0.05092530445782954]
	TIME [epoch: 12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050266124651896116		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.050266124651896116 | validation: 0.06168814691119213]
	TIME [epoch: 12.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04154252030199311		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.04154252030199311 | validation: 0.045460259956251414]
	TIME [epoch: 12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03250391283507834		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.03250391283507834 | validation: 0.046454249927899365]
	TIME [epoch: 12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04315048686749941		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.04315048686749941 | validation: 0.04568018311073952]
	TIME [epoch: 12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033112382539745536		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.033112382539745536 | validation: 0.061597192419764094]
	TIME [epoch: 12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048328943842119054		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.048328943842119054 | validation: 0.045402159452888086]
	TIME [epoch: 12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03384663434539368		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.03384663434539368 | validation: 0.047217355626851865]
	TIME [epoch: 12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03676144503478455		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.03676144503478455 | validation: 0.046552318818067505]
	TIME [epoch: 12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03924922952889244		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.03924922952889244 | validation: 0.05709232651543421]
	TIME [epoch: 12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037662839599750586		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.037662839599750586 | validation: 0.04848221894571571]
	TIME [epoch: 12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049012417757548485		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.049012417757548485 | validation: 0.046169173007820835]
	TIME [epoch: 12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032228789570445976		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.032228789570445976 | validation: 0.04250751585873881]
	TIME [epoch: 12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03190806433266083		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.03190806433266083 | validation: 0.061034206905047975]
	TIME [epoch: 12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0495133977294093		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.0495133977294093 | validation: 0.07323623495010508]
	TIME [epoch: 12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0408748456283238		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.0408748456283238 | validation: 0.043355512270203345]
	TIME [epoch: 12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03378067293850064		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.03378067293850064 | validation: 0.039831220512980825]
	TIME [epoch: 12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03656518086809732		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.03656518086809732 | validation: 0.04334702449671751]
	TIME [epoch: 12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03349429629193907		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.03349429629193907 | validation: 0.053147817636131925]
	TIME [epoch: 12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047829741841845824		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.047829741841845824 | validation: 0.04332975310655088]
	TIME [epoch: 12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0358249998017523		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.0358249998017523 | validation: 0.049675326530432454]
	TIME [epoch: 12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03621018972294241		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.03621018972294241 | validation: 0.06422562099262914]
	TIME [epoch: 12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04086864201357909		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.04086864201357909 | validation: 0.046825288368815735]
	TIME [epoch: 12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03184256414570231		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.03184256414570231 | validation: 0.039602098814457795]
	TIME [epoch: 12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0412280798612684		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.0412280798612684 | validation: 0.0974299421585931]
	TIME [epoch: 12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05835707481836633		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.05835707481836633 | validation: 0.04499239517365239]
	TIME [epoch: 12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03234636671934039		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.03234636671934039 | validation: 0.04357604736825534]
	TIME [epoch: 12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03827776261885733		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.03827776261885733 | validation: 0.050952884531007846]
	TIME [epoch: 12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03931564189533168		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.03931564189533168 | validation: 0.04328303139502883]
	TIME [epoch: 12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028771922863883272		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.028771922863883272 | validation: 0.04306059619687888]
	TIME [epoch: 12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06980044076563659		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.06980044076563659 | validation: 0.04530046154201699]
	TIME [epoch: 12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04045573444094439		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.04045573444094439 | validation: 0.038617166906067896]
	TIME [epoch: 12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030036944543773973		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.030036944543773973 | validation: 0.042247607075864585]
	TIME [epoch: 12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03985193118827307		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.03985193118827307 | validation: 0.04532889690795748]
	TIME [epoch: 12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029741055655461597		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.029741055655461597 | validation: 0.0459623044778187]
	TIME [epoch: 12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04113225470156208		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.04113225470156208 | validation: 0.04217554405429093]
	TIME [epoch: 12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03499266443978691		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.03499266443978691 | validation: 0.04229055480620662]
	TIME [epoch: 12 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031609014167324624		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.031609014167324624 | validation: 0.059859827266889334]
	TIME [epoch: 12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039448312434063215		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.039448312434063215 | validation: 0.08282346590152626]
	TIME [epoch: 12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04778714507673006		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.04778714507673006 | validation: 0.038399010088289345]
	TIME [epoch: 12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0291562670837774		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.0291562670837774 | validation: 0.041707675428401175]
	TIME [epoch: 12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031346308324943936		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.031346308324943936 | validation: 0.05740935184522164]
	TIME [epoch: 12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03821721464601746		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.03821721464601746 | validation: 0.04118662592526387]
	TIME [epoch: 12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033419054827636026		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.033419054827636026 | validation: 0.04988746920074813]
	TIME [epoch: 12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042579969351657275		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.042579969351657275 | validation: 0.042937169567087805]
	TIME [epoch: 407 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02658157215185054		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.02658157215185054 | validation: 0.041067290323750505]
	TIME [epoch: 25.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03137016618051221		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03137016618051221 | validation: 0.06188674555266943]
	TIME [epoch: 25.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03855826207335339		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.03855826207335339 | validation: 0.05096454188241539]
	TIME [epoch: 25.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035917606566034696		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.035917606566034696 | validation: 0.04461306203819648]
	TIME [epoch: 25.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034908849113405936		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.034908849113405936 | validation: 0.04485523544583593]
	TIME [epoch: 25.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04533001273978371		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.04533001273978371 | validation: 0.04787225387846793]
	TIME [epoch: 25.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028119179727171803		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.028119179727171803 | validation: 0.03957680369473052]
	TIME [epoch: 25.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03275336664346009		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.03275336664346009 | validation: 0.05655457637494024]
	TIME [epoch: 25.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036700615755358365		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.036700615755358365 | validation: 0.043998296970409016]
	TIME [epoch: 25.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03706813690699505		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.03706813690699505 | validation: 0.05166045126081187]
	TIME [epoch: 25.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037176216270818206		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.037176216270818206 | validation: 0.04057241841309786]
	TIME [epoch: 25.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028991105289811147		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.028991105289811147 | validation: 0.04959135791371437]
	TIME [epoch: 25.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03605302178485599		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.03605302178485599 | validation: 0.04024952468722025]
	TIME [epoch: 25.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03443168298666113		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.03443168298666113 | validation: 0.0542070798999931]
	TIME [epoch: 25.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03064656375007283		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.03064656375007283 | validation: 0.041107760070678984]
	TIME [epoch: 25.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03901593713657627		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.03901593713657627 | validation: 0.04179015146559326]
	TIME [epoch: 25.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030378942413266413		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.030378942413266413 | validation: 0.04371990434475155]
	TIME [epoch: 25.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029770403473159002		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.029770403473159002 | validation: 0.046158410373045765]
	TIME [epoch: 25.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03577239758113792		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.03577239758113792 | validation: 0.050491167049857066]
	TIME [epoch: 25.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032600533496876416		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.032600533496876416 | validation: 0.050369125969118286]
	TIME [epoch: 25.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03279098282023436		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.03279098282023436 | validation: 0.04091713595004774]
	TIME [epoch: 25.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030508243521674323		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.030508243521674323 | validation: 0.050156821770719826]
	TIME [epoch: 25.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04273197468839299		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.04273197468839299 | validation: 0.04068692077009052]
	TIME [epoch: 25.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02916179633210511		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.02916179633210511 | validation: 0.05183704681234128]
	TIME [epoch: 25.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03025784414699135		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.03025784414699135 | validation: 0.05283275390041295]
	TIME [epoch: 25.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03366927813979028		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.03366927813979028 | validation: 0.04716075921324623]
	TIME [epoch: 25.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03502406366778932		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.03502406366778932 | validation: 0.06606247690014237]
	TIME [epoch: 25.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03608865896312056		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.03608865896312056 | validation: 0.04147199564228206]
	TIME [epoch: 25.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03180268141461551		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.03180268141461551 | validation: 0.03748316837352955]
	TIME [epoch: 25.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030644240313420074		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.030644240313420074 | validation: 0.04055640826356989]
	TIME [epoch: 25.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03256587013684883		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.03256587013684883 | validation: 0.042539328344250074]
	TIME [epoch: 25.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03308267843395421		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.03308267843395421 | validation: 0.044203145529680105]
	TIME [epoch: 25.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02832218991168615		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.02832218991168615 | validation: 0.04050070315620228]
	TIME [epoch: 25.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034227734363644305		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.034227734363644305 | validation: 0.051012827134728184]
	TIME [epoch: 25.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02994297446429984		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.02994297446429984 | validation: 0.04330617503623664]
	TIME [epoch: 25.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03235608311023007		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.03235608311023007 | validation: 0.046512382110654255]
	TIME [epoch: 25.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033743522894524175		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.033743522894524175 | validation: 0.045574445271111715]
	TIME [epoch: 25.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029675103288119918		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.029675103288119918 | validation: 0.04546798550074652]
	TIME [epoch: 25.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03541086317255415		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.03541086317255415 | validation: 0.0447800376383838]
	TIME [epoch: 25.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03210881045212646		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.03210881045212646 | validation: 0.04374263673116748]
	TIME [epoch: 25.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03112435684269094		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.03112435684269094 | validation: 0.05840934391088286]
	TIME [epoch: 25.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03307829808814763		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.03307829808814763 | validation: 0.04645891936768885]
	TIME [epoch: 25.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0327261136813871		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.0327261136813871 | validation: 0.03912848248434049]
	TIME [epoch: 25.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02843607631395055		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.02843607631395055 | validation: 0.04170120743362321]
	TIME [epoch: 25.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0352812715617091		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.0352812715617091 | validation: 0.04046208669333055]
	TIME [epoch: 25.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03982756740990271		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.03982756740990271 | validation: 0.0381877781891732]
	TIME [epoch: 25.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029683563582944904		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.029683563582944904 | validation: 0.042361384902968535]
	TIME [epoch: 25.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026935856254801153		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.026935856254801153 | validation: 0.040615620579942446]
	TIME [epoch: 25.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03263387134861981		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.03263387134861981 | validation: 0.04774506452631195]
	TIME [epoch: 25.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033872945508347636		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.033872945508347636 | validation: 0.04290132330803243]
	TIME [epoch: 25.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028171014441149902		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.028171014441149902 | validation: 0.04080352987790911]
	TIME [epoch: 25.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031969839729033694		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.031969839729033694 | validation: 0.047063455098044396]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1d_1_v_mmd1_20250611_130323/states/model_phi1_1a_saddle_v1d_1_v_mmd1_553.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7354.911 seconds.
