Args:
Namespace(name='model_phi1_1a_saddle_v1c_1_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1c_1/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1c_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05693410709500313, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2213232774

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.203120072451291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.203120072451291 | validation: 7.3497150494487915]
	TIME [epoch: 432 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.033895744843202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.033895744843202 | validation: 6.5155520449342905]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.671732116950758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.671732116950758 | validation: 6.389071364200344]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.485385836920665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.485385836920665 | validation: 6.291078435844581]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.33756592922206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.33756592922206 | validation: 6.216505513857853]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.269683558198917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.269683558198917 | validation: 6.239350893876286]
	TIME [epoch: 6.17 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.105515620016462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.105515620016462 | validation: 6.044365017081576]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.941125490786518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.941125490786518 | validation: 5.88747306095356]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.826747998122992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.826747998122992 | validation: 5.800632198709801]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.73818847349149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.73818847349149 | validation: 5.6398579200683105]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.567283025321978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.567283025321978 | validation: 5.529236469888863]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.439322188068246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.439322188068246 | validation: 5.389717431409679]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.260597761749533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.260597761749533 | validation: 4.989981753620467]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.407144761788771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.407144761788771 | validation: 3.704859382957631]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4215112962247662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4215112962247662 | validation: 2.7619188601946867]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9555812264948633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9555812264948633 | validation: 2.5153467120267554]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5622830296393655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5622830296393655 | validation: 2.553202206158856]
	TIME [epoch: 6.2 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8739873636576023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8739873636576023 | validation: 2.5037310559312176]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.507196686901609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.507196686901609 | validation: 1.956192721912024]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9514440026554194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9514440026554194 | validation: 1.9499488982678483]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.950523479530327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.950523479530327 | validation: 2.653088877564458]
	TIME [epoch: 6.21 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2523987609257494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2523987609257494 | validation: 1.8024285528029518]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.912591958943335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.912591958943335 | validation: 1.6466642034170174]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8714846478391864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8714846478391864 | validation: 1.8253008062675526]
	TIME [epoch: 6.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.627533326588923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.627533326588923 | validation: 2.086834224101223]
	TIME [epoch: 6.22 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0085013733908164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0085013733908164 | validation: 1.833666369695075]
	TIME [epoch: 6.23 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7136001856191405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7136001856191405 | validation: 1.3351736614617895]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.421102372940312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.421102372940312 | validation: 1.6482901954281632]
	TIME [epoch: 6.23 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.398519066527067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.398519066527067 | validation: 1.2580597426160505]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.293930206250749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.293930206250749 | validation: 1.0594904001112866]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2114503207002272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2114503207002272 | validation: 1.1116170019820513]
	TIME [epoch: 6.23 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.164240427649407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.164240427649407 | validation: 1.221012647546456]
	TIME [epoch: 6.21 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2022367034697206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2022367034697206 | validation: 1.0987958028323648]
	TIME [epoch: 6.25 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1282640748051045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1282640748051045 | validation: 1.053839901628986]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.009605709932883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.009605709932883 | validation: 1.1290664979841805]
	TIME [epoch: 6.21 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1027053980415802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1027053980415802 | validation: 1.209680588732632]
	TIME [epoch: 6.23 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1314788834427165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1314788834427165 | validation: 0.8224328689146991]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.880640653727404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.880640653727404 | validation: 0.8767902996099393]
	TIME [epoch: 6.23 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.934724747080892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.934724747080892 | validation: 0.7857444297384223]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.13975380299513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.13975380299513 | validation: 1.603516105853307]
	TIME [epoch: 6.23 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2157856506594282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2157856506594282 | validation: 0.7879394091942677]
	TIME [epoch: 6.19 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9178225155291146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9178225155291146 | validation: 0.8635180836632101]
	TIME [epoch: 6.2 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7736878607662812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736878607662812 | validation: 0.7248364527727029]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7729243312506606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729243312506606 | validation: 0.8321207238171484]
	TIME [epoch: 6.2 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8125255626246745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125255626246745 | validation: 0.8152919279206858]
	TIME [epoch: 6.21 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6939031653354684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6939031653354684 | validation: 0.8967154173100098]
	TIME [epoch: 6.21 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8650624219468399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650624219468399 | validation: 0.7361688386201752]
	TIME [epoch: 6.2 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7843878556757069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7843878556757069 | validation: 0.9395207145735862]
	TIME [epoch: 6.2 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9942240074763053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9942240074763053 | validation: 0.6235872868650277]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.692033258591327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692033258591327 | validation: 0.6399808591296594]
	TIME [epoch: 6.19 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9431861707928865		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.9431861707928865 | validation: 0.6914691094466971]
	TIME [epoch: 6.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7556524650666641		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.7556524650666641 | validation: 0.8838889551476476]
	TIME [epoch: 6.21 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7462150578358426		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.7462150578358426 | validation: 0.6270176037470541]
	TIME [epoch: 6.18 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6944165186100605		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.6944165186100605 | validation: 1.5067724994085103]
	TIME [epoch: 6.18 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.247026952012319		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.247026952012319 | validation: 0.6291523920030593]
	TIME [epoch: 6.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7560746530663819		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.7560746530663819 | validation: 0.5001937203720157]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.541401924255092		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.541401924255092 | validation: 0.5110742416808912]
	TIME [epoch: 6.2 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6635292178194157		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.6635292178194157 | validation: 0.5230797074051297]
	TIME [epoch: 6.21 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6992892707339718		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6992892707339718 | validation: 0.7085885157059983]
	TIME [epoch: 6.19 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6535326178398183		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.6535326178398183 | validation: 0.6176353921261895]
	TIME [epoch: 6.17 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6129708623566664		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.6129708623566664 | validation: 0.5295636291059872]
	TIME [epoch: 6.21 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.626704942513527		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.626704942513527 | validation: 0.8180131746916874]
	TIME [epoch: 6.18 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6837044454444481		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.6837044454444481 | validation: 0.5989051273385422]
	TIME [epoch: 6.19 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5703469973441904		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5703469973441904 | validation: 0.8284176935392485]
	TIME [epoch: 6.2 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7787578073470935		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.7787578073470935 | validation: 0.7943140578154513]
	TIME [epoch: 6.21 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6658691618263313		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.6658691618263313 | validation: 0.5698037929309355]
	TIME [epoch: 6.19 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6023479242174856		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.6023479242174856 | validation: 0.5160363977618447]
	TIME [epoch: 6.21 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5124613731434798		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5124613731434798 | validation: 0.456107606343813]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7900053196420521		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.7900053196420521 | validation: 0.7066820881081451]
	TIME [epoch: 6.21 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7915375587542548		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7915375587542548 | validation: 0.6031976799602838]
	TIME [epoch: 6.16 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5999025036713683		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.5999025036713683 | validation: 0.6026101630967945]
	TIME [epoch: 6.18 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677175562018374		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.5677175562018374 | validation: 0.8496289029801225]
	TIME [epoch: 6.18 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6580794431528265		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.6580794431528265 | validation: 0.45635934064024075]
	TIME [epoch: 6.19 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164133419507602		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5164133419507602 | validation: 0.5797265985644031]
	TIME [epoch: 6.17 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.646403273357522		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.646403273357522 | validation: 0.5821428130153201]
	TIME [epoch: 6.18 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5351888005327189		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5351888005327189 | validation: 0.49517703793627554]
	TIME [epoch: 6.22 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5075088182614553		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.5075088182614553 | validation: 0.45826089772878165]
	TIME [epoch: 6.21 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6120026559576055		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6120026559576055 | validation: 0.5924498569008407]
	TIME [epoch: 6.19 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5240580626830078		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.5240580626830078 | validation: 0.4131738410067505]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.476383380929596		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.476383380929596 | validation: 0.5315896132536481]
	TIME [epoch: 6.2 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5679820471712821		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5679820471712821 | validation: 0.4027339508816107]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4751386973141668		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.4751386973141668 | validation: 0.520384426033401]
	TIME [epoch: 6.19 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49424538548738384		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.49424538548738384 | validation: 0.5480341270528717]
	TIME [epoch: 6.21 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4650512625141181		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.4650512625141181 | validation: 0.4789576724707594]
	TIME [epoch: 6.17 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4838583102226822		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4838583102226822 | validation: 0.4501451944076055]
	TIME [epoch: 6.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4570610225773071		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.4570610225773071 | validation: 0.3680176711698993]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42655849100728577		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.42655849100728577 | validation: 0.4672375392648617]
	TIME [epoch: 6.16 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5557901302021657		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5557901302021657 | validation: 0.5001925873542876]
	TIME [epoch: 6.18 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5170599898254749		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5170599898254749 | validation: 0.4742706613838561]
	TIME [epoch: 6.19 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3881059814193057		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.3881059814193057 | validation: 0.3572780111134845]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3712713040249453		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.3712713040249453 | validation: 0.4956669624502433]
	TIME [epoch: 6.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5758192039073637		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.5758192039073637 | validation: 0.7477293960014321]
	TIME [epoch: 6.17 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5125096188481226		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.5125096188481226 | validation: 0.39038237466654585]
	TIME [epoch: 6.17 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4353135776793204		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.4353135776793204 | validation: 0.35297051040873123]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36641650061742964		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.36641650061742964 | validation: 0.37119734835814017]
	TIME [epoch: 6.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4663366905747991		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.4663366905747991 | validation: 0.35864709448344884]
	TIME [epoch: 6.17 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3576612053596205		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.3576612053596205 | validation: 0.3663366084037314]
	TIME [epoch: 6.18 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4602875512747813		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.4602875512747813 | validation: 0.4570097492316735]
	TIME [epoch: 6.18 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49972091321855583		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.49972091321855583 | validation: 0.38678523904920536]
	TIME [epoch: 6.21 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.314953258309832		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.314953258309832 | validation: 0.3438258815816815]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3993701560233496		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.3993701560233496 | validation: 0.48636283178770434]
	TIME [epoch: 6.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4719973209890353		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4719973209890353 | validation: 0.35060495888118964]
	TIME [epoch: 6.18 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3442405835903438		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.3442405835903438 | validation: 0.4349487993965916]
	TIME [epoch: 6.16 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4358269624106347		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.4358269624106347 | validation: 0.36545013179077634]
	TIME [epoch: 6.2 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33612383418089065		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.33612383418089065 | validation: 0.3442651264055939]
	TIME [epoch: 6.17 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40321074930814693		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.40321074930814693 | validation: 0.3669874726264939]
	TIME [epoch: 6.19 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35300616117865474		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.35300616117865474 | validation: 0.3139027921754871]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.285242348172061		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.285242348172061 | validation: 0.2515746341660302]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3044968871118126		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3044968871118126 | validation: 0.3210758336594758]
	TIME [epoch: 6.23 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42935537922554884		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.42935537922554884 | validation: 0.390323218005171]
	TIME [epoch: 6.25 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41962239693424885		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.41962239693424885 | validation: 0.5995707581447963]
	TIME [epoch: 6.25 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4110573131177825		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.4110573131177825 | validation: 0.28573444662899805]
	TIME [epoch: 6.23 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31844516320036526		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.31844516320036526 | validation: 0.30277305146005773]
	TIME [epoch: 6.25 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151255498425859		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.3151255498425859 | validation: 0.2428173971532592]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37932995890083215		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.37932995890083215 | validation: 0.4514705469135787]
	TIME [epoch: 6.24 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3799009859005359		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.3799009859005359 | validation: 0.37103032056871976]
	TIME [epoch: 6.24 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27385292917756715		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.27385292917756715 | validation: 0.2901393856033462]
	TIME [epoch: 6.24 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36868036139625027		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.36868036139625027 | validation: 0.3941226369501927]
	TIME [epoch: 6.25 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3791206485030478		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.3791206485030478 | validation: 0.3607631279345199]
	TIME [epoch: 6.24 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2488953121187696		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.2488953121187696 | validation: 0.22327075224253845]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3637011669796067		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.3637011669796067 | validation: 0.3031640827566728]
	TIME [epoch: 6.22 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3242548680668059		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3242548680668059 | validation: 0.23522473388538662]
	TIME [epoch: 6.23 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3071527486459953		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.3071527486459953 | validation: 0.4069797036224534]
	TIME [epoch: 6.24 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3425697392518891		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.3425697392518891 | validation: 0.2819880819305476]
	TIME [epoch: 6.24 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25437043811152843		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.25437043811152843 | validation: 0.35974179929626804]
	TIME [epoch: 6.24 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27206186368572777		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.27206186368572777 | validation: 0.2446962844205332]
	TIME [epoch: 6.24 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671623029825233		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.2671623029825233 | validation: 0.4068254536617645]
	TIME [epoch: 6.25 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4456561211632615		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.4456561211632615 | validation: 0.5511189851379129]
	TIME [epoch: 6.24 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3170736802343874		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.3170736802343874 | validation: 0.23317890054145168]
	TIME [epoch: 6.23 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2581424497354414		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2581424497354414 | validation: 0.3160544212138155]
	TIME [epoch: 6.24 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3347196622077007		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.3347196622077007 | validation: 0.24967358187281197]
	TIME [epoch: 6.25 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.348980976452694		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.348980976452694 | validation: 0.2847076389148214]
	TIME [epoch: 6.26 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24277759474241248		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.24277759474241248 | validation: 0.21643735665053498]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3592563022276599		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.3592563022276599 | validation: 0.3591008292322252]
	TIME [epoch: 6.24 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2893220381751919		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.2893220381751919 | validation: 0.27638840228865824]
	TIME [epoch: 6.25 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665178386989177		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2665178386989177 | validation: 0.20977407148331748]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26562795371344244		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.26562795371344244 | validation: 0.34282309395852995]
	TIME [epoch: 6.23 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3389417797561114		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.3389417797561114 | validation: 0.32098371545590154]
	TIME [epoch: 6.24 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736346088469108		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2736346088469108 | validation: 0.2617533434265707]
	TIME [epoch: 6.23 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23469253599165604		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.23469253599165604 | validation: 0.2891574154379303]
	TIME [epoch: 6.23 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25245828911146123		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.25245828911146123 | validation: 0.2637936956111067]
	TIME [epoch: 6.24 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27244534532100967		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.27244534532100967 | validation: 0.19810736170624144]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2351605944110267		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.2351605944110267 | validation: 0.3992646123905137]
	TIME [epoch: 6.25 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3132266220623316		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.3132266220623316 | validation: 0.37965545789069843]
	TIME [epoch: 6.23 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26392250990393656		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.26392250990393656 | validation: 0.24989028157590332]
	TIME [epoch: 6.24 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21152207447911284		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.21152207447911284 | validation: 0.21112468862929196]
	TIME [epoch: 6.23 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2358809912735476		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2358809912735476 | validation: 0.296633076780305]
	TIME [epoch: 6.22 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23797506090968779		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.23797506090968779 | validation: 0.3821725656563729]
	TIME [epoch: 6.22 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2513059123159476		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2513059123159476 | validation: 0.21482819345351234]
	TIME [epoch: 6.23 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1753484767587827		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.1753484767587827 | validation: 0.2050882539221645]
	TIME [epoch: 6.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30316694487333595		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.30316694487333595 | validation: 0.3189494722814177]
	TIME [epoch: 6.22 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434130878220009		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.3434130878220009 | validation: 0.23686087614363005]
	TIME [epoch: 6.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22291271061453594		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.22291271061453594 | validation: 0.24245608207440175]
	TIME [epoch: 6.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19135768348648968		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.19135768348648968 | validation: 0.39445128004873425]
	TIME [epoch: 6.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26395946684402916		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.26395946684402916 | validation: 0.22599789211117702]
	TIME [epoch: 6.2 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18217706599582112		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.18217706599582112 | validation: 0.18452108627475639]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24914986364022082		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.24914986364022082 | validation: 0.20002876637665978]
	TIME [epoch: 6.19 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28367471373795394		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.28367471373795394 | validation: 0.30980616429804825]
	TIME [epoch: 6.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2660071277098258		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2660071277098258 | validation: 0.200923866060427]
	TIME [epoch: 6.19 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19758548533461878		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.19758548533461878 | validation: 0.2228395218358315]
	TIME [epoch: 6.16 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19622801205249818		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.19622801205249818 | validation: 0.20802545188402738]
	TIME [epoch: 6.22 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2288844982929753		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.2288844982929753 | validation: 0.4117699141094361]
	TIME [epoch: 6.18 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26792128988205516		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.26792128988205516 | validation: 0.22124297641940932]
	TIME [epoch: 6.2 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14919970860779402		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.14919970860779402 | validation: 0.1784118300809487]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21451860722868765		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.21451860722868765 | validation: 0.15530819687317482]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21995856062894242		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.21995856062894242 | validation: 0.17729877741888986]
	TIME [epoch: 6.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17272353871533513		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.17272353871533513 | validation: 0.15911169631411637]
	TIME [epoch: 6.19 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19164848931695155		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.19164848931695155 | validation: 0.1678305777758584]
	TIME [epoch: 6.2 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14345678481035015		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.14345678481035015 | validation: 0.3158150541182473]
	TIME [epoch: 6.22 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20514226620670944		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.20514226620670944 | validation: 0.22175245939503346]
	TIME [epoch: 6.23 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24953498384913425		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.24953498384913425 | validation: 0.2098749930281667]
	TIME [epoch: 6.23 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23590482729034906		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.23590482729034906 | validation: 0.2205481663907642]
	TIME [epoch: 6.19 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1958783428113251		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.1958783428113251 | validation: 0.244485622806837]
	TIME [epoch: 6.24 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1657166438388692		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.1657166438388692 | validation: 0.2946017030878568]
	TIME [epoch: 6.22 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25347540275582164		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.25347540275582164 | validation: 0.21817163566874848]
	TIME [epoch: 6.19 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19546519201944765		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.19546519201944765 | validation: 0.2815917446968555]
	TIME [epoch: 6.21 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2973521056006166		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.2973521056006166 | validation: 0.24423248536993786]
	TIME [epoch: 6.23 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17358689413222628		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.17358689413222628 | validation: 0.27032667933365184]
	TIME [epoch: 6.23 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26518352204157586		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.26518352204157586 | validation: 0.21361871379300673]
	TIME [epoch: 6.25 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20476194215264246		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.20476194215264246 | validation: 0.17837112506857894]
	TIME [epoch: 6.2 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14605618851914187		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.14605618851914187 | validation: 0.2048787188336883]
	TIME [epoch: 6.22 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17932682714779632		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.17932682714779632 | validation: 0.28933429114204484]
	TIME [epoch: 6.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1702174334951695		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.1702174334951695 | validation: 0.2066222610744125]
	TIME [epoch: 6.19 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1965382491237773		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.1965382491237773 | validation: 0.1589866180014226]
	TIME [epoch: 6.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17247488010417422		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.17247488010417422 | validation: 0.2118155534921545]
	TIME [epoch: 6.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15103684721264948		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.15103684721264948 | validation: 0.18857986743202898]
	TIME [epoch: 6.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2037977594208858		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.2037977594208858 | validation: 0.160891356819585]
	TIME [epoch: 6.18 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1757170784721385		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.1757170784721385 | validation: 0.15450641269882368]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19728033568092843		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.19728033568092843 | validation: 0.12555723275106684]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15081073062517128		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.15081073062517128 | validation: 0.21318163660518374]
	TIME [epoch: 6.24 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18397403744205992		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.18397403744205992 | validation: 0.16301013498833872]
	TIME [epoch: 6.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13436339344806988		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.13436339344806988 | validation: 0.22058736759883052]
	TIME [epoch: 6.23 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15527102261846784		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.15527102261846784 | validation: 0.24777739995019926]
	TIME [epoch: 6.2 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20431984821831292		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.20431984821831292 | validation: 0.17266868528114904]
	TIME [epoch: 6.24 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1392772963376609		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.1392772963376609 | validation: 0.20344884607724345]
	TIME [epoch: 6.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1553462895813988		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.1553462895813988 | validation: 0.15746317259554654]
	TIME [epoch: 6.21 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15109102045171877		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.15109102045171877 | validation: 0.14687144316071735]
	TIME [epoch: 6.22 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16022664797559288		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.16022664797559288 | validation: 0.12392437517479]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13990373033310383		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.13990373033310383 | validation: 0.18432857240074596]
	TIME [epoch: 6.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13651893068220392		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.13651893068220392 | validation: 0.22430336553028507]
	TIME [epoch: 6.17 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17053777008895782		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.17053777008895782 | validation: 0.11895952268664423]
	TIME [epoch: 428 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1431849100563351		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.1431849100563351 | validation: 0.13214698220040577]
	TIME [epoch: 12.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13336017498366143		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.13336017498366143 | validation: 0.13022730706140037]
	TIME [epoch: 12.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15563097810654125		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.15563097810654125 | validation: 0.22313970064775762]
	TIME [epoch: 12.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1933098651633605		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.1933098651633605 | validation: 0.15593142371715585]
	TIME [epoch: 12.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12249674482433676		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.12249674482433676 | validation: 0.13490917590234758]
	TIME [epoch: 12.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1499230257400208		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.1499230257400208 | validation: 0.13829595906713396]
	TIME [epoch: 12.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11359050473298993		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.11359050473298993 | validation: 0.1466270042423023]
	TIME [epoch: 12.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17192913780777272		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.17192913780777272 | validation: 0.12507265081748947]
	TIME [epoch: 12.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15286798477381522		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.15286798477381522 | validation: 0.13894270926310948]
	TIME [epoch: 12.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13882183362489298		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.13882183362489298 | validation: 0.16656663943792968]
	TIME [epoch: 12.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14657259799189612		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.14657259799189612 | validation: 0.13333127163433608]
	TIME [epoch: 12.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11757403463471167		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.11757403463471167 | validation: 0.2679845622688013]
	TIME [epoch: 12.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17662634429453658		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.17662634429453658 | validation: 0.2248571189796916]
	TIME [epoch: 12.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15133431666963965		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.15133431666963965 | validation: 0.1795635011585947]
	TIME [epoch: 12.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1294309390295095		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.1294309390295095 | validation: 0.13444995428850803]
	TIME [epoch: 12.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13874832341160387		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.13874832341160387 | validation: 0.14615826026657952]
	TIME [epoch: 12.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11241802555916164		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.11241802555916164 | validation: 0.09792039387046361]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1261551498411586		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.1261551498411586 | validation: 0.1558765227356395]
	TIME [epoch: 12.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11964636473099421		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.11964636473099421 | validation: 0.09607073718349958]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11855124771707584		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.11855124771707584 | validation: 0.17614811263509952]
	TIME [epoch: 12.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12104038641735318		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.12104038641735318 | validation: 0.23328776705307652]
	TIME [epoch: 12.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18183414004406812		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.18183414004406812 | validation: 0.14050073262672247]
	TIME [epoch: 12.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10491314602827342		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.10491314602827342 | validation: 0.10613854689654431]
	TIME [epoch: 12.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1512282934741128		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.1512282934741128 | validation: 0.1300626347864603]
	TIME [epoch: 12.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11168324599979178		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.11168324599979178 | validation: 0.16553532454469244]
	TIME [epoch: 12.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12197399909558299		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.12197399909558299 | validation: 0.10675594181002053]
	TIME [epoch: 12.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12242406322478191		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.12242406322478191 | validation: 0.19177756827661713]
	TIME [epoch: 12.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1167416475675178		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.1167416475675178 | validation: 0.12905476779285013]
	TIME [epoch: 12.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1197847940395393		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.1197847940395393 | validation: 0.11959066052886133]
	TIME [epoch: 12.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11967887352203463		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.11967887352203463 | validation: 0.14758634295898732]
	TIME [epoch: 12.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12493410817612677		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.12493410817612677 | validation: 0.09550252723173902]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08358402250249541		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.08358402250249541 | validation: 0.17941537325329593]
	TIME [epoch: 12.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1619853376520466		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.1619853376520466 | validation: 0.1632700371674199]
	TIME [epoch: 12.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12872674821461713		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.12872674821461713 | validation: 0.12593330627230942]
	TIME [epoch: 12.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09026536513546428		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.09026536513546428 | validation: 0.1925824549020014]
	TIME [epoch: 12.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13978109944026115		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.13978109944026115 | validation: 0.1943568931047383]
	TIME [epoch: 12.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14565403314486924		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.14565403314486924 | validation: 0.09763952874576504]
	TIME [epoch: 12.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1093586422680295		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1093586422680295 | validation: 0.10839911613157915]
	TIME [epoch: 12.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11054322331031197		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.11054322331031197 | validation: 0.17314107556792063]
	TIME [epoch: 12.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10528477103296181		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.10528477103296181 | validation: 0.16715051751093332]
	TIME [epoch: 12.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1403879039619916		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.1403879039619916 | validation: 0.16405990297436046]
	TIME [epoch: 12.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10885392657920147		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.10885392657920147 | validation: 0.10875926855738971]
	TIME [epoch: 12.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11495884102150045		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.11495884102150045 | validation: 0.09248108974278715]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1118933366477439		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.1118933366477439 | validation: 0.16273357902799213]
	TIME [epoch: 12.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10245882707004014		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.10245882707004014 | validation: 0.0976671728006086]
	TIME [epoch: 12.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1255441578486326		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.1255441578486326 | validation: 0.07955095462701389]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11888849598044009		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.11888849598044009 | validation: 0.11550076468701352]
	TIME [epoch: 12.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1037411769532601		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.1037411769532601 | validation: 0.11098969226820025]
	TIME [epoch: 12.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08914910164619128		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.08914910164619128 | validation: 0.14865222607746356]
	TIME [epoch: 12.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12252606324045966		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.12252606324045966 | validation: 0.16127327520373952]
	TIME [epoch: 12.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10511326356514021		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.10511326356514021 | validation: 0.10407208420456812]
	TIME [epoch: 12.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08781068505948453		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.08781068505948453 | validation: 0.20696804804856841]
	TIME [epoch: 12.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13722653147466318		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.13722653147466318 | validation: 0.09712690429188037]
	TIME [epoch: 12.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10589097017417314		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.10589097017417314 | validation: 0.07865402062244922]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08708252559150147		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.08708252559150147 | validation: 0.13870261696280844]
	TIME [epoch: 12.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11332504528742862		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.11332504528742862 | validation: 0.188909128316939]
	TIME [epoch: 12.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11706259503520756		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.11706259503520756 | validation: 0.08103453566639188]
	TIME [epoch: 12.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07533935896841985		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.07533935896841985 | validation: 0.08367308850404823]
	TIME [epoch: 12.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09301493462000403		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.09301493462000403 | validation: 0.1498385573436542]
	TIME [epoch: 12.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1390551260529871		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.1390551260529871 | validation: 0.08586071731766488]
	TIME [epoch: 12.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07996595590697547		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.07996595590697547 | validation: 0.09832432819748718]
	TIME [epoch: 12.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09054143539138738		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.09054143539138738 | validation: 0.07749654120541644]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09044585371135933		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09044585371135933 | validation: 0.11532759959395539]
	TIME [epoch: 12.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08685722605037666		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.08685722605037666 | validation: 0.10518885768733846]
	TIME [epoch: 12.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11381671499917445		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.11381671499917445 | validation: 0.14463591619621757]
	TIME [epoch: 12.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11216576610409243		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.11216576610409243 | validation: 0.08966703520697525]
	TIME [epoch: 12.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08809943993759685		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.08809943993759685 | validation: 0.10677962979845373]
	TIME [epoch: 12.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08828885728413975		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.08828885728413975 | validation: 0.14319173408532332]
	TIME [epoch: 12.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09900892912348919		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.09900892912348919 | validation: 0.1356225184233579]
	TIME [epoch: 12.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11142385237536359		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.11142385237536359 | validation: 0.09985205257080965]
	TIME [epoch: 12.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08251446924191368		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.08251446924191368 | validation: 0.08680497974926699]
	TIME [epoch: 12.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08373720600993374		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.08373720600993374 | validation: 0.0882532065475109]
	TIME [epoch: 12.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12553271269964655		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.12553271269964655 | validation: 0.11166281147205759]
	TIME [epoch: 12.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.111329934071734		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.111329934071734 | validation: 0.08621820399801598]
	TIME [epoch: 12.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06721237057543905		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.06721237057543905 | validation: 0.12400491125288536]
	TIME [epoch: 12.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0900695671419679		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.0900695671419679 | validation: 0.10503845766726187]
	TIME [epoch: 12.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09709715525839667		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.09709715525839667 | validation: 0.07147161482229035]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11583815257005059		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11583815257005059 | validation: 0.09281776003321926]
	TIME [epoch: 12.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09432926870146396		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.09432926870146396 | validation: 0.14769526433878644]
	TIME [epoch: 12.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.259988550724094		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.259988550724094 | validation: 0.1371852078685145]
	TIME [epoch: 12.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09990954768950483		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.09990954768950483 | validation: 0.08831881036701306]
	TIME [epoch: 12.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08669775323804511		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.08669775323804511 | validation: 0.08178475287473483]
	TIME [epoch: 12.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07109815822903753		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.07109815822903753 | validation: 0.07812097354379682]
	TIME [epoch: 12.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10835051594992676		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.10835051594992676 | validation: 0.07871408621112327]
	TIME [epoch: 12.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08657523012808287		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.08657523012808287 | validation: 0.08293976480283129]
	TIME [epoch: 12.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07720980206852121		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.07720980206852121 | validation: 0.14095496291043466]
	TIME [epoch: 12.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09626762777429601		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.09626762777429601 | validation: 0.1823083288527615]
	TIME [epoch: 12.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11195348645164963		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.11195348645164963 | validation: 0.07820592750146493]
	TIME [epoch: 12.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061416351469941705		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.061416351469941705 | validation: 0.08248593127764449]
	TIME [epoch: 12.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10976817074806287		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.10976817074806287 | validation: 0.09280395819669358]
	TIME [epoch: 12.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09818633244120649		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.09818633244120649 | validation: 0.07175454097910718]
	TIME [epoch: 12.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0935377750742925		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.0935377750742925 | validation: 0.11087081996457512]
	TIME [epoch: 12.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1006304906589483		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.1006304906589483 | validation: 0.10088504959236878]
	TIME [epoch: 12.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11717576117902101		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.11717576117902101 | validation: 0.10997044860810286]
	TIME [epoch: 12.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0847514801640449		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.0847514801640449 | validation: 0.07705665645924312]
	TIME [epoch: 12.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12034664093912734		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.12034664093912734 | validation: 0.20640587152875167]
	TIME [epoch: 12.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20637915989855682		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.20637915989855682 | validation: 0.1455545072741541]
	TIME [epoch: 12.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10766409670464305		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.10766409670464305 | validation: 0.10099488718945018]
	TIME [epoch: 12.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09933835303235952		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.09933835303235952 | validation: 0.07591488465824359]
	TIME [epoch: 12.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08163182092199264		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.08163182092199264 | validation: 0.08040268925603264]
	TIME [epoch: 12.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05946040187024206		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.05946040187024206 | validation: 0.07515105681956674]
	TIME [epoch: 12.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09669121166867135		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.09669121166867135 | validation: 0.057041755179630366]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07166283071871037		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.07166283071871037 | validation: 0.08514776068971486]
	TIME [epoch: 12.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10254863875663836		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.10254863875663836 | validation: 0.09843872779533455]
	TIME [epoch: 12.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06924840698250462		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.06924840698250462 | validation: 0.07760631026946777]
	TIME [epoch: 12.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06808870237520506		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.06808870237520506 | validation: 0.08232917573350998]
	TIME [epoch: 12.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08161170698669395		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.08161170698669395 | validation: 0.10073725489788884]
	TIME [epoch: 12.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08298303290516391		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.08298303290516391 | validation: 0.10557253858498922]
	TIME [epoch: 12.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07235676356081726		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.07235676356081726 | validation: 0.08849508885296897]
	TIME [epoch: 12.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07896103728085038		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.07896103728085038 | validation: 0.1250173391513088]
	TIME [epoch: 12.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08533594371760128		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.08533594371760128 | validation: 0.06449536840388036]
	TIME [epoch: 12.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0760558658578346		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.0760558658578346 | validation: 0.08992740401674995]
	TIME [epoch: 12.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08406686741956351		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.08406686741956351 | validation: 0.0764076338139181]
	TIME [epoch: 12.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0721109129950193		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.0721109129950193 | validation: 0.10604180496132712]
	TIME [epoch: 12.1 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08920748202332417		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.08920748202332417 | validation: 0.1355227051594423]
	TIME [epoch: 12.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08323665010430788		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.08323665010430788 | validation: 0.06328263977581214]
	TIME [epoch: 12.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053744919058194646		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.053744919058194646 | validation: 0.08262940385931897]
	TIME [epoch: 12.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09337160532174908		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.09337160532174908 | validation: 0.06341301819850238]
	TIME [epoch: 12.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062002952007795965		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.062002952007795965 | validation: 0.12253321917723053]
	TIME [epoch: 12.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08648090115988367		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.08648090115988367 | validation: 0.10682114618240655]
	TIME [epoch: 12.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07516108800588026		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07516108800588026 | validation: 0.0835252260067649]
	TIME [epoch: 12.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08276357664087731		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08276357664087731 | validation: 0.0679444400368853]
	TIME [epoch: 12.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062000088994099485		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.062000088994099485 | validation: 0.0995449658275247]
	TIME [epoch: 12.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06791942394132297		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.06791942394132297 | validation: 0.08118663241473191]
	TIME [epoch: 12.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08277973334628626		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.08277973334628626 | validation: 0.06930290393068164]
	TIME [epoch: 12.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06316588760702366		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.06316588760702366 | validation: 0.08777710957780807]
	TIME [epoch: 12.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09331729491824246		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.09331729491824246 | validation: 0.04838857659039491]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047917463845794706		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.047917463845794706 | validation: 0.07488601254745104]
	TIME [epoch: 12.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06388399645361408		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.06388399645361408 | validation: 0.0852766704472799]
	TIME [epoch: 12.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11026814455366396		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.11026814455366396 | validation: 0.0757017918936469]
	TIME [epoch: 12.2 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06147472103366408		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.06147472103366408 | validation: 0.061838249279275184]
	TIME [epoch: 12.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04986232311757382		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.04986232311757382 | validation: 0.05772223189422312]
	TIME [epoch: 12.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06085549252410001		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.06085549252410001 | validation: 0.138582852801729]
	TIME [epoch: 12.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0942208400713289		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.0942208400713289 | validation: 0.058003657095613656]
	TIME [epoch: 12.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06601051204338343		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.06601051204338343 | validation: 0.08294861429069922]
	TIME [epoch: 12.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05367249641809506		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.05367249641809506 | validation: 0.07698352523584648]
	TIME [epoch: 12.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07798704701087464		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.07798704701087464 | validation: 0.07719898093998503]
	TIME [epoch: 12.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05705579096074883		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.05705579096074883 | validation: 0.06238463388401057]
	TIME [epoch: 12.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0793492852919517		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.0793492852919517 | validation: 0.059325334090064]
	TIME [epoch: 12.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06937435657340589		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.06937435657340589 | validation: 0.09764897949390476]
	TIME [epoch: 12.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07324373496905491		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.07324373496905491 | validation: 0.05441526203293889]
	TIME [epoch: 12.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052107513631893816		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.052107513631893816 | validation: 0.0678366900879172]
	TIME [epoch: 12.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05992497062496739		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.05992497062496739 | validation: 0.0976196471115488]
	TIME [epoch: 12.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07332571172878972		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.07332571172878972 | validation: 0.1130222362846103]
	TIME [epoch: 12.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06866522941105678		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.06866522941105678 | validation: 0.06217150331638699]
	TIME [epoch: 12.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05649906813048184		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.05649906813048184 | validation: 0.06029552231837215]
	TIME [epoch: 12.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06777050456843145		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.06777050456843145 | validation: 0.060006143570336165]
	TIME [epoch: 12.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059776617295784096		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.059776617295784096 | validation: 0.09003429484160531]
	TIME [epoch: 12.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07033837831810219		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.07033837831810219 | validation: 0.06727911740788435]
	TIME [epoch: 12.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06709680880802014		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.06709680880802014 | validation: 0.07965862232922924]
	TIME [epoch: 12.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07615511527555502		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07615511527555502 | validation: 0.07767443500875082]
	TIME [epoch: 12.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0607079529679299		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.0607079529679299 | validation: 0.06956786022809337]
	TIME [epoch: 12.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054937054420380035		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.054937054420380035 | validation: 0.10266798792794171]
	TIME [epoch: 12.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07838070610421967		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.07838070610421967 | validation: 0.08458031503201248]
	TIME [epoch: 12.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05604486249588912		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.05604486249588912 | validation: 0.09385653861890736]
	TIME [epoch: 12.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07056718919742296		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.07056718919742296 | validation: 0.049879394328837895]
	TIME [epoch: 12.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04616569927673793		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.04616569927673793 | validation: 0.06820897227583265]
	TIME [epoch: 12.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0721841285670073		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.0721841285670073 | validation: 0.04804004995373086]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06588758433612434		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.06588758433612434 | validation: 0.06745881306156713]
	TIME [epoch: 12.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057369822232848214		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.057369822232848214 | validation: 0.08849613553706281]
	TIME [epoch: 12.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05967284852790192		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.05967284852790192 | validation: 0.08181977856645015]
	TIME [epoch: 12.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07043244220967596		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.07043244220967596 | validation: 0.06954101614491467]
	TIME [epoch: 12.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05811729853262764		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.05811729853262764 | validation: 0.062324238855834595]
	TIME [epoch: 12.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06233954921379195		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.06233954921379195 | validation: 0.10458414531796498]
	TIME [epoch: 12.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06785102086170752		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.06785102086170752 | validation: 0.07860856782757744]
	TIME [epoch: 12.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06490800106080585		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.06490800106080585 | validation: 0.07214747837694194]
	TIME [epoch: 12.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06794054171154668		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.06794054171154668 | validation: 0.06223928294043668]
	TIME [epoch: 12.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04892791339892892		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.04892791339892892 | validation: 0.049864588239152914]
	TIME [epoch: 12.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054014134637708724		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.054014134637708724 | validation: 0.05666458039433872]
	TIME [epoch: 12.1 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06667356072594015		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.06667356072594015 | validation: 0.06287185756526041]
	TIME [epoch: 12.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048379493960481045		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.048379493960481045 | validation: 0.11064111303840865]
	TIME [epoch: 12.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07357197467028766		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.07357197467028766 | validation: 0.08619470413295313]
	TIME [epoch: 12.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0755307912354804		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.0755307912354804 | validation: 0.08280986782410318]
	TIME [epoch: 12.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05288795965159131		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.05288795965159131 | validation: 0.06904484400023164]
	TIME [epoch: 12.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05154181524608053		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.05154181524608053 | validation: 0.06847001009674922]
	TIME [epoch: 12.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05200678493911044		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.05200678493911044 | validation: 0.10700701558101253]
	TIME [epoch: 12.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06688287268237586		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.06688287268237586 | validation: 0.050195002388479286]
	TIME [epoch: 12.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05089336625212859		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.05089336625212859 | validation: 0.05772210724158604]
	TIME [epoch: 12.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05507258041893952		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.05507258041893952 | validation: 0.06278486335874733]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06630065722195921		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.06630065722195921 | validation: 0.07907960544986148]
	TIME [epoch: 12.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056703057458672775		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.056703057458672775 | validation: 0.3784586995685981]
	TIME [epoch: 12.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20783743162922563		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.20783743162922563 | validation: 0.10299711709212125]
	TIME [epoch: 12.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06858887447473824		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.06858887447473824 | validation: 0.05462799929922104]
	TIME [epoch: 12.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041505528650092736		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.041505528650092736 | validation: 0.0492361888759001]
	TIME [epoch: 12.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03995779091782113		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.03995779091782113 | validation: 0.06593310001727405]
	TIME [epoch: 12.2 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1017099017404533		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.1017099017404533 | validation: 0.06014346812339824]
	TIME [epoch: 12.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058843968083594385		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.058843968083594385 | validation: 0.06607723336010292]
	TIME [epoch: 12.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047926540857745045		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.047926540857745045 | validation: 0.07282249864389737]
	TIME [epoch: 12.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05533385751106425		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.05533385751106425 | validation: 0.05922180477331882]
	TIME [epoch: 12.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04269237871919901		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.04269237871919901 | validation: 0.07810371495160165]
	TIME [epoch: 12.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06971647374568483		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.06971647374568483 | validation: 0.06296881695146923]
	TIME [epoch: 12.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05552448313790793		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.05552448313790793 | validation: 0.06887438216747284]
	TIME [epoch: 12.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14298120046608634		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.14298120046608634 | validation: 0.11324301638396564]
	TIME [epoch: 12.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07772770288381216		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.07772770288381216 | validation: 0.05643378213318225]
	TIME [epoch: 12.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04861417074897682		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.04861417074897682 | validation: 0.10678914734238018]
	TIME [epoch: 12.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057452593738526916		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.057452593738526916 | validation: 0.058981635687524264]
	TIME [epoch: 12.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04033156694413166		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.04033156694413166 | validation: 0.07540345107925721]
	TIME [epoch: 12.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06705566630287159		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.06705566630287159 | validation: 0.05890728758528556]
	TIME [epoch: 12.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0381484632114936		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.0381484632114936 | validation: 0.06594610434896289]
	TIME [epoch: 12.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06324691559516239		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.06324691559516239 | validation: 0.07722392903986915]
	TIME [epoch: 12.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04902286429679145		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.04902286429679145 | validation: 0.056939920195094945]
	TIME [epoch: 12.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05127580715827092		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.05127580715827092 | validation: 0.056196677821535895]
	TIME [epoch: 12.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04969070625377092		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.04969070625377092 | validation: 0.20441501966885914]
	TIME [epoch: 12.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12638848442919345		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.12638848442919345 | validation: 0.050191879977566314]
	TIME [epoch: 12.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04956654260875617		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.04956654260875617 | validation: 0.05041260607007013]
	TIME [epoch: 12.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04409539988390407		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.04409539988390407 | validation: 0.055554545841549105]
	TIME [epoch: 12.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058010090815095006		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.058010090815095006 | validation: 0.07856925195341738]
	TIME [epoch: 12.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04571547971647017		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.04571547971647017 | validation: 0.04485686683518304]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050164656997116724		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.050164656997116724 | validation: 0.05415832572311849]
	TIME [epoch: 12.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04131655000126185		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.04131655000126185 | validation: 0.07140528656505082]
	TIME [epoch: 12.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05129854909918889		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.05129854909918889 | validation: 0.06419067380546895]
	TIME [epoch: 12.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04458560200567355		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04458560200567355 | validation: 0.06815500032317326]
	TIME [epoch: 12.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06058507885115008		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.06058507885115008 | validation: 0.04639706419433848]
	TIME [epoch: 12.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043871657289872024		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.043871657289872024 | validation: 0.07176103002891537]
	TIME [epoch: 12.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15106512877006328		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.15106512877006328 | validation: 0.110961225363234]
	TIME [epoch: 12.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06969260277241723		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.06969260277241723 | validation: 0.05015429113965608]
	TIME [epoch: 12.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05229357632155062		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.05229357632155062 | validation: 0.051148468129649495]
	TIME [epoch: 12.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049132220663502785		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.049132220663502785 | validation: 0.06328855691560015]
	TIME [epoch: 12.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04249976522253787		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04249976522253787 | validation: 0.06353517037311353]
	TIME [epoch: 12.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056565379674936106		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.056565379674936106 | validation: 0.06520391114919563]
	TIME [epoch: 12.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05942675248328428		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.05942675248328428 | validation: 0.04646153741655247]
	TIME [epoch: 12.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048439083523324655		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.048439083523324655 | validation: 0.04916144996828276]
	TIME [epoch: 12.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040066807427365736		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.040066807427365736 | validation: 0.06086992972106886]
	TIME [epoch: 12.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05481834194278491		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.05481834194278491 | validation: 0.06015545299729382]
	TIME [epoch: 12.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04646307912498865		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.04646307912498865 | validation: 0.08108322033876894]
	TIME [epoch: 12.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050714767100539626		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.050714767100539626 | validation: 0.07346616009125745]
	TIME [epoch: 12.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06022947861161425		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.06022947861161425 | validation: 0.07049587685055916]
	TIME [epoch: 12.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04935579023397631		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.04935579023397631 | validation: 0.0649511686149607]
	TIME [epoch: 12.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047479975080193026		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.047479975080193026 | validation: 0.055809040797160064]
	TIME [epoch: 12.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040142038997612		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.040142038997612 | validation: 0.052605325251302106]
	TIME [epoch: 12.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05116561051055417		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.05116561051055417 | validation: 0.0578760226266289]
	TIME [epoch: 12.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052538389594349526		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.052538389594349526 | validation: 0.04831826056355959]
	TIME [epoch: 12.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0450862297868602		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.0450862297868602 | validation: 0.06842587889124457]
	TIME [epoch: 12.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04531922267825159		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.04531922267825159 | validation: 0.06213614883038184]
	TIME [epoch: 12.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03920285006063567		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.03920285006063567 | validation: 0.04905329586224455]
	TIME [epoch: 12.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04980914562449565		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.04980914562449565 | validation: 0.046797524568905804]
	TIME [epoch: 12.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0339586268692434		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.0339586268692434 | validation: 0.061675139846092304]
	TIME [epoch: 12.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06403502666302804		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.06403502666302804 | validation: 0.05410551926784628]
	TIME [epoch: 12.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04763986969096987		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.04763986969096987 | validation: 0.04940748475299091]
	TIME [epoch: 12.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0478962152617652		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.0478962152617652 | validation: 0.04306264033950436]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035596749101793654		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.035596749101793654 | validation: 0.13533466929444013]
	TIME [epoch: 12.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0806025913109647		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.0806025913109647 | validation: 0.09469146245693344]
	TIME [epoch: 12.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09817646030411867		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.09817646030411867 | validation: 0.05425864344100577]
	TIME [epoch: 12.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047354646801016115		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.047354646801016115 | validation: 0.0516008789425221]
	TIME [epoch: 12.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03949316459200046		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.03949316459200046 | validation: 0.04977788353069726]
	TIME [epoch: 12.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05123413857072963		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.05123413857072963 | validation: 0.05138010253367162]
	TIME [epoch: 12.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04816885731115553		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.04816885731115553 | validation: 0.05593634165600682]
	TIME [epoch: 12.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04103138115796305		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.04103138115796305 | validation: 0.05399361758487033]
	TIME [epoch: 12.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04480159842960169		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.04480159842960169 | validation: 0.06903244906124537]
	TIME [epoch: 12.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04337776609748359		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.04337776609748359 | validation: 0.05780660519462302]
	TIME [epoch: 12.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051897322711942015		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.051897322711942015 | validation: 0.06790885088032175]
	TIME [epoch: 12.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05404990349358318		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.05404990349358318 | validation: 0.04484770517899099]
	TIME [epoch: 12.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03737489544657603		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.03737489544657603 | validation: 0.050427994845409566]
	TIME [epoch: 12.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04327353684566545		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.04327353684566545 | validation: 0.053911413010858486]
	TIME [epoch: 12.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044221927162545596		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.044221927162545596 | validation: 0.04760495670772679]
	TIME [epoch: 12.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03905726892978378		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.03905726892978378 | validation: 0.05871263070470052]
	TIME [epoch: 12.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04683299448790457		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.04683299448790457 | validation: 0.0606160175763837]
	TIME [epoch: 12.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045998099521318954		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.045998099521318954 | validation: 0.05458596425667805]
	TIME [epoch: 12.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043794189666816204		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.043794189666816204 | validation: 0.04737809445629732]
	TIME [epoch: 12.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037491904443206256		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.037491904443206256 | validation: 0.048378265806856634]
	TIME [epoch: 12.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0491960849099833		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.0491960849099833 | validation: 0.05808535107821959]
	TIME [epoch: 12.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04066998618428015		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.04066998618428015 | validation: 0.04499802063068377]
	TIME [epoch: 12.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046119400259079735		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.046119400259079735 | validation: 0.05691862958485086]
	TIME [epoch: 12.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04134062408237116		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.04134062408237116 | validation: 0.06041995356592479]
	TIME [epoch: 12.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040979029742657824		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.040979029742657824 | validation: 0.04234797427643883]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035599014913242855		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.035599014913242855 | validation: 0.051044044232714594]
	TIME [epoch: 12.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05097544178786778		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.05097544178786778 | validation: 0.0497941537750491]
	TIME [epoch: 12.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04089521857100161		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.04089521857100161 | validation: 0.0543892584790852]
	TIME [epoch: 12.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04179116422801848		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.04179116422801848 | validation: 0.054928633607585424]
	TIME [epoch: 12.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03796208525229369		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.03796208525229369 | validation: 0.0425084685412573]
	TIME [epoch: 12.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05071508992114181		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.05071508992114181 | validation: 0.08050208135317105]
	TIME [epoch: 12.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04554554067497854		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.04554554067497854 | validation: 0.05780597808396584]
	TIME [epoch: 12.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036149267271158914		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.036149267271158914 | validation: 0.05285641149356776]
	TIME [epoch: 12.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05076137088962102		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.05076137088962102 | validation: 0.05582264810640519]
	TIME [epoch: 12.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039645924763999266		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.039645924763999266 | validation: 0.041336016438693265]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03443054199540141		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.03443054199540141 | validation: 0.04877181824978115]
	TIME [epoch: 12.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048833902510570855		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.048833902510570855 | validation: 0.04042070884932475]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037313318048986535		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.037313318048986535 | validation: 0.05161052762948534]
	TIME [epoch: 12.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037494817673667226		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.037494817673667226 | validation: 0.04622462609842247]
	TIME [epoch: 12.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04165069669958336		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.04165069669958336 | validation: 0.06438299071988943]
	TIME [epoch: 12.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0421185127311892		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.0421185127311892 | validation: 0.04267674763345597]
	TIME [epoch: 12.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031289592405489834		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.031289592405489834 | validation: 0.04959658924772107]
	TIME [epoch: 12.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04908320267128602		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.04908320267128602 | validation: 0.04494339395136433]
	TIME [epoch: 12.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03433109303332105		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.03433109303332105 | validation: 0.09257748533674229]
	TIME [epoch: 12.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050883999577771385		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.050883999577771385 | validation: 0.0460875385740791]
	TIME [epoch: 12.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03287516620875743		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.03287516620875743 | validation: 0.04020140396374771]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038096760811256236		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.038096760811256236 | validation: 0.06948670411604785]
	TIME [epoch: 12.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04977680602503901		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.04977680602503901 | validation: 0.067963082037578]
	TIME [epoch: 12.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041281350853025395		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.041281350853025395 | validation: 0.052166137670035626]
	TIME [epoch: 12.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03787581187584517		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.03787581187584517 | validation: 0.042101980822924816]
	TIME [epoch: 12.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04136885980719296		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.04136885980719296 | validation: 0.045468153569232186]
	TIME [epoch: 12.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032875380079835766		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.032875380079835766 | validation: 0.04402249251112114]
	TIME [epoch: 12.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04571384887381147		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.04571384887381147 | validation: 0.053470269992629244]
	TIME [epoch: 12.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040001000162795994		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.040001000162795994 | validation: 0.039871335571956164]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029737519605731254		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.029737519605731254 | validation: 0.04309955947791619]
	TIME [epoch: 12.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036274754005358345		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.036274754005358345 | validation: 0.04435867018883664]
	TIME [epoch: 12.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05349728108039816		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.05349728108039816 | validation: 0.05117179605140911]
	TIME [epoch: 12.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04010309032372492		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.04010309032372492 | validation: 0.04769257833249539]
	TIME [epoch: 12.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04138538229272143		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.04138538229272143 | validation: 0.05048703196971488]
	TIME [epoch: 12.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033059142924843216		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.033059142924843216 | validation: 0.047181157394300996]
	TIME [epoch: 413 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037809550597297875		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.037809550597297875 | validation: 0.05577518687600086]
	TIME [epoch: 26.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04725233436342202		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.04725233436342202 | validation: 0.04041426223597174]
	TIME [epoch: 26.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03296975816533589		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.03296975816533589 | validation: 0.05348593087494562]
	TIME [epoch: 26.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041525249527694603		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.041525249527694603 | validation: 0.038816864273091195]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03105134346798292		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.03105134346798292 | validation: 0.07821065864456944]
	TIME [epoch: 26.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04715477524942506		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.04715477524942506 | validation: 0.13653396834562578]
	TIME [epoch: 26.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11193498559469525		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.11193498559469525 | validation: 0.05791311078469431]
	TIME [epoch: 26.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03875223826005825		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.03875223826005825 | validation: 0.042480497477166465]
	TIME [epoch: 26.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031913481399475405		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.031913481399475405 | validation: 0.04507978219859825]
	TIME [epoch: 26.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04125638076572811		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.04125638076572811 | validation: 0.04882508171565315]
	TIME [epoch: 26.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03285820600230157		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.03285820600230157 | validation: 0.046371156310431275]
	TIME [epoch: 26.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039101171807625235		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.039101171807625235 | validation: 0.06121770290500146]
	TIME [epoch: 26.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05538851248626341		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.05538851248626341 | validation: 0.04793519135434762]
	TIME [epoch: 26.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03874524092756154		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.03874524092756154 | validation: 0.04329714258024019]
	TIME [epoch: 26.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03413557015476699		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.03413557015476699 | validation: 0.04797731687999795]
	TIME [epoch: 26.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034005163235540994		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.034005163235540994 | validation: 0.04675490668282367]
	TIME [epoch: 26.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03535041769879119		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.03535041769879119 | validation: 0.05017814571027206]
	TIME [epoch: 26.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03588192554568497		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.03588192554568497 | validation: 0.03986100030519075]
	TIME [epoch: 26.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04065235362132574		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.04065235362132574 | validation: 0.04470147840597791]
	TIME [epoch: 26.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0335684802771161		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.0335684802771161 | validation: 0.05880420057044132]
	TIME [epoch: 26.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03365451395091664		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.03365451395091664 | validation: 0.04183793639526342]
	TIME [epoch: 26.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03775703068412252		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.03775703068412252 | validation: 0.04384480853585741]
	TIME [epoch: 26.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05747417790054764		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.05747417790054764 | validation: 0.04407612441703959]
	TIME [epoch: 26.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03614715413230356		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.03614715413230356 | validation: 0.03798063998216397]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03052535916482173		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.03052535916482173 | validation: 0.05102916655736717]
	TIME [epoch: 26.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04091890070124358		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.04091890070124358 | validation: 0.051208816338675626]
	TIME [epoch: 26.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03488816233676005		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.03488816233676005 | validation: 0.040874221236715545]
	TIME [epoch: 26.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030589603151246616		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.030589603151246616 | validation: 0.04722108472604099]
	TIME [epoch: 26.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03313426865653306		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.03313426865653306 | validation: 0.04891251599718892]
	TIME [epoch: 26.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0463075709352503		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.0463075709352503 | validation: 0.03706882518001222]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03032506363273354		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.03032506363273354 | validation: 0.043235098276704104]
	TIME [epoch: 26.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037823376159508273		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.037823376159508273 | validation: 0.04187692719617802]
	TIME [epoch: 26.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03444789290764716		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.03444789290764716 | validation: 0.051795121164148]
	TIME [epoch: 26.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030722424559636252		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.030722424559636252 | validation: 0.045871138532996715]
	TIME [epoch: 26.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03918280142373409		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.03918280142373409 | validation: 0.04343749605819873]
	TIME [epoch: 26.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040787477972064126		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.040787477972064126 | validation: 0.03897025277537944]
	TIME [epoch: 26.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10919622784711866		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.10919622784711866 | validation: 0.09463814716043571]
	TIME [epoch: 26.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04715200057345521		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.04715200057345521 | validation: 0.04170486808281077]
	TIME [epoch: 26.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030926007135717005		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.030926007135717005 | validation: 0.03679442322211558]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030552205603848685		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.030552205603848685 | validation: 0.03819143252378688]
	TIME [epoch: 26.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035275430994930995		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.035275430994930995 | validation: 0.04149546362180724]
	TIME [epoch: 26.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02983201469634753		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.02983201469634753 | validation: 0.04589605764056643]
	TIME [epoch: 26.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03301915449850047		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.03301915449850047 | validation: 0.05114497937599658]
	TIME [epoch: 26.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03408751830663047		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.03408751830663047 | validation: 0.0411028676992754]
	TIME [epoch: 26.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04545618387105983		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.04545618387105983 | validation: 0.04394013725160245]
	TIME [epoch: 26.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03228260900820431		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.03228260900820431 | validation: 0.04451960665328164]
	TIME [epoch: 26.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03144339849185865		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.03144339849185865 | validation: 0.04559513387890393]
	TIME [epoch: 26.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030424346928766235		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.030424346928766235 | validation: 0.04652633677105279]
	TIME [epoch: 26.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04088213381010622		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.04088213381010622 | validation: 0.03976448297939782]
	TIME [epoch: 26.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031161490790582252		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.031161490790582252 | validation: 0.04366059541930037]
	TIME [epoch: 26.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039585361540669786		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.039585361540669786 | validation: 0.04893079396465368]
	TIME [epoch: 26.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03476151485388181		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.03476151485388181 | validation: 0.03951768425517387]
	TIME [epoch: 26.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02823475139627489		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.02823475139627489 | validation: 0.105251219645226]
	TIME [epoch: 26.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08360712620565393		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.08360712620565393 | validation: 0.04622953620232635]
	TIME [epoch: 26.2 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033515921521257507		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.033515921521257507 | validation: 0.04147738498313497]
	TIME [epoch: 26.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028875966332526107		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.028875966332526107 | validation: 0.03957774514223589]
	TIME [epoch: 26.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033064138524299756		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.033064138524299756 | validation: 0.05751990282511234]
	TIME [epoch: 26.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0429332471594214		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.0429332471594214 | validation: 0.042530209460678065]
	TIME [epoch: 26.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030803577618214468		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.030803577618214468 | validation: 0.04564610600526508]
	TIME [epoch: 26.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03307627963763185		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.03307627963763185 | validation: 0.03497321381799257]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02776545431083533		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.02776545431083533 | validation: 0.039730158165555585]
	TIME [epoch: 26.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034058458756089066		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.034058458756089066 | validation: 0.04415690238742222]
	TIME [epoch: 26.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03459053745893063		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.03459053745893063 | validation: 0.03504036642735145]
	TIME [epoch: 26.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03411058792068325		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.03411058792068325 | validation: 0.04873743362010755]
	TIME [epoch: 26.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03563757027534414		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.03563757027534414 | validation: 0.06174524993847787]
	TIME [epoch: 26.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043374400636659455		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.043374400636659455 | validation: 0.04342002707606118]
	TIME [epoch: 26.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03043075403710778		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.03043075403710778 | validation: 0.04194751819385968]
	TIME [epoch: 26.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03631372941549759		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.03631372941549759 | validation: 0.03901574754681256]
	TIME [epoch: 26.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030407403351280997		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.030407403351280997 | validation: 0.04875288070987226]
	TIME [epoch: 26.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030303909136830242		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.030303909136830242 | validation: 0.04065858473821817]
	TIME [epoch: 26.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03816338804824275		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.03816338804824275 | validation: 0.041370594257881865]
	TIME [epoch: 26.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032033156671144015		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.032033156671144015 | validation: 0.04639186434739287]
	TIME [epoch: 26.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03017284715355662		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.03017284715355662 | validation: 0.03862900825790699]
	TIME [epoch: 26.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033695352772315806		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.033695352772315806 | validation: 0.04366332076969026]
	TIME [epoch: 26.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03789079899203621		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.03789079899203621 | validation: 0.04124535006461624]
	TIME [epoch: 26.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027663901532340954		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.027663901532340954 | validation: 0.04471486186469311]
	TIME [epoch: 26.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03448062017785743		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.03448062017785743 | validation: 0.04243254008651774]
	TIME [epoch: 26.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029584917934093912		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.029584917934093912 | validation: 0.040381697587322574]
	TIME [epoch: 26.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033120309387909073		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.033120309387909073 | validation: 0.046814755069465516]
	TIME [epoch: 26.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04917820728358981		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.04917820728358981 | validation: 0.03962787427695895]
	TIME [epoch: 26.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033004529662656414		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.033004529662656414 | validation: 0.03658165135689756]
	TIME [epoch: 26.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027266403919922494		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.027266403919922494 | validation: 0.041082291898282525]
	TIME [epoch: 26.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028070863254306945		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.028070863254306945 | validation: 0.05239029215464396]
	TIME [epoch: 26.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03373046267231822		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.03373046267231822 | validation: 0.034983203822396634]
	TIME [epoch: 26.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033019607665091114		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.033019607665091114 | validation: 0.03830111291149013]
	TIME [epoch: 26.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03099597333929224		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.03099597333929224 | validation: 0.04525769455187177]
	TIME [epoch: 26.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03212105274225437		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.03212105274225437 | validation: 0.04358311428379304]
	TIME [epoch: 26.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030597398443717935		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.030597398443717935 | validation: 0.052212580306934]
	TIME [epoch: 26.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03067064240500672		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.03067064240500672 | validation: 0.04187941253754324]
	TIME [epoch: 26.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03301512505561568		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.03301512505561568 | validation: 0.037353044886340676]
	TIME [epoch: 26.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03675205319930212		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.03675205319930212 | validation: 0.04201358211045173]
	TIME [epoch: 26.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027562375386813973		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.027562375386813973 | validation: 0.040222002249746536]
	TIME [epoch: 26.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03020658933843292		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.03020658933843292 | validation: 0.04053661695945193]
	TIME [epoch: 26.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03374220838418941		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.03374220838418941 | validation: 0.04699799379151326]
	TIME [epoch: 26.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03151767974537424		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.03151767974537424 | validation: 0.04582461430206598]
	TIME [epoch: 26.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03202342897897956		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.03202342897897956 | validation: 0.05084176340147703]
	TIME [epoch: 26.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030829987848183346		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.030829987848183346 | validation: 0.03948757250291838]
	TIME [epoch: 26.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031156132787224632		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.031156132787224632 | validation: 0.03719915967356717]
	TIME [epoch: 26.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031288807687136565		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.031288807687136565 | validation: 0.055482706834703716]
	TIME [epoch: 26.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030118143718854795		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.030118143718854795 | validation: 0.038185133896464975]
	TIME [epoch: 26.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03081038046322794		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.03081038046322794 | validation: 0.0439082268525523]
	TIME [epoch: 26.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036742899635396044		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.036742899635396044 | validation: 0.038209339984233856]
	TIME [epoch: 26.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03043816523740013		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.03043816523740013 | validation: 0.03866922859835229]
	TIME [epoch: 26 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03064183651131877		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.03064183651131877 | validation: 0.04376820508438098]
	TIME [epoch: 26.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028136138245126403		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.028136138245126403 | validation: 0.03808445643357335]
	TIME [epoch: 26.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029110949471859736		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.029110949471859736 | validation: 0.05672402825681111]
	TIME [epoch: 26.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05316328467304455		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.05316328467304455 | validation: 0.05157793347646769]
	TIME [epoch: 26.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032311041695900905		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.032311041695900905 | validation: 0.04881799639523418]
	TIME [epoch: 26.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030723287601757843		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.030723287601757843 | validation: 0.03677621346270272]
	TIME [epoch: 26.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028710122744562323		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.028710122744562323 | validation: 0.0390457250731024]
	TIME [epoch: 26.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028805356527310567		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.028805356527310567 | validation: 0.04062297090497209]
	TIME [epoch: 26.2 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035778212176371595		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.035778212176371595 | validation: 0.038872804993061824]
	TIME [epoch: 26.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02771690992527782		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.02771690992527782 | validation: 0.03580824530332682]
	TIME [epoch: 26.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028604843688976013		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.028604843688976013 | validation: 0.03972199213700184]
	TIME [epoch: 26.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03095926888587172		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.03095926888587172 | validation: 0.03773158072069431]
	TIME [epoch: 26.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0269053261196631		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.0269053261196631 | validation: 0.03610871246531946]
	TIME [epoch: 26.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030747639043954653		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.030747639043954653 | validation: 0.04573814117533792]
	TIME [epoch: 26.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03367321595589665		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.03367321595589665 | validation: 0.035276054643668675]
	TIME [epoch: 26.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02669657000123364		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.02669657000123364 | validation: 0.034891851810659194]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0326839822646048		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.0326839822646048 | validation: 0.04078129156187213]
	TIME [epoch: 26.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029873955584636354		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.029873955584636354 | validation: 0.03746982156085476]
	TIME [epoch: 26.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026416419303550744		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.026416419303550744 | validation: 0.03477957758908055]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03307896864281869		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.03307896864281869 | validation: 0.037697285725680894]
	TIME [epoch: 26.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03316963932924724		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.03316963932924724 | validation: 0.0459424961776752]
	TIME [epoch: 26.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03270853972356373		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.03270853972356373 | validation: 0.035651926457301066]
	TIME [epoch: 26.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02699999994911858		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.02699999994911858 | validation: 0.03480103914934929]
	TIME [epoch: 26.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02837405770295762		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.02837405770295762 | validation: 0.035556701693430684]
	TIME [epoch: 26.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028047479161928998		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.028047479161928998 | validation: 0.03677984750888182]
	TIME [epoch: 26.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03141772846169966		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.03141772846169966 | validation: 0.03955283405051095]
	TIME [epoch: 26.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029738228907337582		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.029738228907337582 | validation: 0.035433433226547024]
	TIME [epoch: 26.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027535652018317418		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.027535652018317418 | validation: 0.03943997706048942]
	TIME [epoch: 26.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030509937422645586		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.030509937422645586 | validation: 0.04460755037748612]
	TIME [epoch: 26.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030184859257324035		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.030184859257324035 | validation: 0.03578786526133809]
	TIME [epoch: 26.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02779880588368759		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.02779880588368759 | validation: 0.03602118925676635]
	TIME [epoch: 26.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026196464524864217		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.026196464524864217 | validation: 0.037662526360139136]
	TIME [epoch: 26.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02930683517253558		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.02930683517253558 | validation: 0.034897456603947975]
	TIME [epoch: 26.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029365431381797195		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.029365431381797195 | validation: 0.03739378886978177]
	TIME [epoch: 26.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028443551723769854		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.028443551723769854 | validation: 0.03601230827927031]
	TIME [epoch: 26.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028263750098573494		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.028263750098573494 | validation: 0.03574020547643732]
	TIME [epoch: 26.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145648994326232		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.03145648994326232 | validation: 0.03889651482863872]
	TIME [epoch: 26.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03283584829726549		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.03283584829726549 | validation: 0.04998447126966044]
	TIME [epoch: 26.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03048781271881122		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.03048781271881122 | validation: 0.04223779815677116]
	TIME [epoch: 26.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029653411119455922		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.029653411119455922 | validation: 0.03827877129886076]
	TIME [epoch: 26.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027309370156770325		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.027309370156770325 | validation: 0.03898928503810179]
	TIME [epoch: 26.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028622714316669635		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.028622714316669635 | validation: 0.03831843468471591]
	TIME [epoch: 26.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028169333980391408		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.028169333980391408 | validation: 0.04079159130929455]
	TIME [epoch: 26.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02932071170458101		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.02932071170458101 | validation: 0.033341737556701265]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026482972145743824		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.026482972145743824 | validation: 0.03589826724269976]
	TIME [epoch: 26.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029207309578780417		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.029207309578780417 | validation: 0.03706145157983839]
	TIME [epoch: 26.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027000814803000168		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.027000814803000168 | validation: 0.04164547745748555]
	TIME [epoch: 26.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02821408380100291		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.02821408380100291 | validation: 0.03564757453609862]
	TIME [epoch: 26.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030372187749785465		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.030372187749785465 | validation: 0.0409517583289038]
	TIME [epoch: 26.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033886441157371626		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.033886441157371626 | validation: 0.03466988409071623]
	TIME [epoch: 26.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02488301525306536		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.02488301525306536 | validation: 0.0325042721829786]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026717957593673396		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.026717957593673396 | validation: 0.03488575454796311]
	TIME [epoch: 26.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02597523903618871		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.02597523903618871 | validation: 0.035250625551888806]
	TIME [epoch: 26.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027699228598884927		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.027699228598884927 | validation: 0.03592296212590909]
	TIME [epoch: 26.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034234952013800314		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.034234952013800314 | validation: 0.043244488647758476]
	TIME [epoch: 26.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028825424232396236		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.028825424232396236 | validation: 0.03323675065261035]
	TIME [epoch: 26.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02799277855628064		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.02799277855628064 | validation: 0.03794042638345879]
	TIME [epoch: 26.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02560049452677598		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.02560049452677598 | validation: 0.0331507259643924]
	TIME [epoch: 26.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02684478073553732		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.02684478073553732 | validation: 0.03898691425363849]
	TIME [epoch: 26.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03193781339248766		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.03193781339248766 | validation: 0.03770882987379451]
	TIME [epoch: 26.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02608971688678994		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.02608971688678994 | validation: 0.04089368513336085]
	TIME [epoch: 26.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025589021736012466		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.025589021736012466 | validation: 0.036679981909409146]
	TIME [epoch: 26.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03502961539641947		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.03502961539641947 | validation: 0.06069739788229036]
	TIME [epoch: 26.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03660235988976706		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.03660235988976706 | validation: 0.033352652763559254]
	TIME [epoch: 26.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023551110466795894		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.023551110466795894 | validation: 0.033220700864737024]
	TIME [epoch: 26.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02799786982511027		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.02799786982511027 | validation: 0.035681675658075326]
	TIME [epoch: 26.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02731058632648932		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.02731058632648932 | validation: 0.03620333468629745]
	TIME [epoch: 26.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02466865286575403		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.02466865286575403 | validation: 0.03543990996943762]
	TIME [epoch: 26.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028371607967602568		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.028371607967602568 | validation: 0.05016726944937158]
	TIME [epoch: 26.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028183316022905504		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.028183316022905504 | validation: 0.03558574603715019]
	TIME [epoch: 26.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02433581219227371		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.02433581219227371 | validation: 0.039308426548093564]
	TIME [epoch: 26.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02980882799036264		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.02980882799036264 | validation: 0.03280738816197181]
	TIME [epoch: 26.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02680620960018192		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.02680620960018192 | validation: 0.032871960102394494]
	TIME [epoch: 26.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025710897034079694		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.025710897034079694 | validation: 0.0347006500522141]
	TIME [epoch: 26.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02579639909212357		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.02579639909212357 | validation: 0.03308210426850821]
	TIME [epoch: 26.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024436954514509994		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.024436954514509994 | validation: 0.03714952063562945]
	TIME [epoch: 26.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027968851415804998		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.027968851415804998 | validation: 0.04288653725707689]
	TIME [epoch: 26.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02579609216425822		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.02579609216425822 | validation: 0.034623023855358315]
	TIME [epoch: 26.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024329899158022597		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.024329899158022597 | validation: 0.036594810021609844]
	TIME [epoch: 26.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027007239392243428		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.027007239392243428 | validation: 0.03580262558697517]
	TIME [epoch: 26.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025828426926845782		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.025828426926845782 | validation: 0.0352630935842389]
	TIME [epoch: 26.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029381625550221585		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.029381625550221585 | validation: 0.03952134663428106]
	TIME [epoch: 26.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026111154301954265		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.026111154301954265 | validation: 0.034624802148478157]
	TIME [epoch: 26.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02645837836644906		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.02645837836644906 | validation: 0.03414462335219248]
	TIME [epoch: 26.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02510084580753902		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.02510084580753902 | validation: 0.033411143026048466]
	TIME [epoch: 26.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026270602629862316		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.026270602629862316 | validation: 0.038121028789027706]
	TIME [epoch: 26.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028108798285666297		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.028108798285666297 | validation: 0.0354599658171718]
	TIME [epoch: 26.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02608691084144148		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.02608691084144148 | validation: 0.031199880671629418]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02549923580694778		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.02549923580694778 | validation: 0.03213567224540961]
	TIME [epoch: 26.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024398487499375596		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.024398487499375596 | validation: 0.03800129282615897]
	TIME [epoch: 26.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02784202836272147		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.02784202836272147 | validation: 0.03804208154849721]
	TIME [epoch: 26.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02579064607315246		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.02579064607315246 | validation: 0.04015229998173105]
	TIME [epoch: 26.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028609599841381853		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.028609599841381853 | validation: 0.037369876074725694]
	TIME [epoch: 26.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0269557998966328		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.0269557998966328 | validation: 0.04590149916415462]
	TIME [epoch: 26.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031198353272642657		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.031198353272642657 | validation: 0.03410357556748599]
	TIME [epoch: 26.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024723313386575317		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.024723313386575317 | validation: 0.031008306404532424]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022475481699568936		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.022475481699568936 | validation: 0.032684287129026705]
	TIME [epoch: 26.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023786310338762745		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.023786310338762745 | validation: 0.035388533753087384]
	TIME [epoch: 26.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028478428422308112		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.028478428422308112 | validation: 0.03912943737680268]
	TIME [epoch: 26.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02648520505287169		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.02648520505287169 | validation: 0.036776367389123626]
	TIME [epoch: 26.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027091377737094666		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.027091377737094666 | validation: 0.038136761935419775]
	TIME [epoch: 26.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024064214166860422		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.024064214166860422 | validation: 0.032066134259082495]
	TIME [epoch: 26.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02335751931372003		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.02335751931372003 | validation: 0.03413334134105869]
	TIME [epoch: 26.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024378952579486073		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.024378952579486073 | validation: 0.03320196186686542]
	TIME [epoch: 26.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025349821776221883		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.025349821776221883 | validation: 0.03511472027835219]
	TIME [epoch: 26.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030394263996800425		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.030394263996800425 | validation: 0.03701745193892991]
	TIME [epoch: 26 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025005515056100686		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.025005515056100686 | validation: 0.0316706049368686]
	TIME [epoch: 26.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025700877088348707		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.025700877088348707 | validation: 0.03499694505325754]
	TIME [epoch: 26.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023944654472621417		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.023944654472621417 | validation: 0.03404418839572242]
	TIME [epoch: 26.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025411422054511745		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.025411422054511745 | validation: 0.04003452726610336]
	TIME [epoch: 26.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025386088598048855		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.025386088598048855 | validation: 0.032697388217957424]
	TIME [epoch: 26.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02305814036720277		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.02305814036720277 | validation: 0.0336736119868566]
	TIME [epoch: 26.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024318322271317354		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.024318322271317354 | validation: 0.03608222503800071]
	TIME [epoch: 26.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024729388368627262		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.024729388368627262 | validation: 0.03252122378089146]
	TIME [epoch: 26.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027283845055292348		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.027283845055292348 | validation: 0.042243090288404145]
	TIME [epoch: 26.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02425406560698582		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.02425406560698582 | validation: 0.035088213289124544]
	TIME [epoch: 26.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027107914472143675		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.027107914472143675 | validation: 0.03300682229104845]
	TIME [epoch: 26.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024010855410773163		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.024010855410773163 | validation: 0.03591181388999503]
	TIME [epoch: 26.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025548932968858712		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.025548932968858712 | validation: 0.03730571876027608]
	TIME [epoch: 26.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024056294260615705		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.024056294260615705 | validation: 0.03338159621625794]
	TIME [epoch: 26.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023601002625745143		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.023601002625745143 | validation: 0.03185548038879491]
	TIME [epoch: 26.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02446530905567283		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.02446530905567283 | validation: 0.036582825429626334]
	TIME [epoch: 26.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023921290984134994		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.023921290984134994 | validation: 0.03570811000908786]
	TIME [epoch: 26.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024513000586066612		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.024513000586066612 | validation: 0.03579811419468149]
	TIME [epoch: 26.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026860555182132484		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.026860555182132484 | validation: 0.0316695232493843]
	TIME [epoch: 26.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024772549172037597		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.024772549172037597 | validation: 0.034119047063146474]
	TIME [epoch: 26.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024017865361405214		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.024017865361405214 | validation: 0.03307296394363806]
	TIME [epoch: 26.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023724250313077396		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.023724250313077396 | validation: 0.03805752316663313]
	TIME [epoch: 26.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02598257214623595		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.02598257214623595 | validation: 0.03437636273067891]
	TIME [epoch: 26.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025609733662363715		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.025609733662363715 | validation: 0.03612550950190739]
	TIME [epoch: 26.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02508163544090102		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.02508163544090102 | validation: 0.035602913788104276]
	TIME [epoch: 26.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02495506272257559		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.02495506272257559 | validation: 0.030505609686159663]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02375247558273667		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.02375247558273667 | validation: 0.037466152795519514]
	TIME [epoch: 26.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023810327005033532		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.023810327005033532 | validation: 0.03303900304978244]
	TIME [epoch: 26.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02417185656606296		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.02417185656606296 | validation: 0.03280055362919681]
	TIME [epoch: 26 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024700150848953193		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.024700150848953193 | validation: 0.03218771090188055]
	TIME [epoch: 26.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025764401296507016		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.025764401296507016 | validation: 0.03143892839628498]
	TIME [epoch: 26.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024658366947295014		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.024658366947295014 | validation: 0.035600019759043915]
	TIME [epoch: 26.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023766501412992854		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.023766501412992854 | validation: 0.03582443161992532]
	TIME [epoch: 26.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02624701576080718		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.02624701576080718 | validation: 0.032665536693993094]
	TIME [epoch: 26.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02363679671919907		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.02363679671919907 | validation: 0.031631416423603266]
	TIME [epoch: 26.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024168711154803763		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.024168711154803763 | validation: 0.03019157978893992]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023386944113398027		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.023386944113398027 | validation: 0.033603425723449334]
	TIME [epoch: 26.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025258021514713267		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.025258021514713267 | validation: 0.03086359753002737]
	TIME [epoch: 26.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02324321452070363		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.02324321452070363 | validation: 0.03343043093898398]
	TIME [epoch: 26.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024647586541939666		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.024647586541939666 | validation: 0.03559584734084422]
	TIME [epoch: 26.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024184186480930417		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.024184186480930417 | validation: 0.032904070425772876]
	TIME [epoch: 26.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023544646961148174		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.023544646961148174 | validation: 0.03469314246238618]
	TIME [epoch: 26.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02240515347221095		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.02240515347221095 | validation: 0.029700088998715485]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02412898649164284		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.02412898649164284 | validation: 0.033617544793112304]
	TIME [epoch: 26 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02314659728376213		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.02314659728376213 | validation: 0.032401557577261784]
	TIME [epoch: 26.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024216241213228847		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.024216241213228847 | validation: 0.03245658293179376]
	TIME [epoch: 26.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023369764623407792		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.023369764623407792 | validation: 0.036010355114481904]
	TIME [epoch: 26.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030273692629008715		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.030273692629008715 | validation: 0.03172223105697403]
	TIME [epoch: 26.1 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024631540331126255		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.024631540331126255 | validation: 0.037841250724832784]
	TIME [epoch: 26.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023867449729145666		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.023867449729145666 | validation: 0.031149260515259816]
	TIME [epoch: 26.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023454566789390813		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.023454566789390813 | validation: 0.03916831730596179]
	TIME [epoch: 26.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023817978880524816		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.023817978880524816 | validation: 0.031518789047896546]
	TIME [epoch: 26.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02475276757414372		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.02475276757414372 | validation: 0.03160700748566867]
	TIME [epoch: 26.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022959972678937526		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.022959972678937526 | validation: 0.031850398601909954]
	TIME [epoch: 26.1 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02348781854887025		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.02348781854887025 | validation: 0.031214529465774847]
	TIME [epoch: 26.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02329699465221302		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.02329699465221302 | validation: 0.03016767101132381]
	TIME [epoch: 26.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023412794466361353		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.023412794466361353 | validation: 0.0404721099166212]
	TIME [epoch: 26.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026302598091741156		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.026302598091741156 | validation: 0.0319194755439643]
	TIME [epoch: 26.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02292969533466518		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.02292969533466518 | validation: 0.032355359682179674]
	TIME [epoch: 26.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02127470041154068		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.02127470041154068 | validation: 0.033267190181370455]
	TIME [epoch: 26.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023950739005892816		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.023950739005892816 | validation: 0.030147871289508674]
	TIME [epoch: 26.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024139504345376518		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.024139504345376518 | validation: 0.029409418841287907]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023665604750344238		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.023665604750344238 | validation: 0.03403596252706472]
	TIME [epoch: 26.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022111328368027115		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.022111328368027115 | validation: 0.03312662368108286]
	TIME [epoch: 26.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02414491441647002		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.02414491441647002 | validation: 0.03321289390900379]
	TIME [epoch: 26.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02352831084715956		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.02352831084715956 | validation: 0.03318469203321829]
	TIME [epoch: 26.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02171195995988932		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.02171195995988932 | validation: 0.034774005227859714]
	TIME [epoch: 26.1 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02276190734417746		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.02276190734417746 | validation: 0.03089604570434068]
	TIME [epoch: 26.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02314919318055975		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.02314919318055975 | validation: 0.0389173612584992]
	TIME [epoch: 26.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02191029142522486		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.02191029142522486 | validation: 0.03156696714814086]
	TIME [epoch: 26 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023226434756650965		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.023226434756650965 | validation: 0.03201750397153302]
	TIME [epoch: 26.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022328240334560105		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.022328240334560105 | validation: 0.03002899982426168]
	TIME [epoch: 26.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022505293224993995		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.022505293224993995 | validation: 0.030799641774968065]
	TIME [epoch: 26.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023727021546121493		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.023727021546121493 | validation: 0.036612604213010755]
	TIME [epoch: 26.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02270970610049295		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.02270970610049295 | validation: 0.031736327970734446]
	TIME [epoch: 26.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022848774396024247		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.022848774396024247 | validation: 0.03320388881890449]
	TIME [epoch: 26.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023371133450465986		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.023371133450465986 | validation: 0.030107400201996506]
	TIME [epoch: 26.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021977414888543777		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.021977414888543777 | validation: 0.030617949300539016]
	TIME [epoch: 26.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022739946028801267		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.022739946028801267 | validation: 0.029667509948639825]
	TIME [epoch: 26.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021239885716392334		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.021239885716392334 | validation: 0.03271143840657598]
	TIME [epoch: 26.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023555036971948173		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.023555036971948173 | validation: 0.043279519038488104]
	TIME [epoch: 26.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023267820827811043		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.023267820827811043 | validation: 0.03247777842361507]
	TIME [epoch: 26.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026038296734269207		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.026038296734269207 | validation: 0.036002258393381295]
	TIME [epoch: 26.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023535088582029372		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.023535088582029372 | validation: 0.031546476341705244]
	TIME [epoch: 26.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021375264087215795		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.021375264087215795 | validation: 0.03059383525682597]
	TIME [epoch: 26.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022149404526174386		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.022149404526174386 | validation: 0.03151889299455806]
	TIME [epoch: 26.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02372367628947957		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.02372367628947957 | validation: 0.03184663004575834]
	TIME [epoch: 26.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02144258743855662		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.02144258743855662 | validation: 0.029604882405475506]
	TIME [epoch: 26.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023013649134396654		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.023013649134396654 | validation: 0.03037659851922797]
	TIME [epoch: 26.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022316728700863646		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.022316728700863646 | validation: 0.030075945207367137]
	TIME [epoch: 26.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023670810436569344		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.023670810436569344 | validation: 0.032247129985196736]
	TIME [epoch: 26.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021980811305010513		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.021980811305010513 | validation: 0.030084641046893644]
	TIME [epoch: 26.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023157761118988658		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.023157761118988658 | validation: 0.029537904757510523]
	TIME [epoch: 26 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023097208655754105		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.023097208655754105 | validation: 0.030143047621842053]
	TIME [epoch: 26.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0233612855891151		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.0233612855891151 | validation: 0.031139216530231682]
	TIME [epoch: 26 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020529175902465075		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.020529175902465075 | validation: 0.03144134640322027]
	TIME [epoch: 26.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022787237914812707		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.022787237914812707 | validation: 0.0351046189447666]
	TIME [epoch: 26.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022496831249909057		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.022496831249909057 | validation: 0.030786478919245144]
	TIME [epoch: 26.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02263376006037282		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.02263376006037282 | validation: 0.03403749279006664]
	TIME [epoch: 26.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02242522174623589		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.02242522174623589 | validation: 0.029843695718260614]
	TIME [epoch: 26.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021169133641045525		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.021169133641045525 | validation: 0.03298185398675347]
	TIME [epoch: 26.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02109755418460802		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.02109755418460802 | validation: 0.03203171744359297]
	TIME [epoch: 26.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02295642494080578		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.02295642494080578 | validation: 0.03189868987053389]
	TIME [epoch: 26.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021840166130227545		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.021840166130227545 | validation: 0.032441077310287246]
	TIME [epoch: 26.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02369507765205706		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.02369507765205706 | validation: 0.03243072799027712]
	TIME [epoch: 26.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021788759091305607		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.021788759091305607 | validation: 0.04006496566880778]
	TIME [epoch: 26.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022783156362674112		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.022783156362674112 | validation: 0.030471079429135087]
	TIME [epoch: 26.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023062295364130323		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.023062295364130323 | validation: 0.029846284846186587]
	TIME [epoch: 26.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02195980730317644		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.02195980730317644 | validation: 0.031661682429618795]
	TIME [epoch: 26.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021764460445944392		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.021764460445944392 | validation: 0.03057878936009128]
	TIME [epoch: 26.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021007701939875355		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.021007701939875355 | validation: 0.03145274890051981]
	TIME [epoch: 26.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024957501079585425		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.024957501079585425 | validation: 0.03893687089978339]
	TIME [epoch: 26.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02317346225399323		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.02317346225399323 | validation: 0.033235349699314305]
	TIME [epoch: 26.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022674467008090594		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.022674467008090594 | validation: 0.03052598141198256]
	TIME [epoch: 26.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021527560995862945		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.021527560995862945 | validation: 0.032628245738163406]
	TIME [epoch: 26.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022752747352369394		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.022752747352369394 | validation: 0.030290593531783847]
	TIME [epoch: 26.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021147450551426084		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.021147450551426084 | validation: 0.03469123417552709]
	TIME [epoch: 26.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023375232409409023		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.023375232409409023 | validation: 0.03242478536844719]
	TIME [epoch: 26.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021205303798386976		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.021205303798386976 | validation: 0.030566097965465118]
	TIME [epoch: 26.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021242482592012273		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.021242482592012273 | validation: 0.04580579080497714]
	TIME [epoch: 26.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024537678333230555		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.024537678333230555 | validation: 0.030569822031710065]
	TIME [epoch: 26.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021488935946401545		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.021488935946401545 | validation: 0.03111482057321428]
	TIME [epoch: 26.1 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021042364653359496		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.021042364653359496 | validation: 0.03144740294354993]
	TIME [epoch: 26.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022087235737187225		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.022087235737187225 | validation: 0.03239122246795908]
	TIME [epoch: 26.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021452293949735718		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.021452293949735718 | validation: 0.03094007396851948]
	TIME [epoch: 26.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021748428885878107		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.021748428885878107 | validation: 0.034725074859867144]
	TIME [epoch: 26 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02120240166323056		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.02120240166323056 | validation: 0.028942607302963858]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022187291538209024		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.022187291538209024 | validation: 0.029067852456554504]
	TIME [epoch: 26.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021181383539973606		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.021181383539973606 | validation: 0.0374124715785283]
	TIME [epoch: 26.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022458311985095146		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.022458311985095146 | validation: 0.029812490928315977]
	TIME [epoch: 26.1 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020915907323143555		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.020915907323143555 | validation: 0.030299728165362933]
	TIME [epoch: 26.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020206388111586145		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.020206388111586145 | validation: 0.03143887303450118]
	TIME [epoch: 26 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02190893404600705		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.02190893404600705 | validation: 0.032043661710658165]
	TIME [epoch: 26.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02097543378606003		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.02097543378606003 | validation: 0.03363557339159974]
	TIME [epoch: 26.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021767957684894625		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.021767957684894625 | validation: 0.030375822311402123]
	TIME [epoch: 26.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021216848831911616		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.021216848831911616 | validation: 0.029923762279552756]
	TIME [epoch: 26.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0211548762126069		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.0211548762126069 | validation: 0.03459076679219089]
	TIME [epoch: 26.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021632172918551133		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.021632172918551133 | validation: 0.03079368099570233]
	TIME [epoch: 26.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021905319461157157		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.021905319461157157 | validation: 0.02899964496958661]
	TIME [epoch: 26.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020356732255588355		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.020356732255588355 | validation: 0.028590841573214343]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021540519378804492		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.021540519378804492 | validation: 0.027324097796093495]
	TIME [epoch: 26.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021328718576161518		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.021328718576161518 | validation: 0.03561109264709339]
	TIME [epoch: 26.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021371099636740873		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.021371099636740873 | validation: 0.03369640907188979]
	TIME [epoch: 26.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021325610335358403		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.021325610335358403 | validation: 0.03184476734843253]
	TIME [epoch: 26.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021485648414982003		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.021485648414982003 | validation: 0.029425060380197557]
	TIME [epoch: 26.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020658423935781847		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.020658423935781847 | validation: 0.033862128157218584]
	TIME [epoch: 26.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021274684141010154		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.021274684141010154 | validation: 0.02964144255449954]
	TIME [epoch: 26.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02066924152084659		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.02066924152084659 | validation: 0.028728243842227465]
	TIME [epoch: 26.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02073616029733164		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.02073616029733164 | validation: 0.031591354298291366]
	TIME [epoch: 26.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022083857115623277		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.022083857115623277 | validation: 0.030505366496770124]
	TIME [epoch: 26.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021555934967200645		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.021555934967200645 | validation: 0.027448467802584357]
	TIME [epoch: 26.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02015932162959947		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.02015932162959947 | validation: 0.030198723164863932]
	TIME [epoch: 26.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020360124163904033		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.020360124163904033 | validation: 0.032987752863530756]
	TIME [epoch: 26.1 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028194854801428963		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.028194854801428963 | validation: 0.031884788365045845]
	TIME [epoch: 26.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021873058892215767		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.021873058892215767 | validation: 0.028164977091547985]
	TIME [epoch: 26.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021371339843368517		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.021371339843368517 | validation: 0.027610633192235026]
	TIME [epoch: 26.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01982803169981212		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.01982803169981212 | validation: 0.029597416710720553]
	TIME [epoch: 26.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020631898443435166		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.020631898443435166 | validation: 0.03087155073933678]
	TIME [epoch: 26.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020352538449967494		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.020352538449967494 | validation: 0.03138990874797369]
	TIME [epoch: 26.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022643429704546227		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.022643429704546227 | validation: 0.028217306180369578]
	TIME [epoch: 26.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020425288679423413		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.020425288679423413 | validation: 0.0301452329818325]
	TIME [epoch: 26.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020703524645376586		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.020703524645376586 | validation: 0.030573323061799825]
	TIME [epoch: 26.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020732423311022815		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.020732423311022815 | validation: 0.028025005279799557]
	TIME [epoch: 26.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021401095322113595		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.021401095322113595 | validation: 0.02765703803665965]
	TIME [epoch: 26.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02033390222838796		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.02033390222838796 | validation: 0.036292106790310634]
	TIME [epoch: 26.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02261380339264433		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.02261380339264433 | validation: 0.0288648083063191]
	TIME [epoch: 26.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021494533094527105		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.021494533094527105 | validation: 0.030835468406073863]
	TIME [epoch: 26.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02111564980001235		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.02111564980001235 | validation: 0.029312095555375617]
	TIME [epoch: 26 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020239931762276557		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.020239931762276557 | validation: 0.02825394909179979]
	TIME [epoch: 26 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019962017559698016		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.019962017559698016 | validation: 0.029330738215792813]
	TIME [epoch: 26.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020217952426985222		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.020217952426985222 | validation: 0.03151328937955515]
	TIME [epoch: 26.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0249584711537403		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0249584711537403 | validation: 0.03434854896670548]
	TIME [epoch: 26.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022102894163577386		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.022102894163577386 | validation: 0.030005660723473396]
	TIME [epoch: 26.1 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02075548445916158		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.02075548445916158 | validation: 0.03166638136494236]
	TIME [epoch: 26 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020763378460326914		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.020763378460326914 | validation: 0.027989072393526666]
	TIME [epoch: 26.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01988335924480996		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.01988335924480996 | validation: 0.030194370783014346]
	TIME [epoch: 26.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020012850810933635		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.020012850810933635 | validation: 0.02988339379043474]
	TIME [epoch: 26.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020612598901247534		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.020612598901247534 | validation: 0.03476484941978408]
	TIME [epoch: 26.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020881657772851607		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.020881657772851607 | validation: 0.03128065496999841]
	TIME [epoch: 26.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01929697263827518		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.01929697263827518 | validation: 0.03261136262133198]
	TIME [epoch: 26.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02538311754557954		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.02538311754557954 | validation: 0.0371509246621994]
	TIME [epoch: 26.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024107416822332813		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.024107416822332813 | validation: 0.030019659385919602]
	TIME [epoch: 26.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020916732484585084		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.020916732484585084 | validation: 0.028810734739449582]
	TIME [epoch: 26.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02143574631988348		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.02143574631988348 | validation: 0.030299830210471633]
	TIME [epoch: 26.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019932751484611426		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.019932751484611426 | validation: 0.029371491184056644]
	TIME [epoch: 26.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021041362291495128		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.021041362291495128 | validation: 0.03248153638496062]
	TIME [epoch: 26.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02078358642747101		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.02078358642747101 | validation: 0.031448828591218114]
	TIME [epoch: 26.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019714289657518293		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.019714289657518293 | validation: 0.029795794673595048]
	TIME [epoch: 26.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019352869663753627		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.019352869663753627 | validation: 0.02995802463723879]
	TIME [epoch: 26.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02023191303097974		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.02023191303097974 | validation: 0.03104545236898065]
	TIME [epoch: 26.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020343344749617857		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.020343344749617857 | validation: 0.03259438543081905]
	TIME [epoch: 26.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020173663186039317		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.020173663186039317 | validation: 0.02932554053008666]
	TIME [epoch: 26.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01976662708273058		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.01976662708273058 | validation: 0.031150852523990833]
	TIME [epoch: 26.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021661774253100508		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.021661774253100508 | validation: 0.030157622859392746]
	TIME [epoch: 26.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02057552368065755		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.02057552368065755 | validation: 0.02968022202987062]
	TIME [epoch: 26.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020137807642890172		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.020137807642890172 | validation: 0.030232064196622402]
	TIME [epoch: 26.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019973164466943798		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.019973164466943798 | validation: 0.030346905354035947]
	TIME [epoch: 26 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021552791779791657		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.021552791779791657 | validation: 0.030514383788577543]
	TIME [epoch: 26.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01946868051149867		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.01946868051149867 | validation: 0.030586653586365864]
	TIME [epoch: 26 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020319672479669555		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.020319672479669555 | validation: 0.029764197103403053]
	TIME [epoch: 26.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020453099905429		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.020453099905429 | validation: 0.029116947932911925]
	TIME [epoch: 26.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01959772184071243		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.01959772184071243 | validation: 0.03061088798704506]
	TIME [epoch: 26 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02634837293747894		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.02634837293747894 | validation: 0.03290427031513646]
	TIME [epoch: 26.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021998392949768165		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.021998392949768165 | validation: 0.02916167216313269]
	TIME [epoch: 26.1 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020822866375643494		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.020822866375643494 | validation: 0.02861408191235531]
	TIME [epoch: 26.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020266072506555433		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.020266072506555433 | validation: 0.029585777451045868]
	TIME [epoch: 26.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019587952356626186		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.019587952356626186 | validation: 0.028819131827141027]
	TIME [epoch: 26 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019308670167418333		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.019308670167418333 | validation: 0.028824200248385015]
	TIME [epoch: 26.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020082462764311037		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.020082462764311037 | validation: 0.03034783982408782]
	TIME [epoch: 26.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020422834329558423		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.020422834329558423 | validation: 0.028130632666706713]
	TIME [epoch: 26.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02071454712984628		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.02071454712984628 | validation: 0.030612538043581633]
	TIME [epoch: 26.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019997998704356392		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.019997998704356392 | validation: 0.03300122581487661]
	TIME [epoch: 26 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022346139023982817		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.022346139023982817 | validation: 0.02908773582612945]
	TIME [epoch: 26.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01964253163854931		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.01964253163854931 | validation: 0.026749930477733247]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020478985687430798		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.020478985687430798 | validation: 0.027871624356575093]
	TIME [epoch: 26.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01935688273385216		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.01935688273385216 | validation: 0.0292256056082848]
	TIME [epoch: 26.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01974572976002603		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.01974572976002603 | validation: 0.027058593823905963]
	TIME [epoch: 26.1 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020231344005939476		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.020231344005939476 | validation: 0.039023814664319756]
	TIME [epoch: 26.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02449236876868817		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.02449236876868817 | validation: 0.028423733042643014]
	TIME [epoch: 26.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020348550748005522		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.020348550748005522 | validation: 0.029846301018359923]
	TIME [epoch: 26.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020095342616398927		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.020095342616398927 | validation: 0.02737742567029289]
	TIME [epoch: 26.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01929538272883692		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.01929538272883692 | validation: 0.029648436849779053]
	TIME [epoch: 26.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01955268256320628		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.01955268256320628 | validation: 0.02938966650211724]
	TIME [epoch: 26.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0199108143232034		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.0199108143232034 | validation: 0.029496754016093987]
	TIME [epoch: 26.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02069506307780079		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.02069506307780079 | validation: 0.031005828174469265]
	TIME [epoch: 26.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019971591795054887		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.019971591795054887 | validation: 0.028317168575744844]
	TIME [epoch: 26.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019553102679960823		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.019553102679960823 | validation: 0.03229292420748166]
	TIME [epoch: 26.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02081480984679089		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.02081480984679089 | validation: 0.03054342765705466]
	TIME [epoch: 26.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019736872417197297		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.019736872417197297 | validation: 0.03007371728706168]
	TIME [epoch: 26.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01971529642862545		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.01971529642862545 | validation: 0.030728923099307047]
	TIME [epoch: 26.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019114961918364384		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.019114961918364384 | validation: 0.028987826304093404]
	TIME [epoch: 26.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0200266429395462		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0200266429395462 | validation: 0.028735656280629407]
	TIME [epoch: 26.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019251074216894253		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.019251074216894253 | validation: 0.029763616119556975]
	TIME [epoch: 26.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01972646843441605		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.01972646843441605 | validation: 0.028673279921783243]
	TIME [epoch: 26.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018563992332806985		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.018563992332806985 | validation: 0.03200355697833074]
	TIME [epoch: 26.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018921467677756112		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.018921467677756112 | validation: 0.03113791498936734]
	TIME [epoch: 26.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02053357296509787		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.02053357296509787 | validation: 0.027933881507521328]
	TIME [epoch: 26.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02052285685356394		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.02052285685356394 | validation: 0.027882985896007635]
	TIME [epoch: 26.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019152649304086267		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.019152649304086267 | validation: 0.03118053190138414]
	TIME [epoch: 26.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019458753114529822		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.019458753114529822 | validation: 0.02761864996297018]
	TIME [epoch: 26.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02004875532633478		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.02004875532633478 | validation: 0.03075891018705293]
	TIME [epoch: 26.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019445778135106777		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.019445778135106777 | validation: 0.030307927064716064]
	TIME [epoch: 26.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021520421836639332		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.021520421836639332 | validation: 0.030260640450410083]
	TIME [epoch: 26.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019103124718838283		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.019103124718838283 | validation: 0.029223731314301466]
	TIME [epoch: 26.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019435724754219598		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.019435724754219598 | validation: 0.02804796055746287]
	TIME [epoch: 26.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0198003108742371		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0198003108742371 | validation: 0.03048724610077838]
	TIME [epoch: 26.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019459628936510962		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.019459628936510962 | validation: 0.028046613527502304]
	TIME [epoch: 26.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020484774908480604		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.020484774908480604 | validation: 0.03116309582995648]
	TIME [epoch: 26.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019882074419018826		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.019882074419018826 | validation: 0.027950480327385223]
	TIME [epoch: 26.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018945976465737445		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.018945976465737445 | validation: 0.030278450193438763]
	TIME [epoch: 26 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020615001687874165		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.020615001687874165 | validation: 0.02895202920783067]
	TIME [epoch: 26.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019648351755548285		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.019648351755548285 | validation: 0.0279995263303136]
	TIME [epoch: 26 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01911199641328641		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.01911199641328641 | validation: 0.02849728761454892]
	TIME [epoch: 26.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019766908650534556		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.019766908650534556 | validation: 0.032389902637946485]
	TIME [epoch: 26.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019095747050573486		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.019095747050573486 | validation: 0.03008570890447719]
	TIME [epoch: 26.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019038539956894984		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.019038539956894984 | validation: 0.028284096860411407]
	TIME [epoch: 26 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01856660257757331		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.01856660257757331 | validation: 0.02944298169126351]
	TIME [epoch: 26.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019912307494362465		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.019912307494362465 | validation: 0.028829979906209086]
	TIME [epoch: 26 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020004450152309446		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.020004450152309446 | validation: 0.02878134417513966]
	TIME [epoch: 26.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018704619251842965		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.018704619251842965 | validation: 0.028553351341344554]
	TIME [epoch: 26.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019379346655620217		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.019379346655620217 | validation: 0.030178389809475684]
	TIME [epoch: 26.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019317512258707224		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.019317512258707224 | validation: 0.026839130159085577]
	TIME [epoch: 26 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0198504416074746		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.0198504416074746 | validation: 0.028534165142976266]
	TIME [epoch: 26 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019301420769447453		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.019301420769447453 | validation: 0.0315913012637848]
	TIME [epoch: 26.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019982482276339298		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.019982482276339298 | validation: 0.0317347698090498]
	TIME [epoch: 26.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01967157470423688		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.01967157470423688 | validation: 0.02985433780136601]
	TIME [epoch: 26.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01889017419755623		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.01889017419755623 | validation: 0.02876940675202916]
	TIME [epoch: 26.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01934714966651982		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.01934714966651982 | validation: 0.027854588487412072]
	TIME [epoch: 26 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018195205451644658		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.018195205451644658 | validation: 0.0306888818772744]
	TIME [epoch: 26.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018159759527490008		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.018159759527490008 | validation: 0.028240057688803118]
	TIME [epoch: 26.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018707169700398995		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.018707169700398995 | validation: 0.03233528818366136]
	TIME [epoch: 26.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01970466213951891		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.01970466213951891 | validation: 0.027403204201474858]
	TIME [epoch: 26.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019048763468680584		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.019048763468680584 | validation: 0.027582297289087183]
	TIME [epoch: 26 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019066366227633965		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.019066366227633965 | validation: 0.028278783953539895]
	TIME [epoch: 26 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01855927571209512		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.01855927571209512 | validation: 0.02893942868416586]
	TIME [epoch: 26 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01797820827716854		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.01797820827716854 | validation: 0.02670829054687531]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_986.pth
	Model improved!!!
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019188636122017724		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.019188636122017724 | validation: 0.028876876168778966]
	TIME [epoch: 26.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019267768004297626		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.019267768004297626 | validation: 0.027702470380044683]
	TIME [epoch: 26.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018531967788914683		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.018531967788914683 | validation: 0.031287176958446045]
	TIME [epoch: 26.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01968650065601144		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.01968650065601144 | validation: 0.028711018539546204]
	TIME [epoch: 26.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019316492566534976		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.019316492566534976 | validation: 0.028156940154565636]
	TIME [epoch: 26.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020243954033542025		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.020243954033542025 | validation: 0.02762036326505755]
	TIME [epoch: 26.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01935949732059948		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.01935949732059948 | validation: 0.026717177061991948]
	TIME [epoch: 26.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019616470212529425		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.019616470212529425 | validation: 0.02869069594376148]
	TIME [epoch: 26 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018747717450651166		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.018747717450651166 | validation: 0.02659504082700105]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018592267599735243		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.018592267599735243 | validation: 0.027460778786583023]
	TIME [epoch: 26.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018879931271626113		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.018879931271626113 | validation: 0.03151784634388882]
	TIME [epoch: 26.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019362978566401786		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.019362978566401786 | validation: 0.028239733435050204]
	TIME [epoch: 26.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01892579941015425		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.01892579941015425 | validation: 0.027642198318753874]
	TIME [epoch: 26 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019340752885369288		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.019340752885369288 | validation: 0.028087107793973453]
	TIME [epoch: 26.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01891131057084364		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.01891131057084364 | validation: 0.026674195430470185]
	TIME [epoch: 459 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018396246823735654		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.018396246823735654 | validation: 0.02755507331653948]
	TIME [epoch: 55.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018673404192929833		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.018673404192929833 | validation: 0.029680540052918217]
	TIME [epoch: 55.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018316466523508438		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.018316466523508438 | validation: 0.028320446128196876]
	TIME [epoch: 55.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01829629795058272		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.01829629795058272 | validation: 0.028013420755173338]
	TIME [epoch: 55.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019109790979715033		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.019109790979715033 | validation: 0.027853388109609117]
	TIME [epoch: 55.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019214384952881726		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.019214384952881726 | validation: 0.02877644114101204]
	TIME [epoch: 55.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019097129728248032		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.019097129728248032 | validation: 0.027565241343258176]
	TIME [epoch: 55.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019285891020243043		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.019285891020243043 | validation: 0.02887137433597805]
	TIME [epoch: 55.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021866027853226		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.02021866027853226 | validation: 0.029752865293715534]
	TIME [epoch: 55.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019114876478160035		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.019114876478160035 | validation: 0.026695531102308462]
	TIME [epoch: 55.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01823639580583252		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.01823639580583252 | validation: 0.028415591737599784]
	TIME [epoch: 55.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019094491020400187		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.019094491020400187 | validation: 0.027329463791427457]
	TIME [epoch: 55.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018937148916226502		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.018937148916226502 | validation: 0.027563163258969132]
	TIME [epoch: 55.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01860191509618219		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.01860191509618219 | validation: 0.02780299524587368]
	TIME [epoch: 55.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01840128289584256		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.01840128289584256 | validation: 0.027558890728962553]
	TIME [epoch: 55.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018646995186041855		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.018646995186041855 | validation: 0.026818914080533357]
	TIME [epoch: 55.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018427669233554267		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.018427669233554267 | validation: 0.027693392376260055]
	TIME [epoch: 55.2 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018691894669539168		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.018691894669539168 | validation: 0.02875515917804725]
	TIME [epoch: 55.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018463028803698236		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.018463028803698236 | validation: 0.02733008935975629]
	TIME [epoch: 55.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018560706354162414		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.018560706354162414 | validation: 0.027721543714338475]
	TIME [epoch: 55.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018203847134544763		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.018203847134544763 | validation: 0.029802346619182116]
	TIME [epoch: 55.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018312199758968428		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.018312199758968428 | validation: 0.03004820103933047]
	TIME [epoch: 55.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01944909596270564		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.01944909596270564 | validation: 0.028653151256852673]
	TIME [epoch: 55.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018232557202105336		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.018232557202105336 | validation: 0.027272923540536793]
	TIME [epoch: 55.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017936646389563694		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.017936646389563694 | validation: 0.029338123956313127]
	TIME [epoch: 55.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01821992966554785		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.01821992966554785 | validation: 0.03031202940727969]
	TIME [epoch: 55.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019091017879192228		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.019091017879192228 | validation: 0.028823211083302012]
	TIME [epoch: 55.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018657968056239017		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.018657968056239017 | validation: 0.02781232332928962]
	TIME [epoch: 55.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020010489916625802		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.020010489916625802 | validation: 0.029761457872789138]
	TIME [epoch: 55.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01923506548421205		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.01923506548421205 | validation: 0.028823974282707446]
	TIME [epoch: 55.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01855615353783058		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.01855615353783058 | validation: 0.02630562579441033]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019029658218578417		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.019029658218578417 | validation: 0.028941434498683565]
	TIME [epoch: 55.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018085577226286332		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.018085577226286332 | validation: 0.02825122254359081]
	TIME [epoch: 55.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019040787184196495		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.019040787184196495 | validation: 0.027569842712694012]
	TIME [epoch: 55.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018103138080499535		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.018103138080499535 | validation: 0.029146374390756694]
	TIME [epoch: 55.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018390949370446667		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.018390949370446667 | validation: 0.02772493979168602]
	TIME [epoch: 55.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019247987529379774		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.019247987529379774 | validation: 0.029960562213648047]
	TIME [epoch: 55.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018610068162323686		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.018610068162323686 | validation: 0.028479060475211634]
	TIME [epoch: 55.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018346748999981333		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.018346748999981333 | validation: 0.028059439177172695]
	TIME [epoch: 55 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019329707606079043		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.019329707606079043 | validation: 0.027022632574356317]
	TIME [epoch: 55.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0178399229463291		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0178399229463291 | validation: 0.025875906537548225]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1042.pth
	Model improved!!!
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018086002307746082		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.018086002307746082 | validation: 0.02916687736900404]
	TIME [epoch: 55.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01823073005598338		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.01823073005598338 | validation: 0.02775356669724524]
	TIME [epoch: 55.1 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018655308330825036		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.018655308330825036 | validation: 0.027499187868209207]
	TIME [epoch: 54.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019057372557745395		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.019057372557745395 | validation: 0.026627167697236106]
	TIME [epoch: 55.1 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018533134539323055		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.018533134539323055 | validation: 0.02810112520374227]
	TIME [epoch: 55.1 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018079899160919635		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.018079899160919635 | validation: 0.026846059170907086]
	TIME [epoch: 55.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018930462825056858		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.018930462825056858 | validation: 0.030782801167746857]
	TIME [epoch: 55.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01789382254372192		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.01789382254372192 | validation: 0.028426575338195197]
	TIME [epoch: 55.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018245847811333343		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.018245847811333343 | validation: 0.02956880812818036]
	TIME [epoch: 55 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018384837541568444		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.018384837541568444 | validation: 0.0303760469564128]
	TIME [epoch: 55.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019172848955144685		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.019172848955144685 | validation: 0.027431517733134736]
	TIME [epoch: 55.1 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01782765682300578		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.01782765682300578 | validation: 0.028162651535215723]
	TIME [epoch: 55.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017975450189003567		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.017975450189003567 | validation: 0.027755432210448672]
	TIME [epoch: 55.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018421718113247783		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.018421718113247783 | validation: 0.026383374886157933]
	TIME [epoch: 55.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018170186647451427		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.018170186647451427 | validation: 0.027377144315560847]
	TIME [epoch: 55.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01831815958976406		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.01831815958976406 | validation: 0.026323524719847752]
	TIME [epoch: 55.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018432392928891112		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.018432392928891112 | validation: 0.026503360779969863]
	TIME [epoch: 55.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019276177185599524		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.019276177185599524 | validation: 0.027441717337877557]
	TIME [epoch: 55.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017932297193550602		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.017932297193550602 | validation: 0.027079190005412115]
	TIME [epoch: 55.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01839309983661534		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.01839309983661534 | validation: 0.02948674783949036]
	TIME [epoch: 55.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018766110306307315		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.018766110306307315 | validation: 0.02769280964865792]
	TIME [epoch: 55.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01798486454355396		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.01798486454355396 | validation: 0.02800930287940337]
	TIME [epoch: 55.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017972596242048066		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.017972596242048066 | validation: 0.027348580440431813]
	TIME [epoch: 55.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01859832754220873		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.01859832754220873 | validation: 0.027847835138024413]
	TIME [epoch: 55.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018370213395909454		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.018370213395909454 | validation: 0.02977372460223498]
	TIME [epoch: 55.1 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018021515036070787		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.018021515036070787 | validation: 0.027771695376463033]
	TIME [epoch: 55.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018314665550002003		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.018314665550002003 | validation: 0.029188332245632706]
	TIME [epoch: 55.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01797485039227813		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.01797485039227813 | validation: 0.02706180523687566]
	TIME [epoch: 55.1 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018026813248619593		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.018026813248619593 | validation: 0.028097636119244276]
	TIME [epoch: 55.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01772727521112797		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.01772727521112797 | validation: 0.026757235255164786]
	TIME [epoch: 55.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017413551842738198		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.017413551842738198 | validation: 0.0266777371254305]
	TIME [epoch: 55.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018346643947977935		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.018346643947977935 | validation: 0.02952013790587699]
	TIME [epoch: 55.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01813920074717359		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.01813920074717359 | validation: 0.028398245317611023]
	TIME [epoch: 55.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018058974089678246		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.018058974089678246 | validation: 0.028738984036653023]
	TIME [epoch: 55.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01834258815824319		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.01834258815824319 | validation: 0.028552470310034824]
	TIME [epoch: 55 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018214260204174087		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.018214260204174087 | validation: 0.027053597032112513]
	TIME [epoch: 55.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017739862230511976		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.017739862230511976 | validation: 0.026034636350694827]
	TIME [epoch: 55.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01809309580378319		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.01809309580378319 | validation: 0.02756447172443962]
	TIME [epoch: 55.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018178336139225364		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.018178336139225364 | validation: 0.02651472846861739]
	TIME [epoch: 55.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018096492683229593		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.018096492683229593 | validation: 0.02563971809071778]
	TIME [epoch: 55.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1082.pth
	Model improved!!!
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018551266673557013		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.018551266673557013 | validation: 0.027860671617777548]
	TIME [epoch: 55.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017945421421825956		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.017945421421825956 | validation: 0.028201926774842928]
	TIME [epoch: 55.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01853610981483637		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.01853610981483637 | validation: 0.027745506411027777]
	TIME [epoch: 55 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018030306595548752		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.018030306595548752 | validation: 0.028384041009426975]
	TIME [epoch: 55.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017931349309637214		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.017931349309637214 | validation: 0.02707520830384185]
	TIME [epoch: 55.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017331718696852808		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.017331718696852808 | validation: 0.026693167839499835]
	TIME [epoch: 55.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018629440317845282		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.018629440317845282 | validation: 0.029988282045570733]
	TIME [epoch: 55.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017426100794958696		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.017426100794958696 | validation: 0.028177174476197442]
	TIME [epoch: 55.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018053538979857207		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.018053538979857207 | validation: 0.02804324344394351]
	TIME [epoch: 55.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017617925321172963		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.017617925321172963 | validation: 0.027718538800199657]
	TIME [epoch: 55.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01766201361598028		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.01766201361598028 | validation: 0.025846305505203983]
	TIME [epoch: 55.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017850738655144655		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.017850738655144655 | validation: 0.026919311490139347]
	TIME [epoch: 55.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01731876476312929		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.01731876476312929 | validation: 0.027838548250625563]
	TIME [epoch: 55.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01729200353078246		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.01729200353078246 | validation: 0.029299213620633424]
	TIME [epoch: 55.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018110751769284204		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.018110751769284204 | validation: 0.027078430112961335]
	TIME [epoch: 55.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018359551803519076		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.018359551803519076 | validation: 0.025025041408778474]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1098.pth
	Model improved!!!
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017495971184598613		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.017495971184598613 | validation: 0.027838831402439857]
	TIME [epoch: 55.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174927134262475		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0174927134262475 | validation: 0.02853404207688815]
	TIME [epoch: 55.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017717706629876043		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.017717706629876043 | validation: 0.02619871691256908]
	TIME [epoch: 55.1 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018853952663634897		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.018853952663634897 | validation: 0.025988547273994408]
	TIME [epoch: 55.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01744579253455291		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.01744579253455291 | validation: 0.027118237892701097]
	TIME [epoch: 55.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01746630561145598		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.01746630561145598 | validation: 0.030249652665365737]
	TIME [epoch: 55.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017499998338651852		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.017499998338651852 | validation: 0.026438489740375125]
	TIME [epoch: 55.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01785881019399738		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.01785881019399738 | validation: 0.026284326632284]
	TIME [epoch: 55.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017981625626954694		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.017981625626954694 | validation: 0.028159771523504913]
	TIME [epoch: 55.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017516058368276206		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.017516058368276206 | validation: 0.025415614669989046]
	TIME [epoch: 55.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017530113347140837		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.017530113347140837 | validation: 0.027996427305392133]
	TIME [epoch: 55.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017882644700988284		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.017882644700988284 | validation: 0.02807842173311386]
	TIME [epoch: 55.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017294951209445446		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.017294951209445446 | validation: 0.02773036722037773]
	TIME [epoch: 55.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017666676386762902		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.017666676386762902 | validation: 0.02690045111246309]
	TIME [epoch: 55.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017936250311640853		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.017936250311640853 | validation: 0.028588333672039522]
	TIME [epoch: 55.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018251379512581666		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.018251379512581666 | validation: 0.027437062162554045]
	TIME [epoch: 55.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018091154676347056		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.018091154676347056 | validation: 0.02643048545186118]
	TIME [epoch: 55.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01784283230056596		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.01784283230056596 | validation: 0.026037649470942108]
	TIME [epoch: 55.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017851289011802107		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.017851289011802107 | validation: 0.025745556349606777]
	TIME [epoch: 55.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017922067508245494		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.017922067508245494 | validation: 0.02642321036833247]
	TIME [epoch: 55.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017268556412702432		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.017268556412702432 | validation: 0.026174603778706777]
	TIME [epoch: 55.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017741812100858834		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.017741812100858834 | validation: 0.02833426085923461]
	TIME [epoch: 55.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174802956472973		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.0174802956472973 | validation: 0.026888677540570102]
	TIME [epoch: 55.2 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01835777490413834		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.01835777490413834 | validation: 0.027012267030959262]
	TIME [epoch: 55.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707376858152074		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.01707376858152074 | validation: 0.02831443391270653]
	TIME [epoch: 55 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01737757726371692		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.01737757726371692 | validation: 0.026680712200678483]
	TIME [epoch: 55.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018366347412057754		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.018366347412057754 | validation: 0.026856392477355218]
	TIME [epoch: 55.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017029128104344027		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.017029128104344027 | validation: 0.025702026666758585]
	TIME [epoch: 55.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018929995183206417		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.018929995183206417 | validation: 0.02605522741382159]
	TIME [epoch: 55.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01765791903315775		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.01765791903315775 | validation: 0.0253701081069867]
	TIME [epoch: 55.2 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686867402850302		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.01686867402850302 | validation: 0.027794437514850917]
	TIME [epoch: 55.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01717945447299555		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.01717945447299555 | validation: 0.02649172419304194]
	TIME [epoch: 55.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018233512736618242		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.018233512736618242 | validation: 0.028263855157072998]
	TIME [epoch: 55.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017187397078198715		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.017187397078198715 | validation: 0.02660155400370234]
	TIME [epoch: 55.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017662226220887346		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.017662226220887346 | validation: 0.027499017707787045]
	TIME [epoch: 55.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017849298094801076		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.017849298094801076 | validation: 0.027346830669215876]
	TIME [epoch: 55.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018069140619175996		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.018069140619175996 | validation: 0.029313061054485945]
	TIME [epoch: 55.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017441906349162352		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.017441906349162352 | validation: 0.027470725929071682]
	TIME [epoch: 55.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017562317666564343		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.017562317666564343 | validation: 0.026955156037783555]
	TIME [epoch: 55.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017765746340243457		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.017765746340243457 | validation: 0.02734321055923511]
	TIME [epoch: 55.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018290615277545403		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.018290615277545403 | validation: 0.02875828135669259]
	TIME [epoch: 55.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017525532379646828		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.017525532379646828 | validation: 0.029313983682470512]
	TIME [epoch: 55.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018042440842697717		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.018042440842697717 | validation: 0.027278491676704486]
	TIME [epoch: 55.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017055934037006622		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.017055934037006622 | validation: 0.025796418033760985]
	TIME [epoch: 55.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017471514748251517		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.017471514748251517 | validation: 0.025618788252094044]
	TIME [epoch: 55.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017155191508659802		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.017155191508659802 | validation: 0.027073214562364212]
	TIME [epoch: 55.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016982718483366355		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.016982718483366355 | validation: 0.026780600564586365]
	TIME [epoch: 55.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017335919455778802		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.017335919455778802 | validation: 0.02622288897565442]
	TIME [epoch: 55.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017020958263625208		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.017020958263625208 | validation: 0.02665191737957434]
	TIME [epoch: 55.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01721930710446524		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.01721930710446524 | validation: 0.028130141822517774]
	TIME [epoch: 55.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018468845263046263		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.018468845263046263 | validation: 0.026609352447048498]
	TIME [epoch: 55.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017260642034012275		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.017260642034012275 | validation: 0.027902705075420843]
	TIME [epoch: 55.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017107688946163973		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.017107688946163973 | validation: 0.02567622676429552]
	TIME [epoch: 55.2 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01748651362885205		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.01748651362885205 | validation: 0.026769993278478316]
	TIME [epoch: 55.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01758804209383397		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.01758804209383397 | validation: 0.026068237057946544]
	TIME [epoch: 55.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01747447789252397		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.01747447789252397 | validation: 0.028065431733132792]
	TIME [epoch: 55.2 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017609158297916077		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.017609158297916077 | validation: 0.026539973095800563]
	TIME [epoch: 55.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017198662733953853		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.017198662733953853 | validation: 0.027515175695032257]
	TIME [epoch: 55.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017346515598232394		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.017346515598232394 | validation: 0.026742238197195865]
	TIME [epoch: 55.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017434363123283534		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.017434363123283534 | validation: 0.027582864382755812]
	TIME [epoch: 55.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707279597679617		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.01707279597679617 | validation: 0.027388499420683016]
	TIME [epoch: 55.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017183921611462566		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.017183921611462566 | validation: 0.02704123412000342]
	TIME [epoch: 55.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017344534582571673		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.017344534582571673 | validation: 0.02631742192360155]
	TIME [epoch: 55.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0170632827725318		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.0170632827725318 | validation: 0.027312158473274967]
	TIME [epoch: 55.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017299352446216606		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.017299352446216606 | validation: 0.025046346611162937]
	TIME [epoch: 55.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01748334005858692		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.01748334005858692 | validation: 0.025697850319108293]
	TIME [epoch: 55.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016973199994216037		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.016973199994216037 | validation: 0.025752008178516667]
	TIME [epoch: 55.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017112493547562564		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.017112493547562564 | validation: 0.026869552509359282]
	TIME [epoch: 55.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017328845395611452		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.017328845395611452 | validation: 0.0275303038433499]
	TIME [epoch: 55.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016937560505663616		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.016937560505663616 | validation: 0.026952100613202308]
	TIME [epoch: 55.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016840486296089106		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.016840486296089106 | validation: 0.028662375422990827]
	TIME [epoch: 55.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017331446298579266		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.017331446298579266 | validation: 0.0281442718508037]
	TIME [epoch: 55.2 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017238624315586405		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.017238624315586405 | validation: 0.02697103628142314]
	TIME [epoch: 55.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017853601870685576		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.017853601870685576 | validation: 0.0270731219120511]
	TIME [epoch: 55.2 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01755091314420595		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.01755091314420595 | validation: 0.025932668578386584]
	TIME [epoch: 55.2 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017063312239634636		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.017063312239634636 | validation: 0.027707548402838192]
	TIME [epoch: 55.2 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016897568237605385		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.016897568237605385 | validation: 0.02713599124772803]
	TIME [epoch: 55.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01692061296858173		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.01692061296858173 | validation: 0.029434911208605436]
	TIME [epoch: 55.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01755798625470376		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.01755798625470376 | validation: 0.02729644183114174]
	TIME [epoch: 55.2 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017683054432091446		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.017683054432091446 | validation: 0.025526692432362285]
	TIME [epoch: 55.2 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017268484335003796		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.017268484335003796 | validation: 0.026989115183620495]
	TIME [epoch: 55.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017324378011214024		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.017324378011214024 | validation: 0.028097068694782117]
	TIME [epoch: 55.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174881496620457		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.0174881496620457 | validation: 0.028087375287434737]
	TIME [epoch: 55.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017610780248409223		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.017610780248409223 | validation: 0.02591493292484402]
	TIME [epoch: 55.1 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01806041562289285		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.01806041562289285 | validation: 0.02687349971130026]
	TIME [epoch: 55.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686930014529523		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.01686930014529523 | validation: 0.027100611466167296]
	TIME [epoch: 55.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016540770387516782		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.016540770387516782 | validation: 0.026616848538234393]
	TIME [epoch: 55.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017561092897192297		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.017561092897192297 | validation: 0.027673828926224706]
	TIME [epoch: 55.1 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017595883449595583		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.017595883449595583 | validation: 0.027169832969572975]
	TIME [epoch: 55.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016839151953876228		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.016839151953876228 | validation: 0.02572410304508066]
	TIME [epoch: 55.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017323641342472516		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.017323641342472516 | validation: 0.0279795517354364]
	TIME [epoch: 55.1 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01698681925946002		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.01698681925946002 | validation: 0.02705609335569714]
	TIME [epoch: 55.1 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016812164791177874		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.016812164791177874 | validation: 0.02708450806955338]
	TIME [epoch: 55.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018130108323585404		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.018130108323585404 | validation: 0.027731408790167614]
	TIME [epoch: 55.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01701766835732735		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.01701766835732735 | validation: 0.02738049550893777]
	TIME [epoch: 55.2 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016921058752160746		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.016921058752160746 | validation: 0.02613432517761474]
	TIME [epoch: 55.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017336812776774957		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.017336812776774957 | validation: 0.026087257906267945]
	TIME [epoch: 55.2 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01765341466121216		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.01765341466121216 | validation: 0.0258972037391037]
	TIME [epoch: 55.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01776945000696737		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.01776945000696737 | validation: 0.027085058979767568]
	TIME [epoch: 55.2 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017079823493568593		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.017079823493568593 | validation: 0.024783787389119197]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017319731612397155		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.017319731612397155 | validation: 0.027970834954473313]
	TIME [epoch: 55.1 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016779358400741302		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.016779358400741302 | validation: 0.028117714876646417]
	TIME [epoch: 55.4 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017060138653720795		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.017060138653720795 | validation: 0.026949219515132378]
	TIME [epoch: 55.2 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016646060776420515		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.016646060776420515 | validation: 0.028659334883188503]
	TIME [epoch: 55.2 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017548821155621723		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.017548821155621723 | validation: 0.026631356492059104]
	TIME [epoch: 55.1 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01684907720765755		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.01684907720765755 | validation: 0.026576944281524916]
	TIME [epoch: 55.2 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016797950914541983		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.016797950914541983 | validation: 0.027092816366707885]
	TIME [epoch: 55.1 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016621269342149133		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.016621269342149133 | validation: 0.025183473016906502]
	TIME [epoch: 55.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01740447343779384		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.01740447343779384 | validation: 0.026753843213432868]
	TIME [epoch: 55.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016818729051723904		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.016818729051723904 | validation: 0.026613958172334758]
	TIME [epoch: 55.2 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01678152666999073		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.01678152666999073 | validation: 0.026606355872993905]
	TIME [epoch: 55.2 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01696950647841977		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.01696950647841977 | validation: 0.02583349323310319]
	TIME [epoch: 55.2 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016969753269223932		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.016969753269223932 | validation: 0.026860720952724036]
	TIME [epoch: 55.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0171449987945141		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.0171449987945141 | validation: 0.026623328228967222]
	TIME [epoch: 55.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016349044639835202		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.016349044639835202 | validation: 0.02582428849811977]
	TIME [epoch: 55.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016697563259378236		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.016697563259378236 | validation: 0.026774653062415946]
	TIME [epoch: 55.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017014485904594902		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.017014485904594902 | validation: 0.028362311197141873]
	TIME [epoch: 55.1 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01754194190728103		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.01754194190728103 | validation: 0.02824191778125855]
	TIME [epoch: 55.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017821027206552636		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.017821027206552636 | validation: 0.027097461777728107]
	TIME [epoch: 55.2 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01708012541955156		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.01708012541955156 | validation: 0.02688489582029093]
	TIME [epoch: 55.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016678672130704945		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.016678672130704945 | validation: 0.025452561161221524]
	TIME [epoch: 55.2 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016830199326294384		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.016830199326294384 | validation: 0.025344225986253653]
	TIME [epoch: 55.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017360162829644928		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.017360162829644928 | validation: 0.026848856583328445]
	TIME [epoch: 55.2 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016451710033257724		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.016451710033257724 | validation: 0.025304758772381673]
	TIME [epoch: 55.1 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017009042891682192		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.017009042891682192 | validation: 0.026106735805528018]
	TIME [epoch: 55.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016986003204952003		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.016986003204952003 | validation: 0.027862248958280003]
	TIME [epoch: 55.2 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017124685278757944		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.017124685278757944 | validation: 0.025797556709405475]
	TIME [epoch: 55.2 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0167107868217729		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.0167107868217729 | validation: 0.02682253306900237]
	TIME [epoch: 55.2 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01653722495018044		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.01653722495018044 | validation: 0.03176591410111544]
	TIME [epoch: 55.2 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01791166301736943		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.01791166301736943 | validation: 0.027837333745081603]
	TIME [epoch: 55.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01675542999037207		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.01675542999037207 | validation: 0.02703606003120545]
	TIME [epoch: 55.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016486834413726177		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.016486834413726177 | validation: 0.025666313626568328]
	TIME [epoch: 55.2 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017429178076771277		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.017429178076771277 | validation: 0.02518254266982277]
	TIME [epoch: 55.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01677962676439645		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.01677962676439645 | validation: 0.027325896963344257]
	TIME [epoch: 55.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01734729489471775		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.01734729489471775 | validation: 0.027463433583356307]
	TIME [epoch: 55.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016852797732176614		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.016852797732176614 | validation: 0.026918430472721022]
	TIME [epoch: 55.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017289335814003187		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.017289335814003187 | validation: 0.027704426107290167]
	TIME [epoch: 55.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016126593258339015		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.016126593258339015 | validation: 0.0275056077556158]
	TIME [epoch: 55.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01678196113531112		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.01678196113531112 | validation: 0.026462534914893254]
	TIME [epoch: 55.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016467938583055775		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.016467938583055775 | validation: 0.026274872318575438]
	TIME [epoch: 55.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0162360984427194		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.0162360984427194 | validation: 0.027605125406331414]
	TIME [epoch: 55.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01730469296673967		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.01730469296673967 | validation: 0.025416594832425272]
	TIME [epoch: 55.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0164624455527013		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.0164624455527013 | validation: 0.024964181987289582]
	TIME [epoch: 55.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016844207892367055		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.016844207892367055 | validation: 0.025863965905398537]
	TIME [epoch: 55.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016633417802807637		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.016633417802807637 | validation: 0.02630756651857191]
	TIME [epoch: 55.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017209967905629772		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.017209967905629772 | validation: 0.02526867387073853]
	TIME [epoch: 55.1 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017042899715513163		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.017042899715513163 | validation: 0.025665043820650338]
	TIME [epoch: 55.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016722356487479214		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.016722356487479214 | validation: 0.026854054657092084]
	TIME [epoch: 55.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01701008506143483		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.01701008506143483 | validation: 0.026495427048628412]
	TIME [epoch: 55.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016399237327825854		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.016399237327825854 | validation: 0.026680599574805404]
	TIME [epoch: 55.1 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017027483806888167		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.017027483806888167 | validation: 0.026356521118387995]
	TIME [epoch: 55.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016414208292474152		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.016414208292474152 | validation: 0.025726865308138325]
	TIME [epoch: 55.2 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01679641252358318		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.01679641252358318 | validation: 0.027226500688944624]
	TIME [epoch: 55.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01680415605947044		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.01680415605947044 | validation: 0.025991075820651195]
	TIME [epoch: 55.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016983616864086913		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.016983616864086913 | validation: 0.028508653778348346]
	TIME [epoch: 55.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017312736682451226		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.017312736682451226 | validation: 0.027152266729703382]
	TIME [epoch: 55.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01771991009959114		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.01771991009959114 | validation: 0.02855878290241163]
	TIME [epoch: 55.2 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016763517070282487		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.016763517070282487 | validation: 0.026445604101666402]
	TIME [epoch: 55.4 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016881450858914585		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.016881450858914585 | validation: 0.02688579172997827]
	TIME [epoch: 55.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017225671217555895		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.017225671217555895 | validation: 0.0257490760564483]
	TIME [epoch: 55.1 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016382818431283856		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.016382818431283856 | validation: 0.02750958916691796]
	TIME [epoch: 55.1 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01638208567121539		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.01638208567121539 | validation: 0.027081732813696316]
	TIME [epoch: 55.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016718563116628043		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.016718563116628043 | validation: 0.02479301587172003]
	TIME [epoch: 55.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017270141147042825		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.017270141147042825 | validation: 0.02713619579167753]
	TIME [epoch: 55.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686120000156158		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.01686120000156158 | validation: 0.0256443309418576]
	TIME [epoch: 55.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016890786971025324		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.016890786971025324 | validation: 0.02558769340169402]
	TIME [epoch: 55.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01723642414561115		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.01723642414561115 | validation: 0.02703271615044239]
	TIME [epoch: 55.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01685949732361914		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.01685949732361914 | validation: 0.02603847212876613]
	TIME [epoch: 55.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016998197932309892		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.016998197932309892 | validation: 0.02619255441206747]
	TIME [epoch: 55.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016292895987378982		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.016292895987378982 | validation: 0.027040857996300877]
	TIME [epoch: 55.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016680440220342853		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.016680440220342853 | validation: 0.02775192486111172]
	TIME [epoch: 55.4 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01691893818884291		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.01691893818884291 | validation: 0.027100671245391075]
	TIME [epoch: 55.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017053209128075973		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.017053209128075973 | validation: 0.02857890557401454]
	TIME [epoch: 55.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01702110443330452		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.01702110443330452 | validation: 0.025868808167770582]
	TIME [epoch: 55.2 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017189142993178533		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.017189142993178533 | validation: 0.02595808494369444]
	TIME [epoch: 55.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016643148028161216		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.016643148028161216 | validation: 0.024952085622383144]
	TIME [epoch: 55.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016457057198669266		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.016457057198669266 | validation: 0.02670448648839809]
	TIME [epoch: 55.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016049812723981097		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.016049812723981097 | validation: 0.027078924823146333]
	TIME [epoch: 55.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01634165809043438		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.01634165809043438 | validation: 0.02503559039262082]
	TIME [epoch: 55.2 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01703794485056741		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.01703794485056741 | validation: 0.02536148543755834]
	TIME [epoch: 55.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01659200319103919		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.01659200319103919 | validation: 0.025971828514132995]
	TIME [epoch: 55.2 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01696306673323532		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.01696306673323532 | validation: 0.025624776399756726]
	TIME [epoch: 55.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016591434504124702		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.016591434504124702 | validation: 0.027357239300143176]
	TIME [epoch: 55.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016521410849567503		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.016521410849567503 | validation: 0.025539893864382192]
	TIME [epoch: 55.2 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01633074554932645		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.01633074554932645 | validation: 0.02445083735027697]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1283.pth
	Model improved!!!
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01660593571765201		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.01660593571765201 | validation: 0.026134794804753086]
	TIME [epoch: 55.1 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016665457804839995		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.016665457804839995 | validation: 0.026782184706294242]
	TIME [epoch: 55.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01678512095440941		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.01678512095440941 | validation: 0.02766200653857495]
	TIME [epoch: 55.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017002554627538503		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.017002554627538503 | validation: 0.02625494771307578]
	TIME [epoch: 55.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016453972863618466		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.016453972863618466 | validation: 0.026598672105548102]
	TIME [epoch: 55.2 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016148914215261434		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.016148914215261434 | validation: 0.026203422156118546]
	TIME [epoch: 55.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016515795614687398		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.016515795614687398 | validation: 0.02501048373825307]
	TIME [epoch: 55.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017022569667048404		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.017022569667048404 | validation: 0.026012335759592546]
	TIME [epoch: 55.2 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01627035337870813		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.01627035337870813 | validation: 0.025430902668612927]
	TIME [epoch: 55.1 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01653335243888328		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.01653335243888328 | validation: 0.02545036106945037]
	TIME [epoch: 55.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016717593108533105		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.016717593108533105 | validation: 0.025446811387729237]
	TIME [epoch: 55.2 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016413170240757877		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.016413170240757877 | validation: 0.02665861435295549]
	TIME [epoch: 55.2 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016355132492698765		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.016355132492698765 | validation: 0.02590885911695692]
	TIME [epoch: 55.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01656657593763267		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.01656657593763267 | validation: 0.026512961194767773]
	TIME [epoch: 55.1 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016772188478947733		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.016772188478947733 | validation: 0.026214258356125845]
	TIME [epoch: 55.1 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016815584347236665		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.016815584347236665 | validation: 0.02595989608032199]
	TIME [epoch: 55.2 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016083352064546315		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.016083352064546315 | validation: 0.02791915558949887]
	TIME [epoch: 55.1 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016827469329622338		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.016827469329622338 | validation: 0.027426790490426053]
	TIME [epoch: 55.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01591134534853398		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.01591134534853398 | validation: 0.026668450371021852]
	TIME [epoch: 55.2 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016832022251870506		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.016832022251870506 | validation: 0.025678379774464494]
	TIME [epoch: 55.2 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016505206750885133		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.016505206750885133 | validation: 0.027610948603029515]
	TIME [epoch: 55.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016584017500306164		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.016584017500306164 | validation: 0.027372456711723524]
	TIME [epoch: 55.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016768305072194168		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.016768305072194168 | validation: 0.02629623974759529]
	TIME [epoch: 55.2 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016238078845237498		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.016238078845237498 | validation: 0.026108551734837115]
	TIME [epoch: 55.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016495689309434555		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.016495689309434555 | validation: 0.02557069300604157]
	TIME [epoch: 55.1 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01629585813550228		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.01629585813550228 | validation: 0.026728005533764455]
	TIME [epoch: 55.2 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016127554010471758		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.016127554010471758 | validation: 0.025928039515900024]
	TIME [epoch: 55.2 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01694091050935621		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.01694091050935621 | validation: 0.026640143367708313]
	TIME [epoch: 55.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016684129532791525		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.016684129532791525 | validation: 0.025486927463511028]
	TIME [epoch: 55.2 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01680698235533214		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.01680698235533214 | validation: 0.026141552427214658]
	TIME [epoch: 55.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016148810943048254		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.016148810943048254 | validation: 0.02625712155690186]
	TIME [epoch: 55.2 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01657930476610986		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.01657930476610986 | validation: 0.026352658487968122]
	TIME [epoch: 55.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016973347451189825		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.016973347451189825 | validation: 0.02573132948413099]
	TIME [epoch: 55.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016274344297527662		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.016274344297527662 | validation: 0.026207006975893316]
	TIME [epoch: 55.1 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016768941675946933		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.016768941675946933 | validation: 0.026676528072539625]
	TIME [epoch: 55.2 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01659159374277571		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.01659159374277571 | validation: 0.024924569060480233]
	TIME [epoch: 55.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016111890290077435		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.016111890290077435 | validation: 0.027637924531188893]
	TIME [epoch: 55.1 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016568101777953214		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.016568101777953214 | validation: 0.026426947130928906]
	TIME [epoch: 55.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016424557533906092		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.016424557533906092 | validation: 0.026116482313708556]
	TIME [epoch: 55.2 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0161395653916772		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.0161395653916772 | validation: 0.025106025491514916]
	TIME [epoch: 55.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016607616116806348		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.016607616116806348 | validation: 0.026029443761130904]
	TIME [epoch: 55.2 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01618378658960883		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.01618378658960883 | validation: 0.025503109136622883]
	TIME [epoch: 55.3 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01570883063459643		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.01570883063459643 | validation: 0.02585966782492074]
	TIME [epoch: 55.2 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01712280684230187		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.01712280684230187 | validation: 0.026181867645025506]
	TIME [epoch: 55.2 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016259862971212995		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.016259862971212995 | validation: 0.024467326331024563]
	TIME [epoch: 55.1 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016663188565247136		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.016663188565247136 | validation: 0.026929530060819454]
	TIME [epoch: 55.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016235610802608127		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.016235610802608127 | validation: 0.026736447333899543]
	TIME [epoch: 55.2 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016461242815278825		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.016461242815278825 | validation: 0.025529487749378677]
	TIME [epoch: 55.4 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016194263582544943		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.016194263582544943 | validation: 0.02574034485249219]
	TIME [epoch: 55.2 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01645676849711707		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.01645676849711707 | validation: 0.026100575978111298]
	TIME [epoch: 55.4 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016047006408873823		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.016047006408873823 | validation: 0.028536112920735705]
	TIME [epoch: 55.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01653222895266556		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.01653222895266556 | validation: 0.025909643277057432]
	TIME [epoch: 55.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01650653806936419		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.01650653806936419 | validation: 0.025878604439776715]
	TIME [epoch: 55.2 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01598018363547125		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.01598018363547125 | validation: 0.024705044154453473]
	TIME [epoch: 55.4 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016666189490508448		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.016666189490508448 | validation: 0.02680538230129298]
	TIME [epoch: 55.2 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01618202111594234		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.01618202111594234 | validation: 0.028430518892528955]
	TIME [epoch: 55.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01687703790180173		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.01687703790180173 | validation: 0.025326031872243575]
	TIME [epoch: 55.1 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0165448553476841		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.0165448553476841 | validation: 0.02499190087999871]
	TIME [epoch: 55.2 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015797710586213785		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.015797710586213785 | validation: 0.025098315873642092]
	TIME [epoch: 55.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0166703210198433		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.0166703210198433 | validation: 0.02539927249469803]
	TIME [epoch: 55.2 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016780409682205562		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.016780409682205562 | validation: 0.025861730569477295]
	TIME [epoch: 55.2 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01680041005009829		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.01680041005009829 | validation: 0.026625533574663712]
	TIME [epoch: 55.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016149082012221656		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.016149082012221656 | validation: 0.025068789760443643]
	TIME [epoch: 55.2 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015823024952874062		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.015823024952874062 | validation: 0.02643834459124953]
	TIME [epoch: 55.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015924210270819233		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.015924210270819233 | validation: 0.02642662672343336]
	TIME [epoch: 55.2 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016392795747485072		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.016392795747485072 | validation: 0.024691814741788193]
	TIME [epoch: 55.2 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604173948086709		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.01604173948086709 | validation: 0.0252135972303723]
	TIME [epoch: 55.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01620078906825554		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.01620078906825554 | validation: 0.025154249915934783]
	TIME [epoch: 55.2 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016377788811019617		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.016377788811019617 | validation: 0.02614725835465754]
	TIME [epoch: 55.2 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016374767463021286		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.016374767463021286 | validation: 0.02518298602974147]
	TIME [epoch: 55.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016176915737320906		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.016176915737320906 | validation: 0.024742166671114063]
	TIME [epoch: 55.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016267138837013743		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.016267138837013743 | validation: 0.025099938419882638]
	TIME [epoch: 55.2 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016355750429832033		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.016355750429832033 | validation: 0.02675026237769841]
	TIME [epoch: 55.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01610958456841918		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.01610958456841918 | validation: 0.027332223810607893]
	TIME [epoch: 55.4 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016450871903711597		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.016450871903711597 | validation: 0.02511864325455821]
	TIME [epoch: 55.4 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016525672695487704		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.016525672695487704 | validation: 0.02934118067793439]
	TIME [epoch: 55.2 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01670921659074684		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.01670921659074684 | validation: 0.025503494841845747]
	TIME [epoch: 55.4 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016618703379094515		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.016618703379094515 | validation: 0.02575589048198995]
	TIME [epoch: 55.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016019230714192412		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.016019230714192412 | validation: 0.026379430342409693]
	TIME [epoch: 55.2 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0165324795485247		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.0165324795485247 | validation: 0.02590764137083216]
	TIME [epoch: 55.2 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016090170968302883		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.016090170968302883 | validation: 0.02423148534246953]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1364.pth
	Model improved!!!
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01557368123624011		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.01557368123624011 | validation: 0.025263451382193852]
	TIME [epoch: 55.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01641172405872585		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.01641172405872585 | validation: 0.024239813981165294]
	TIME [epoch: 55.3 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016525588773868873		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.016525588773868873 | validation: 0.024919047225633047]
	TIME [epoch: 55.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015661648922747094		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.015661648922747094 | validation: 0.026044503199504167]
	TIME [epoch: 55.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016577834246329245		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.016577834246329245 | validation: 0.02539702521144769]
	TIME [epoch: 55.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016085687896736908		[learning rate: 9.3243e-05]
	Learning Rate: 9.32429e-05
	LOSS [training: 0.016085687896736908 | validation: 0.026129241999143966]
	TIME [epoch: 55.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016229292782128833		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.016229292782128833 | validation: 0.024619845378865583]
	TIME [epoch: 55.4 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016283402159076713		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.016283402159076713 | validation: 0.02632458431549578]
	TIME [epoch: 55.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016106759569478456		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.016106759569478456 | validation: 0.025674126648684296]
	TIME [epoch: 55.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015922072377306345		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.015922072377306345 | validation: 0.024032415734997155]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1c_1_v_mmd1_20250611_071928/states/model_phi1_1a_saddle_v1c_1_v_mmd1_1374.pth
	Model improved!!!
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01602043993852009		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.01602043993852009 | validation: 0.025951838599580324]
	TIME [epoch: 55.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015903190463228047		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.015903190463228047 | validation: 0.02662984669729041]
	TIME [epoch: 55.1 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016293111051221103		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.016293111051221103 | validation: 0.027732494021323155]
	TIME [epoch: 55.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015932561870502716		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.015932561870502716 | validation: 0.025746810861403818]
	TIME [epoch: 55.2 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016005459338818525		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.016005459338818525 | validation: 0.02663893924018324]
	TIME [epoch: 55.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015797878487598394		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.015797878487598394 | validation: 0.02632284470548535]
	TIME [epoch: 55.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016472989037509697		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.016472989037509697 | validation: 0.027305763729283013]
	TIME [epoch: 55.2 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016427152072127072		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.016427152072127072 | validation: 0.026468078204232526]
	TIME [epoch: 55.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0167515225338121		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.0167515225338121 | validation: 0.026316054860450827]
	TIME [epoch: 55.1 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01635558923183845		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.01635558923183845 | validation: 0.028336015329171002]
	TIME [epoch: 55.2 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016052077961287076		[learning rate: 8.8418e-05]
	Learning Rate: 8.84176e-05
	LOSS [training: 0.016052077961287076 | validation: 0.02484258046982051]
	TIME [epoch: 55.2 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01594948394318192		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.01594948394318192 | validation: 0.02609439443462839]
	TIME [epoch: 55.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01601951395428844		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.01601951395428844 | validation: 0.02740444270513861]
	TIME [epoch: 55.1 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01594153811777753		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.01594153811777753 | validation: 0.02738745015125814]
	TIME [epoch: 55.3 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01603143861716539		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.01603143861716539 | validation: 0.02576850893833082]
	TIME [epoch: 55.2 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015963854099494898		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.015963854099494898 | validation: 0.024395553752899424]
	TIME [epoch: 55.1 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016109984211297494		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.016109984211297494 | validation: 0.02646389396582311]
	TIME [epoch: 55.1 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016338184931980623		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.016338184931980623 | validation: 0.025693421748482922]
	TIME [epoch: 55.1 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01622249895658098		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.01622249895658098 | validation: 0.024046465072847422]
	TIME [epoch: 55.2 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016237219365991994		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.016237219365991994 | validation: 0.02715427731495696]
	TIME [epoch: 55.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015868307354400573		[learning rate: 8.534e-05]
	Learning Rate: 8.53403e-05
	LOSS [training: 0.015868307354400573 | validation: 0.02562138015955894]
	TIME [epoch: 55.2 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01657655690890079		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.01657655690890079 | validation: 0.02555957601505158]
	TIME [epoch: 55.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016009344826158976		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.016009344826158976 | validation: 0.025620980365942216]
	TIME [epoch: 55.2 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016134297591757718		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.016134297591757718 | validation: 0.02627911734538234]
	TIME [epoch: 55.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016463713090085		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.016463713090085 | validation: 0.025009547203628114]
	TIME [epoch: 55.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01574981194004995		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.01574981194004995 | validation: 0.02627524018933306]
	TIME [epoch: 55.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015470648300521365		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.015470648300521365 | validation: 0.02779479859233411]
	TIME [epoch: 55.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016343258870669713		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.016343258870669713 | validation: 0.0269425014994705]
	TIME [epoch: 55.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015988719390450913		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.015988719390450913 | validation: 0.026128284383161587]
	TIME [epoch: 55.1 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016061448228055525		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.016061448228055525 | validation: 0.026483015499917825]
	TIME [epoch: 55.2 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015984053649414166		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.015984053649414166 | validation: 0.02524275948849177]
	TIME [epoch: 55.2 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01606078349870415		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.01606078349870415 | validation: 0.02641538443276968]
	TIME [epoch: 55.1 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01571360697693754		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.01571360697693754 | validation: 0.024689266988047933]
	TIME [epoch: 55.1 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015586444523583981		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.015586444523583981 | validation: 0.02624879152157631]
	TIME [epoch: 55.1 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01642159207163079		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.01642159207163079 | validation: 0.026770360872993684]
	TIME [epoch: 55.1 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016200952976927186		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.016200952976927186 | validation: 0.024568966109852226]
	TIME [epoch: 55.1 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0164680534838511		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.0164680534838511 | validation: 0.026576019348388352]
	TIME [epoch: 55.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015882362109078924		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.015882362109078924 | validation: 0.026676067229962047]
	TIME [epoch: 55.2 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015997416228408976		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.015997416228408976 | validation: 0.0259608556567239]
	TIME [epoch: 55.2 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01588787431163775		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.01588787431163775 | validation: 0.026324432030316557]
	TIME [epoch: 55.3 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0156458881429111		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.0156458881429111 | validation: 0.026219883306869828]
	TIME [epoch: 55.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016066306117108468		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.016066306117108468 | validation: 0.025175045774716152]
	TIME [epoch: 55.1 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0157871862997572		[learning rate: 7.8942e-05]
	Learning Rate: 7.89419e-05
	LOSS [training: 0.0157871862997572 | validation: 0.025783252889017203]
	TIME [epoch: 55.1 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01585817654881729		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.01585817654881729 | validation: 0.025675612302885606]
	TIME [epoch: 55.1 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0156426353250394		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.0156426353250394 | validation: 0.026045874587601697]
	TIME [epoch: 55.2 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015824074695041483		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.015824074695041483 | validation: 0.026860617183132134]
	TIME [epoch: 55.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016064322161928934		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.016064322161928934 | validation: 0.025117939718845805]
	TIME [epoch: 55.2 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015938098066507474		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.015938098066507474 | validation: 0.025150969884861464]
	TIME [epoch: 55.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016406406480150966		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.016406406480150966 | validation: 0.026043112803836185]
	TIME [epoch: 55.2 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016510625799265696		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.016510625799265696 | validation: 0.02605433816509143]
	TIME [epoch: 55.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016264569238942927		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.016264569238942927 | validation: 0.024171701202754713]
	TIME [epoch: 55.1 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01571821959286995		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.01571821959286995 | validation: 0.026701546494800388]
	TIME [epoch: 55.2 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015640901277990234		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.015640901277990234 | validation: 0.02533239898083505]
	TIME [epoch: 55.1 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016258251869108717		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.016258251869108717 | validation: 0.025626336618639986]
	TIME [epoch: 55.2 sec]
EPOCH 1429/2000:
	Training over batches...
