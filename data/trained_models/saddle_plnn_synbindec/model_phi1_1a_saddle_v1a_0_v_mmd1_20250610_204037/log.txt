Args:
Namespace(name='model_phi1_1a_saddle_v1a_0_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_0/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05612786114215851, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4090508591

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.120485273486083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.120485273486083 | validation: 6.303281248438825]
	TIME [epoch: 437 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.845929683286288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.845929683286288 | validation: 5.977552318009053]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.594258017054182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.594258017054182 | validation: 5.945139219357489]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.537186170015313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.537186170015313 | validation: 5.525379526202675]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.339980787098393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.339980787098393 | validation: 5.487679737122932]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.181117572117236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.181117572117236 | validation: 5.477342667948317]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.525657681673174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.525657681673174 | validation: 4.79308082621582]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.765108815460987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.765108815460987 | validation: 4.164728613564243]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.210452133133841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.210452133133841 | validation: 3.933964696837055]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8681184042556103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8681184042556103 | validation: 3.347167680873358]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4782372105306205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4782372105306205 | validation: 3.2059094909731494]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323331115813193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.323331115813193 | validation: 2.9822944151251143]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.07617798842846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.07617798842846 | validation: 2.8626655841069875]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.921055387177984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.921055387177984 | validation: 2.8372738651999425]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.855479436774927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.855479436774927 | validation: 2.9711961050177544]
	TIME [epoch: 6.15 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8766108122534417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8766108122534417 | validation: 2.84866251454561]
	TIME [epoch: 6.15 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.833663246664514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.833663246664514 | validation: 2.8494741428728716]
	TIME [epoch: 6.15 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8241744923476952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8241744923476952 | validation: 2.845967212717486]
	TIME [epoch: 6.13 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.819657899960151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.819657899960151 | validation: 2.854088453587856]
	TIME [epoch: 6.14 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8056643497250664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8056643497250664 | validation: 2.844814721420939]
	TIME [epoch: 6.15 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.80065065519459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.80065065519459 | validation: 2.8942326675117775]
	TIME [epoch: 6.15 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.804011521432968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.804011521432968 | validation: 2.8491639898928285]
	TIME [epoch: 6.14 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.775483294875371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.775483294875371 | validation: 2.8703833627354065]
	TIME [epoch: 6.15 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.784009517871387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.784009517871387 | validation: 2.905213610496439]
	TIME [epoch: 6.14 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7861754538455292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7861754538455292 | validation: 2.858931816899787]
	TIME [epoch: 6.14 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7225933485145064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7225933485145064 | validation: 2.9349838275808278]
	TIME [epoch: 6.14 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.764378240807715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.764378240807715 | validation: 2.9509383324837772]
	TIME [epoch: 6.15 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6757723829348636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6757723829348636 | validation: 2.8223695219961584]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.549866423011719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.549866423011719 | validation: 2.8035719933302605]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.428931709568459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.428931709568459 | validation: 2.7299509811739995]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.428819146907797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.428819146907797 | validation: 2.8694056197570106]
	TIME [epoch: 6.16 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3585511662298257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3585511662298257 | validation: 2.7141830251923373]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4084711407133708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4084711407133708 | validation: 2.620980547545207]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.385292752743735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.385292752743735 | validation: 2.695957944808126]
	TIME [epoch: 6.16 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2416547122830837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2416547122830837 | validation: 2.5340837243400887]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.206450081656741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.206450081656741 | validation: 2.569248382812366]
	TIME [epoch: 6.16 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.193293239339616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.193293239339616 | validation: 2.5911712636231012]
	TIME [epoch: 6.16 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1188469681716384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1188469681716384 | validation: 2.4326753477304424]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.168977327012838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.168977327012838 | validation: 2.5747182599515854]
	TIME [epoch: 6.17 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0525114035768652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0525114035768652 | validation: 2.3409723702711958]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1153026709282505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1153026709282505 | validation: 2.266019207884879]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9078745660784127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9078745660784127 | validation: 2.0047404827491637]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7884348022179741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7884348022179741 | validation: 2.043013026398468]
	TIME [epoch: 6.16 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8575656866250583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8575656866250583 | validation: 1.7811947631343126]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7064875327635476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7064875327635476 | validation: 2.0813208321240375]
	TIME [epoch: 6.16 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7680326485725844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7680326485725844 | validation: 1.7962835395843364]
	TIME [epoch: 6.15 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5728799422638426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5728799422638426 | validation: 1.640441396195739]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4460459420564247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4460459420564247 | validation: 1.939287388430183]
	TIME [epoch: 6.17 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6422695285897149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6422695285897149 | validation: 1.621023986589059]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3759069521196765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3759069521196765 | validation: 1.3994311424843082]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4990840716635885		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.4990840716635885 | validation: 1.5873241283225719]
	TIME [epoch: 6.16 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3955112498554108		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.3955112498554108 | validation: 1.2189193673856744]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2799727370300942		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.2799727370300942 | validation: 1.3139509410010182]
	TIME [epoch: 6.17 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2211008348532526		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.2211008348532526 | validation: 1.0352463602145807]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1528009442740188		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.1528009442740188 | validation: 1.161492636496453]
	TIME [epoch: 6.16 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2762196529134757		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.2762196529134757 | validation: 0.9263404767686689]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1531891559585206		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.1531891559585206 | validation: 1.2484441527172483]
	TIME [epoch: 6.15 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1777028971943968		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.1777028971943968 | validation: 1.0144444357964124]
	TIME [epoch: 6.15 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1594357857238837		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.1594357857238837 | validation: 0.9313258410221673]
	TIME [epoch: 6.14 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0875432570121546		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.0875432570121546 | validation: 1.0831106831594817]
	TIME [epoch: 6.14 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0738445799122707		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.0738445799122707 | validation: 1.0897236219906061]
	TIME [epoch: 6.14 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9494762221398699		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.9494762221398699 | validation: 1.1209278032369974]
	TIME [epoch: 6.14 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8790510511848239		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.8790510511848239 | validation: 1.1463637533460744]
	TIME [epoch: 6.15 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9961819129779634		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.9961819129779634 | validation: 0.7720817405178749]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0428424471473912		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.0428424471473912 | validation: 0.7596682203475974]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8436092220408034		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.8436092220408034 | validation: 0.9022731164263746]
	TIME [epoch: 6.15 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8224508881863339		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.8224508881863339 | validation: 0.9226593849975595]
	TIME [epoch: 6.14 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9490574500217348		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.9490574500217348 | validation: 0.8959084803065199]
	TIME [epoch: 6.15 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8038987593417782		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.8038987593417782 | validation: 0.8556166433684396]
	TIME [epoch: 6.15 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7309742675965836		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.7309742675965836 | validation: 0.5875143420214859]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.899322950513405		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.899322950513405 | validation: 0.6558331885243228]
	TIME [epoch: 6.16 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8114122829798621		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.8114122829798621 | validation: 0.7785138680431172]
	TIME [epoch: 6.16 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7392543976004488		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.7392543976004488 | validation: 0.7827242237111393]
	TIME [epoch: 6.15 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.731056833132155		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.731056833132155 | validation: 0.6784795809861996]
	TIME [epoch: 6.15 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8544813113056434		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.8544813113056434 | validation: 0.9322898667720984]
	TIME [epoch: 6.15 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8337781721246476		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.8337781721246476 | validation: 1.0941396002600756]
	TIME [epoch: 6.16 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8656598368543262		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.8656598368543262 | validation: 0.6244437579805262]
	TIME [epoch: 6.17 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6789246672964847		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.6789246672964847 | validation: 0.655738213437851]
	TIME [epoch: 6.16 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7774089070028286		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.7774089070028286 | validation: 0.632208300683849]
	TIME [epoch: 6.16 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6351987823766129		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.6351987823766129 | validation: 0.6415948447142124]
	TIME [epoch: 6.16 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7224126100344932		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.7224126100344932 | validation: 0.9306061624506166]
	TIME [epoch: 6.17 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7645285182466415		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.7645285182466415 | validation: 0.8396886150423495]
	TIME [epoch: 6.17 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6775111399844175		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6775111399844175 | validation: 0.5707363553121367]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5154883821217543		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.5154883821217543 | validation: 0.48505524990713345]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.687885058176901		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.687885058176901 | validation: 0.6081336222653141]
	TIME [epoch: 6.17 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7077866344698996		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.7077866344698996 | validation: 0.7656139107494153]
	TIME [epoch: 6.16 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7274109699673315		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.7274109699673315 | validation: 0.5261648220697785]
	TIME [epoch: 6.16 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5607556335205982		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.5607556335205982 | validation: 0.6769250504741114]
	TIME [epoch: 6.16 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6626396384573291		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.6626396384573291 | validation: 0.7948124902889043]
	TIME [epoch: 6.16 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.591483951448208		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.591483951448208 | validation: 0.6903484491258347]
	TIME [epoch: 6.15 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428201061699022		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.6428201061699022 | validation: 0.9430059813927194]
	TIME [epoch: 6.16 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6154216452192145		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.6154216452192145 | validation: 0.5175114636157294]
	TIME [epoch: 6.15 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6458416363037128		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.6458416363037128 | validation: 0.697747715006817]
	TIME [epoch: 6.16 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5992423179899035		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.5992423179899035 | validation: 0.5304643850810685]
	TIME [epoch: 6.15 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5445923288324284		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.5445923288324284 | validation: 0.4774693982733845]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5302092219235754		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5302092219235754 | validation: 0.40265438793023733]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46769271566849335		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.46769271566849335 | validation: 0.5270493309072715]
	TIME [epoch: 6.16 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4938128925934282		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.4938128925934282 | validation: 0.6746154079555982]
	TIME [epoch: 6.15 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5872405128912802		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.5872405128912802 | validation: 0.7898815613594539]
	TIME [epoch: 6.15 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5709337494805443		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.5709337494805443 | validation: 0.5923726888765016]
	TIME [epoch: 6.15 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.502372268706467		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.502372268706467 | validation: 0.43069198506955375]
	TIME [epoch: 6.16 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4895132134640606		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4895132134640606 | validation: 0.38786921533728175]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4383285993802991		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.4383285993802991 | validation: 0.5066563268143106]
	TIME [epoch: 6.16 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5169509606371872		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5169509606371872 | validation: 0.46710058258857123]
	TIME [epoch: 6.16 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46688466674617374		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.46688466674617374 | validation: 0.43574402838906046]
	TIME [epoch: 6.16 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5643076560223336		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.5643076560223336 | validation: 0.5963225595141834]
	TIME [epoch: 6.16 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49613789671584696		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.49613789671584696 | validation: 0.5922632920624442]
	TIME [epoch: 6.16 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5213161208137024		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.5213161208137024 | validation: 0.4210449535650197]
	TIME [epoch: 6.16 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43391523786978		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.43391523786978 | validation: 0.4634405822970926]
	TIME [epoch: 6.17 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4633609920449492		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.4633609920449492 | validation: 0.4698481332211797]
	TIME [epoch: 6.17 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3971843436492357		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.3971843436492357 | validation: 0.34778771355149085]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46505285945968033		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.46505285945968033 | validation: 0.4966418998995779]
	TIME [epoch: 6.17 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5064733848068637		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.5064733848068637 | validation: 0.6200675930937647]
	TIME [epoch: 6.17 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48106456186497437		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.48106456186497437 | validation: 0.6243028558146021]
	TIME [epoch: 6.17 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5383855966922324		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.5383855966922324 | validation: 0.6149333916453419]
	TIME [epoch: 6.16 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5020329135935698		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.5020329135935698 | validation: 0.34152148597836335]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46540731574418315		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.46540731574418315 | validation: 0.4351674487889542]
	TIME [epoch: 6.16 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39554487787103676		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.39554487787103676 | validation: 0.4086179838710787]
	TIME [epoch: 6.15 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5153866303563401		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.5153866303563401 | validation: 0.47087348485945096]
	TIME [epoch: 6.15 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43978866930066746		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.43978866930066746 | validation: 0.33193229125940715]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4044745734536178		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.4044745734536178 | validation: 0.38296974328740074]
	TIME [epoch: 6.15 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3834695894323864		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.3834695894323864 | validation: 0.6464323494402648]
	TIME [epoch: 6.15 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4448627792521619		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.4448627792521619 | validation: 0.3908473505150112]
	TIME [epoch: 6.15 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38033052660552835		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.38033052660552835 | validation: 0.6222878575653725]
	TIME [epoch: 6.14 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5265774985471818		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.5265774985471818 | validation: 0.8095483820434346]
	TIME [epoch: 6.15 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5209163256793485		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.5209163256793485 | validation: 0.5753531853457102]
	TIME [epoch: 6.15 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38559153631896814		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.38559153631896814 | validation: 0.40308193396171377]
	TIME [epoch: 6.15 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.451347801114221		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.451347801114221 | validation: 0.3877514921256842]
	TIME [epoch: 6.16 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3620603176538124		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.3620603176538124 | validation: 0.36056360334498727]
	TIME [epoch: 6.16 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3891308793528494		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.3891308793528494 | validation: 0.4051138878648006]
	TIME [epoch: 6.17 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4589264103878252		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.4589264103878252 | validation: 0.3640930257269762]
	TIME [epoch: 6.16 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3805485599175426		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.3805485599175426 | validation: 0.36684461139780533]
	TIME [epoch: 6.16 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3603915320075793		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3603915320075793 | validation: 0.3076907716246692]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4027676774457323		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.4027676774457323 | validation: 0.3927727613051568]
	TIME [epoch: 6.16 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3436476346297076		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3436476346297076 | validation: 0.5646678254643225]
	TIME [epoch: 6.15 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4864466722227248		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.4864466722227248 | validation: 0.331889798863257]
	TIME [epoch: 6.16 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34259953846998153		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.34259953846998153 | validation: 0.4098684420841291]
	TIME [epoch: 6.16 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3872598961233553		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.3872598961233553 | validation: 0.31769248703988756]
	TIME [epoch: 6.17 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3558152350367144		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.3558152350367144 | validation: 0.7875073874132417]
	TIME [epoch: 6.17 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5144761600598295		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.5144761600598295 | validation: 0.38914256328309904]
	TIME [epoch: 6.16 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32271744517095496		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.32271744517095496 | validation: 0.31521626961085303]
	TIME [epoch: 6.16 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3741603006437385		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.3741603006437385 | validation: 0.3601836322455209]
	TIME [epoch: 6.16 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137397468982166		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.3137397468982166 | validation: 0.2689054239645716]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3817558655378096		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.3817558655378096 | validation: 0.7685502225662244]
	TIME [epoch: 6.16 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5009422814867766		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.5009422814867766 | validation: 0.3299220935988577]
	TIME [epoch: 6.16 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3534863819638158		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3534863819638158 | validation: 0.3434949152645276]
	TIME [epoch: 6.16 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31103258019301927		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.31103258019301927 | validation: 0.45439147345489916]
	TIME [epoch: 6.16 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34817109052738315		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.34817109052738315 | validation: 0.39305420219979914]
	TIME [epoch: 6.16 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28369860615068193		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.28369860615068193 | validation: 0.23519353011753694]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072654189211818		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.4072654189211818 | validation: 0.4152634714760203]
	TIME [epoch: 6.16 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33748576217344295		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.33748576217344295 | validation: 0.28196051065136624]
	TIME [epoch: 6.16 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3273815763855889		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.3273815763855889 | validation: 0.29545035770661077]
	TIME [epoch: 6.16 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781214331635865		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.2781214331635865 | validation: 0.2349751498811218]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3536925988384748		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.3536925988384748 | validation: 0.3878431955504532]
	TIME [epoch: 6.15 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3010791749998788		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.3010791749998788 | validation: 0.3026346919519427]
	TIME [epoch: 6.15 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34764299929474357		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.34764299929474357 | validation: 0.24494316416242887]
	TIME [epoch: 6.14 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30205406883663494		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.30205406883663494 | validation: 0.2786773379435221]
	TIME [epoch: 6.15 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3155103247435119		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.3155103247435119 | validation: 0.320085327172401]
	TIME [epoch: 6.15 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30784760204281547		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.30784760204281547 | validation: 0.4027529188792618]
	TIME [epoch: 6.14 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.294609012127714		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.294609012127714 | validation: 0.2407551633205978]
	TIME [epoch: 6.14 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29488408861119975		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.29488408861119975 | validation: 0.39521780971556075]
	TIME [epoch: 6.15 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.379983216140357		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.379983216140357 | validation: 0.32553133908014015]
	TIME [epoch: 6.15 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29451131883450987		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.29451131883450987 | validation: 0.24924743262879395]
	TIME [epoch: 6.16 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22466298191315354		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.22466298191315354 | validation: 0.26174300758655933]
	TIME [epoch: 6.15 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665446342750898		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.2665446342750898 | validation: 0.45256814243123544]
	TIME [epoch: 6.15 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41013141699189365		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.41013141699189365 | validation: 0.39988380373773064]
	TIME [epoch: 6.15 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3212457987382467		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.3212457987382467 | validation: 0.2605688268794936]
	TIME [epoch: 6.15 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721763573266504		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.2721763573266504 | validation: 0.37026468993327244]
	TIME [epoch: 6.16 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28909277825756763		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.28909277825756763 | validation: 0.3431395283630216]
	TIME [epoch: 6.15 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25801377223926		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.25801377223926 | validation: 0.22705198138234728]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2212123065231923		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.2212123065231923 | validation: 0.5173590374454314]
	TIME [epoch: 6.16 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3066260512269377		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.3066260512269377 | validation: 0.2879457673347331]
	TIME [epoch: 6.15 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657990629952722		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.2657990629952722 | validation: 0.233656386507705]
	TIME [epoch: 6.16 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2360779979482352		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.2360779979482352 | validation: 0.22511593343162545]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22319027473506806		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.22319027473506806 | validation: 0.3650991150841489]
	TIME [epoch: 6.16 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29538208531615334		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.29538208531615334 | validation: 0.40567325659778164]
	TIME [epoch: 6.16 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3430030923504804		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.3430030923504804 | validation: 0.2246147560147488]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22225495554340843		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.22225495554340843 | validation: 0.3334487834273969]
	TIME [epoch: 6.16 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2632352788580843		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.2632352788580843 | validation: 0.21197967607096813]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21687688071274394		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.21687688071274394 | validation: 0.19510834396382276]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26513043983290635		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.26513043983290635 | validation: 0.3153751695719851]
	TIME [epoch: 6.15 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2332436768005334		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.2332436768005334 | validation: 0.38060840559304976]
	TIME [epoch: 6.15 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2716365067583649		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.2716365067583649 | validation: 0.2593193860250165]
	TIME [epoch: 6.14 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20626882029519547		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.20626882029519547 | validation: 0.17799840409264167]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2229774102218811		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.2229774102218811 | validation: 0.3153997102323074]
	TIME [epoch: 6.15 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26372019101172745		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.26372019101172745 | validation: 0.25465261463454314]
	TIME [epoch: 6.14 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2257926321440649		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.2257926321440649 | validation: 0.2788453919308655]
	TIME [epoch: 6.14 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19683004891206557		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.19683004891206557 | validation: 0.2914861671028016]
	TIME [epoch: 6.14 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23392980877190728		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.23392980877190728 | validation: 0.2785984192694925]
	TIME [epoch: 6.15 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21591550829272832		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.21591550829272832 | validation: 0.24086558164139574]
	TIME [epoch: 6.14 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19415065917139945		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.19415065917139945 | validation: 0.26345798232245016]
	TIME [epoch: 6.15 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22046865752575395		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.22046865752575395 | validation: 0.19127952357391795]
	TIME [epoch: 6.15 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19220263722304298		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.19220263722304298 | validation: 0.2550081327228514]
	TIME [epoch: 6.15 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22427360378161462		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.22427360378161462 | validation: 0.35813099954645505]
	TIME [epoch: 6.16 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24808623355318782		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.24808623355318782 | validation: 0.2841423711305518]
	TIME [epoch: 6.15 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736871822271057		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.2736871822271057 | validation: 0.28473344228787006]
	TIME [epoch: 6.16 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2139043409990177		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.2139043409990177 | validation: 0.20271570118971693]
	TIME [epoch: 6.16 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21020897932305288		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.21020897932305288 | validation: 0.27979027392345446]
	TIME [epoch: 6.17 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2148791228843079		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.2148791228843079 | validation: 0.2192823302798791]
	TIME [epoch: 6.16 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1815575555834904		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.1815575555834904 | validation: 0.3267932634194578]
	TIME [epoch: 6.17 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23782305477765406		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.23782305477765406 | validation: 0.20411317992039602]
	TIME [epoch: 461 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24332560066202796		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.24332560066202796 | validation: 0.29512460960690395]
	TIME [epoch: 12.1 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20693811581861365		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.20693811581861365 | validation: 0.1792021681697657]
	TIME [epoch: 12.1 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21365294281116357		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.21365294281116357 | validation: 0.15701458847476385]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18700950910288078		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.18700950910288078 | validation: 0.23046978594581638]
	TIME [epoch: 12.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22322585808740975		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.22322585808740975 | validation: 0.14233077468552569]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2429097319979212		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.2429097319979212 | validation: 0.1857021772926598]
	TIME [epoch: 12.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18657449254793856		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.18657449254793856 | validation: 0.20104203793891967]
	TIME [epoch: 12.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18309050650235548		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.18309050650235548 | validation: 0.17214489884287199]
	TIME [epoch: 12.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15845495086214506		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.15845495086214506 | validation: 0.3194480395673883]
	TIME [epoch: 12.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20512747350194338		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.20512747350194338 | validation: 0.16042369228886505]
	TIME [epoch: 12.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14507487508834366		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.14507487508834366 | validation: 0.27100889682867646]
	TIME [epoch: 12.1 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23195801268342697		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.23195801268342697 | validation: 0.10833315502267554]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17367960483068393		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.17367960483068393 | validation: 0.2993582744452848]
	TIME [epoch: 12.1 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21767258530142533		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.21767258530142533 | validation: 0.14955508687540403]
	TIME [epoch: 12.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15802567680383328		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.15802567680383328 | validation: 0.16468148688331963]
	TIME [epoch: 12.1 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18865789762835655		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.18865789762835655 | validation: 0.15529396477721202]
	TIME [epoch: 12.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16515133586551942		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.16515133586551942 | validation: 0.1966556900069193]
	TIME [epoch: 12.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19068303466952466		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.19068303466952466 | validation: 0.18131324298166127]
	TIME [epoch: 12.1 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17581037025810572		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.17581037025810572 | validation: 0.1444129813370131]
	TIME [epoch: 12.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1586457155760535		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.1586457155760535 | validation: 0.21547432611066153]
	TIME [epoch: 12.1 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16157905292927682		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.16157905292927682 | validation: 0.23424893940882435]
	TIME [epoch: 12.1 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18854467795945187		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.18854467795945187 | validation: 0.16492444157606084]
	TIME [epoch: 12.1 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17397126664016285		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.17397126664016285 | validation: 0.2573526253226953]
	TIME [epoch: 12.1 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1645714645600994		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.1645714645600994 | validation: 0.13603027334203596]
	TIME [epoch: 12.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17447419406889964		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.17447419406889964 | validation: 0.2651808303048981]
	TIME [epoch: 12.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1644193061198614		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.1644193061198614 | validation: 0.14102384479257737]
	TIME [epoch: 12.1 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13235442203091882		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.13235442203091882 | validation: 0.17712946543297958]
	TIME [epoch: 12.1 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1820565673560724		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.1820565673560724 | validation: 0.12449658117304298]
	TIME [epoch: 12.1 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14086110390895315		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.14086110390895315 | validation: 0.2489449222952814]
	TIME [epoch: 12.1 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.167875688869529		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.167875688869529 | validation: 0.1414706877375622]
	TIME [epoch: 12.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1540685343655096		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.1540685343655096 | validation: 0.20940807483410348]
	TIME [epoch: 12.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1304791878583441		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1304791878583441 | validation: 0.18641461554390248]
	TIME [epoch: 12.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14746775890767533		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.14746775890767533 | validation: 0.30392066755648334]
	TIME [epoch: 12.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1946497882339738		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.1946497882339738 | validation: 0.1346383424292312]
	TIME [epoch: 12.1 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11420453998272152		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.11420453998272152 | validation: 0.15299497134129675]
	TIME [epoch: 12.1 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15469222457690268		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.15469222457690268 | validation: 0.13558512649510435]
	TIME [epoch: 12.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1539392890339284		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1539392890339284 | validation: 0.1300524182205654]
	TIME [epoch: 12.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1392065017785134		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1392065017785134 | validation: 0.1955525029617488]
	TIME [epoch: 12.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14313468522904754		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.14313468522904754 | validation: 0.12430823683940068]
	TIME [epoch: 12.1 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11120589732872488		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.11120589732872488 | validation: 0.1369651712382194]
	TIME [epoch: 12.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18104665400279457		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.18104665400279457 | validation: 0.1261738046544493]
	TIME [epoch: 12.1 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09958374176839423		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.09958374176839423 | validation: 0.1548500144515611]
	TIME [epoch: 12.1 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16018973463943378		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.16018973463943378 | validation: 0.17195235211043514]
	TIME [epoch: 12.1 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18073338731129437		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.18073338731129437 | validation: 0.12196608965951478]
	TIME [epoch: 12.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11825580931336091		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.11825580931336091 | validation: 0.11111234057344138]
	TIME [epoch: 12.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15311902067727862		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.15311902067727862 | validation: 0.1325808179371344]
	TIME [epoch: 12.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1257980309702452		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.1257980309702452 | validation: 0.13074874047674717]
	TIME [epoch: 12.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11754740410163472		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.11754740410163472 | validation: 0.20763103721187695]
	TIME [epoch: 12.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15316709348824106		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.15316709348824106 | validation: 0.11054275832666137]
	TIME [epoch: 12.1 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12417606577746226		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.12417606577746226 | validation: 0.13717337441623237]
	TIME [epoch: 12.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11179367447766077		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.11179367447766077 | validation: 0.12986341335540924]
	TIME [epoch: 12.1 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11577554476509969		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.11577554476509969 | validation: 0.2698992277453324]
	TIME [epoch: 12.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1651089057005615		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.1651089057005615 | validation: 0.08230467476654849]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09640064596603495		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.09640064596603495 | validation: 0.17102227518001384]
	TIME [epoch: 12.1 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16076931979657008		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.16076931979657008 | validation: 0.10312671493249434]
	TIME [epoch: 12.1 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10424321213703477		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.10424321213703477 | validation: 0.11427313607385989]
	TIME [epoch: 12.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1465120895035253		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.1465120895035253 | validation: 0.1423042824623606]
	TIME [epoch: 12.1 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1362215841066401		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.1362215841066401 | validation: 0.1552273814175414]
	TIME [epoch: 12.1 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10902620315930692		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.10902620315930692 | validation: 0.10870602402219925]
	TIME [epoch: 12.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13080350167033497		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.13080350167033497 | validation: 0.20029265917992037]
	TIME [epoch: 12.1 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11138463002415283		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.11138463002415283 | validation: 0.13863771258997204]
	TIME [epoch: 12.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11493040101778446		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.11493040101778446 | validation: 0.11521883917719482]
	TIME [epoch: 12.1 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09246455243165713		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09246455243165713 | validation: 0.15076472652731224]
	TIME [epoch: 12.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15292817868426106		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.15292817868426106 | validation: 0.09972413869593165]
	TIME [epoch: 12.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10838240750993983		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.10838240750993983 | validation: 0.1397469043214815]
	TIME [epoch: 12.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11888578938010912		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.11888578938010912 | validation: 0.17091822305048024]
	TIME [epoch: 12.1 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5878740691360973		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.5878740691360973 | validation: 0.46482264846119836]
	TIME [epoch: 12.1 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3621033510559591		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.3621033510559591 | validation: 0.15298061669110607]
	TIME [epoch: 12.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1462188411233486		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.1462188411233486 | validation: 0.24895465452479548]
	TIME [epoch: 12.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19907387062033263		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.19907387062033263 | validation: 0.1247580966156534]
	TIME [epoch: 12.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10267304901206299		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.10267304901206299 | validation: 0.10193825335143523]
	TIME [epoch: 12.1 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07414371540462972		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.07414371540462972 | validation: 0.08648177988861926]
	TIME [epoch: 12.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10533210515842373		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.10533210515842373 | validation: 0.12519643898173075]
	TIME [epoch: 12.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10839737192919456		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.10839737192919456 | validation: 0.1537414775143256]
	TIME [epoch: 12.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10797667878221553		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.10797667878221553 | validation: 0.10977475173049317]
	TIME [epoch: 12.1 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10009847481769601		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.10009847481769601 | validation: 0.16299552106307663]
	TIME [epoch: 12.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08698989012335703		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.08698989012335703 | validation: 0.11957939371826334]
	TIME [epoch: 12.1 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.102786527101553		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.102786527101553 | validation: 0.21411777021698503]
	TIME [epoch: 12.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1284824674185118		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.1284824674185118 | validation: 0.12284757285883927]
	TIME [epoch: 12.1 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09393947127068966		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.09393947127068966 | validation: 0.1447718101805816]
	TIME [epoch: 12.1 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12261556421252455		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.12261556421252455 | validation: 0.1036221401872025]
	TIME [epoch: 12.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09738085786730835		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.09738085786730835 | validation: 0.0971508972567873]
	TIME [epoch: 12.1 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.095427644883981		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.095427644883981 | validation: 0.13335102800967802]
	TIME [epoch: 12.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07054645087109582		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.07054645087109582 | validation: 0.05958017295870739]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060573142063776154		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.060573142063776154 | validation: 0.1471653146795809]
	TIME [epoch: 12.1 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17378601665816157		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.17378601665816157 | validation: 0.09356047467884564]
	TIME [epoch: 12.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09125122035497314		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.09125122035497314 | validation: 0.10173713786646887]
	TIME [epoch: 12.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.094467774950388		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.094467774950388 | validation: 0.1315513900637372]
	TIME [epoch: 12.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11057294012463309		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.11057294012463309 | validation: 0.10992868240250042]
	TIME [epoch: 12.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08175367129542872		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.08175367129542872 | validation: 0.06415418238757314]
	TIME [epoch: 12.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10819206173811435		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.10819206173811435 | validation: 0.13914928186007255]
	TIME [epoch: 12.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11890345475922853		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.11890345475922853 | validation: 0.10361842857790315]
	TIME [epoch: 12.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08707710054662937		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.08707710054662937 | validation: 0.0906705422181249]
	TIME [epoch: 12.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10244981090328487		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.10244981090328487 | validation: 0.09696321875075015]
	TIME [epoch: 12.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07586979306818373		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.07586979306818373 | validation: 0.14180110410665708]
	TIME [epoch: 12.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10168739858224518		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.10168739858224518 | validation: 0.1258742683947057]
	TIME [epoch: 12.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08839189450791339		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.08839189450791339 | validation: 0.13829810177740365]
	TIME [epoch: 12.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09906735255012798		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.09906735255012798 | validation: 0.07198209986769487]
	TIME [epoch: 12.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0857302200276702		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.0857302200276702 | validation: 0.13719066280374909]
	TIME [epoch: 12.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10286944331736311		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.10286944331736311 | validation: 0.10857669974956087]
	TIME [epoch: 12.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0931397368907756		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.0931397368907756 | validation: 0.08779924933229667]
	TIME [epoch: 12.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08163010710724694		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.08163010710724694 | validation: 0.11009652231149988]
	TIME [epoch: 12.1 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08784304212618303		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.08784304212618303 | validation: 0.09031500485905583]
	TIME [epoch: 12.1 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07040846295979407		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.07040846295979407 | validation: 0.12826190683233873]
	TIME [epoch: 12.1 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08367545671182555		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.08367545671182555 | validation: 0.1588693170676769]
	TIME [epoch: 12.1 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10085948903836955		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.10085948903836955 | validation: 0.11544094034857706]
	TIME [epoch: 12.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08494490989898824		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.08494490989898824 | validation: 0.09956625313207422]
	TIME [epoch: 12.1 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06878095097744803		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.06878095097744803 | validation: 0.11209491907643851]
	TIME [epoch: 12.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10020818399666656		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.10020818399666656 | validation: 0.10298709072463678]
	TIME [epoch: 12.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06892279179818858		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.06892279179818858 | validation: 0.0891827140630044]
	TIME [epoch: 12.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10607833309808942		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.10607833309808942 | validation: 0.12262256242744904]
	TIME [epoch: 12.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0861331592807384		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.0861331592807384 | validation: 0.09092722518062352]
	TIME [epoch: 12.1 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05918818205291304		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.05918818205291304 | validation: 0.07794820953496939]
	TIME [epoch: 12.1 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09640867605369813		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.09640867605369813 | validation: 0.07378066779399384]
	TIME [epoch: 12.1 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053553644455654456		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.053553644455654456 | validation: 0.11337268265815244]
	TIME [epoch: 12.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08336567235994377		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.08336567235994377 | validation: 0.07472440218452817]
	TIME [epoch: 12.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07666378274921354		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.07666378274921354 | validation: 0.2193322496648215]
	TIME [epoch: 12.1 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11192747452627796		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.11192747452627796 | validation: 0.08737225162565258]
	TIME [epoch: 12.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07446776611428965		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.07446776611428965 | validation: 0.0634501267636223]
	TIME [epoch: 12.1 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08509271800594348		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.08509271800594348 | validation: 0.1028931514608443]
	TIME [epoch: 12.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06657249556874653		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.06657249556874653 | validation: 0.08754133449894792]
	TIME [epoch: 12.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06419097993897098		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.06419097993897098 | validation: 0.05432923153307465]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06985741965946175		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.06985741965946175 | validation: 0.19988181706781627]
	TIME [epoch: 12.1 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09917566119015497		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.09917566119015497 | validation: 0.12032779208970587]
	TIME [epoch: 12.1 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06458785438277864		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.06458785438277864 | validation: 0.06138882793517174]
	TIME [epoch: 12.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06150025728112782		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.06150025728112782 | validation: 0.10626132276665157]
	TIME [epoch: 12.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06853150375173747		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.06853150375173747 | validation: 0.1397368002459991]
	TIME [epoch: 12.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09785825338807938		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.09785825338807938 | validation: 0.060521353092496954]
	TIME [epoch: 12.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06397664686460919		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.06397664686460919 | validation: 0.09910857668674908]
	TIME [epoch: 12.1 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09129153387307543		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.09129153387307543 | validation: 0.07828554552988373]
	TIME [epoch: 12.1 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05415785623640752		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.05415785623640752 | validation: 0.07518703481558583]
	TIME [epoch: 12.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07842160426897102		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.07842160426897102 | validation: 0.11415020682737761]
	TIME [epoch: 12.1 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07227208180391739		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.07227208180391739 | validation: 0.06703963181668765]
	TIME [epoch: 12.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07785239928384739		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.07785239928384739 | validation: 0.07503269291124878]
	TIME [epoch: 12.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0633295030884823		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.0633295030884823 | validation: 0.08051752561756656]
	TIME [epoch: 12.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07037774582897802		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.07037774582897802 | validation: 0.09235269172235598]
	TIME [epoch: 12.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07844771094803557		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.07844771094803557 | validation: 0.08602446705658537]
	TIME [epoch: 12.1 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07654594367204884		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.07654594367204884 | validation: 0.05726124347010129]
	TIME [epoch: 12.1 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06314290216467101		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.06314290216467101 | validation: 0.07082687000642021]
	TIME [epoch: 12.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0690764254842888		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.0690764254842888 | validation: 0.1371004139063332]
	TIME [epoch: 12.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06773074852294127		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.06773074852294127 | validation: 0.05910154513125563]
	TIME [epoch: 12.1 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05361129927863371		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.05361129927863371 | validation: 0.06040409413754173]
	TIME [epoch: 12.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06674989727436453		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.06674989727436453 | validation: 0.08040178491680966]
	TIME [epoch: 12.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0635155953153662		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.0635155953153662 | validation: 0.09249029087797322]
	TIME [epoch: 12.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06638124802985194		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.06638124802985194 | validation: 0.08978722926415135]
	TIME [epoch: 12.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07045500473771649		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.07045500473771649 | validation: 0.07777028469395017]
	TIME [epoch: 12.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06491960423566281		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.06491960423566281 | validation: 0.08512775135410296]
	TIME [epoch: 12.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06092020403358447		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.06092020403358447 | validation: 0.07663640541529795]
	TIME [epoch: 12.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07585979726240183		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.07585979726240183 | validation: 0.056246771288269666]
	TIME [epoch: 12.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04630380823494258		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.04630380823494258 | validation: 0.06461804062571255]
	TIME [epoch: 12.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07464289398116468		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07464289398116468 | validation: 0.09889148249504887]
	TIME [epoch: 12.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05748612285740903		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.05748612285740903 | validation: 0.06001407138351068]
	TIME [epoch: 12.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04829462744395734		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.04829462744395734 | validation: 0.11020302381308417]
	TIME [epoch: 12.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08133516351883624		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.08133516351883624 | validation: 0.0879348070775987]
	TIME [epoch: 12.1 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07131531957362088		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.07131531957362088 | validation: 0.07791945318924856]
	TIME [epoch: 12.1 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05417728294908804		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.05417728294908804 | validation: 0.07160839522137598]
	TIME [epoch: 12.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0662285038954736		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.0662285038954736 | validation: 0.06850639828859784]
	TIME [epoch: 12.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047370679949282915		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.047370679949282915 | validation: 0.07458370398017519]
	TIME [epoch: 12.1 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07489023741590936		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.07489023741590936 | validation: 0.053755788403161496]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03927027280953769		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.03927027280953769 | validation: 0.08316490126474083]
	TIME [epoch: 12.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06463279433048068		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.06463279433048068 | validation: 0.10923473664166619]
	TIME [epoch: 12.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060562550426973116		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.060562550426973116 | validation: 0.06354166521466996]
	TIME [epoch: 12.1 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05639536041579984		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.05639536041579984 | validation: 0.06701527889575123]
	TIME [epoch: 12.1 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06240436653070462		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.06240436653070462 | validation: 0.0700217826365513]
	TIME [epoch: 12.1 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050201150502910095		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.050201150502910095 | validation: 0.06070870193045363]
	TIME [epoch: 12.1 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058335865327412945		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.058335865327412945 | validation: 0.0509543848891164]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05946294370363189		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.05946294370363189 | validation: 0.05634157876459556]
	TIME [epoch: 12.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05219400979203972		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.05219400979203972 | validation: 0.06770719375455098]
	TIME [epoch: 12.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04923593619861527		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.04923593619861527 | validation: 0.10659112650366456]
	TIME [epoch: 12.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06772260287325434		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.06772260287325434 | validation: 0.07256055996928568]
	TIME [epoch: 12.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04821212740255816		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.04821212740255816 | validation: 0.05578726783431033]
	TIME [epoch: 12.1 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07115201120203118		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.07115201120203118 | validation: 0.05766673286032874]
	TIME [epoch: 12.1 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05314282973305514		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.05314282973305514 | validation: 0.05973887712640399]
	TIME [epoch: 12.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0522276819972603		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.0522276819972603 | validation: 0.0788247975982668]
	TIME [epoch: 12.1 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055891488687517366		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.055891488687517366 | validation: 0.05185901331712469]
	TIME [epoch: 12.1 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0529068349863181		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.0529068349863181 | validation: 0.05492485946136456]
	TIME [epoch: 12.1 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05470945600424715		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.05470945600424715 | validation: 0.07400665523801829]
	TIME [epoch: 12.1 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04658818553016611		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.04658818553016611 | validation: 0.07901556348603656]
	TIME [epoch: 12.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056812179001619424		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.056812179001619424 | validation: 0.07459750646147083]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05648980050713483		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.05648980050713483 | validation: 0.07528382379092924]
	TIME [epoch: 12.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04838457593697279		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.04838457593697279 | validation: 0.04861221099174763]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03901588516057684		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.03901588516057684 | validation: 0.06989572043092437]
	TIME [epoch: 12.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0631045270857293		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.0631045270857293 | validation: 0.05262820605371964]
	TIME [epoch: 12.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05001082112873816		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.05001082112873816 | validation: 0.0760472023663711]
	TIME [epoch: 12.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039347079846974754		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.039347079846974754 | validation: 0.07835244911101073]
	TIME [epoch: 12.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04805411426636054		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.04805411426636054 | validation: 0.07917386327605277]
	TIME [epoch: 12.1 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06352346760591658		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.06352346760591658 | validation: 0.040731609741389146]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0380653617643959		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.0380653617643959 | validation: 0.07381161307692936]
	TIME [epoch: 12.1 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0605632693597781		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.0605632693597781 | validation: 0.04653041008116125]
	TIME [epoch: 12.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04079462207370267		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.04079462207370267 | validation: 0.045577196364884734]
	TIME [epoch: 12.1 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06167430075603583		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.06167430075603583 | validation: 0.06968943074030923]
	TIME [epoch: 12.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06333633764645075		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.06333633764645075 | validation: 0.06484056942081093]
	TIME [epoch: 12.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05392258044805821		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.05392258044805821 | validation: 0.06987965409838912]
	TIME [epoch: 12.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047600446390706826		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.047600446390706826 | validation: 0.03980062915467699]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03486599619024045		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.03486599619024045 | validation: 0.08274978577755951]
	TIME [epoch: 12.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0560231691410272		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.0560231691410272 | validation: 0.07066446470581284]
	TIME [epoch: 12.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04397593427783317		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.04397593427783317 | validation: 0.057143360041501653]
	TIME [epoch: 12.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05080289610731903		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.05080289610731903 | validation: 0.0930167402846822]
	TIME [epoch: 12.1 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056085791846021214		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.056085791846021214 | validation: 0.06349916897645555]
	TIME [epoch: 12.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041183045955983655		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.041183045955983655 | validation: 0.047715784288485594]
	TIME [epoch: 12.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037371156533764854		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.037371156533764854 | validation: 0.0894037609498036]
	TIME [epoch: 12.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060760807401121955		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.060760807401121955 | validation: 0.06156526693329615]
	TIME [epoch: 12.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02882149317662882		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.02882149317662882 | validation: 0.047314114273488486]
	TIME [epoch: 12.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04478450012579848		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.04478450012579848 | validation: 0.1299212381740949]
	TIME [epoch: 12.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06679905789784099		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.06679905789784099 | validation: 0.07452026968239026]
	TIME [epoch: 12.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06099292860931805		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.06099292860931805 | validation: 0.04284795341597889]
	TIME [epoch: 12.1 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029106754966470545		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.029106754966470545 | validation: 0.04660106269769345]
	TIME [epoch: 12.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03861301213867058		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.03861301213867058 | validation: 0.10762765771530669]
	TIME [epoch: 12.1 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06157123610091582		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.06157123610091582 | validation: 0.058396584770449504]
	TIME [epoch: 12.1 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04994624355445506		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.04994624355445506 | validation: 0.052734411944338117]
	TIME [epoch: 12.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03490741937946017		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.03490741937946017 | validation: 0.028937171899060747]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04309573141751204		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04309573141751204 | validation: 0.08125253685383019]
	TIME [epoch: 12.1 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04770276020520658		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.04770276020520658 | validation: 0.053605931033818964]
	TIME [epoch: 12.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04051849258479308		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.04051849258479308 | validation: 0.048106143523694196]
	TIME [epoch: 12.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051895663812621645		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.051895663812621645 | validation: 0.05007074946736867]
	TIME [epoch: 12.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028452432136977852		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.028452432136977852 | validation: 0.0426010554822332]
	TIME [epoch: 12.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04628274927734615		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.04628274927734615 | validation: 0.11364057678944775]
	TIME [epoch: 12.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052214530563848095		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.052214530563848095 | validation: 0.04499081031657091]
	TIME [epoch: 12.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03865485023044897		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.03865485023044897 | validation: 0.05388366738424874]
	TIME [epoch: 12.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045462841170389345		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.045462841170389345 | validation: 0.07672327570136543]
	TIME [epoch: 12.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04637392523720259		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.04637392523720259 | validation: 0.03663085545857805]
	TIME [epoch: 12.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029849673865480333		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.029849673865480333 | validation: 0.04765878374053281]
	TIME [epoch: 12.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05329253398636542		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.05329253398636542 | validation: 0.07238632541979727]
	TIME [epoch: 12.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036870821697344354		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.036870821697344354 | validation: 0.053618460855669896]
	TIME [epoch: 12.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03725191913490051		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.03725191913490051 | validation: 0.03201489986594134]
	TIME [epoch: 12.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024031404857727656		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.024031404857727656 | validation: 0.05826948987746747]
	TIME [epoch: 12.1 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06804829968735622		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.06804829968735622 | validation: 0.07538381507466915]
	TIME [epoch: 12.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031673260760629124		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.031673260760629124 | validation: 0.02810723497214194]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024387190980419093		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.024387190980419093 | validation: 0.05029040078939864]
	TIME [epoch: 12.1 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07108974446997798		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.07108974446997798 | validation: 0.04806521482057147]
	TIME [epoch: 12.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03822291663148213		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.03822291663148213 | validation: 0.04752705619594647]
	TIME [epoch: 12.1 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040861862329552356		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.040861862329552356 | validation: 0.04382783855940013]
	TIME [epoch: 12.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030588384112766438		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.030588384112766438 | validation: 0.03089403673943243]
	TIME [epoch: 12.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03487220265068579		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.03487220265068579 | validation: 0.04414131711643306]
	TIME [epoch: 12.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04036452441813203		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.04036452441813203 | validation: 0.09200776384713237]
	TIME [epoch: 12.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042990470032426134		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.042990470032426134 | validation: 0.05034328838053041]
	TIME [epoch: 12.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562693683057257		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.03562693683057257 | validation: 0.05657873086270474]
	TIME [epoch: 12.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041183811358346055		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.041183811358346055 | validation: 0.03027604004737461]
	TIME [epoch: 12.1 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024845195362063633		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.024845195362063633 | validation: 0.036140435104070556]
	TIME [epoch: 12.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0578871910537567		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.0578871910537567 | validation: 0.04251025374869569]
	TIME [epoch: 12.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032127185758953916		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.032127185758953916 | validation: 0.06441963985326248]
	TIME [epoch: 12.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04688023289742862		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.04688023289742862 | validation: 0.04761655090745793]
	TIME [epoch: 12.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03573866655127589		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.03573866655127589 | validation: 0.036194364004098784]
	TIME [epoch: 12.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03876842434634256		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.03876842434634256 | validation: 0.03396510669730957]
	TIME [epoch: 12.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04387393811241157		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04387393811241157 | validation: 0.0582128273818556]
	TIME [epoch: 12.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029775741481378945		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.029775741481378945 | validation: 0.04346087248408506]
	TIME [epoch: 12.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04224888203675184		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.04224888203675184 | validation: 0.047283468784944466]
	TIME [epoch: 12.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03273242197710942		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.03273242197710942 | validation: 0.03759050686262885]
	TIME [epoch: 12.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03275476485692423		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.03275476485692423 | validation: 0.045512449623153754]
	TIME [epoch: 12.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03772301797476928		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.03772301797476928 | validation: 0.04551925304547066]
	TIME [epoch: 12.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030021995840511607		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.030021995840511607 | validation: 0.10703676585050133]
	TIME [epoch: 12.1 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056852662953384384		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.056852662953384384 | validation: 0.058293139075744724]
	TIME [epoch: 12.1 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035729964551471136		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.035729964551471136 | validation: 0.04228851224694896]
	TIME [epoch: 12.1 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03157710343005581		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.03157710343005581 | validation: 0.057339708710569315]
	TIME [epoch: 12.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03864220577462415		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.03864220577462415 | validation: 0.04014478501576943]
	TIME [epoch: 12.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03383058639234742		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.03383058639234742 | validation: 0.06135391619906351]
	TIME [epoch: 12.1 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03553633919001811		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.03553633919001811 | validation: 0.03761967231699322]
	TIME [epoch: 12.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026919145159678793		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.026919145159678793 | validation: 0.0307841514040545]
	TIME [epoch: 12.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04177058316912529		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.04177058316912529 | validation: 0.043413394737042185]
	TIME [epoch: 12.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025079804806960294		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.025079804806960294 | validation: 0.0639735281183649]
	TIME [epoch: 12.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04117528122622612		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.04117528122622612 | validation: 0.04045052090807842]
	TIME [epoch: 12.1 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02803641362613781		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.02803641362613781 | validation: 0.03261456104816241]
	TIME [epoch: 12.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03738397516379216		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.03738397516379216 | validation: 0.05593713344802151]
	TIME [epoch: 12.1 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03052795348738637		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.03052795348738637 | validation: 0.029376253561483323]
	TIME [epoch: 12.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024918775721901047		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.024918775721901047 | validation: 0.07310071925836054]
	TIME [epoch: 12.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046388258708449905		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.046388258708449905 | validation: 0.05485721266409723]
	TIME [epoch: 12.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035138141246956		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.035138141246956 | validation: 0.023738260811469153]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019409779821056176		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.019409779821056176 | validation: 0.03703280494837174]
	TIME [epoch: 12.1 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04775275780713535		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.04775275780713535 | validation: 0.03425304447949261]
	TIME [epoch: 12.1 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02798587843103236		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.02798587843103236 | validation: 0.039855548368268885]
	TIME [epoch: 12.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026493402254235278		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.026493402254235278 | validation: 0.04764127385383834]
	TIME [epoch: 12.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038113954822204005		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.038113954822204005 | validation: 0.028722936487645775]
	TIME [epoch: 12.1 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029540165412622443		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.029540165412622443 | validation: 0.06830450329259202]
	TIME [epoch: 12.1 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03731481860173628		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.03731481860173628 | validation: 0.04214464815154669]
	TIME [epoch: 12.1 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03146704271253092		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.03146704271253092 | validation: 0.04544803739452341]
	TIME [epoch: 12.1 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024302053986447414		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.024302053986447414 | validation: 0.05791611804503961]
	TIME [epoch: 12.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03787876831375479		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.03787876831375479 | validation: 0.02495785374976832]
	TIME [epoch: 12.1 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02083294822850585		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.02083294822850585 | validation: 0.026940997485203996]
	TIME [epoch: 12.1 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03804831963185525		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.03804831963185525 | validation: 0.06535629664328582]
	TIME [epoch: 12.1 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03261551572609697		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03261551572609697 | validation: 0.041649437055631004]
	TIME [epoch: 12.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025407088336301124		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.025407088336301124 | validation: 0.026473268192546258]
	TIME [epoch: 12.1 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03660145094109674		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.03660145094109674 | validation: 0.044656736625227414]
	TIME [epoch: 12.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027833910924309695		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.027833910924309695 | validation: 0.031181006092025572]
	TIME [epoch: 12.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03268045039272471		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.03268045039272471 | validation: 0.03851341866195919]
	TIME [epoch: 12.1 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024560273636588678		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.024560273636588678 | validation: 0.029245133304962846]
	TIME [epoch: 12.1 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03516043220656657		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.03516043220656657 | validation: 0.0510176138630819]
	TIME [epoch: 12.1 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02979718000763195		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.02979718000763195 | validation: 0.03577300533612643]
	TIME [epoch: 12.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02852278741605465		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.02852278741605465 | validation: 0.04184591106975555]
	TIME [epoch: 12.1 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03012840234905817		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.03012840234905817 | validation: 0.04286032854808283]
	TIME [epoch: 12.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0235400360472266		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.0235400360472266 | validation: 0.03164053184292751]
	TIME [epoch: 12.1 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029166558232370944		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.029166558232370944 | validation: 0.03372615058920469]
	TIME [epoch: 12.1 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251981679467219		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.03251981679467219 | validation: 0.05847588693290447]
	TIME [epoch: 12.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030580126467845668		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.030580126467845668 | validation: 0.04520461699507693]
	TIME [epoch: 12.1 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03250523420403273		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.03250523420403273 | validation: 0.034224009837718984]
	TIME [epoch: 12.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030044514863500148		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.030044514863500148 | validation: 0.03544031641985476]
	TIME [epoch: 12.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02434475150993718		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.02434475150993718 | validation: 0.03980458495013467]
	TIME [epoch: 12.1 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026833433576410177		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.026833433576410177 | validation: 0.027655820862184596]
	TIME [epoch: 12.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03615009356172232		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.03615009356172232 | validation: 0.03129385989733559]
	TIME [epoch: 12.1 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020662211978737393		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.020662211978737393 | validation: 0.026384045212358546]
	TIME [epoch: 12.1 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025301184669356616		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.025301184669356616 | validation: 0.06030664510998185]
	TIME [epoch: 477 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04406100716579805		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.04406100716579805 | validation: 0.023392826713286498]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02104657232742669		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.02104657232742669 | validation: 0.04753531864819957]
	TIME [epoch: 25.9 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02859620311547643		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.02859620311547643 | validation: 0.026879057019506263]
	TIME [epoch: 25.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02676323813609658		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.02676323813609658 | validation: 0.022688767226276232]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02290608219264472		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.02290608219264472 | validation: 0.06106434228351687]
	TIME [epoch: 25.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03367278404692426		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.03367278404692426 | validation: 0.024596116046921697]
	TIME [epoch: 25.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020753076450941946		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.020753076450941946 | validation: 0.03842279685827102]
	TIME [epoch: 25.9 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027602486184431813		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.027602486184431813 | validation: 0.06665159808717365]
	TIME [epoch: 26 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035102570144836864		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.035102570144836864 | validation: 0.028385081307783423]
	TIME [epoch: 26 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021288129452461942		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.021288129452461942 | validation: 0.04115618061743986]
	TIME [epoch: 26 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025772731003243364		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.025772731003243364 | validation: 0.04635156021960515]
	TIME [epoch: 26 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034090124242065166		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.034090124242065166 | validation: 0.02920759248476388]
	TIME [epoch: 26 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02983667511388107		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.02983667511388107 | validation: 0.03869426959454619]
	TIME [epoch: 26 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027539126733622812		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.027539126733622812 | validation: 0.03892366181169245]
	TIME [epoch: 26 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026480265700250704		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.026480265700250704 | validation: 0.03499122026542325]
	TIME [epoch: 26 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02412474228876975		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.02412474228876975 | validation: 0.03891396704659404]
	TIME [epoch: 25.9 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024324760267053096		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.024324760267053096 | validation: 0.04263741548686645]
	TIME [epoch: 25.9 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026158336850612553		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.026158336850612553 | validation: 0.022931742768929207]
	TIME [epoch: 26 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020870771273391743		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.020870771273391743 | validation: 0.048263718702959806]
	TIME [epoch: 26 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029779929983232135		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.029779929983232135 | validation: 0.027800709674429547]
	TIME [epoch: 26 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0207774008514907		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.0207774008514907 | validation: 0.05179339043979225]
	TIME [epoch: 25.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0324471061157791		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.0324471061157791 | validation: 0.0453316333699388]
	TIME [epoch: 25.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02828816709129712		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.02828816709129712 | validation: 0.022818543156476665]
	TIME [epoch: 26 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020220921437908896		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.020220921437908896 | validation: 0.023803162210570227]
	TIME [epoch: 26 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021069373778174357		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.021069373778174357 | validation: 0.02766242860800861]
	TIME [epoch: 26 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023982310510982662		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.023982310510982662 | validation: 0.051731325901993086]
	TIME [epoch: 26 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03494761413059774		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.03494761413059774 | validation: 0.026804293253074804]
	TIME [epoch: 26 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024003844059752543		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.024003844059752543 | validation: 0.027167170735917748]
	TIME [epoch: 26 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02569877982277363		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.02569877982277363 | validation: 0.03847596349939908]
	TIME [epoch: 25.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017928667119064223		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.017928667119064223 | validation: 0.019374566671001472]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027791987669333416		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.027791987669333416 | validation: 0.05070186314798554]
	TIME [epoch: 26 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02772854660351195		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.02772854660351195 | validation: 0.02560261773463199]
	TIME [epoch: 26 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02114150452309861		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.02114150452309861 | validation: 0.0450783903620008]
	TIME [epoch: 25.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029641376799877616		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.029641376799877616 | validation: 0.023823911735740295]
	TIME [epoch: 25.9 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01629402094570869		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.01629402094570869 | validation: 0.02228105790914854]
	TIME [epoch: 26 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026702405589166235		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.026702405589166235 | validation: 0.03887335024144546]
	TIME [epoch: 26 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018913842113634265		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.018913842113634265 | validation: 0.030568509035384224]
	TIME [epoch: 26 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027108398392576977		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.027108398392576977 | validation: 0.034055266808754724]
	TIME [epoch: 26 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023183733980994152		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.023183733980994152 | validation: 0.02903658299543391]
	TIME [epoch: 26 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025808816509984636		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.025808816509984636 | validation: 0.026795370892279992]
	TIME [epoch: 26 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023089518776918026		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.023089518776918026 | validation: 0.03215555111145428]
	TIME [epoch: 25.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019930575937803518		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.019930575937803518 | validation: 0.02781467562392609]
	TIME [epoch: 25.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025192191473448963		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.025192191473448963 | validation: 0.04302910952562275]
	TIME [epoch: 25.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021377273062807844		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.021377273062807844 | validation: 0.020169330870163996]
	TIME [epoch: 26 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021205077553888715		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.021205077553888715 | validation: 0.03320128449020344]
	TIME [epoch: 26 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021649514084450452		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.021649514084450452 | validation: 0.025258859378771138]
	TIME [epoch: 26 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02077250594430626		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.02077250594430626 | validation: 0.06318971899763573]
	TIME [epoch: 26 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031673677170946754		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.031673677170946754 | validation: 0.03616683222565344]
	TIME [epoch: 25.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023178972994965463		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.023178972994965463 | validation: 0.02391407061754231]
	TIME [epoch: 25.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0200631809384122		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.0200631809384122 | validation: 0.029297386569386383]
	TIME [epoch: 26 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021357688045260094		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.021357688045260094 | validation: 0.025180264470691802]
	TIME [epoch: 26 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026184401767583465		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.026184401767583465 | validation: 0.025755825461364838]
	TIME [epoch: 26 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018945865320262267		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.018945865320262267 | validation: 0.03222059386031122]
	TIME [epoch: 26 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019778155570248326		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.019778155570248326 | validation: 0.021167752010211142]
	TIME [epoch: 26 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020732103876686017		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.020732103876686017 | validation: 0.039977725225611244]
	TIME [epoch: 26 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02834343584084077		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.02834343584084077 | validation: 0.030417824431849254]
	TIME [epoch: 26 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020511215841292528		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.020511215841292528 | validation: 0.03221614806316526]
	TIME [epoch: 25.9 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018022548400548837		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.018022548400548837 | validation: 0.03327172193259474]
	TIME [epoch: 25.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023826381111767346		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.023826381111767346 | validation: 0.02456179347494101]
	TIME [epoch: 25.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021727939500332433		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.021727939500332433 | validation: 0.03273054512847144]
	TIME [epoch: 26 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022733157192123165		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.022733157192123165 | validation: 0.033791800093412185]
	TIME [epoch: 26 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024405246656841587		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.024405246656841587 | validation: 0.02509247089764778]
	TIME [epoch: 26 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017141750526857846		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.017141750526857846 | validation: 0.02372295737307831]
	TIME [epoch: 25.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017797590426339355		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.017797590426339355 | validation: 0.026072972727589783]
	TIME [epoch: 25.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019645467777744126		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.019645467777744126 | validation: 0.03664536213471809]
	TIME [epoch: 25.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024937930704407855		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.024937930704407855 | validation: 0.02975999854130443]
	TIME [epoch: 25.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01836812563113978		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.01836812563113978 | validation: 0.02869339680684905]
	TIME [epoch: 26 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022813107177494955		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.022813107177494955 | validation: 0.02221693578990524]
	TIME [epoch: 26 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020827583034187		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.020827583034187 | validation: 0.03319126920359119]
	TIME [epoch: 26 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02381193559878255		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.02381193559878255 | validation: 0.01989786964489573]
	TIME [epoch: 26 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016447159521934828		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.016447159521934828 | validation: 0.028360869534645755]
	TIME [epoch: 25.9 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022193841281914983		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.022193841281914983 | validation: 0.01980070895488116]
	TIME [epoch: 26 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018178296607210608		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.018178296607210608 | validation: 0.04620888038622202]
	TIME [epoch: 26 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027824415297770192		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.027824415297770192 | validation: 0.0238192613262226]
	TIME [epoch: 25.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020443452397445938		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.020443452397445938 | validation: 0.03158214296498102]
	TIME [epoch: 25.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02161567013285755		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.02161567013285755 | validation: 0.02408299777608526]
	TIME [epoch: 25.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01675150237383579		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.01675150237383579 | validation: 0.02201304994460126]
	TIME [epoch: 26 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022404578510289888		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.022404578510289888 | validation: 0.024409335724758112]
	TIME [epoch: 26 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022207971445909757		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.022207971445909757 | validation: 0.02546849634228978]
	TIME [epoch: 25.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01976696426927184		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.01976696426927184 | validation: 0.016804905631746945]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017452793987284283		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.017452793987284283 | validation: 0.02198420581719024]
	TIME [epoch: 26 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02133844125214401		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.02133844125214401 | validation: 0.022832606478109124]
	TIME [epoch: 26 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01770982751173674		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.01770982751173674 | validation: 0.020894140167063303]
	TIME [epoch: 25.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020489626623899856		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.020489626623899856 | validation: 0.033147943497580414]
	TIME [epoch: 26 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017291956998180622		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.017291956998180622 | validation: 0.031891763356335714]
	TIME [epoch: 25.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02559423001306313		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.02559423001306313 | validation: 0.025458133686101513]
	TIME [epoch: 26 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01665383453461151		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.01665383453461151 | validation: 0.019619926380971802]
	TIME [epoch: 26 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01839567058393925		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.01839567058393925 | validation: 0.024698434200268626]
	TIME [epoch: 26 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015621935662301734		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.015621935662301734 | validation: 0.02180723682962905]
	TIME [epoch: 25.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019891856664726478		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.019891856664726478 | validation: 0.0403858289550726]
	TIME [epoch: 25.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024416303950765498		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.024416303950765498 | validation: 0.02425863339302063]
	TIME [epoch: 25.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01481825869859294		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.01481825869859294 | validation: 0.021702154695966112]
	TIME [epoch: 26 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02088077397505558		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.02088077397505558 | validation: 0.03115810916744181]
	TIME [epoch: 26 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018213427855934505		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.018213427855934505 | validation: 0.01927992095821704]
	TIME [epoch: 26 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019406584279229207		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.019406584279229207 | validation: 0.023656692273865465]
	TIME [epoch: 25.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016269998161548963		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.016269998161548963 | validation: 0.01944769556776995]
	TIME [epoch: 26 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02154430294423131		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.02154430294423131 | validation: 0.03877520182745373]
	TIME [epoch: 25.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01841973168576927		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.01841973168576927 | validation: 0.026817144115899683]
	TIME [epoch: 26 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01804744869806189		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.01804744869806189 | validation: 0.02107319016874081]
	TIME [epoch: 26 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015327575988604052		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.015327575988604052 | validation: 0.02437736884418394]
	TIME [epoch: 25.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018064047067286725		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.018064047067286725 | validation: 0.03318150183123944]
	TIME [epoch: 25.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021869592710847402		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.021869592710847402 | validation: 0.03229383734530529]
	TIME [epoch: 25.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019833280279185902		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.019833280279185902 | validation: 0.0176136486537382]
	TIME [epoch: 25.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016024450977399168		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.016024450977399168 | validation: 0.025127224466618952]
	TIME [epoch: 25.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01972913541811032		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.01972913541811032 | validation: 0.02265764523896284]
	TIME [epoch: 25.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01980017444453064		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.01980017444453064 | validation: 0.01896280907972977]
	TIME [epoch: 25.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014641014324603506		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.014641014324603506 | validation: 0.017715582591184185]
	TIME [epoch: 25.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01870171521049807		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.01870171521049807 | validation: 0.02267410222073446]
	TIME [epoch: 25.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01888957625496319		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.01888957625496319 | validation: 0.034362191081336695]
	TIME [epoch: 25.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01773312897626726		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.01773312897626726 | validation: 0.02041843544002871]
	TIME [epoch: 25.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015966795481555184		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.015966795481555184 | validation: 0.030651742618892065]
	TIME [epoch: 25.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01886107556941675		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.01886107556941675 | validation: 0.029957325461672676]
	TIME [epoch: 25.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707313098953067		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.01707313098953067 | validation: 0.02431203269969715]
	TIME [epoch: 25.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01771489346471509		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.01771489346471509 | validation: 0.03817641516527606]
	TIME [epoch: 25.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017459535091536728		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.017459535091536728 | validation: 0.022668742401865605]
	TIME [epoch: 25.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019004839227469282		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.019004839227469282 | validation: 0.016011650606896778]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014264059676791476		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.014264059676791476 | validation: 0.03033203825918248]
	TIME [epoch: 26 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020418070232523743		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.020418070232523743 | validation: 0.029713449859739116]
	TIME [epoch: 25.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017618677580723323		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.017618677580723323 | validation: 0.02137600137756873]
	TIME [epoch: 25.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017390915397693406		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.017390915397693406 | validation: 0.025726905976242886]
	TIME [epoch: 25.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018433947309223563		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.018433947309223563 | validation: 0.02026798666527998]
	TIME [epoch: 25.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015670912055895995		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.015670912055895995 | validation: 0.02370093722979172]
	TIME [epoch: 25.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0167664950892014		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.0167664950892014 | validation: 0.020424889323880747]
	TIME [epoch: 25.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017390949983472445		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.017390949983472445 | validation: 0.02812180847164581]
	TIME [epoch: 25.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015166618126206795		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.015166618126206795 | validation: 0.02613622446594711]
	TIME [epoch: 25.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020022808369538975		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.020022808369538975 | validation: 0.019334805682029988]
	TIME [epoch: 25.9 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014209618069012678		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.014209618069012678 | validation: 0.021880798767750412]
	TIME [epoch: 25.9 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019714837865725858		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.019714837865725858 | validation: 0.02385632606598763]
	TIME [epoch: 26 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014421987020407367		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.014421987020407367 | validation: 0.01613564474276239]
	TIME [epoch: 26 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015938217080424713		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.015938217080424713 | validation: 0.01896012701450738]
	TIME [epoch: 25.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017866065314496665		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.017866065314496665 | validation: 0.026269203853922552]
	TIME [epoch: 25.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014765208656451293		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.014765208656451293 | validation: 0.020802959864225937]
	TIME [epoch: 25.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016642252001795897		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.016642252001795897 | validation: 0.021646715973718804]
	TIME [epoch: 26 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014192500614209722		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.014192500614209722 | validation: 0.018814947250971485]
	TIME [epoch: 25.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016272213545627103		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.016272213545627103 | validation: 0.02929217257308668]
	TIME [epoch: 25.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016511851946740146		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.016511851946740146 | validation: 0.01898937753126908]
	TIME [epoch: 26 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014824080572923101		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.014824080572923101 | validation: 0.02170829197445092]
	TIME [epoch: 25.9 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01639521734167903		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.01639521734167903 | validation: 0.025853460551848173]
	TIME [epoch: 26 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019733187726963454		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.019733187726963454 | validation: 0.01925913242542776]
	TIME [epoch: 25.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013673352285937112		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.013673352285937112 | validation: 0.016376004069586815]
	TIME [epoch: 25.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015642018077385734		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.015642018077385734 | validation: 0.022108453488164894]
	TIME [epoch: 25.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013808092284882445		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.013808092284882445 | validation: 0.016124653514153445]
	TIME [epoch: 25.9 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01656992906934644		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.01656992906934644 | validation: 0.021616413251162224]
	TIME [epoch: 25.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018699871252868414		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.018699871252868414 | validation: 0.022782838235683844]
	TIME [epoch: 26 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020036788850658266		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.020036788850658266 | validation: 0.023545375696548937]
	TIME [epoch: 25.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014222384304644417		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.014222384304644417 | validation: 0.024452830454307434]
	TIME [epoch: 25.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0144791335934587		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.0144791335934587 | validation: 0.01761420997878621]
	TIME [epoch: 25.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012438902969766711		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.012438902969766711 | validation: 0.016623210991018225]
	TIME [epoch: 25.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018416840383427432		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.018416840383427432 | validation: 0.024333068030829755]
	TIME [epoch: 26 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019278987791919014		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.019278987791919014 | validation: 0.02092091870855184]
	TIME [epoch: 26 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014661204420526796		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.014661204420526796 | validation: 0.019218979225233114]
	TIME [epoch: 25.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016737744830333183		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.016737744830333183 | validation: 0.01975784383586315]
	TIME [epoch: 26 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01342276113273593		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.01342276113273593 | validation: 0.025719838357950574]
	TIME [epoch: 25.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018409277361191283		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.018409277361191283 | validation: 0.01978973024054588]
	TIME [epoch: 26 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013623810192866247		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.013623810192866247 | validation: 0.026768785321741866]
	TIME [epoch: 25.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01854295496465435		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.01854295496465435 | validation: 0.0177533889063419]
	TIME [epoch: 25.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012359582589835097		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.012359582589835097 | validation: 0.01700371212088193]
	TIME [epoch: 25.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014222846029240874		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.014222846029240874 | validation: 0.01794436693250056]
	TIME [epoch: 25.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012867675855169583		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.012867675855169583 | validation: 0.01590382138564318]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01929416315145793		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.01929416315145793 | validation: 0.020012705124321892]
	TIME [epoch: 25.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016169468569424302		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.016169468569424302 | validation: 0.02256123210345848]
	TIME [epoch: 25.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01766394016444496		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.01766394016444496 | validation: 0.019868329702852763]
	TIME [epoch: 25.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011751620924724772		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.011751620924724772 | validation: 0.01769259705979356]
	TIME [epoch: 25.9 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016648870999760333		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.016648870999760333 | validation: 0.01980557709873906]
	TIME [epoch: 25.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025293446578161665		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.025293446578161665 | validation: 0.02123834054542438]
	TIME [epoch: 25.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014666875358279832		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.014666875358279832 | validation: 0.016795975755692284]
	TIME [epoch: 25.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012417209644841857		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.012417209644841857 | validation: 0.02182999711572453]
	TIME [epoch: 25.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014773131868342379		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.014773131868342379 | validation: 0.020953214121544064]
	TIME [epoch: 25.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015773925460042814		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.015773925460042814 | validation: 0.017400818730698486]
	TIME [epoch: 25.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013712009477868447		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.013712009477868447 | validation: 0.021294911741728904]
	TIME [epoch: 25.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015201626180677986		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.015201626180677986 | validation: 0.02901104747474309]
	TIME [epoch: 25.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015781267571806874		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.015781267571806874 | validation: 0.018923596517069624]
	TIME [epoch: 25.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013167228938235355		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.013167228938235355 | validation: 0.01902479589497132]
	TIME [epoch: 25.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013507822876176467		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.013507822876176467 | validation: 0.022511867016153278]
	TIME [epoch: 25.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020633753201659718		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.020633753201659718 | validation: 0.02131980189691153]
	TIME [epoch: 25.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014330183376273677		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.014330183376273677 | validation: 0.016465357876710707]
	TIME [epoch: 25.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009904373574198329		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.009904373574198329 | validation: 0.015099543619589416]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0162320878158006		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0162320878158006 | validation: 0.01940647982333502]
	TIME [epoch: 25.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013763426927378121		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.013763426927378121 | validation: 0.017408282891798233]
	TIME [epoch: 25.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012311856411440983		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.012311856411440983 | validation: 0.019329144140377216]
	TIME [epoch: 25.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017139896164778866		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.017139896164778866 | validation: 0.019913986603326748]
	TIME [epoch: 25.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015572038455964976		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.015572038455964976 | validation: 0.01968187349441805]
	TIME [epoch: 25.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011342667270304453		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.011342667270304453 | validation: 0.016266830860677536]
	TIME [epoch: 25.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017958886729507177		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.017958886729507177 | validation: 0.01798827685695934]
	TIME [epoch: 25.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04604201251794362		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.04604201251794362 | validation: 0.03484580685272553]
	TIME [epoch: 25.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021308001966217044		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.021308001966217044 | validation: 0.01604311184174908]
	TIME [epoch: 25.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011911617793377046		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.011911617793377046 | validation: 0.01622273854202013]
	TIME [epoch: 25.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012310940174003158		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.012310940174003158 | validation: 0.015319900491738047]
	TIME [epoch: 25.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010384293154726446		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.010384293154726446 | validation: 0.01568762610710562]
	TIME [epoch: 25.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012528612697297308		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.012528612697297308 | validation: 0.01794184074691959]
	TIME [epoch: 25.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015822935662921263		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.015822935662921263 | validation: 0.016501012549295858]
	TIME [epoch: 25.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011926593075861436		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.011926593075861436 | validation: 0.016286319253647367]
	TIME [epoch: 25.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013470533688880624		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.013470533688880624 | validation: 0.01649797208424435]
	TIME [epoch: 25.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010879057382946648		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.010879057382946648 | validation: 0.016646299638874977]
	TIME [epoch: 25.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017862742011789716		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.017862742011789716 | validation: 0.01835394536823195]
	TIME [epoch: 25.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012683974144202744		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.012683974144202744 | validation: 0.01851141593359708]
	TIME [epoch: 25.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012288726559266085		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.012288726559266085 | validation: 0.02316193129218918]
	TIME [epoch: 25.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012500233833898634		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.012500233833898634 | validation: 0.018020843063189435]
	TIME [epoch: 25.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015103097298162034		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.015103097298162034 | validation: 0.023474454870914336]
	TIME [epoch: 25.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015966832454559748		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.015966832454559748 | validation: 0.019307981276050375]
	TIME [epoch: 25.9 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013288598270005492		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.013288598270005492 | validation: 0.017924382899864165]
	TIME [epoch: 25.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01198132714358893		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.01198132714358893 | validation: 0.015498011142640019]
	TIME [epoch: 25.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012121760124088058		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.012121760124088058 | validation: 0.018519885358378497]
	TIME [epoch: 25.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015007186477900577		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.015007186477900577 | validation: 0.01341233371718028]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012492335475854117		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.012492335475854117 | validation: 0.016970889698300483]
	TIME [epoch: 25.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01230735859538687		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.01230735859538687 | validation: 0.014939302970978976]
	TIME [epoch: 25.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012022542719434735		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.012022542719434735 | validation: 0.016648609031715736]
	TIME [epoch: 25.9 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01532944420163413		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.01532944420163413 | validation: 0.02307134760168985]
	TIME [epoch: 25.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012883593347851083		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.012883593347851083 | validation: 0.020074247917489886]
	TIME [epoch: 25.9 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012444724083587527		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.012444724083587527 | validation: 0.017998046737718813]
	TIME [epoch: 25.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01242132642469845		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.01242132642469845 | validation: 0.018345283623342055]
	TIME [epoch: 25.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013553299395108982		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.013553299395108982 | validation: 0.014914684404749468]
	TIME [epoch: 25.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01160561760116086		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.01160561760116086 | validation: 0.0198984405822004]
	TIME [epoch: 25.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013167766648520662		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.013167766648520662 | validation: 0.018277277109303523]
	TIME [epoch: 25.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013890637585136714		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.013890637585136714 | validation: 0.015766470037125974]
	TIME [epoch: 25.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010939305895657777		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.010939305895657777 | validation: 0.02081869469166638]
	TIME [epoch: 25.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013640325927506736		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.013640325927506736 | validation: 0.018214746654694955]
	TIME [epoch: 25.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012247032236441264		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.012247032236441264 | validation: 0.014860321083163177]
	TIME [epoch: 25.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011944100132330064		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.011944100132330064 | validation: 0.02525231548404474]
	TIME [epoch: 25.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014864289892359068		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.014864289892359068 | validation: 0.01721898904005008]
	TIME [epoch: 25.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012243876436570383		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.012243876436570383 | validation: 0.018820196554487564]
	TIME [epoch: 25.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011671765684775643		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.011671765684775643 | validation: 0.0181768678909571]
	TIME [epoch: 25.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012274520291613727		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.012274520291613727 | validation: 0.021827428096069943]
	TIME [epoch: 25.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018066611787526044		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.018066611787526044 | validation: 0.017277344507408247]
	TIME [epoch: 25.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011359023856858906		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.011359023856858906 | validation: 0.015315247013218899]
	TIME [epoch: 25.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01063714582677482		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.01063714582677482 | validation: 0.014540502173153709]
	TIME [epoch: 25.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010411712299298125		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.010411712299298125 | validation: 0.021304472880625425]
	TIME [epoch: 25.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014274592657422508		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.014274592657422508 | validation: 0.02087160914891063]
	TIME [epoch: 25.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013121239554900542		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.013121239554900542 | validation: 0.016851099989606384]
	TIME [epoch: 25.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011074306527359476		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.011074306527359476 | validation: 0.014263523651922577]
	TIME [epoch: 25.9 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012771198315670052		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.012771198315670052 | validation: 0.017048102981935397]
	TIME [epoch: 25.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012061926992290601		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.012061926992290601 | validation: 0.013139230470247931]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011598995713281816		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.011598995713281816 | validation: 0.01940785262278425]
	TIME [epoch: 25.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014039341829810764		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.014039341829810764 | validation: 0.020236083571836545]
	TIME [epoch: 25.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01168522450630095		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.01168522450630095 | validation: 0.016261592127833107]
	TIME [epoch: 25.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012410700021078169		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.012410700021078169 | validation: 0.01922380028156986]
	TIME [epoch: 25.9 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013496399563295434		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.013496399563295434 | validation: 0.024176094765576812]
	TIME [epoch: 25.9 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014848467025504367		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.014848467025504367 | validation: 0.014091947363986203]
	TIME [epoch: 25.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010437659653448776		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.010437659653448776 | validation: 0.013736839734573885]
	TIME [epoch: 25.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010952650493547099		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.010952650493547099 | validation: 0.01571441179831482]
	TIME [epoch: 25.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026182856639445917		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.026182856639445917 | validation: 0.02903221526554721]
	TIME [epoch: 25.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015763378571947234		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.015763378571947234 | validation: 0.015569859223790838]
	TIME [epoch: 25.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011936878114845895		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.011936878114845895 | validation: 0.01507647711771891]
	TIME [epoch: 25.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010007450937723875		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.010007450937723875 | validation: 0.01525723038753854]
	TIME [epoch: 25.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010469399394090391		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.010469399394090391 | validation: 0.014580060603347968]
	TIME [epoch: 25.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010963409403197268		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.010963409403197268 | validation: 0.014931091991802658]
	TIME [epoch: 25.9 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010064046346387061		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.010064046346387061 | validation: 0.015035924769652622]
	TIME [epoch: 25.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012129850724593232		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.012129850724593232 | validation: 0.012957764208974043]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010915382490851284		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.010915382490851284 | validation: 0.013729566971950696]
	TIME [epoch: 25.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009808494548002835		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.009808494548002835 | validation: 0.017114318888466942]
	TIME [epoch: 25.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012038254035386465		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.012038254035386465 | validation: 0.024447260335268214]
	TIME [epoch: 25.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012723345002295708		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.012723345002295708 | validation: 0.016817378172081378]
	TIME [epoch: 26 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011994331776370513		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.011994331776370513 | validation: 0.015417267128487826]
	TIME [epoch: 25.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010528559720790656		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.010528559720790656 | validation: 0.012264699541664849]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009919544778188233		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.009919544778188233 | validation: 0.014349143258406164]
	TIME [epoch: 25.9 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01002634916270764		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.01002634916270764 | validation: 0.015220529457429317]
	TIME [epoch: 25.9 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0144012144711556		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0144012144711556 | validation: 0.014386653271067914]
	TIME [epoch: 25.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01226909027813021		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.01226909027813021 | validation: 0.014492970202319273]
	TIME [epoch: 25.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010922118221120611		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.010922118221120611 | validation: 0.014114762717694362]
	TIME [epoch: 25.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011499913636416597		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.011499913636416597 | validation: 0.015296825290374525]
	TIME [epoch: 25.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01018690073972318		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.01018690073972318 | validation: 0.012546805261282195]
	TIME [epoch: 25.9 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009314105334438256		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.009314105334438256 | validation: 0.021540934028191416]
	TIME [epoch: 25.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012981235030287573		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.012981235030287573 | validation: 0.015628335863425342]
	TIME [epoch: 25.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010590558563467184		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.010590558563467184 | validation: 0.015077388215025252]
	TIME [epoch: 25.9 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012811702933209317		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.012811702933209317 | validation: 0.014131730854750335]
	TIME [epoch: 25.9 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009586140030662799		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.009586140030662799 | validation: 0.015079656914935458]
	TIME [epoch: 25.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011339950719373212		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.011339950719373212 | validation: 0.020274932194993822]
	TIME [epoch: 25.9 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011218293652137326		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.011218293652137326 | validation: 0.016686370666529657]
	TIME [epoch: 25.9 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010584983058310668		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.010584983058310668 | validation: 0.012754205102840647]
	TIME [epoch: 25.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010926251525688041		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.010926251525688041 | validation: 0.012520119789880426]
	TIME [epoch: 25.9 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009173145982411462		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.009173145982411462 | validation: 0.014001497254902512]
	TIME [epoch: 25.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011178119824947963		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.011178119824947963 | validation: 0.019203726113028757]
	TIME [epoch: 25.9 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011170746936630872		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.011170746936630872 | validation: 0.015089797459040622]
	TIME [epoch: 25.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011113433572501427		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.011113433572501427 | validation: 0.013220763429810958]
	TIME [epoch: 25.9 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013264961541168942		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.013264961541168942 | validation: 0.01715990368730623]
	TIME [epoch: 25.9 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010654976443305837		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.010654976443305837 | validation: 0.01280907487383351]
	TIME [epoch: 25.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011668454298842158		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.011668454298842158 | validation: 0.016111943995138822]
	TIME [epoch: 25.9 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01073315648369826		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.01073315648369826 | validation: 0.015901803243615738]
	TIME [epoch: 25.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013280486994796706		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.013280486994796706 | validation: 0.012465970970978455]
	TIME [epoch: 25.9 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009725010715428518		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.009725010715428518 | validation: 0.01498915040199868]
	TIME [epoch: 25.9 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009760369092489028		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.009760369092489028 | validation: 0.015352661755679646]
	TIME [epoch: 25.9 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011976946823832345		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.011976946823832345 | validation: 0.01592671600225931]
	TIME [epoch: 25.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009684489053518167		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.009684489053518167 | validation: 0.014288926355068732]
	TIME [epoch: 25.9 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011817226763787988		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.011817226763787988 | validation: 0.014043760391794294]
	TIME [epoch: 25.9 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009516808560057398		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.009516808560057398 | validation: 0.01540667995706214]
	TIME [epoch: 25.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011694023925630353		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.011694023925630353 | validation: 0.016208967828912207]
	TIME [epoch: 25.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01120361264356231		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.01120361264356231 | validation: 0.013599116967793177]
	TIME [epoch: 25.9 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009170953767686356		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.009170953767686356 | validation: 0.015351626413295499]
	TIME [epoch: 25.9 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011677092824177603		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.011677092824177603 | validation: 0.014312729041177922]
	TIME [epoch: 25.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009856089589660385		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.009856089589660385 | validation: 0.01284184865659719]
	TIME [epoch: 25.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010482372779262375		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.010482372779262375 | validation: 0.01338283983850312]
	TIME [epoch: 25.9 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010256476306536158		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.010256476306536158 | validation: 0.01632170816370225]
	TIME [epoch: 25.9 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01087084399217621		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.01087084399217621 | validation: 0.015456741697089995]
	TIME [epoch: 25.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009303496915445505		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.009303496915445505 | validation: 0.0150698385474047]
	TIME [epoch: 25.9 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011478241066669424		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.011478241066669424 | validation: 0.01473414021398451]
	TIME [epoch: 25.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009659875661476873		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.009659875661476873 | validation: 0.014574289017188288]
	TIME [epoch: 25.9 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010341464926891723		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.010341464926891723 | validation: 0.012618969851212216]
	TIME [epoch: 25.9 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009244371608608764		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.009244371608608764 | validation: 0.01400285363421079]
	TIME [epoch: 25.9 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010632916094268006		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.010632916094268006 | validation: 0.018521952892446104]
	TIME [epoch: 25.9 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011283644813089792		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.011283644813089792 | validation: 0.01449412880446864]
	TIME [epoch: 25.9 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009160606948974557		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.009160606948974557 | validation: 0.01435773078656406]
	TIME [epoch: 25.9 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010500806317919262		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.010500806317919262 | validation: 0.011881079418604225]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_803.pth
	Model improved!!!
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00984485391324687		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.00984485391324687 | validation: 0.01353278234162668]
	TIME [epoch: 25.9 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009546552385411619		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.009546552385411619 | validation: 0.01514733137460613]
	TIME [epoch: 25.9 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011830618278094153		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.011830618278094153 | validation: 0.012011541527837177]
	TIME [epoch: 25.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009695731525069468		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.009695731525069468 | validation: 0.01141161141791422]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008833098065823344		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.008833098065823344 | validation: 0.012820437469455069]
	TIME [epoch: 25.9 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010801101263958111		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.010801101263958111 | validation: 0.015253960704871141]
	TIME [epoch: 26 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011232175975643507		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.011232175975643507 | validation: 0.01438200726546132]
	TIME [epoch: 25.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0097166844330496		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.0097166844330496 | validation: 0.01328171808998908]
	TIME [epoch: 25.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009939261890450043		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.009939261890450043 | validation: 0.012814293285225712]
	TIME [epoch: 25.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010088235462720413		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.010088235462720413 | validation: 0.021875326598745845]
	TIME [epoch: 25.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017662898272962527		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.017662898272962527 | validation: 0.018286505265966285]
	TIME [epoch: 25.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01277782664172856		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.01277782664172856 | validation: 0.010859843924010886]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009513525774619482		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.009513525774619482 | validation: 0.01103937382803472]
	TIME [epoch: 25.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010736579812279287		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.010736579812279287 | validation: 0.012675331815938936]
	TIME [epoch: 25.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009276718626025122		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.009276718626025122 | validation: 0.013191693850679993]
	TIME [epoch: 25.9 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009441099419968083		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.009441099419968083 | validation: 0.013835236999537035]
	TIME [epoch: 25.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009082200768827932		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.009082200768827932 | validation: 0.01546236674223785]
	TIME [epoch: 25.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010283816108740844		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.010283816108740844 | validation: 0.01212569470240244]
	TIME [epoch: 25.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009214468098361621		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.009214468098361621 | validation: 0.014387717019895073]
	TIME [epoch: 26 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011129965294892382		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.011129965294892382 | validation: 0.014205776288656751]
	TIME [epoch: 25.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009623229632004		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.009623229632004 | validation: 0.012168047312702532]
	TIME [epoch: 25.9 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009896995431738321		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.009896995431738321 | validation: 0.013528618222115762]
	TIME [epoch: 26 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009307578964422804		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.009307578964422804 | validation: 0.012125583225552258]
	TIME [epoch: 26 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010775196886482407		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.010775196886482407 | validation: 0.013746889043912153]
	TIME [epoch: 25.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010122943493548977		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.010122943493548977 | validation: 0.013972954684983687]
	TIME [epoch: 25.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009131160543564616		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.009131160543564616 | validation: 0.011479444574067252]
	TIME [epoch: 25.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009681135263194455		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.009681135263194455 | validation: 0.010755382940024264]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00940597660227007		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.00940597660227007 | validation: 0.016077811343933684]
	TIME [epoch: 25.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01072255069179884		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.01072255069179884 | validation: 0.013377668191015147]
	TIME [epoch: 25.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009416820792281936		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.009416820792281936 | validation: 0.011384260832682043]
	TIME [epoch: 25.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008420351451102506		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.008420351451102506 | validation: 0.013234635575885282]
	TIME [epoch: 25.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010947496698090753		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.010947496698090753 | validation: 0.013987339478015996]
	TIME [epoch: 25.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009274507504480192		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.009274507504480192 | validation: 0.013516968188576653]
	TIME [epoch: 25.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009875693938443916		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.009875693938443916 | validation: 0.013808267035714741]
	TIME [epoch: 25.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009428415255761663		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.009428415255761663 | validation: 0.011639841782755088]
	TIME [epoch: 25.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009609611018235757		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.009609611018235757 | validation: 0.014451998257423961]
	TIME [epoch: 25.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0085127334738207		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0085127334738207 | validation: 0.01444266148680056]
	TIME [epoch: 25.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010984129130849963		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.010984129130849963 | validation: 0.012943410388618883]
	TIME [epoch: 25.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009665209851790901		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.009665209851790901 | validation: 0.01432435892239671]
	TIME [epoch: 25.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00876525525004869		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.00876525525004869 | validation: 0.011740791651809806]
	TIME [epoch: 25.9 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008977172337300056		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.008977172337300056 | validation: 0.012514081104302778]
	TIME [epoch: 25.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008502221581727011		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.008502221581727011 | validation: 0.01906965370279482]
	TIME [epoch: 25.8 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0098167782797088		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0098167782797088 | validation: 0.012623107813285791]
	TIME [epoch: 25.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009031955151623423		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.009031955151623423 | validation: 0.011435280540100661]
	TIME [epoch: 25.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009258645356765995		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.009258645356765995 | validation: 0.013224338461588568]
	TIME [epoch: 25.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008641675603859538		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.008641675603859538 | validation: 0.011178743877386517]
	TIME [epoch: 25.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008529289018777819		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.008529289018777819 | validation: 0.011522467673704628]
	TIME [epoch: 25.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01169267195873549		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.01169267195873549 | validation: 0.01055607017878951]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009287856902821393		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.009287856902821393 | validation: 0.012136587142830127]
	TIME [epoch: 25.9 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007998143822562122		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.007998143822562122 | validation: 0.013680181722897323]
	TIME [epoch: 25.9 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009514453912083543		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.009514453912083543 | validation: 0.012020052294993514]
	TIME [epoch: 25.9 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008624157902332053		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.008624157902332053 | validation: 0.014730343581235227]
	TIME [epoch: 25.9 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009129131146303312		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.009129131146303312 | validation: 0.0142999993484405]
	TIME [epoch: 26 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009479731106958718		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.009479731106958718 | validation: 0.009523386887174447]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007476112574784572		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.007476112574784572 | validation: 0.013238901124344025]
	TIME [epoch: 26 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011534897912276854		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.011534897912276854 | validation: 0.011939632755217209]
	TIME [epoch: 25.9 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008856223359230142		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.008856223359230142 | validation: 0.011957748652305932]
	TIME [epoch: 25.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007561237885331832		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.007561237885331832 | validation: 0.01158155365167065]
	TIME [epoch: 25.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008305096236448681		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.008305096236448681 | validation: 0.010891449155071476]
	TIME [epoch: 25.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010190782992195275		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.010190782992195275 | validation: 0.015115023601371505]
	TIME [epoch: 25.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009239503737891914		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.009239503737891914 | validation: 0.014074536834498308]
	TIME [epoch: 25.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00819804033350853		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.00819804033350853 | validation: 0.013978167685484481]
	TIME [epoch: 25.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008534912977338972		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.008534912977338972 | validation: 0.013222305883842583]
	TIME [epoch: 25.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00948194097307372		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.00948194097307372 | validation: 0.01757831970173273]
	TIME [epoch: 25.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009628388795322445		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.009628388795322445 | validation: 0.01135951103522555]
	TIME [epoch: 25.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009725944041039486		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.009725944041039486 | validation: 0.01276528670940964]
	TIME [epoch: 25.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008555816198828217		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.008555816198828217 | validation: 0.012756218387890778]
	TIME [epoch: 25.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008734738619744543		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.008734738619744543 | validation: 0.012711082168876765]
	TIME [epoch: 25.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00872559875690433		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.00872559875690433 | validation: 0.012137560295220742]
	TIME [epoch: 26 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009084171616374297		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.009084171616374297 | validation: 0.011222695491756914]
	TIME [epoch: 25.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008868869356629994		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.008868869356629994 | validation: 0.01128189452515842]
	TIME [epoch: 25.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008287422088659232		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.008287422088659232 | validation: 0.01164278634351422]
	TIME [epoch: 25.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009414355352156938		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.009414355352156938 | validation: 0.01231848192905658]
	TIME [epoch: 25.9 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008086159132083822		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.008086159132083822 | validation: 0.01434486050407935]
	TIME [epoch: 25.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00881313074542459		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.00881313074542459 | validation: 0.010730483672978845]
	TIME [epoch: 25.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009154693068017361		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.009154693068017361 | validation: 0.012846768484024985]
	TIME [epoch: 25.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008409427748822106		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.008409427748822106 | validation: 0.012242802026921018]
	TIME [epoch: 25.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009015983607664901		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.009015983607664901 | validation: 0.01515089558317779]
	TIME [epoch: 25.9 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008861787271690364		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.008861787271690364 | validation: 0.012644340085961575]
	TIME [epoch: 25.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008076254237400075		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.008076254237400075 | validation: 0.011978431394371589]
	TIME [epoch: 25.9 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008064476137307219		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.008064476137307219 | validation: 0.011088365342404273]
	TIME [epoch: 25.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009687664049128443		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.009687664049128443 | validation: 0.01223800484983267]
	TIME [epoch: 25.9 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007760752672001716		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.007760752672001716 | validation: 0.012204189156332248]
	TIME [epoch: 25.9 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007723097332751264		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.007723097332751264 | validation: 0.011626488396592567]
	TIME [epoch: 25.9 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009321654904657414		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.009321654904657414 | validation: 0.013926539752294367]
	TIME [epoch: 25.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009100476127536535		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.009100476127536535 | validation: 0.011489740037730331]
	TIME [epoch: 25.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008489244363938296		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.008489244363938296 | validation: 0.012037007818584458]
	TIME [epoch: 25.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008843249317734664		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.008843249317734664 | validation: 0.012630329172477004]
	TIME [epoch: 25.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008392066380339387		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.008392066380339387 | validation: 0.011538496719782675]
	TIME [epoch: 26 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008148724592583236		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.008148724592583236 | validation: 0.012073548894984602]
	TIME [epoch: 25.9 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009323103058753637		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.009323103058753637 | validation: 0.014386420625781708]
	TIME [epoch: 25.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008624568572372766		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.008624568572372766 | validation: 0.012029648772226242]
	TIME [epoch: 25.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008009158312493953		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.008009158312493953 | validation: 0.011422039248356625]
	TIME [epoch: 25.9 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008947397811019421		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.008947397811019421 | validation: 0.010117910779847543]
	TIME [epoch: 25.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007523721068622439		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.007523721068622439 | validation: 0.013674072066169055]
	TIME [epoch: 26 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077126115232000825		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.0077126115232000825 | validation: 0.010485965962653476]
	TIME [epoch: 26 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008719672155167698		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.008719672155167698 | validation: 0.014932710494811026]
	TIME [epoch: 25.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008721963887134945		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.008721963887134945 | validation: 0.010721475890617161]
	TIME [epoch: 25.9 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008432781780570785		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.008432781780570785 | validation: 0.011152409551724272]
	TIME [epoch: 25.9 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00831902924555901		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.00831902924555901 | validation: 0.009597717308205215]
	TIME [epoch: 25.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00830628471857725		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.00830628471857725 | validation: 0.013916146115130389]
	TIME [epoch: 25.9 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008977718021377897		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.008977718021377897 | validation: 0.010768309436230465]
	TIME [epoch: 25.9 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007841430097532612		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.007841430097532612 | validation: 0.012132994803124405]
	TIME [epoch: 25.9 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00822455746237029		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.00822455746237029 | validation: 0.010759685957927747]
	TIME [epoch: 25.9 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007546314905145551		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.007546314905145551 | validation: 0.010720355144025546]
	TIME [epoch: 26 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008411578718981768		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.008411578718981768 | validation: 0.013333923645669939]
	TIME [epoch: 26 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008030721870301618		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.008030721870301618 | validation: 0.012000258241889671]
	TIME [epoch: 25.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008434265820314516		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.008434265820314516 | validation: 0.01113327674775729]
	TIME [epoch: 25.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007363776408950784		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.007363776408950784 | validation: 0.011080475111083472]
	TIME [epoch: 25.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008402612315168053		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.008402612315168053 | validation: 0.010430357068525176]
	TIME [epoch: 25.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007911205450280887		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.007911205450280887 | validation: 0.011441056837819149]
	TIME [epoch: 25.9 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007889745633736001		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.007889745633736001 | validation: 0.011755784068882055]
	TIME [epoch: 26 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007825320432390868		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.007825320432390868 | validation: 0.010809797477728698]
	TIME [epoch: 26 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00807692485358046		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.00807692485358046 | validation: 0.014690116774946933]
	TIME [epoch: 26 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009283002693640824		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.009283002693640824 | validation: 0.012608698542188395]
	TIME [epoch: 25.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008253646162005962		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.008253646162005962 | validation: 0.01024917001707933]
	TIME [epoch: 25.9 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007018213720049039		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.007018213720049039 | validation: 0.009265805430287068]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008656618186985803		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.008656618186985803 | validation: 0.010344162994174217]
	TIME [epoch: 25.9 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007537899415238997		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.007537899415238997 | validation: 0.009355992213368159]
	TIME [epoch: 25.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007096724831244304		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.007096724831244304 | validation: 0.010884640766868129]
	TIME [epoch: 25.9 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008413562513489773		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.008413562513489773 | validation: 0.00917335008784378]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007995243823091488		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.007995243823091488 | validation: 0.010379338859403541]
	TIME [epoch: 25.9 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007700691440608224		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.007700691440608224 | validation: 0.010678582922039666]
	TIME [epoch: 25.9 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007801553135024019		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.007801553135024019 | validation: 0.010187530053869326]
	TIME [epoch: 25.9 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008660577500210136		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.008660577500210136 | validation: 0.011690164887737062]
	TIME [epoch: 25.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007855932403665637		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.007855932403665637 | validation: 0.012097975874408959]
	TIME [epoch: 25.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007343978066718149		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.007343978066718149 | validation: 0.011209158114066857]
	TIME [epoch: 25.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007727195487386351		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.007727195487386351 | validation: 0.012467466208933794]
	TIME [epoch: 25.9 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00835679081722377		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.00835679081722377 | validation: 0.011271517608503694]
	TIME [epoch: 25.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007411700131860007		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.007411700131860007 | validation: 0.010568916790849314]
	TIME [epoch: 25.8 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007701477538047257		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.007701477538047257 | validation: 0.009678383035698014]
	TIME [epoch: 25.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008076876404028065		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.008076876404028065 | validation: 0.012412303053929792]
	TIME [epoch: 25.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0074631004003792335		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0074631004003792335 | validation: 0.012852529165432999]
	TIME [epoch: 25.9 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007738486336575525		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.007738486336575525 | validation: 0.009893708774795302]
	TIME [epoch: 25.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007156906284868396		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.007156906284868396 | validation: 0.010857874296794095]
	TIME [epoch: 25.9 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007912023071988294		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.007912023071988294 | validation: 0.012165257676696782]
	TIME [epoch: 25.9 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00865489363779337		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.00865489363779337 | validation: 0.010432676741480818]
	TIME [epoch: 25.9 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007724137369892692		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.007724137369892692 | validation: 0.012676796260511217]
	TIME [epoch: 25.9 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007134864476498261		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.007134864476498261 | validation: 0.00988277398164869]
	TIME [epoch: 25.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007592709911178537		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.007592709911178537 | validation: 0.010474793030005286]
	TIME [epoch: 25.9 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007646759368385004		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.007646759368385004 | validation: 0.011507703622881171]
	TIME [epoch: 25.9 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007804723132035188		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.007804723132035188 | validation: 0.010942947278067326]
	TIME [epoch: 25.9 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007152905666492804		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.007152905666492804 | validation: 0.011604879591219922]
	TIME [epoch: 25.9 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008538704186845531		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.008538704186845531 | validation: 0.010707720056029137]
	TIME [epoch: 25.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007704972803986595		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.007704972803986595 | validation: 0.010339972061347054]
	TIME [epoch: 25.9 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076188536952510755		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.0076188536952510755 | validation: 0.008666003242804848]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008137101519501576		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.008137101519501576 | validation: 0.009233732955918998]
	TIME [epoch: 25.9 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007963417152829081		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.007963417152829081 | validation: 0.01130965019778273]
	TIME [epoch: 25.9 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007695559120362101		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.007695559120362101 | validation: 0.00942688834559307]
	TIME [epoch: 25.9 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076160080619338665		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0076160080619338665 | validation: 0.009999213025070926]
	TIME [epoch: 25.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00806619272830258		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.00806619272830258 | validation: 0.010146325334400652]
	TIME [epoch: 25.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007202029188563807		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.007202029188563807 | validation: 0.01002637760800245]
	TIME [epoch: 25.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007214825392971601		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.007214825392971601 | validation: 0.010616670203626445]
	TIME [epoch: 25.9 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007722289442875235		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.007722289442875235 | validation: 0.010369595784856876]
	TIME [epoch: 25.9 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007754278505014617		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.007754278505014617 | validation: 0.011312855934479216]
	TIME [epoch: 25.9 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007843026027568496		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.007843026027568496 | validation: 0.010513305851482361]
	TIME [epoch: 25.9 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007377708646775359		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.007377708646775359 | validation: 0.009062236130877309]
	TIME [epoch: 25.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00742693090825744		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.00742693090825744 | validation: 0.009901434082001954]
	TIME [epoch: 25.9 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006388242831418803		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.006388242831418803 | validation: 0.009878684261566939]
	TIME [epoch: 25.9 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008700705409088377		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.008700705409088377 | validation: 0.0142327605677613]
	TIME [epoch: 25.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010586571249871876		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.010586571249871876 | validation: 0.011617146069785344]
	TIME [epoch: 25.9 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006991308779921106		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.006991308779921106 | validation: 0.011852427928973137]
	TIME [epoch: 25.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006981767802030072		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.006981767802030072 | validation: 0.0103308844452101]
	TIME [epoch: 26 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007433278371763961		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.007433278371763961 | validation: 0.011995467312066466]
	TIME [epoch: 26 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007997767335850132		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.007997767335850132 | validation: 0.008757284898398447]
	TIME [epoch: 25.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006689410071464756		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.006689410071464756 | validation: 0.01055571368124628]
	TIME [epoch: 25.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007015993473113552		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.007015993473113552 | validation: 0.010518974268784053]
	TIME [epoch: 26 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008069716162967061		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.008069716162967061 | validation: 0.01127947351420913]
	TIME [epoch: 25.9 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006511085120595499		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.006511085120595499 | validation: 0.010508683356310496]
	TIME [epoch: 26 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007516354176546646		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.007516354176546646 | validation: 0.010192762280315478]
	TIME [epoch: 25.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007781126481732441		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.007781126481732441 | validation: 0.010469513572416395]
	TIME [epoch: 25.9 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006906189211830838		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.006906189211830838 | validation: 0.011359569426042083]
	TIME [epoch: 25.9 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007326983577554789		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.007326983577554789 | validation: 0.012881686924066615]
	TIME [epoch: 25.9 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00696489904108394		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.00696489904108394 | validation: 0.008307962439070551]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006680123211493256		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.006680123211493256 | validation: 0.01044119586342513]
	TIME [epoch: 25.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007178355754693021		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.007178355754693021 | validation: 0.011618064828542638]
	TIME [epoch: 25.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072734234803870975		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.0072734234803870975 | validation: 0.014093597219059055]
	TIME [epoch: 25.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073105931726502255		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.0073105931726502255 | validation: 0.009593807343593264]
	TIME [epoch: 25.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0067655979058253565		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.0067655979058253565 | validation: 0.00971670053885969]
	TIME [epoch: 25.9 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007700648768970101		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.007700648768970101 | validation: 0.01086023488738383]
	TIME [epoch: 25.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00790493545753021		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.00790493545753021 | validation: 0.011047494606241184]
	TIME [epoch: 25.9 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007329555109838582		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.007329555109838582 | validation: 0.009970073776076133]
	TIME [epoch: 25.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006972708457525743		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.006972708457525743 | validation: 0.010324864842257054]
	TIME [epoch: 25.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006939370130407107		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.006939370130407107 | validation: 0.009629733871230799]
	TIME [epoch: 25.9 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006464902940807302		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.006464902940807302 | validation: 0.010608332989498719]
	TIME [epoch: 26 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007107884874831941		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.007107884874831941 | validation: 0.010639675700444877]
	TIME [epoch: 26 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070588293797036305		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0070588293797036305 | validation: 0.01094446239357386]
	TIME [epoch: 25.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007880879809328238		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.007880879809328238 | validation: 0.011845574394961996]
	TIME [epoch: 25.9 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007533978848312601		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.007533978848312601 | validation: 0.008887484816724427]
	TIME [epoch: 26 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064909591175652676		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.0064909591175652676 | validation: 0.00924611544118573]
	TIME [epoch: 25.9 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006646681353993905		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.006646681353993905 | validation: 0.009005744238922302]
	TIME [epoch: 26 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006704402701734228		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.006704402701734228 | validation: 0.0106053303642488]
	TIME [epoch: 25.9 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006378704964652098		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.006378704964652098 | validation: 0.010916770292832166]
	TIME [epoch: 25.9 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007169834407154182		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.007169834407154182 | validation: 0.010073148750358953]
	TIME [epoch: 25.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006759227286676699		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.006759227286676699 | validation: 0.010633665443516873]
	TIME [epoch: 25.9 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006849525855979664		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.006849525855979664 | validation: 0.008871157573209752]
	TIME [epoch: 25.9 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076442153881324925		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.0076442153881324925 | validation: 0.010478754132975065]
	TIME [epoch: 25.9 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007222003908675887		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.007222003908675887 | validation: 0.009363126031224961]
	TIME [epoch: 462 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006822571482091955		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.006822571482091955 | validation: 0.00993063157735691]
	TIME [epoch: 54.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006150738689637503		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.006150738689637503 | validation: 0.010798355906592477]
	TIME [epoch: 54.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006235086661892959		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.006235086661892959 | validation: 0.00944822949852016]
	TIME [epoch: 54.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006962351127713774		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.006962351127713774 | validation: 0.010415473171996759]
	TIME [epoch: 55 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006797023547739589		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.006797023547739589 | validation: 0.009468279013616326]
	TIME [epoch: 55 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006573290421067817		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.006573290421067817 | validation: 0.009303539315749223]
	TIME [epoch: 55 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006822948369996293		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.006822948369996293 | validation: 0.010649073093813957]
	TIME [epoch: 54.9 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006750287626968628		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.006750287626968628 | validation: 0.0093542924633575]
	TIME [epoch: 54.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00627719161961731		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.00627719161961731 | validation: 0.010990537574700185]
	TIME [epoch: 55 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006176695833351481		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.006176695833351481 | validation: 0.009326440113759448]
	TIME [epoch: 54.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00665741657659425		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.00665741657659425 | validation: 0.00861105790088737]
	TIME [epoch: 55 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006732025806829276		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.006732025806829276 | validation: 0.011055436987203392]
	TIME [epoch: 54.9 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006694700223248687		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.006694700223248687 | validation: 0.009083618421024342]
	TIME [epoch: 54.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006535696454882574		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.006535696454882574 | validation: 0.010655498347756613]
	TIME [epoch: 54.9 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007666882289057616		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.007666882289057616 | validation: 0.010164200234533642]
	TIME [epoch: 55 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006284981747179494		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.006284981747179494 | validation: 0.009918097083866687]
	TIME [epoch: 55 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006445279338660488		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.006445279338660488 | validation: 0.00962014262959257]
	TIME [epoch: 54.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007043986054856591		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.007043986054856591 | validation: 0.012289235175408971]
	TIME [epoch: 54.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006323965846447939		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.006323965846447939 | validation: 0.009028177536390349]
	TIME [epoch: 54.9 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007927365446234068		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.007927365446234068 | validation: 0.009099537054528496]
	TIME [epoch: 54.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006693633918332429		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.006693633918332429 | validation: 0.010603335601694349]
	TIME [epoch: 55 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006142654767814924		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.006142654767814924 | validation: 0.009431747557639458]
	TIME [epoch: 54.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006365264284241044		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.006365264284241044 | validation: 0.009842441772762745]
	TIME [epoch: 54.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00650079981487474		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.00650079981487474 | validation: 0.007913220097347266]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007199857554247782		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.007199857554247782 | validation: 0.008833485344859811]
	TIME [epoch: 55 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006407631725757653		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.006407631725757653 | validation: 0.009378015909017231]
	TIME [epoch: 55 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006973735497421063		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.006973735497421063 | validation: 0.00946296146587319]
	TIME [epoch: 54.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006755750237451296		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.006755750237451296 | validation: 0.008059957337917303]
	TIME [epoch: 54.9 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064805799051407004		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.0064805799051407004 | validation: 0.010414698973045214]
	TIME [epoch: 55 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006869809394369453		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.006869809394369453 | validation: 0.00949694096633963]
	TIME [epoch: 54.9 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006883713058501744		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.006883713058501744 | validation: 0.009122775910143566]
	TIME [epoch: 54.9 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006403910234613158		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.006403910234613158 | validation: 0.010347414229693888]
	TIME [epoch: 55 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008864681682937801		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.008864681682937801 | validation: 0.009895911352415158]
	TIME [epoch: 54.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007329219180751837		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.007329219180751837 | validation: 0.008540309693537508]
	TIME [epoch: 55 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063633557370935435		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0063633557370935435 | validation: 0.009730102955999492]
	TIME [epoch: 55 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006214546807921195		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.006214546807921195 | validation: 0.0110967399071593]
	TIME [epoch: 54.9 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006579320644129791		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.006579320644129791 | validation: 0.008837768395398229]
	TIME [epoch: 54.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006611416788836809		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.006611416788836809 | validation: 0.008058020896799342]
	TIME [epoch: 54.8 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006642930807071949		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.006642930807071949 | validation: 0.010407451253501523]
	TIME [epoch: 55 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007160226841758284		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.007160226841758284 | validation: 0.010205078885552953]
	TIME [epoch: 54.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006272994041621861		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.006272994041621861 | validation: 0.009202655716295364]
	TIME [epoch: 54.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064430003895655855		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0064430003895655855 | validation: 0.008729075894220553]
	TIME [epoch: 55 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00617309867465553		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.00617309867465553 | validation: 0.010581657996869843]
	TIME [epoch: 54.8 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006441720173210102		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.006441720173210102 | validation: 0.009507670868183571]
	TIME [epoch: 54.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060840428960714675		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0060840428960714675 | validation: 0.008671748937077206]
	TIME [epoch: 55 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006422640789738408		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.006422640789738408 | validation: 0.010862091472687182]
	TIME [epoch: 54.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0069558327234385126		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0069558327234385126 | validation: 0.008249487206533858]
	TIME [epoch: 54.9 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006952095036447237		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.006952095036447237 | validation: 0.009452249507717862]
	TIME [epoch: 54.9 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063693678860310445		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.0063693678860310445 | validation: 0.009393678358716927]
	TIME [epoch: 55 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006409211162335265		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.006409211162335265 | validation: 0.01018489866457414]
	TIME [epoch: 54.9 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061435910492408895		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0061435910492408895 | validation: 0.008596094589324853]
	TIME [epoch: 55 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006045077601946042		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.006045077601946042 | validation: 0.008945473464217334]
	TIME [epoch: 54.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006359611615866849		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.006359611615866849 | validation: 0.009887678311529396]
	TIME [epoch: 55 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006417822727166187		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.006417822727166187 | validation: 0.00751114472404164]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1055.pth
	Model improved!!!
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006754084241859113		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.006754084241859113 | validation: 0.009681478789051092]
	TIME [epoch: 54.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071014207301587084		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0071014207301587084 | validation: 0.009667605216009213]
	TIME [epoch: 54.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006367624103433445		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.006367624103433445 | validation: 0.0119484079293874]
	TIME [epoch: 54.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068239135984742415		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0068239135984742415 | validation: 0.00897440635060253]
	TIME [epoch: 54.9 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006352755661821386		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.006352755661821386 | validation: 0.008740367351464051]
	TIME [epoch: 54.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006446500408391109		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.006446500408391109 | validation: 0.010341722492848062]
	TIME [epoch: 55 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006784264690367085		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.006784264690367085 | validation: 0.009326339569913668]
	TIME [epoch: 54.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006027596390472992		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.006027596390472992 | validation: 0.00856390942577772]
	TIME [epoch: 55 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006844268436812092		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.006844268436812092 | validation: 0.010106758326033334]
	TIME [epoch: 55 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005975077061253687		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.005975077061253687 | validation: 0.009841614525413333]
	TIME [epoch: 55 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00632473213981717		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.00632473213981717 | validation: 0.010100696724320728]
	TIME [epoch: 54.9 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006889442147767402		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.006889442147767402 | validation: 0.008648396623057664]
	TIME [epoch: 55 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006609935365478935		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.006609935365478935 | validation: 0.011023404776654953]
	TIME [epoch: 55 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006260315328278288		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.006260315328278288 | validation: 0.009089531037394113]
	TIME [epoch: 54.9 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006625914505042447		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.006625914505042447 | validation: 0.008860510384741973]
	TIME [epoch: 54.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005714623776347844		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.005714623776347844 | validation: 0.009358316319622258]
	TIME [epoch: 54.9 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006344343552351217		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.006344343552351217 | validation: 0.009133085859523136]
	TIME [epoch: 54.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065168733301926365		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0065168733301926365 | validation: 0.00881629328754554]
	TIME [epoch: 55 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061936165266256		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0061936165266256 | validation: 0.008528210718425338]
	TIME [epoch: 55 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006220056669449545		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.006220056669449545 | validation: 0.008825463699781147]
	TIME [epoch: 55 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006236501878633852		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.006236501878633852 | validation: 0.00817716432166842]
	TIME [epoch: 55 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060643761274499774		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0060643761274499774 | validation: 0.008707270784282415]
	TIME [epoch: 54.9 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005716698755249963		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.005716698755249963 | validation: 0.011311930758471454]
	TIME [epoch: 54.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065674873615073365		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0065674873615073365 | validation: 0.009664645989326169]
	TIME [epoch: 55 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006219520297139616		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.006219520297139616 | validation: 0.00908529936912083]
	TIME [epoch: 55 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060477258402237555		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.0060477258402237555 | validation: 0.008257799531269922]
	TIME [epoch: 54.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006315078012911275		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.006315078012911275 | validation: 0.00823017745150753]
	TIME [epoch: 54.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006793579008091257		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.006793579008091257 | validation: 0.009186910054367677]
	TIME [epoch: 54.9 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006300450636510601		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.006300450636510601 | validation: 0.00872507004692393]
	TIME [epoch: 55 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006432178422991402		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.006432178422991402 | validation: 0.008139168895486519]
	TIME [epoch: 55 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006113854192012833		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.006113854192012833 | validation: 0.00888156439257869]
	TIME [epoch: 55 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006296097413610845		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.006296097413610845 | validation: 0.009232722126622139]
	TIME [epoch: 55 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054921008071867105		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0054921008071867105 | validation: 0.010087863391788583]
	TIME [epoch: 54.9 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006034739352882286		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.006034739352882286 | validation: 0.007707240381086979]
	TIME [epoch: 55 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005935847289358328		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.005935847289358328 | validation: 0.008242857269046264]
	TIME [epoch: 55 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005887796492240081		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.005887796492240081 | validation: 0.009045172227305828]
	TIME [epoch: 54.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005698815729066967		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.005698815729066967 | validation: 0.008271778401392508]
	TIME [epoch: 54.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063202087864435465		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.0063202087864435465 | validation: 0.009151102303922133]
	TIME [epoch: 55 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005621257736481006		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.005621257736481006 | validation: 0.008855390086101467]
	TIME [epoch: 54.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005906424758305013		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.005906424758305013 | validation: 0.008129140325364964]
	TIME [epoch: 55 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00577667322232488		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.00577667322232488 | validation: 0.00815511132173209]
	TIME [epoch: 54.9 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006174176249929263		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.006174176249929263 | validation: 0.009358239443955432]
	TIME [epoch: 54.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006250596093596713		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.006250596093596713 | validation: 0.009504353372079548]
	TIME [epoch: 55 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063269492909790535		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.0063269492909790535 | validation: 0.009052949875793402]
	TIME [epoch: 54.9 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006479435413902914		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.006479435413902914 | validation: 0.0104696068213007]
	TIME [epoch: 55 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005970280489648267		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.005970280489648267 | validation: 0.008107630504212029]
	TIME [epoch: 54.9 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005677761843000268		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.005677761843000268 | validation: 0.009367493391810843]
	TIME [epoch: 54.9 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006379916465530651		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.006379916465530651 | validation: 0.008473601959403216]
	TIME [epoch: 55 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055165374366248875		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.0055165374366248875 | validation: 0.009337855234972107]
	TIME [epoch: 54.9 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00634666342669529		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.00634666342669529 | validation: 0.009025863274274253]
	TIME [epoch: 54.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006187306061442266		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.006187306061442266 | validation: 0.008980712525918194]
	TIME [epoch: 55 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005683670076759847		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.005683670076759847 | validation: 0.009858687524403286]
	TIME [epoch: 54.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006342655933432759		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.006342655933432759 | validation: 0.00965937264184862]
	TIME [epoch: 54.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061655571071071984		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.0061655571071071984 | validation: 0.007798963893891617]
	TIME [epoch: 55 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005932060689842271		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.005932060689842271 | validation: 0.00981241083340661]
	TIME [epoch: 55 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006056506273979054		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.006056506273979054 | validation: 0.009922271837819586]
	TIME [epoch: 54.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060401740205403495		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.0060401740205403495 | validation: 0.009611230149131707]
	TIME [epoch: 54.9 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006275115726383066		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.006275115726383066 | validation: 0.009842404258046649]
	TIME [epoch: 55 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006041479362069228		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.006041479362069228 | validation: 0.008951426499150462]
	TIME [epoch: 55 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006347473316300561		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.006347473316300561 | validation: 0.008906930458531082]
	TIME [epoch: 54.9 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00569477850763161		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.00569477850763161 | validation: 0.008035329970116299]
	TIME [epoch: 54.9 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005606668747527497		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.005606668747527497 | validation: 0.008081438499199494]
	TIME [epoch: 55 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005844007954176473		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.005844007954176473 | validation: 0.007974885249222024]
	TIME [epoch: 55 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005670623875957821		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.005670623875957821 | validation: 0.009356972281040304]
	TIME [epoch: 54.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006611434973602527		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.006611434973602527 | validation: 0.009478518430586743]
	TIME [epoch: 55 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005690097258559132		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.005690097258559132 | validation: 0.010296775559685133]
	TIME [epoch: 55.1 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005607886187404942		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.005607886187404942 | validation: 0.008294998910775592]
	TIME [epoch: 54.9 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006200145106039804		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.006200145106039804 | validation: 0.010289627638423436]
	TIME [epoch: 55 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006192067938376124		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.006192067938376124 | validation: 0.009238967614642749]
	TIME [epoch: 55 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00564841947194109		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.00564841947194109 | validation: 0.007850512302729599]
	TIME [epoch: 54.9 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005298953460277457		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.005298953460277457 | validation: 0.00983376445524471]
	TIME [epoch: 54.9 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005320308990142319		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.005320308990142319 | validation: 0.008001611883351347]
	TIME [epoch: 55 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060996752136453815		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0060996752136453815 | validation: 0.007951954984883838]
	TIME [epoch: 55 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005518608509097064		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.005518608509097064 | validation: 0.007546923595414801]
	TIME [epoch: 55 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005652259108830032		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.005652259108830032 | validation: 0.008823658356839952]
	TIME [epoch: 55 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005794027603995687		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.005794027603995687 | validation: 0.009383219461555552]
	TIME [epoch: 54.9 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006096989145851248		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.006096989145851248 | validation: 0.007192053119743334]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1132.pth
	Model improved!!!
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005974682129522852		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.005974682129522852 | validation: 0.008111107690903358]
	TIME [epoch: 54.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005657090807593383		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.005657090807593383 | validation: 0.008529450846553405]
	TIME [epoch: 54.9 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006427464733904955		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.006427464733904955 | validation: 0.008992530683872934]
	TIME [epoch: 54.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060818449917574855		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.0060818449917574855 | validation: 0.008449843143342716]
	TIME [epoch: 54.9 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005373787698926864		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.005373787698926864 | validation: 0.00784356838215362]
	TIME [epoch: 55 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005763406690404454		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.005763406690404454 | validation: 0.007113648465286495]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1138.pth
	Model improved!!!
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00557605929501381		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.00557605929501381 | validation: 0.00864361942789008]
	TIME [epoch: 54.9 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057318282402134945		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0057318282402134945 | validation: 0.007960446727818038]
	TIME [epoch: 55 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006054148677136236		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.006054148677136236 | validation: 0.008875322005745635]
	TIME [epoch: 54.9 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005847071766458496		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.005847071766458496 | validation: 0.008417043462802427]
	TIME [epoch: 55 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00587215656086355		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.00587215656086355 | validation: 0.008103957358482319]
	TIME [epoch: 54.9 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006140845626463494		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.006140845626463494 | validation: 0.00972284638001586]
	TIME [epoch: 54.9 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005924130841936045		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.005924130841936045 | validation: 0.008551730408758413]
	TIME [epoch: 54.9 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051215967166208425		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.0051215967166208425 | validation: 0.00763671938394068]
	TIME [epoch: 54.9 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006247879447892177		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.006247879447892177 | validation: 0.0073469272568906525]
	TIME [epoch: 54.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005201815646123742		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.005201815646123742 | validation: 0.008091183547611845]
	TIME [epoch: 54.9 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005854266712051616		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.005854266712051616 | validation: 0.009385212915766663]
	TIME [epoch: 54.9 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005520062766642237		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.005520062766642237 | validation: 0.008798504041154148]
	TIME [epoch: 54.8 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005807291261615209		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.005807291261615209 | validation: 0.007541037043336269]
	TIME [epoch: 55 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005798093521021943		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.005798093521021943 | validation: 0.00932529061922906]
	TIME [epoch: 54.9 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060579331732519845		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0060579331732519845 | validation: 0.009797038108845664]
	TIME [epoch: 54.9 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005268606073891501		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.005268606073891501 | validation: 0.007986639461158775]
	TIME [epoch: 55 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00522452147850687		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.00522452147850687 | validation: 0.009162964953069322]
	TIME [epoch: 55 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005393347949688805		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.005393347949688805 | validation: 0.009211696015559838]
	TIME [epoch: 54.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005721149961380402		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.005721149961380402 | validation: 0.007621293224519465]
	TIME [epoch: 55 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005959356261898193		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.005959356261898193 | validation: 0.006883186067876029]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006240180129978807		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.006240180129978807 | validation: 0.0076397260169380286]
	TIME [epoch: 55 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005695037092151224		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.005695037092151224 | validation: 0.008656720863942702]
	TIME [epoch: 54.9 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005676147370936649		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.005676147370936649 | validation: 0.007905138939931085]
	TIME [epoch: 54.9 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005595636229209814		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.005595636229209814 | validation: 0.007628121477918049]
	TIME [epoch: 55 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00528375974250396		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.00528375974250396 | validation: 0.00842736508668816]
	TIME [epoch: 54.9 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005651168065654974		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.005651168065654974 | validation: 0.0077310022702834855]
	TIME [epoch: 54.8 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005966503675016041		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.005966503675016041 | validation: 0.008109336732193093]
	TIME [epoch: 54.9 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054917068197047406		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.0054917068197047406 | validation: 0.009285368615557194]
	TIME [epoch: 54.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005703655648311693		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.005703655648311693 | validation: 0.00865366474021301]
	TIME [epoch: 55 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005145493304259633		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.005145493304259633 | validation: 0.008491584697349931]
	TIME [epoch: 55 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005678155021414312		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.005678155021414312 | validation: 0.008289160181374683]
	TIME [epoch: 55 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052014027171909194		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.0052014027171909194 | validation: 0.008480655206969024]
	TIME [epoch: 54.9 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005472910055934302		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.005472910055934302 | validation: 0.008600034624696537]
	TIME [epoch: 54.9 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005198678240261885		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.005198678240261885 | validation: 0.007320195212558449]
	TIME [epoch: 54.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00517363388229213		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.00517363388229213 | validation: 0.008709246084849943]
	TIME [epoch: 55 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00526401598278866		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.00526401598278866 | validation: 0.007807102654629551]
	TIME [epoch: 55 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005406828268772793		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.005406828268772793 | validation: 0.008490577036969222]
	TIME [epoch: 55 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005395912974065417		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.005395912974065417 | validation: 0.007369257780096231]
	TIME [epoch: 55 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006138908792396627		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.006138908792396627 | validation: 0.007457569611767811]
	TIME [epoch: 54.9 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054824809663241904		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.0054824809663241904 | validation: 0.00807279041280519]
	TIME [epoch: 55 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005291407520531375		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.005291407520531375 | validation: 0.008276936032284469]
	TIME [epoch: 54.9 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005056497777082312		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.005056497777082312 | validation: 0.006950126502621793]
	TIME [epoch: 55 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005316406063173741		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.005316406063173741 | validation: 0.007965883255897768]
	TIME [epoch: 55 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005312126802853552		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.005312126802853552 | validation: 0.00912769451477386]
	TIME [epoch: 55 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005469256475059667		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.005469256475059667 | validation: 0.00706569632511678]
	TIME [epoch: 54.9 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005524803194212276		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.005524803194212276 | validation: 0.0077433963719032405]
	TIME [epoch: 54.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005657858617567282		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.005657858617567282 | validation: 0.007889808987947788]
	TIME [epoch: 54.9 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056393149952496955		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.0056393149952496955 | validation: 0.007846480323811076]
	TIME [epoch: 55 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005421969530677003		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.005421969530677003 | validation: 0.008442660806637522]
	TIME [epoch: 55 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005690794863022539		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.005690794863022539 | validation: 0.008559458533173268]
	TIME [epoch: 54.9 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006099656120832241		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.006099656120832241 | validation: 0.00890672397791362]
	TIME [epoch: 55 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005474532781198796		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.005474532781198796 | validation: 0.009316642918588625]
	TIME [epoch: 54.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054285891805778		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.0054285891805778 | validation: 0.008082862339881211]
	TIME [epoch: 55 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005478320357560063		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.005478320357560063 | validation: 0.007545435527176112]
	TIME [epoch: 54.9 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055073792315097		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.0055073792315097 | validation: 0.009152534152348651]
	TIME [epoch: 55 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005322845178169181		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.005322845178169181 | validation: 0.009325992731402173]
	TIME [epoch: 54.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005450817650526712		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.005450817650526712 | validation: 0.007550730109120933]
	TIME [epoch: 54.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00583448140303404		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.00583448140303404 | validation: 0.008152468655491987]
	TIME [epoch: 55 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005698997917665204		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.005698997917665204 | validation: 0.007973119847217207]
	TIME [epoch: 54.9 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005360991991931404		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.005360991991931404 | validation: 0.007900688311537242]
	TIME [epoch: 55 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005290239318704575		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.005290239318704575 | validation: 0.007603210927455523]
	TIME [epoch: 55 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005260011795094785		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.005260011795094785 | validation: 0.007562323947129116]
	TIME [epoch: 54.9 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005787624221044625		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.005787624221044625 | validation: 0.007533088993005914]
	TIME [epoch: 54.9 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057936518354251626		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.0057936518354251626 | validation: 0.007287256157044443]
	TIME [epoch: 54.9 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005888897632149335		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.005888897632149335 | validation: 0.008503520590535394]
	TIME [epoch: 54.9 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005148124595322016		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.005148124595322016 | validation: 0.007941515122512333]
	TIME [epoch: 55 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005305024763604252		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.005305024763604252 | validation: 0.006976392823136874]
	TIME [epoch: 54.9 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053628698875046584		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.0053628698875046584 | validation: 0.007464403523108124]
	TIME [epoch: 55 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055143950474271924		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.0055143950474271924 | validation: 0.007544176353858917]
	TIME [epoch: 55 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005457688657340638		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.005457688657340638 | validation: 0.007571082079022716]
	TIME [epoch: 54.9 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005126065537760476		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.005126065537760476 | validation: 0.006458567919210395]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1209.pth
	Model improved!!!
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005532452716193792		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.005532452716193792 | validation: 0.00889650623436502]
	TIME [epoch: 54.9 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004914744510908053		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.004914744510908053 | validation: 0.008017688760399459]
	TIME [epoch: 54.9 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005307228052724537		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.005307228052724537 | validation: 0.008780699388333577]
	TIME [epoch: 54.9 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005197858157634486		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.005197858157634486 | validation: 0.01078731701444701]
	TIME [epoch: 55 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050287432636418675		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.0050287432636418675 | validation: 0.008559288804607965]
	TIME [epoch: 54.9 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004981692989124042		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.004981692989124042 | validation: 0.007799653561158465]
	TIME [epoch: 54.9 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005356256990095792		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.005356256990095792 | validation: 0.008034462996454137]
	TIME [epoch: 54.9 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049926224181395225		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.0049926224181395225 | validation: 0.007553562895420357]
	TIME [epoch: 55 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005086545197297061		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.005086545197297061 | validation: 0.006726161120109552]
	TIME [epoch: 55 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005036825886606448		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.005036825886606448 | validation: 0.00851369751711915]
	TIME [epoch: 55 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005346154122597767		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.005346154122597767 | validation: 0.010134507990531307]
	TIME [epoch: 55 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00522534023204099		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.00522534023204099 | validation: 0.008354161462508534]
	TIME [epoch: 55 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057325964640569625		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.0057325964640569625 | validation: 0.008719544513797473]
	TIME [epoch: 54.9 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053082241080277615		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.0053082241080277615 | validation: 0.008662247862202502]
	TIME [epoch: 54.9 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052179537428398304		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.0052179537428398304 | validation: 0.007574801021414895]
	TIME [epoch: 55 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054284057767106375		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.0054284057767106375 | validation: 0.0075355921995875754]
	TIME [epoch: 54.9 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004957025186304401		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.004957025186304401 | validation: 0.008854841210423845]
	TIME [epoch: 54.9 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005021804381611927		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.005021804381611927 | validation: 0.007516231036733601]
	TIME [epoch: 55 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005445278214883997		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.005445278214883997 | validation: 0.007429397292655952]
	TIME [epoch: 54.9 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00529739510424352		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.00529739510424352 | validation: 0.008329365965878101]
	TIME [epoch: 54.9 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004983788597546745		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.004983788597546745 | validation: 0.007264974261598714]
	TIME [epoch: 54.9 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056439882881996065		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.0056439882881996065 | validation: 0.00862004025341279]
	TIME [epoch: 54.9 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005072155321036968		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.005072155321036968 | validation: 0.008007962930059195]
	TIME [epoch: 54.9 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005076879615775461		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.005076879615775461 | validation: 0.006858665873537421]
	TIME [epoch: 54.9 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005141637993808616		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.005141637993808616 | validation: 0.007432162499352277]
	TIME [epoch: 55 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057336386911508355		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.0057336386911508355 | validation: 0.007584797171026948]
	TIME [epoch: 54.9 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00526354235212047		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.00526354235212047 | validation: 0.007012311285347211]
	TIME [epoch: 55 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005305611311283936		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.005305611311283936 | validation: 0.00699675288568548]
	TIME [epoch: 54.9 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004963535389571231		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.004963535389571231 | validation: 0.007452888796613363]
	TIME [epoch: 54.9 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005553638577198499		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.005553638577198499 | validation: 0.0074488115763221355]
	TIME [epoch: 55 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052137296535896185		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.0052137296535896185 | validation: 0.007921723570091768]
	TIME [epoch: 54.9 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049033381950624125		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.0049033381950624125 | validation: 0.006951386311063365]
	TIME [epoch: 54.9 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005315469206662647		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.005315469206662647 | validation: 0.0072855697061657045]
	TIME [epoch: 54.9 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004658052684715843		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.004658052684715843 | validation: 0.00731063386126565]
	TIME [epoch: 54.9 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004764294809088041		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.004764294809088041 | validation: 0.008329754350022847]
	TIME [epoch: 54.9 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005014732675673352		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.005014732675673352 | validation: 0.009247694648963304]
	TIME [epoch: 54.9 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055493008290092		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.0055493008290092 | validation: 0.008011121157133434]
	TIME [epoch: 55 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00540959547957473		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.00540959547957473 | validation: 0.00799129316770952]
	TIME [epoch: 54.9 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00588517557323472		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.00588517557323472 | validation: 0.007735162285696113]
	TIME [epoch: 54.9 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055179525727120406		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.0055179525727120406 | validation: 0.008099784939271333]
	TIME [epoch: 54.9 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005088258825221109		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.005088258825221109 | validation: 0.007267208472911587]
	TIME [epoch: 54.9 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005340674722461172		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.005340674722461172 | validation: 0.00865405851659363]
	TIME [epoch: 54.9 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005203449009984651		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.005203449009984651 | validation: 0.0069196373441251565]
	TIME [epoch: 55 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051211395580661855		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.0051211395580661855 | validation: 0.007523720871879076]
	TIME [epoch: 55 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004915421284724181		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.004915421284724181 | validation: 0.006567061249084034]
	TIME [epoch: 55 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005347608234961547		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.005347608234961547 | validation: 0.0076357141248310535]
	TIME [epoch: 54.9 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004784337726447669		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.004784337726447669 | validation: 0.007020201720393096]
	TIME [epoch: 55 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004901535206008681		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.004901535206008681 | validation: 0.00742751033926252]
	TIME [epoch: 54.9 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005039058751426603		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.005039058751426603 | validation: 0.007073897703825509]
	TIME [epoch: 55 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005449322107584247		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.005449322107584247 | validation: 0.007845072495681537]
	TIME [epoch: 55 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052977835075927945		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.0052977835075927945 | validation: 0.008415399014802111]
	TIME [epoch: 55 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005131827375242755		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.005131827375242755 | validation: 0.008368879770437371]
	TIME [epoch: 54.9 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004655736897444806		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.004655736897444806 | validation: 0.009371553757089101]
	TIME [epoch: 54.9 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005298769544324083		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.005298769544324083 | validation: 0.008136564702960888]
	TIME [epoch: 54.9 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048165024283605955		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.0048165024283605955 | validation: 0.008970991149143688]
	TIME [epoch: 55 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053754991955587295		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.0053754991955587295 | validation: 0.007354286070569851]
	TIME [epoch: 54.9 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053757259704969515		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.0053757259704969515 | validation: 0.008160924570306982]
	TIME [epoch: 54.8 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004987426848462291		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.004987426848462291 | validation: 0.006370679482643959]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1267.pth
	Model improved!!!
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005149065425647971		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.005149065425647971 | validation: 0.0075720227005708195]
	TIME [epoch: 54.9 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004951562245517068		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.004951562245517068 | validation: 0.007258653407130637]
	TIME [epoch: 54.9 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004984540687389804		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.004984540687389804 | validation: 0.008155635178564121]
	TIME [epoch: 54.9 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004747860409715355		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.004747860409715355 | validation: 0.007853755262960277]
	TIME [epoch: 55 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004938926533598486		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.004938926533598486 | validation: 0.007649339326603451]
	TIME [epoch: 55 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004964805365525852		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.004964805365525852 | validation: 0.006421407717061137]
	TIME [epoch: 54.9 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004946753223912562		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.004946753223912562 | validation: 0.008843396998723623]
	TIME [epoch: 54.9 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005407301245576986		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.005407301245576986 | validation: 0.00741721906783808]
	TIME [epoch: 55 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005228687181244192		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.005228687181244192 | validation: 0.006746175905290239]
	TIME [epoch: 55 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004764807775572638		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.004764807775572638 | validation: 0.006217896309443702]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1277.pth
	Model improved!!!
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004879683581247311		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.004879683581247311 | validation: 0.00785512385709216]
	TIME [epoch: 54.9 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004979233353751098		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.004979233353751098 | validation: 0.00859123639628923]
	TIME [epoch: 55 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051961751392303225		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.0051961751392303225 | validation: 0.007342153402840232]
	TIME [epoch: 54.9 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005152151278022902		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.005152151278022902 | validation: 0.006887575984499329]
	TIME [epoch: 54.9 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005072477589245795		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.005072477589245795 | validation: 0.007867704122328135]
	TIME [epoch: 54.9 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005008726596023233		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.005008726596023233 | validation: 0.007128143083629269]
	TIME [epoch: 54.9 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049640072808977125		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.0049640072808977125 | validation: 0.007065124157977245]
	TIME [epoch: 54.9 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004732232247476633		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.004732232247476633 | validation: 0.0075529915434798]
	TIME [epoch: 54.9 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005068393197812479		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.005068393197812479 | validation: 0.007288593555401437]
	TIME [epoch: 54.9 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050046434986682885		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.0050046434986682885 | validation: 0.007394161150348674]
	TIME [epoch: 54.9 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005064185902526527		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.005064185902526527 | validation: 0.007549871345694828]
	TIME [epoch: 54.9 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049776900210007375		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.0049776900210007375 | validation: 0.0080138985674838]
	TIME [epoch: 54.9 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004979935992652994		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.004979935992652994 | validation: 0.008104174716285016]
	TIME [epoch: 54.9 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005054261698802904		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.005054261698802904 | validation: 0.0073257729410640925]
	TIME [epoch: 54.9 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004865946532052951		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.004865946532052951 | validation: 0.00735547168504249]
	TIME [epoch: 54.9 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004745628374164972		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.004745628374164972 | validation: 0.007238607690097787]
	TIME [epoch: 54.9 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004602113752191957		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.004602113752191957 | validation: 0.007213349079802652]
	TIME [epoch: 54.9 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004927055105532766		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.004927055105532766 | validation: 0.007865803891657199]
	TIME [epoch: 54.9 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004912661479851126		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.004912661479851126 | validation: 0.0069070565328855395]
	TIME [epoch: 54.9 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00501707848727434		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.00501707848727434 | validation: 0.007074320314523557]
	TIME [epoch: 54.9 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004873744033608303		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.004873744033608303 | validation: 0.006771416162352378]
	TIME [epoch: 54.8 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00475759136225764		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.00475759136225764 | validation: 0.006490019555133532]
	TIME [epoch: 54.9 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004827560877855996		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.004827560877855996 | validation: 0.008464329240079758]
	TIME [epoch: 54.8 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046951590732727255		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.0046951590732727255 | validation: 0.007294343079940497]
	TIME [epoch: 55 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004841541133019659		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.004841541133019659 | validation: 0.007833198699196782]
	TIME [epoch: 54.9 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004681911908943914		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.004681911908943914 | validation: 0.007269115550461027]
	TIME [epoch: 54.9 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004962665780208295		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.004962665780208295 | validation: 0.00689603839913962]
	TIME [epoch: 54.8 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048381732052228295		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.0048381732052228295 | validation: 0.007810847552608302]
	TIME [epoch: 54.9 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004606618107301543		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.004606618107301543 | validation: 0.007779245527135555]
	TIME [epoch: 54.9 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004872286497535043		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.004872286497535043 | validation: 0.007166094985971257]
	TIME [epoch: 54.9 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004963064365675469		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.004963064365675469 | validation: 0.00780333641841117]
	TIME [epoch: 54.9 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045015136790580125		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.0045015136790580125 | validation: 0.009319204197527587]
	TIME [epoch: 54.9 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004939777772086665		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.004939777772086665 | validation: 0.007658737252782211]
	TIME [epoch: 54.9 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051083151881181465		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.0051083151881181465 | validation: 0.00788850939658991]
	TIME [epoch: 54.8 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004894780398122089		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.004894780398122089 | validation: 0.007751992154171216]
	TIME [epoch: 54.9 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004470103685602399		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.004470103685602399 | validation: 0.007543792882721659]
	TIME [epoch: 54.9 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004786767635645079		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.004786767635645079 | validation: 0.007546793176210081]
	TIME [epoch: 54.9 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004728603854940592		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.004728603854940592 | validation: 0.008130758579857412]
	TIME [epoch: 54.9 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004774123077488526		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.004774123077488526 | validation: 0.0065461165195650406]
	TIME [epoch: 54.9 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004673330045726213		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.004673330045726213 | validation: 0.007391761390646303]
	TIME [epoch: 54.8 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004820919916534539		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.004820919916534539 | validation: 0.008146912534511516]
	TIME [epoch: 54.9 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004688602096120591		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.004688602096120591 | validation: 0.007344765217491193]
	TIME [epoch: 55 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004638376114707714		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.004638376114707714 | validation: 0.0073610129947801525]
	TIME [epoch: 54.9 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005095971431790421		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.005095971431790421 | validation: 0.006755758645340001]
	TIME [epoch: 54.9 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004529523628684614		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.004529523628684614 | validation: 0.006322968457899281]
	TIME [epoch: 54.9 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005182044073007873		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.005182044073007873 | validation: 0.0068902566018221555]
	TIME [epoch: 54.9 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004710019646708764		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.004710019646708764 | validation: 0.007985768757010805]
	TIME [epoch: 54.9 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004652841277393469		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.004652841277393469 | validation: 0.007472107615397015]
	TIME [epoch: 54.9 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004597789856392357		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.004597789856392357 | validation: 0.007745579683074496]
	TIME [epoch: 54.9 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004944264911651871		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.004944264911651871 | validation: 0.007583350023661253]
	TIME [epoch: 54.9 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004750094805848434		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.004750094805848434 | validation: 0.00766402755157308]
	TIME [epoch: 55 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004564900049905708		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.004564900049905708 | validation: 0.0069521483293778965]
	TIME [epoch: 54.9 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004476280686100712		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.004476280686100712 | validation: 0.0073523596061627495]
	TIME [epoch: 54.9 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004622265277369031		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.004622265277369031 | validation: 0.007027700388633094]
	TIME [epoch: 54.9 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048162365220162755		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.0048162365220162755 | validation: 0.006700511512664159]
	TIME [epoch: 54.9 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005124050865045758		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.005124050865045758 | validation: 0.007441407997607556]
	TIME [epoch: 54.9 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004560733546140696		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.004560733546140696 | validation: 0.007486962963801389]
	TIME [epoch: 54.9 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004771263606280983		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.004771263606280983 | validation: 0.006249280335926759]
	TIME [epoch: 54.9 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004851491673982186		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.004851491673982186 | validation: 0.008655746582341202]
	TIME [epoch: 54.8 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004774894745390507		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.004774894745390507 | validation: 0.0065470966111998465]
	TIME [epoch: 55 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004660878550142576		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.004660878550142576 | validation: 0.007446392763815537]
	TIME [epoch: 54.9 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045371862938708585		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.0045371862938708585 | validation: 0.009437211686110773]
	TIME [epoch: 54.9 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045513586407061645		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.0045513586407061645 | validation: 0.0084186768885023]
	TIME [epoch: 54.9 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004828521164036195		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.004828521164036195 | validation: 0.00799798836367291]
	TIME [epoch: 54.9 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004658713886469962		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.004658713886469962 | validation: 0.006882780105595113]
	TIME [epoch: 54.9 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00440690149395363		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.00440690149395363 | validation: 0.007333440062400843]
	TIME [epoch: 54.9 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048247857306425055		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.0048247857306425055 | validation: 0.00628779502022931]
	TIME [epoch: 54.9 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043844590619850805		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.0043844590619850805 | validation: 0.008743107107788054]
	TIME [epoch: 54.9 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004649749393165948		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.004649749393165948 | validation: 0.007428699925255832]
	TIME [epoch: 54.9 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004731322667245096		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.004731322667245096 | validation: 0.006840410269152137]
	TIME [epoch: 54.9 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005210033026993844		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.005210033026993844 | validation: 0.006430341020512323]
	TIME [epoch: 54.9 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004746598943001265		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.004746598943001265 | validation: 0.007083925243251268]
	TIME [epoch: 54.8 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004851984914588945		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.004851984914588945 | validation: 0.00703897340356974]
	TIME [epoch: 54.9 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00445991124093854		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.00445991124093854 | validation: 0.005740185355342572]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1351.pth
	Model improved!!!
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004885309334103734		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.004885309334103734 | validation: 0.006226166649150058]
	TIME [epoch: 54.9 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004686431029651431		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.004686431029651431 | validation: 0.007364136380831493]
	TIME [epoch: 54.9 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005142883893341393		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.005142883893341393 | validation: 0.006670693455048656]
	TIME [epoch: 54.9 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004682511178770103		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.004682511178770103 | validation: 0.0066608535816431065]
	TIME [epoch: 55 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004626645356942655		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.004626645356942655 | validation: 0.006937440055317131]
	TIME [epoch: 54.9 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004453006605335831		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.004453006605335831 | validation: 0.0071346759971706]
	TIME [epoch: 54.9 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004740392888918033		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.004740392888918033 | validation: 0.006759393934342435]
	TIME [epoch: 54.9 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004891053492789208		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.004891053492789208 | validation: 0.008008158490947777]
	TIME [epoch: 54.9 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049379359759422		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.0049379359759422 | validation: 0.007235352318481144]
	TIME [epoch: 54.9 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00490142579818933		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.00490142579818933 | validation: 0.007415184295365118]
	TIME [epoch: 54.9 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004685946656397682		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.004685946656397682 | validation: 0.006319158160145158]
	TIME [epoch: 54.9 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004645036037411155		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.004645036037411155 | validation: 0.006897496203540299]
	TIME [epoch: 54.9 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042752295887901505		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.0042752295887901505 | validation: 0.006780762250178192]
	TIME [epoch: 54.9 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004950674156730144		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.004950674156730144 | validation: 0.008562035211487932]
	TIME [epoch: 54.9 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00500767819875159		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.00500767819875159 | validation: 0.007649119291233644]
	TIME [epoch: 55 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004849067625892651		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.004849067625892651 | validation: 0.007986183009239759]
	TIME [epoch: 54.9 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004722793203528968		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.004722793203528968 | validation: 0.0075863075302513225]
	TIME [epoch: 54.9 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004597277492959432		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.004597277492959432 | validation: 0.006978733235861702]
	TIME [epoch: 54.9 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004658098789551956		[learning rate: 9.3243e-05]
	Learning Rate: 9.32429e-05
	LOSS [training: 0.004658098789551956 | validation: 0.0077452128008169]
	TIME [epoch: 55 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004371118285786954		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.004371118285786954 | validation: 0.007070800488400097]
	TIME [epoch: 54.9 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00478127646618836		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.00478127646618836 | validation: 0.007080441990124553]
	TIME [epoch: 54.9 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004858922908005365		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.004858922908005365 | validation: 0.006323946920277075]
	TIME [epoch: 55 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004407899953703634		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.004407899953703634 | validation: 0.007177664762041497]
	TIME [epoch: 54.9 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004409048517691085		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.004409048517691085 | validation: 0.006876155237528407]
	TIME [epoch: 55 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048101837569532015		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.0048101837569532015 | validation: 0.007315795826760561]
	TIME [epoch: 54.9 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00443495193728346		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.00443495193728346 | validation: 0.008242247775966055]
	TIME [epoch: 54.9 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005144989937336655		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.005144989937336655 | validation: 0.006448844532959897]
	TIME [epoch: 55 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004470325389464609		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.004470325389464609 | validation: 0.007212938040904894]
	TIME [epoch: 54.9 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004317372138761041		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.004317372138761041 | validation: 0.007124595148420495]
	TIME [epoch: 54.9 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00477583604309549		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.00477583604309549 | validation: 0.006832403582532117]
	TIME [epoch: 55 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004588748081103653		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.004588748081103653 | validation: 0.006836549280606499]
	TIME [epoch: 54.9 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004548851265811334		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.004548851265811334 | validation: 0.006364209369357103]
	TIME [epoch: 54.9 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004152262494312144		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.004152262494312144 | validation: 0.005609475927612414]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1a_0_v_mmd1_1384.pth
	Model improved!!!
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004619507031798217		[learning rate: 8.8418e-05]
	Learning Rate: 8.84176e-05
	LOSS [training: 0.004619507031798217 | validation: 0.008379926981003469]
	TIME [epoch: 55 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00454956247287944		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.00454956247287944 | validation: 0.007675257555231079]
	TIME [epoch: 54.9 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004562583792141358		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.004562583792141358 | validation: 0.006819954639962022]
	TIME [epoch: 54.9 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004777684492111055		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.004777684492111055 | validation: 0.0063580460051908426]
	TIME [epoch: 54.9 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004844828840242144		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.004844828840242144 | validation: 0.007240556114099503]
	TIME [epoch: 54.8 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004942057015748318		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.004942057015748318 | validation: 0.006674019017179379]
	TIME [epoch: 55 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00490236298629248		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.00490236298629248 | validation: 0.007488828681301203]
	TIME [epoch: 54.9 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004425319491484421		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.004425319491484421 | validation: 0.008391714844361603]
	TIME [epoch: 54.8 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004425926189494013		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.004425926189494013 | validation: 0.006880672566438576]
	TIME [epoch: 54.9 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004281585599769514		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.004281585599769514 | validation: 0.008124968409690835]
	TIME [epoch: 54.9 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005167612828431032		[learning rate: 8.534e-05]
	Learning Rate: 8.53403e-05
	LOSS [training: 0.005167612828431032 | validation: 0.005994899277312283]
	TIME [epoch: 54.9 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004724697349566699		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.004724697349566699 | validation: 0.006494919128587085]
	TIME [epoch: 54.9 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004760202288180298		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.004760202288180298 | validation: 0.006529980903012117]
	TIME [epoch: 54.9 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004403905152326266		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.004403905152326266 | validation: 0.00735992215988805]
	TIME [epoch: 55 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042421659555064335		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.0042421659555064335 | validation: 0.007064667874588738]
	TIME [epoch: 54.9 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004861097779899533		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.004861097779899533 | validation: 0.0065636372490385265]
	TIME [epoch: 54.9 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049220205269348335		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.0049220205269348335 | validation: 0.007077097964525871]
	TIME [epoch: 54.9 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004885669378699721		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.004885669378699721 | validation: 0.006878236862290204]
	TIME [epoch: 54.9 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004745068409608301		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.004745068409608301 | validation: 0.005935899904599355]
	TIME [epoch: 54.9 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004721486926928194		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.004721486926928194 | validation: 0.006024835880840488]
	TIME [epoch: 54.9 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004618711767174916		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.004618711767174916 | validation: 0.007825894745971916]
	TIME [epoch: 54.9 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004721044137555271		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.004721044137555271 | validation: 0.006466727727860109]
	TIME [epoch: 54.9 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004486613784553623		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.004486613784553623 | validation: 0.007499742528107189]
	TIME [epoch: 54.9 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046618900835311075		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.0046618900835311075 | validation: 0.006997856322146454]
	TIME [epoch: 55 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00417660600846782		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.00417660600846782 | validation: 0.007295012583720337]
	TIME [epoch: 54.9 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045946225585455965		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.0045946225585455965 | validation: 0.008070937350847107]
	TIME [epoch: 55 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004829899193891096		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.004829899193891096 | validation: 0.006826635926093783]
	TIME [epoch: 54.9 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004552921460831952		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.004552921460831952 | validation: 0.006974710002583853]
	TIME [epoch: 54.9 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004720507437500659		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.004720507437500659 | validation: 0.007871316402878841]
	TIME [epoch: 54.9 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004400673220557767		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.004400673220557767 | validation: 0.006262507665962751]
	TIME [epoch: 54.9 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004405777552171544		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.004405777552171544 | validation: 0.007355272896419947]
	TIME [epoch: 55 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004676957421134021		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.004676957421134021 | validation: 0.006649460121024772]
	TIME [epoch: 54.9 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004384333910188684		[learning rate: 7.8942e-05]
	Learning Rate: 7.89419e-05
	LOSS [training: 0.004384333910188684 | validation: 0.006945345563149189]
	TIME [epoch: 55 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004128864148472976		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.004128864148472976 | validation: 0.006452079454139838]
	TIME [epoch: 55 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004361078148527615		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.004361078148527615 | validation: 0.006428525255195538]
	TIME [epoch: 55 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004604963750081263		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.004604963750081263 | validation: 0.005879293616300907]
	TIME [epoch: 54.9 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004559213884553385		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 0.004559213884553385 | validation: 0.006272580843573162]
	TIME [epoch: 54.9 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004490433719102512		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 0.004490433719102512 | validation: 0.0067096947950585]
	TIME [epoch: 55 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004590086784114176		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 0.004590086784114176 | validation: 0.007164510975643678]
	TIME [epoch: 55 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004539674784837744		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 0.004539674784837744 | validation: 0.006631387290368496]
	TIME [epoch: 54.9 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004748351833049472		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.004748351833049472 | validation: 0.007212842157644377]
	TIME [epoch: 54.9 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004531670035546154		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 0.004531670035546154 | validation: 0.006880519747716371]
	TIME [epoch: 55 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004545379318622415		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 0.004545379318622415 | validation: 0.007911252419385091]
	TIME [epoch: 54.9 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004543039817250537		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 0.004543039817250537 | validation: 0.007156210201648728]
	TIME [epoch: 54.9 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004273419082640433		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 0.004273419082640433 | validation: 0.007620443254598929]
	TIME [epoch: 54.9 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00434373849772061		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 0.00434373849772061 | validation: 0.006985300787457798]
	TIME [epoch: 54.9 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043953368222696895		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 0.0043953368222696895 | validation: 0.007582033644092347]
	TIME [epoch: 54.9 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004343697481490842		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 0.004343697481490842 | validation: 0.006566419057062009]
	TIME [epoch: 54.9 sec]
EPOCH 1433/2000:
	Training over batches...
