Args:
Namespace(name='model_phi1_1a_saddle_v1a_3_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_3/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1a_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.050754182040691376, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2279857043

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.914605443618305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.914605443618305 | validation: 5.926485994871232]
	TIME [epoch: 374 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.315028374500338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.315028374500338 | validation: 5.641922296165483]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.894025836838257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.894025836838257 | validation: 5.167467659503188]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.579605954050107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.579605954050107 | validation: 5.187435205998607]
	TIME [epoch: 6.19 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.342461665557134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.342461665557134 | validation: 5.02176142001651]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.068890156075726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.068890156075726 | validation: 4.733473530822886]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.758131781348782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.758131781348782 | validation: 4.574191382953892]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.712863007730503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.712863007730503 | validation: 4.431523853286723]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5130669342774565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5130669342774565 | validation: 4.101251276735157]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.121142779979064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121142779979064 | validation: 4.3599748864162144]
	TIME [epoch: 6.19 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.269549282829143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.269549282829143 | validation: 3.9358825292583357]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.948263064061293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.948263064061293 | validation: 3.719884354450901]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.947858481556648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.947858481556648 | validation: 3.7266478204917606]
	TIME [epoch: 6.2 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7347987641793177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7347987641793177 | validation: 3.5852374154558406]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6250913631932793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6250913631932793 | validation: 3.906895993983958]
	TIME [epoch: 6.19 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.765191282642024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.765191282642024 | validation: 3.848533564417706]
	TIME [epoch: 6.19 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6550410732991527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6550410732991527 | validation: 3.4252677039452384]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.48713726484601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.48713726484601 | validation: 3.33982904768125]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4204520876444384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4204520876444384 | validation: 3.363852142422398]
	TIME [epoch: 6.18 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.416896704789689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.416896704789689 | validation: 3.1810541400122228]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286810172040895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.286810172040895 | validation: 3.124560858895923]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083308884321795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2083308884321795 | validation: 2.941659222158531]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.305574938148223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.305574938148223 | validation: 2.945785776277099]
	TIME [epoch: 6.18 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1049095472748087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1049095472748087 | validation: 2.807156348991897]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.061800662672721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.061800662672721 | validation: 2.8606451534173187]
	TIME [epoch: 6.19 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.01829922344705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.01829922344705 | validation: 2.6157101778183307]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.030626091326369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.030626091326369 | validation: 2.6238500925608688]
	TIME [epoch: 6.2 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.99755370732976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.99755370732976 | validation: 2.661189406646164]
	TIME [epoch: 6.18 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9475294633262754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9475294633262754 | validation: 2.4872699561286336]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.004641778777101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.004641778777101 | validation: 2.786187143527034]
	TIME [epoch: 6.19 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.051398191369464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.051398191369464 | validation: 2.6693842509892165]
	TIME [epoch: 6.19 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.927680633317869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.927680633317869 | validation: 2.434827350540237]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9202733146751543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9202733146751543 | validation: 2.7516925920005173]
	TIME [epoch: 6.2 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.003951710846258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.003951710846258 | validation: 2.566383702109653]
	TIME [epoch: 6.19 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8703362958907954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8703362958907954 | validation: 2.2607544495576364]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.905050765080668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.905050765080668 | validation: 2.492797664154159]
	TIME [epoch: 6.2 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8157413565442337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8157413565442337 | validation: 2.165135736065872]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.835488485822756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.835488485822756 | validation: 2.3458475495872984]
	TIME [epoch: 6.19 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8258018465650228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8258018465650228 | validation: 3.07704680648375]
	TIME [epoch: 6.18 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0222662094470074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0222662094470074 | validation: 2.1991087471103468]
	TIME [epoch: 6.2 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6024900419977586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6024900419977586 | validation: 1.956955870381765]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6121884086040774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6121884086040774 | validation: 2.143686383448669]
	TIME [epoch: 6.19 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6279412553659403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6279412553659403 | validation: 2.115501556955608]
	TIME [epoch: 6.18 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6047175521816897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6047175521816897 | validation: 1.8413022733269417]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4312164138031394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4312164138031394 | validation: 1.8606770269468216]
	TIME [epoch: 6.2 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4830962935452554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4830962935452554 | validation: 2.026933594058087]
	TIME [epoch: 6.19 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.416464525318683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.416464525318683 | validation: 1.7596002135396622]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4970562296246874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4970562296246874 | validation: 1.7257881995141053]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4660548669112727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4660548669112727 | validation: 1.9037565928163778]
	TIME [epoch: 6.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370956871268754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.370956871268754 | validation: 1.8142132218542473]
	TIME [epoch: 6.19 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3524517674275223		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 2.3524517674275223 | validation: 1.6773895792936737]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4458122004984837		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 2.4458122004984837 | validation: 1.9889232408475608]
	TIME [epoch: 6.21 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4893092302542055		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 2.4893092302542055 | validation: 1.6660078123872875]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3032888877695523		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 2.3032888877695523 | validation: 1.6456418222429594]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3863778966109392		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 2.3863778966109392 | validation: 1.656135526457358]
	TIME [epoch: 6.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2731753403825663		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 2.2731753403825663 | validation: 1.6486933320122832]
	TIME [epoch: 6.19 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4291488669952352		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 2.4291488669952352 | validation: 1.8802110377695054]
	TIME [epoch: 6.18 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3953278730285152		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 2.3953278730285152 | validation: 1.590312936360729]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2460041240609003		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 2.2460041240609003 | validation: 1.550135639220307]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.290788826765148		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 2.290788826765148 | validation: 2.2593848236027427]
	TIME [epoch: 6.19 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.109080366417839		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 2.109080366417839 | validation: 1.246472252739975]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.303394820417425		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.303394820417425 | validation: 1.2521874144560021]
	TIME [epoch: 6.19 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1087619704682878		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.1087619704682878 | validation: 0.882373739845054]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0280370684425972		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.0280370684425972 | validation: 1.7161450608413422]
	TIME [epoch: 6.2 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4137367669666268		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.4137367669666268 | validation: 0.8588753449150613]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8938453916901602		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.8938453916901602 | validation: 0.8310715408902499]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.917386948804469		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.917386948804469 | validation: 1.3998365052507529]
	TIME [epoch: 6.19 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3724279128489156		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 1.3724279128489156 | validation: 0.8886647924439253]
	TIME [epoch: 6.2 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.001108240586613		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 1.001108240586613 | validation: 0.9134850542108344]
	TIME [epoch: 6.18 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0393346493265174		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 1.0393346493265174 | validation: 0.8541323831993106]
	TIME [epoch: 6.19 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8372964359073858		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.8372964359073858 | validation: 0.7465617874123927]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9417197366762803		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.9417197366762803 | validation: 0.6956586186476712]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7855801095463254		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.7855801095463254 | validation: 0.7684921521867051]
	TIME [epoch: 6.21 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9892446195270296		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.9892446195270296 | validation: 0.6683666997717812]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8125149115025324		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.8125149115025324 | validation: 0.9880842263813349]
	TIME [epoch: 6.19 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8352295346016203		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.8352295346016203 | validation: 0.7745308967595337]
	TIME [epoch: 6.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9429206453630838		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.9429206453630838 | validation: 0.7056120986799415]
	TIME [epoch: 6.19 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7491483420026255		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.7491483420026255 | validation: 1.0120520301712963]
	TIME [epoch: 6.2 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9642878598907961		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.9642878598907961 | validation: 0.6262753088591781]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7285206577770985		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.7285206577770985 | validation: 1.3577915909497076]
	TIME [epoch: 6.19 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0039890100450184		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 1.0039890100450184 | validation: 0.7953042603096774]
	TIME [epoch: 6.19 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8509870293765556		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.8509870293765556 | validation: 0.6524752113142652]
	TIME [epoch: 6.19 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.828165309585737		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.828165309585737 | validation: 0.7185931900088832]
	TIME [epoch: 6.18 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7550621469867739		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.7550621469867739 | validation: 1.8689181322020652]
	TIME [epoch: 6.19 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5114329172872991		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 1.5114329172872991 | validation: 1.0090559034294195]
	TIME [epoch: 6.19 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7821756087472854		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.7821756087472854 | validation: 0.6083170338460362]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8836386575304579		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.8836386575304579 | validation: 1.175678876444672]
	TIME [epoch: 6.2 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0121406521657923		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 1.0121406521657923 | validation: 1.2509017147625459]
	TIME [epoch: 6.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8224599497637073		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.8224599497637073 | validation: 0.6664986121913721]
	TIME [epoch: 6.18 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6589327556950666		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.6589327556950666 | validation: 0.4941002893242694]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7342955462453843		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.7342955462453843 | validation: 0.6988241577968496]
	TIME [epoch: 6.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8652457667352851		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.8652457667352851 | validation: 0.8683814177389443]
	TIME [epoch: 6.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7381633293120797		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.7381633293120797 | validation: 0.5635363942060083]
	TIME [epoch: 6.18 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7532243433614136		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.7532243433614136 | validation: 0.584898067977641]
	TIME [epoch: 6.18 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6661251400943992		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.6661251400943992 | validation: 0.6540623541863828]
	TIME [epoch: 6.18 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.958970596381076		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.958970596381076 | validation: 0.9264714708026713]
	TIME [epoch: 6.19 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8793028605241346		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.8793028605241346 | validation: 0.5536153079444127]
	TIME [epoch: 6.2 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5700748252571165		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.5700748252571165 | validation: 0.5845815724430299]
	TIME [epoch: 6.18 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.521056407021369		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.521056407021369 | validation: 0.7628256206135435]
	TIME [epoch: 6.19 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7619396578120823		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.7619396578120823 | validation: 0.5554408370657207]
	TIME [epoch: 6.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5750369106126427		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.5750369106126427 | validation: 0.8199259698748493]
	TIME [epoch: 6.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6432154539957918		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.6432154539957918 | validation: 0.46365854785619764]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7968891509876662		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.7968891509876662 | validation: 0.6026551353158429]
	TIME [epoch: 6.19 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5215214977135348		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5215214977135348 | validation: 0.5602446825451473]
	TIME [epoch: 6.18 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.633327264490704		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.633327264490704 | validation: 0.4400185442679966]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5879039376073617		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.5879039376073617 | validation: 0.4585180207215046]
	TIME [epoch: 6.2 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5550366198549892		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.5550366198549892 | validation: 0.6580345336713781]
	TIME [epoch: 6.2 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5485369982393735		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.5485369982393735 | validation: 0.5611493501459448]
	TIME [epoch: 6.19 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5590889824701665		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.5590889824701665 | validation: 0.5000193839737654]
	TIME [epoch: 6.18 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.580411534064735		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.580411534064735 | validation: 0.5148426671946924]
	TIME [epoch: 6.19 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44799112710984		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.44799112710984 | validation: 0.563139909202912]
	TIME [epoch: 6.19 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5210452995594033		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.5210452995594033 | validation: 0.39588370008178886]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6318192441114737		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.6318192441114737 | validation: 0.7217224043707688]
	TIME [epoch: 6.19 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027942815632705		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.6027942815632705 | validation: 0.7935982765842606]
	TIME [epoch: 6.19 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6329682387379308		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.6329682387379308 | validation: 0.3874062938194469]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6501118051983312		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.6501118051983312 | validation: 0.5252207041525678]
	TIME [epoch: 6.2 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4345907732838796		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.4345907732838796 | validation: 0.4916854656363886]
	TIME [epoch: 6.18 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5138033956091638		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.5138033956091638 | validation: 0.4050633682247759]
	TIME [epoch: 6.19 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5055569509771016		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.5055569509771016 | validation: 0.6238203822498958]
	TIME [epoch: 6.19 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5020942662884199		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.5020942662884199 | validation: 0.5838047454162434]
	TIME [epoch: 6.18 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4428972980043887		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.4428972980043887 | validation: 0.4253401852596119]
	TIME [epoch: 6.18 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5662344099455232		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.5662344099455232 | validation: 0.7783535582079255]
	TIME [epoch: 6.19 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6444959347277053		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.6444959347277053 | validation: 0.7109842874798029]
	TIME [epoch: 6.18 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47775907654199024		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.47775907654199024 | validation: 0.5109174886521572]
	TIME [epoch: 6.18 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.421131226372252		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.421131226372252 | validation: 0.602568132340549]
	TIME [epoch: 6.18 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5135473168826657		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.5135473168826657 | validation: 0.4210810893328574]
	TIME [epoch: 6.19 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45216257368274165		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.45216257368274165 | validation: 0.39529397599440536]
	TIME [epoch: 6.17 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44914340044468215		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.44914340044468215 | validation: 0.44303326563827383]
	TIME [epoch: 6.18 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4616630766274068		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.4616630766274068 | validation: 0.5554295711736353]
	TIME [epoch: 6.18 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3947305830126581		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.3947305830126581 | validation: 0.32828438137566796]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3237516001280619		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.3237516001280619 | validation: 0.7709617977288266]
	TIME [epoch: 6.19 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5564698117514936		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.5564698117514936 | validation: 0.501943541540432]
	TIME [epoch: 6.18 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4110690763248882		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.4110690763248882 | validation: 0.3296549709561467]
	TIME [epoch: 6.18 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.396427317081927		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.396427317081927 | validation: 0.37358145738993087]
	TIME [epoch: 6.19 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3478708820850367		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.3478708820850367 | validation: 0.4019227835224885]
	TIME [epoch: 6.19 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4973454724609264		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.4973454724609264 | validation: 0.5710507292423718]
	TIME [epoch: 6.19 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3841477634768592		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.3841477634768592 | validation: 0.3979524179418082]
	TIME [epoch: 6.17 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35957724736300045		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.35957724736300045 | validation: 0.3235840042097248]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.425056259978407		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.425056259978407 | validation: 0.3433024881117259]
	TIME [epoch: 6.18 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3518471851007182		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.3518471851007182 | validation: 0.46511591792096585]
	TIME [epoch: 6.19 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4359238772521614		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.4359238772521614 | validation: 0.49842820458602405]
	TIME [epoch: 6.18 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4280445170836961		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.4280445170836961 | validation: 0.4835416578937288]
	TIME [epoch: 6.19 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4299482810048758		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.4299482810048758 | validation: 0.45839283125130315]
	TIME [epoch: 6.18 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36185632820075464		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.36185632820075464 | validation: 0.4489956129077009]
	TIME [epoch: 6.19 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3167514319045699		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.3167514319045699 | validation: 0.4928097042880386]
	TIME [epoch: 6.18 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3244491976919952		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3244491976919952 | validation: 0.3099905590609605]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3702759073873575		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.3702759073873575 | validation: 0.5966666071421906]
	TIME [epoch: 6.19 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4124935066347865		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.4124935066347865 | validation: 0.30777969803236926]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4411241644486617		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.4411241644486617 | validation: 0.4081952013586755]
	TIME [epoch: 6.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.334917489815909		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.334917489815909 | validation: 0.3285172020007349]
	TIME [epoch: 6.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3254228585905085		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.3254228585905085 | validation: 0.35135915271643553]
	TIME [epoch: 6.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33806070659515886		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.33806070659515886 | validation: 0.3346471347518305]
	TIME [epoch: 6.18 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38686793856598733		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.38686793856598733 | validation: 0.3583090107628522]
	TIME [epoch: 6.19 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3427104317344094		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.3427104317344094 | validation: 0.380617695769463]
	TIME [epoch: 6.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37038612858349707		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.37038612858349707 | validation: 0.3953138051058994]
	TIME [epoch: 6.19 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40991156190273986		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.40991156190273986 | validation: 0.2731227591620439]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29137329114935195		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.29137329114935195 | validation: 0.3686848359426884]
	TIME [epoch: 6.2 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3277200810453841		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.3277200810453841 | validation: 0.418250017271093]
	TIME [epoch: 6.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31798303881700274		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.31798303881700274 | validation: 0.5152849865263072]
	TIME [epoch: 6.21 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4043290996137605		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.4043290996137605 | validation: 0.2663025477783775]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856695679473271		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.2856695679473271 | validation: 0.3102894117816406]
	TIME [epoch: 6.19 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3669673843761889		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.3669673843761889 | validation: 0.31613545546795413]
	TIME [epoch: 6.18 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28069383039317786		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.28069383039317786 | validation: 0.48962907744412804]
	TIME [epoch: 6.19 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.345852807886912		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.345852807886912 | validation: 0.46573116832924444]
	TIME [epoch: 6.19 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3735829063512527		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.3735829063512527 | validation: 0.28920948178477035]
	TIME [epoch: 6.18 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2696596130423732		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.2696596130423732 | validation: 0.25265410691636536]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27927772255306466		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.27927772255306466 | validation: 0.4071779291914258]
	TIME [epoch: 6.19 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37660736001704		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.37660736001704 | validation: 0.3086339538066295]
	TIME [epoch: 6.19 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30011836045074003		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.30011836045074003 | validation: 0.22680477659448267]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2901472789707207		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.2901472789707207 | validation: 0.3142119735696628]
	TIME [epoch: 6.19 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30301330160887086		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.30301330160887086 | validation: 0.3486192815221776]
	TIME [epoch: 6.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27311738408047637		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.27311738408047637 | validation: 0.28224233709961666]
	TIME [epoch: 6.19 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30930965805010646		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.30930965805010646 | validation: 0.29056877215045107]
	TIME [epoch: 6.19 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.379892882784176		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.379892882784176 | validation: 0.36752993434113457]
	TIME [epoch: 6.19 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28262503515611626		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.28262503515611626 | validation: 0.3497133410922627]
	TIME [epoch: 6.19 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2884123609944358		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.2884123609944358 | validation: 0.304147751719184]
	TIME [epoch: 6.19 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32861462156481636		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.32861462156481636 | validation: 0.2537266139101062]
	TIME [epoch: 6.19 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22413503980777122		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.22413503980777122 | validation: 0.251058916738397]
	TIME [epoch: 6.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3579467759795971		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.3579467759795971 | validation: 0.34212553204575263]
	TIME [epoch: 6.18 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593266942188321		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.2593266942188321 | validation: 0.23544916955816608]
	TIME [epoch: 6.18 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649146441148984		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.2649146441148984 | validation: 0.22911215703628657]
	TIME [epoch: 6.19 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2932541807301217		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.2932541807301217 | validation: 0.34759535802959557]
	TIME [epoch: 6.19 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27165734173118566		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.27165734173118566 | validation: 0.21964704001722057]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2150439762876925		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.2150439762876925 | validation: 0.3082037105798595]
	TIME [epoch: 6.19 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30639620761816055		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.30639620761816055 | validation: 0.4191271047469455]
	TIME [epoch: 6.19 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28124461194469685		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.28124461194469685 | validation: 0.24168459127824754]
	TIME [epoch: 6.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23896583225304907		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.23896583225304907 | validation: 0.28411385166311864]
	TIME [epoch: 6.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25344678138520066		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.25344678138520066 | validation: 0.3946560267724751]
	TIME [epoch: 6.21 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30602976257935477		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.30602976257935477 | validation: 0.27524271375542325]
	TIME [epoch: 6.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2213070412102482		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.2213070412102482 | validation: 0.3514199127438553]
	TIME [epoch: 6.19 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2271566212179863		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.2271566212179863 | validation: 0.41643834356715004]
	TIME [epoch: 6.18 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28795940271702997		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.28795940271702997 | validation: 0.22362478500429278]
	TIME [epoch: 6.19 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23517637442482264		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.23517637442482264 | validation: 0.26617582535718914]
	TIME [epoch: 6.19 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24945214651587144		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.24945214651587144 | validation: 0.40891140858260056]
	TIME [epoch: 6.19 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28672257850043203		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.28672257850043203 | validation: 0.25752376954342693]
	TIME [epoch: 6.19 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26251711401885996		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.26251711401885996 | validation: 0.30445365051130086]
	TIME [epoch: 6.19 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18014780963605317		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.18014780963605317 | validation: 0.19011566465175617]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23270013615918653		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.23270013615918653 | validation: 0.18063197681282744]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22928036775267058		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.22928036775267058 | validation: 0.20372592858131866]
	TIME [epoch: 6.19 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20203942733375588		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.20203942733375588 | validation: 0.3185691924632464]
	TIME [epoch: 6.19 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2612767700891417		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.2612767700891417 | validation: 0.2036024245865785]
	TIME [epoch: 398 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20401297457102197		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.20401297457102197 | validation: 0.22951878818594995]
	TIME [epoch: 12.2 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2404522205987949		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.2404522205987949 | validation: 0.26301241751760396]
	TIME [epoch: 12.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2931989236942812		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.2931989236942812 | validation: 0.24624615606411954]
	TIME [epoch: 12.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24147797594380246		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.24147797594380246 | validation: 0.31716609354259273]
	TIME [epoch: 12.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20089360781471108		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.20089360781471108 | validation: 0.15907029344643572]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23126050190362185		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.23126050190362185 | validation: 0.33583985997028265]
	TIME [epoch: 12.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26954830661071216		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.26954830661071216 | validation: 0.20726572497320828]
	TIME [epoch: 12.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18607804781257203		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.18607804781257203 | validation: 0.19318699086637242]
	TIME [epoch: 12.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2260580610992337		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.2260580610992337 | validation: 0.28423959488638145]
	TIME [epoch: 12.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22173396828787623		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.22173396828787623 | validation: 0.23781635273486285]
	TIME [epoch: 12.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942287070630196		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.2942287070630196 | validation: 0.27584387772022134]
	TIME [epoch: 12.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629252479125787		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.2629252479125787 | validation: 0.19568038274366767]
	TIME [epoch: 12.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16783150293034516		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.16783150293034516 | validation: 0.15427551604784093]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17111094550232753		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.17111094550232753 | validation: 0.1611473104443079]
	TIME [epoch: 12.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19414909655970036		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.19414909655970036 | validation: 0.19244043946544892]
	TIME [epoch: 12.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20481518578600627		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.20481518578600627 | validation: 0.24500557102333897]
	TIME [epoch: 12.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17388213172288458		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.17388213172288458 | validation: 0.2560129939566607]
	TIME [epoch: 12.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21673884857143977		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.21673884857143977 | validation: 0.21403805379646657]
	TIME [epoch: 12.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18438055492050698		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.18438055492050698 | validation: 0.1576976887307885]
	TIME [epoch: 12.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17690519672546112		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.17690519672546112 | validation: 0.2522870245026494]
	TIME [epoch: 12.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21981115604037588		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.21981115604037588 | validation: 0.25135176927697167]
	TIME [epoch: 12.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18691242220733842		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.18691242220733842 | validation: 0.1406597541513276]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1815125756886428		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.1815125756886428 | validation: 0.25136925724911635]
	TIME [epoch: 12.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20591258714716423		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.20591258714716423 | validation: 0.1498740633144482]
	TIME [epoch: 12.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15986026736589676		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.15986026736589676 | validation: 0.21199455484013865]
	TIME [epoch: 12.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2584439941993047		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.2584439941993047 | validation: 0.15966821360157657]
	TIME [epoch: 12.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16161724416850695		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.16161724416850695 | validation: 0.1846671772586468]
	TIME [epoch: 12.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19620213491910926		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.19620213491910926 | validation: 0.2313601792626206]
	TIME [epoch: 12.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18304829721051633		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.18304829721051633 | validation: 0.14834559182018975]
	TIME [epoch: 12.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16622830577006092		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.16622830577006092 | validation: 0.1479490838881636]
	TIME [epoch: 12.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12702825269165854		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.12702825269165854 | validation: 0.1778656687310105]
	TIME [epoch: 12.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24829328975750725		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.24829328975750725 | validation: 0.4329151366475802]
	TIME [epoch: 12.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21396397041125492		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.21396397041125492 | validation: 0.18604290756721303]
	TIME [epoch: 12.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16340097323794475		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.16340097323794475 | validation: 0.13756497576137178]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15810445159145464		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.15810445159145464 | validation: 0.1514583938229231]
	TIME [epoch: 12.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16775073167879836		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.16775073167879836 | validation: 0.32063445512946026]
	TIME [epoch: 12.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16571109372699466		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.16571109372699466 | validation: 0.21360829007902815]
	TIME [epoch: 12.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12920992426308917		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.12920992426308917 | validation: 0.131162638065921]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2008263053982527		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.2008263053982527 | validation: 0.24626734506325962]
	TIME [epoch: 12.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1522474929947858		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1522474929947858 | validation: 0.23021381825090792]
	TIME [epoch: 12.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19823548728766172		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.19823548728766172 | validation: 0.12540401148165828]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11215384672547832		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11215384672547832 | validation: 0.19382378648135545]
	TIME [epoch: 12.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20039347501073526		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.20039347501073526 | validation: 0.12139076575584058]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16388578749566676		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.16388578749566676 | validation: 0.15401632554737257]
	TIME [epoch: 12.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13761701142839097		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.13761701142839097 | validation: 0.1265099573040541]
	TIME [epoch: 12.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16708623709593437		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.16708623709593437 | validation: 0.13807538293793498]
	TIME [epoch: 12.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10617134394158953		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.10617134394158953 | validation: 0.13391329918775424]
	TIME [epoch: 12.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1596327823230602		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.1596327823230602 | validation: 0.12696564463562704]
	TIME [epoch: 12.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13098343762825604		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.13098343762825604 | validation: 0.33809957148987047]
	TIME [epoch: 12.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.221788678240679		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.221788678240679 | validation: 0.20323062529648667]
	TIME [epoch: 12.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12468389357642559		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.12468389357642559 | validation: 0.1265193407230548]
	TIME [epoch: 12.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10346472866557889		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.10346472866557889 | validation: 0.11980790643939682]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1476000555530813		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.1476000555530813 | validation: 0.18248126352961985]
	TIME [epoch: 12.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15272186786403297		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.15272186786403297 | validation: 0.11686691026721191]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09926862893652391		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.09926862893652391 | validation: 0.21930889881040974]
	TIME [epoch: 12.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16178701899865833		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.16178701899865833 | validation: 0.1447672465200781]
	TIME [epoch: 12.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1542161603070818		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.1542161603070818 | validation: 0.1542668621455169]
	TIME [epoch: 12.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12951953874044442		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.12951953874044442 | validation: 0.1231089155283564]
	TIME [epoch: 12.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14660295258350328		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.14660295258350328 | validation: 0.17439931473483078]
	TIME [epoch: 12.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10237221747266757		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.10237221747266757 | validation: 0.10879244340979943]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12591194488899232		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.12591194488899232 | validation: 0.22876519228115116]
	TIME [epoch: 12.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12768154080968325		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.12768154080968325 | validation: 0.09743165054687128]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13684207438958013		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.13684207438958013 | validation: 0.2828902562326844]
	TIME [epoch: 12.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13759945062030715		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.13759945062030715 | validation: 0.22001469028459533]
	TIME [epoch: 12.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16827866129224317		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.16827866129224317 | validation: 0.09413898657571913]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08931175238901852		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.08931175238901852 | validation: 0.09625387445151029]
	TIME [epoch: 12.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14583004614145836		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.14583004614145836 | validation: 0.21191788548841245]
	TIME [epoch: 12.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18431886482685067		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.18431886482685067 | validation: 0.14704320137870436]
	TIME [epoch: 12.2 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12616575524554677		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.12616575524554677 | validation: 0.13640332375074604]
	TIME [epoch: 12.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11157989544060137		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.11157989544060137 | validation: 0.13225914402854616]
	TIME [epoch: 12.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10325874192065009		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.10325874192065009 | validation: 0.08130260695449763]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12013546710482487		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.12013546710482487 | validation: 0.12157319052559949]
	TIME [epoch: 12.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15699817193004542		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.15699817193004542 | validation: 0.12093354543667267]
	TIME [epoch: 12.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11018906846524784		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.11018906846524784 | validation: 0.1762872661026754]
	TIME [epoch: 12.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15784956432952676		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.15784956432952676 | validation: 0.12941675876224482]
	TIME [epoch: 12.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10058291849629532		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.10058291849629532 | validation: 0.09203932126077957]
	TIME [epoch: 12.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1136835543074518		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.1136835543074518 | validation: 0.17281626326147137]
	TIME [epoch: 12.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15162386644824652		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.15162386644824652 | validation: 0.1143896342066448]
	TIME [epoch: 12.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3267624695738054		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.3267624695738054 | validation: 0.27085861112424725]
	TIME [epoch: 12.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.250319169730769		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.250319169730769 | validation: 0.14640514077327269]
	TIME [epoch: 12.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10910871800057734		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.10910871800057734 | validation: 0.11529319426228489]
	TIME [epoch: 12.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1021655507580359		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.1021655507580359 | validation: 0.17702209719216067]
	TIME [epoch: 12.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1338445585585238		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1338445585585238 | validation: 0.11318137588404328]
	TIME [epoch: 12.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08897836215831688		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.08897836215831688 | validation: 0.08209072719046684]
	TIME [epoch: 12.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13514744602779635		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.13514744602779635 | validation: 0.09200724571442498]
	TIME [epoch: 12.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06957667249236946		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.06957667249236946 | validation: 0.10746457941379543]
	TIME [epoch: 12.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1299747256299943		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.1299747256299943 | validation: 0.1005323449679019]
	TIME [epoch: 12.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10944673838744368		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.10944673838744368 | validation: 0.0898961304736392]
	TIME [epoch: 12.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09611864160629323		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.09611864160629323 | validation: 0.1729207900508185]
	TIME [epoch: 12.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10285564574709656		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.10285564574709656 | validation: 0.14077479367199272]
	TIME [epoch: 12.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1279286376585927		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.1279286376585927 | validation: 0.06570181747771896]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060646094794314334		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.060646094794314334 | validation: 0.25850408199488273]
	TIME [epoch: 12.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921727142178826		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.2921727142178826 | validation: 0.15291452610097286]
	TIME [epoch: 12.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.107905436380678		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.107905436380678 | validation: 0.09845228579343868]
	TIME [epoch: 12.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.091997403396151		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.091997403396151 | validation: 0.5368425783157516]
	TIME [epoch: 12.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40195755794096283		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.40195755794096283 | validation: 0.15772684023927483]
	TIME [epoch: 12.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12769873252496025		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.12769873252496025 | validation: 0.07893760367582835]
	TIME [epoch: 12.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08672635976639896		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.08672635976639896 | validation: 0.15570851392290813]
	TIME [epoch: 12.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1108061114594033		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.1108061114594033 | validation: 0.07515413469065982]
	TIME [epoch: 12.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07103789691660377		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.07103789691660377 | validation: 0.07917560199880816]
	TIME [epoch: 12.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07949005103903373		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.07949005103903373 | validation: 0.1270080419190364]
	TIME [epoch: 12.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11477132100208898		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.11477132100208898 | validation: 0.1396877648214149]
	TIME [epoch: 12.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09047581855655573		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.09047581855655573 | validation: 0.09631806235554771]
	TIME [epoch: 12.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08045156791355501		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.08045156791355501 | validation: 0.10498868708684037]
	TIME [epoch: 12.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09791809539305252		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.09791809539305252 | validation: 0.11223788471815382]
	TIME [epoch: 12.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08578031751041179		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.08578031751041179 | validation: 0.09010802305923071]
	TIME [epoch: 12.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10437177057176188		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.10437177057176188 | validation: 0.10609306573909251]
	TIME [epoch: 12.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09338273063447991		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.09338273063447991 | validation: 0.11318484594808963]
	TIME [epoch: 12.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08815838922689404		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.08815838922689404 | validation: 0.09772260130460544]
	TIME [epoch: 12.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09694952475126808		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.09694952475126808 | validation: 0.1897071002952188]
	TIME [epoch: 12.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11298019023668295		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.11298019023668295 | validation: 0.06263340991038159]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640407940904725		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.06640407940904725 | validation: 0.10658242271281018]
	TIME [epoch: 12.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0973601587566142		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.0973601587566142 | validation: 0.0879112834358644]
	TIME [epoch: 12.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06660339321208172		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.06660339321208172 | validation: 0.10126931768567077]
	TIME [epoch: 12.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09519316428937488		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.09519316428937488 | validation: 0.07571581664645693]
	TIME [epoch: 12.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11191747160746905		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.11191747160746905 | validation: 0.12746984471997852]
	TIME [epoch: 12.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07008471442215113		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.07008471442215113 | validation: 0.062187315423980385]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08844537783188053		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.08844537783188053 | validation: 0.20001976318071138]
	TIME [epoch: 12.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10205037195610615		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.10205037195610615 | validation: 0.15650733599281547]
	TIME [epoch: 12.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07162724606156129		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.07162724606156129 | validation: 0.06371672796161193]
	TIME [epoch: 12.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05528888407501576		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.05528888407501576 | validation: 0.12100373256555133]
	TIME [epoch: 12.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08883658804636306		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08883658804636306 | validation: 0.09237041892293316]
	TIME [epoch: 12.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0847234201755738		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.0847234201755738 | validation: 0.13611029596488228]
	TIME [epoch: 12.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09342945477475251		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.09342945477475251 | validation: 0.07785262099388712]
	TIME [epoch: 12.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07656384535587804		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.07656384535587804 | validation: 0.10482212065077089]
	TIME [epoch: 12.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07468136652732352		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.07468136652732352 | validation: 0.06041719879283952]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057470893453882704		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.057470893453882704 | validation: 0.09301594366039027]
	TIME [epoch: 12.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0716407018852697		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.0716407018852697 | validation: 0.11604765382213307]
	TIME [epoch: 12.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10833729448763946		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.10833729448763946 | validation: 0.07259124699344832]
	TIME [epoch: 12.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06987509295298658		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.06987509295298658 | validation: 0.05648326986739717]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09565075215516691		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.09565075215516691 | validation: 0.11102900957957723]
	TIME [epoch: 12.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12253749079879292		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.12253749079879292 | validation: 0.07280913099329611]
	TIME [epoch: 12.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06075916088561701		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.06075916088561701 | validation: 0.07268003796980935]
	TIME [epoch: 12.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058250007886213304		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.058250007886213304 | validation: 0.07890802952879651]
	TIME [epoch: 12.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06735146161968518		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.06735146161968518 | validation: 0.07581107214348226]
	TIME [epoch: 12.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1170077467879183		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.1170077467879183 | validation: 0.062538037678503]
	TIME [epoch: 12.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06985517254465354		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.06985517254465354 | validation: 0.058816202857635294]
	TIME [epoch: 12.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06310405152809656		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.06310405152809656 | validation: 0.07821474933934547]
	TIME [epoch: 12.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05775667521488624		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.05775667521488624 | validation: 0.06042310171468855]
	TIME [epoch: 12.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09505575185800544		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.09505575185800544 | validation: 0.06635896513869258]
	TIME [epoch: 12.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061087727225076334		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.061087727225076334 | validation: 0.07425312959110594]
	TIME [epoch: 12.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08334394492369611		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.08334394492369611 | validation: 0.059415954481577826]
	TIME [epoch: 12.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06605753864974517		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.06605753864974517 | validation: 0.11696310320472905]
	TIME [epoch: 12.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060964125351919266		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.060964125351919266 | validation: 0.056989256194799345]
	TIME [epoch: 12.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07965346143592647		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.07965346143592647 | validation: 0.10964077784499318]
	TIME [epoch: 12.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06799260132635591		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.06799260132635591 | validation: 0.05237390428015182]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0528278070484625		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.0528278070484625 | validation: 0.0529828904297278]
	TIME [epoch: 12.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0779522496911296		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.0779522496911296 | validation: 0.1048462259183032]
	TIME [epoch: 12.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06973628383851652		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.06973628383851652 | validation: 0.08230967375847412]
	TIME [epoch: 12.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0500393034219479		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.0500393034219479 | validation: 0.09264016232148874]
	TIME [epoch: 12.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07863461377221011		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07863461377221011 | validation: 0.05840158688373741]
	TIME [epoch: 12.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054931258816075454		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.054931258816075454 | validation: 0.07512278126025165]
	TIME [epoch: 12.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05783264096013569		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.05783264096013569 | validation: 0.09305324764078382]
	TIME [epoch: 12.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06281510758581846		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.06281510758581846 | validation: 0.07262074202222246]
	TIME [epoch: 12.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09352336692865182		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.09352336692865182 | validation: 0.08396299652555386]
	TIME [epoch: 12.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0782848458845738		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.0782848458845738 | validation: 0.05960992991805092]
	TIME [epoch: 12.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042810735308996095		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.042810735308996095 | validation: 0.06752115875336515]
	TIME [epoch: 12.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058344626877150474		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.058344626877150474 | validation: 0.11271269000100784]
	TIME [epoch: 12.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08164422596050623		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.08164422596050623 | validation: 0.08173827067542323]
	TIME [epoch: 12.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051709959324493036		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.051709959324493036 | validation: 0.0526562197902227]
	TIME [epoch: 12.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06680067790581863		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.06680067790581863 | validation: 0.09749616391466685]
	TIME [epoch: 12.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05104299066357867		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.05104299066357867 | validation: 0.04711262977819725]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061441060892586434		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.061441060892586434 | validation: 0.0657745419590344]
	TIME [epoch: 12.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0737608321161996		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.0737608321161996 | validation: 0.06095193191079637]
	TIME [epoch: 12.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05999302517659019		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.05999302517659019 | validation: 0.057601522040179966]
	TIME [epoch: 12.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05557165265238058		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.05557165265238058 | validation: 0.057596837008637713]
	TIME [epoch: 12.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04297103120095425		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.04297103120095425 | validation: 0.072831856817628]
	TIME [epoch: 12.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08233375124461288		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.08233375124461288 | validation: 0.08096788317647535]
	TIME [epoch: 12.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04569703153628134		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.04569703153628134 | validation: 0.05494676588172933]
	TIME [epoch: 12.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04959021995628489		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.04959021995628489 | validation: 0.08769693298925513]
	TIME [epoch: 12.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0803553174033836		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.0803553174033836 | validation: 0.0447121419882868]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050927647569315726		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.050927647569315726 | validation: 0.061581097987771005]
	TIME [epoch: 12.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04100570879233649		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.04100570879233649 | validation: 0.06416727277759962]
	TIME [epoch: 12.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09217206494828473		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.09217206494828473 | validation: 0.05392627797778844]
	TIME [epoch: 12.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050609606329684816		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.050609606329684816 | validation: 0.04867158128339198]
	TIME [epoch: 12.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04247489361117911		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.04247489361117911 | validation: 0.062805072136435]
	TIME [epoch: 12.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059687270386716215		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.059687270386716215 | validation: 0.07427278740974969]
	TIME [epoch: 12.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06098855197485278		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.06098855197485278 | validation: 0.05887712457608453]
	TIME [epoch: 12.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04912812752913843		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.04912812752913843 | validation: 0.06607979376317803]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046609506533956804		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.046609506533956804 | validation: 0.07200549569875654]
	TIME [epoch: 12.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058664741971314965		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.058664741971314965 | validation: 0.06162815921368581]
	TIME [epoch: 12.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05691962312565709		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.05691962312565709 | validation: 0.06085067563868528]
	TIME [epoch: 12.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043663074164744534		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.043663074164744534 | validation: 0.05421425287847689]
	TIME [epoch: 12.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06651795552017654		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.06651795552017654 | validation: 0.05604795685781317]
	TIME [epoch: 12.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0536630040349009		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.0536630040349009 | validation: 0.07031891805437368]
	TIME [epoch: 12.2 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04632068019895154		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.04632068019895154 | validation: 0.04344624951161912]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04394964697068926		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.04394964697068926 | validation: 0.09625338041754467]
	TIME [epoch: 12.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06788251043873285		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.06788251043873285 | validation: 0.038707739651109505]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042895742860658934		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.042895742860658934 | validation: 0.052244545232433305]
	TIME [epoch: 12.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05222190820882114		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.05222190820882114 | validation: 0.06523916321150147]
	TIME [epoch: 12.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06269656898468512		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.06269656898468512 | validation: 0.06364571667037171]
	TIME [epoch: 12.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042568250261307		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.042568250261307 | validation: 0.03291310890760837]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03539436453679697		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.03539436453679697 | validation: 0.08022325406157552]
	TIME [epoch: 12.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05511943853364034		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.05511943853364034 | validation: 0.09480750151231121]
	TIME [epoch: 12.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0711001549224543		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.0711001549224543 | validation: 0.04295487501826452]
	TIME [epoch: 12.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04283155395267215		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04283155395267215 | validation: 0.04120992445004108]
	TIME [epoch: 12.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044846287820895676		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.044846287820895676 | validation: 0.05434495241306686]
	TIME [epoch: 12.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060638141758595054		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.060638141758595054 | validation: 0.08049435037805033]
	TIME [epoch: 12.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044361721460341286		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.044361721460341286 | validation: 0.035156809675430105]
	TIME [epoch: 12.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02920508326874481		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.02920508326874481 | validation: 0.05531989674572366]
	TIME [epoch: 12.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0654777939431324		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.0654777939431324 | validation: 0.057279317548398566]
	TIME [epoch: 12.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044156412300338614		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.044156412300338614 | validation: 0.07113490854529225]
	TIME [epoch: 12.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07715130193343608		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.07715130193343608 | validation: 0.06108008321071594]
	TIME [epoch: 12.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038331186682103155		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.038331186682103155 | validation: 0.04244861550505546]
	TIME [epoch: 12.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059349109408914025		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.059349109408914025 | validation: 0.06547716928407209]
	TIME [epoch: 12.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04746861899897541		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.04746861899897541 | validation: 0.03556577801242547]
	TIME [epoch: 12.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03642110663864723		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.03642110663864723 | validation: 0.0700940955777978]
	TIME [epoch: 12.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052988083024298235		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.052988083024298235 | validation: 0.037801247719685724]
	TIME [epoch: 12.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038511777420514134		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.038511777420514134 | validation: 0.06052786359662905]
	TIME [epoch: 12.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044972031616875754		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.044972031616875754 | validation: 0.04024416674190726]
	TIME [epoch: 12.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03279527051015412		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.03279527051015412 | validation: 0.07199252095013747]
	TIME [epoch: 12.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04580870306185		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04580870306185 | validation: 0.04130374092120711]
	TIME [epoch: 12.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043699444648982115		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.043699444648982115 | validation: 0.0816522858332974]
	TIME [epoch: 12.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06478496894182655		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.06478496894182655 | validation: 0.04525035643232514]
	TIME [epoch: 12.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03465747234378968		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.03465747234378968 | validation: 0.034042489528801095]
	TIME [epoch: 12.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03680977077026408		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.03680977077026408 | validation: 0.04033070732946982]
	TIME [epoch: 12.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037153086128384416		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.037153086128384416 | validation: 0.05629880318763074]
	TIME [epoch: 12.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06558753396848505		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.06558753396848505 | validation: 0.04449724356837971]
	TIME [epoch: 12.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04701683206132785		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04701683206132785 | validation: 0.03582744916727341]
	TIME [epoch: 12.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03545424392825434		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.03545424392825434 | validation: 0.050150980634722814]
	TIME [epoch: 12.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041409607729240204		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.041409607729240204 | validation: 0.0608193855448515]
	TIME [epoch: 12.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03943067714912625		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.03943067714912625 | validation: 0.03336830614369453]
	TIME [epoch: 12.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035969694814314064		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.035969694814314064 | validation: 0.03559926873634932]
	TIME [epoch: 12.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05424752399842932		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.05424752399842932 | validation: 0.03472837020951876]
	TIME [epoch: 12.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027145351506470856		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.027145351506470856 | validation: 0.03540574138997245]
	TIME [epoch: 12.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02918518966175703		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.02918518966175703 | validation: 0.04546750349371992]
	TIME [epoch: 12.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05603558962106231		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.05603558962106231 | validation: 0.06765426593240093]
	TIME [epoch: 12.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035826165307656796		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.035826165307656796 | validation: 0.0338660151678073]
	TIME [epoch: 12.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042982219459088554		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.042982219459088554 | validation: 0.043147845412830586]
	TIME [epoch: 12.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03563887270459747		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.03563887270459747 | validation: 0.05337293041032634]
	TIME [epoch: 12.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036988103514527804		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.036988103514527804 | validation: 0.0459719601148216]
	TIME [epoch: 12.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051322155347517745		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.051322155347517745 | validation: 0.04099140438431559]
	TIME [epoch: 12.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03854275167856018		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.03854275167856018 | validation: 0.03152811164967943]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03332379971960161		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.03332379971960161 | validation: 0.07652745310255699]
	TIME [epoch: 12.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03329662594431168		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.03329662594431168 | validation: 0.04164327422533978]
	TIME [epoch: 12.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04731431470772986		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.04731431470772986 | validation: 0.05028464813128336]
	TIME [epoch: 12.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03300765744186644		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.03300765744186644 | validation: 0.03697754800739397]
	TIME [epoch: 12.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038170848554574234		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.038170848554574234 | validation: 0.13285621310998721]
	TIME [epoch: 12.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05549181139128734		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.05549181139128734 | validation: 0.03480556335203684]
	TIME [epoch: 12.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02711099088912027		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.02711099088912027 | validation: 0.03378412705019933]
	TIME [epoch: 12.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03398445200037798		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.03398445200037798 | validation: 0.0583544618727818]
	TIME [epoch: 12.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04694198536995592		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.04694198536995592 | validation: 0.036960735335873504]
	TIME [epoch: 12.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03058799818752509		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.03058799818752509 | validation: 0.03727943771908597]
	TIME [epoch: 12.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04357306132643152		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.04357306132643152 | validation: 0.0911343065956627]
	TIME [epoch: 12.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044000619221251654		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.044000619221251654 | validation: 0.03455567414887116]
	TIME [epoch: 12.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02522389371409909		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.02522389371409909 | validation: 0.032125315761759816]
	TIME [epoch: 12.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03567681029881094		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.03567681029881094 | validation: 0.04383840110246092]
	TIME [epoch: 12.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04004214651776727		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.04004214651776727 | validation: 0.03222305989279847]
	TIME [epoch: 12.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03229617807276088		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.03229617807276088 | validation: 0.03886159715254238]
	TIME [epoch: 12.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027362910606607865		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.027362910606607865 | validation: 0.035085092291267286]
	TIME [epoch: 12.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04569571933956341		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.04569571933956341 | validation: 0.03736450556842382]
	TIME [epoch: 12.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033063858210181735		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.033063858210181735 | validation: 0.04816030272554042]
	TIME [epoch: 12.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028649446348744716		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.028649446348744716 | validation: 0.03498420298632316]
	TIME [epoch: 12.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04499559529363876		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.04499559529363876 | validation: 0.053079448891265604]
	TIME [epoch: 12.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03220439483714363		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.03220439483714363 | validation: 0.03765134839211048]
	TIME [epoch: 12.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030478294799258496		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.030478294799258496 | validation: 0.03252147001101234]
	TIME [epoch: 12.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030783453731251895		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.030783453731251895 | validation: 0.03750107978317832]
	TIME [epoch: 12.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03605188286587906		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03605188286587906 | validation: 0.03483060849601493]
	TIME [epoch: 12.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03372772559059501		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.03372772559059501 | validation: 0.04958751437904751]
	TIME [epoch: 12.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031053285916122393		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.031053285916122393 | validation: 0.026078156399368488]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02302576048410939		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.02302576048410939 | validation: 0.028380916400990747]
	TIME [epoch: 12.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03487323501648586		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.03487323501648586 | validation: 0.07497848027411971]
	TIME [epoch: 12.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04875037930979513		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.04875037930979513 | validation: 0.03449684635876332]
	TIME [epoch: 12.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02900547334833569		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.02900547334833569 | validation: 0.03724385154172426]
	TIME [epoch: 12.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03310013891168591		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.03310013891168591 | validation: 0.02910484794302158]
	TIME [epoch: 12.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034189716299015405		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.034189716299015405 | validation: 0.03466188305991768]
	TIME [epoch: 12.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031146395995641937		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.031146395995641937 | validation: 0.032390011349951744]
	TIME [epoch: 12.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029652171059774763		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.029652171059774763 | validation: 0.04026701549232247]
	TIME [epoch: 12.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029186197481719564		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.029186197481719564 | validation: 0.026766181019709318]
	TIME [epoch: 12.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035964765986233785		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.035964765986233785 | validation: 0.04195830258156693]
	TIME [epoch: 12.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03090177788132862		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.03090177788132862 | validation: 0.033274365668570484]
	TIME [epoch: 12.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03146088968530698		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.03146088968530698 | validation: 0.03771249056370883]
	TIME [epoch: 12.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027055168989884974		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.027055168989884974 | validation: 0.024638959525692825]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02438115598047746		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.02438115598047746 | validation: 0.02653986422645373]
	TIME [epoch: 12.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02706230408743429		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.02706230408743429 | validation: 0.025974927592651062]
	TIME [epoch: 12.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0430699736208367		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.0430699736208367 | validation: 0.049204741061664364]
	TIME [epoch: 12.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029018602066168967		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.029018602066168967 | validation: 0.03143607687430527]
	TIME [epoch: 12.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026179364687614396		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.026179364687614396 | validation: 0.027167760976976]
	TIME [epoch: 12.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023695061990529998		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.023695061990529998 | validation: 0.023612833059803183]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03711247832441665		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03711247832441665 | validation: 0.05090385066813377]
	TIME [epoch: 12.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032073602210028924		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.032073602210028924 | validation: 0.029642004163378817]
	TIME [epoch: 12.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0261231448791803		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.0261231448791803 | validation: 0.04632871095905477]
	TIME [epoch: 12.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0349096011617751		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0349096011617751 | validation: 0.0357866935714095]
	TIME [epoch: 12.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033305118224959795		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.033305118224959795 | validation: 0.03299520631098729]
	TIME [epoch: 12.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021132311124728742		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.021132311124728742 | validation: 0.025088792391129315]
	TIME [epoch: 12.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01825060334972055		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.01825060334972055 | validation: 0.05133556654599185]
	TIME [epoch: 12.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03465799521765108		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.03465799521765108 | validation: 0.05946012318814692]
	TIME [epoch: 12.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03695361074992096		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03695361074992096 | validation: 0.025251395716125197]
	TIME [epoch: 12.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03165223885501448		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.03165223885501448 | validation: 0.031514178518356806]
	TIME [epoch: 12.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023259498445759585		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.023259498445759585 | validation: 0.024253856548001648]
	TIME [epoch: 12.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02841380991372241		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.02841380991372241 | validation: 0.02783630180921539]
	TIME [epoch: 12.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025482116987815696		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.025482116987815696 | validation: 0.04498841416923566]
	TIME [epoch: 12.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0320976365888576		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.0320976365888576 | validation: 0.025197154439842044]
	TIME [epoch: 12.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024465466786666556		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.024465466786666556 | validation: 0.02699909663046221]
	TIME [epoch: 12.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023339122426204575		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.023339122426204575 | validation: 0.047905216695448054]
	TIME [epoch: 12.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03258433555311822		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.03258433555311822 | validation: 0.033043384355032734]
	TIME [epoch: 12.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016583909617854765		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.016583909617854765 | validation: 0.03095184437852477]
	TIME [epoch: 12.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03368995755909979		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.03368995755909979 | validation: 0.027470611306194992]
	TIME [epoch: 12.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030468842281526236		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.030468842281526236 | validation: 0.034122318560051844]
	TIME [epoch: 12.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021831776690229447		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.021831776690229447 | validation: 0.027557781554385166]
	TIME [epoch: 418 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026268173746478697		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.026268173746478697 | validation: 0.05574127632925476]
	TIME [epoch: 26.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03599038830914172		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03599038830914172 | validation: 0.027962944749037856]
	TIME [epoch: 26.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024905514152982318		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.024905514152982318 | validation: 0.023624840238137913]
	TIME [epoch: 26.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022321670062268383		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.022321670062268383 | validation: 0.03179083615716419]
	TIME [epoch: 26.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03428321826232738		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.03428321826232738 | validation: 0.022421853082037313]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017286115991458413		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.017286115991458413 | validation: 0.03117800872808005]
	TIME [epoch: 26.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024673969972919728		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.024673969972919728 | validation: 0.030962974097216447]
	TIME [epoch: 26.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02779969997688173		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.02779969997688173 | validation: 0.033998192383159556]
	TIME [epoch: 26.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021952201595394606		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.021952201595394606 | validation: 0.023671596994656676]
	TIME [epoch: 26.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03301502785101927		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.03301502785101927 | validation: 0.02858303783951185]
	TIME [epoch: 26.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019512812423515692		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.019512812423515692 | validation: 0.037975718431665945]
	TIME [epoch: 26.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027302254149674774		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.027302254149674774 | validation: 0.026465870450674785]
	TIME [epoch: 26.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025292954167613457		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.025292954167613457 | validation: 0.051050969432478124]
	TIME [epoch: 26.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026377319104940344		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.026377319104940344 | validation: 0.025420167570013544]
	TIME [epoch: 26.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022947517868807093		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.022947517868807093 | validation: 0.031612437604370644]
	TIME [epoch: 26.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031068938373445022		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.031068938373445022 | validation: 0.03343618622353858]
	TIME [epoch: 26.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017928464227574922		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.017928464227574922 | validation: 0.01847527059601402]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02761768443993079		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.02761768443993079 | validation: 0.03534363960765877]
	TIME [epoch: 26.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023310122135326655		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.023310122135326655 | validation: 0.03466513883489342]
	TIME [epoch: 26.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022598141471461525		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.022598141471461525 | validation: 0.023407010542583595]
	TIME [epoch: 26.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02032373285698451		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.02032373285698451 | validation: 0.04047571229247323]
	TIME [epoch: 26.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030838705214596515		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.030838705214596515 | validation: 0.017512207296389602]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02150454655105988		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.02150454655105988 | validation: 0.04810118038714063]
	TIME [epoch: 26.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03267677616251154		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.03267677616251154 | validation: 0.02346679963117828]
	TIME [epoch: 26.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024521686853207336		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.024521686853207336 | validation: 0.027543592002846264]
	TIME [epoch: 26.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02014087336591268		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.02014087336591268 | validation: 0.028744332298829618]
	TIME [epoch: 26.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026083499440067645		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.026083499440067645 | validation: 0.04101357305605538]
	TIME [epoch: 26.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02390667576294636		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.02390667576294636 | validation: 0.01719523044244688]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016237149570404237		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.016237149570404237 | validation: 0.027987344452044517]
	TIME [epoch: 26.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025111194781203033		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.025111194781203033 | validation: 0.0313444846256115]
	TIME [epoch: 26.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020510534857495107		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.020510534857495107 | validation: 0.03545891849123125]
	TIME [epoch: 26.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023529922004443732		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.023529922004443732 | validation: 0.03681733359907573]
	TIME [epoch: 26.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030116649622947432		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.030116649622947432 | validation: 0.020031726697248113]
	TIME [epoch: 26.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020661480495471004		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.020661480495471004 | validation: 0.030439059164756796]
	TIME [epoch: 26.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01911953295053484		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.01911953295053484 | validation: 0.020816876321390156]
	TIME [epoch: 26.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02291350746482743		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02291350746482743 | validation: 0.035866647227188964]
	TIME [epoch: 26.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02423870174716007		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.02423870174716007 | validation: 0.027474487830521047]
	TIME [epoch: 26.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02042970120340628		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.02042970120340628 | validation: 0.02440936900273121]
	TIME [epoch: 26.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01949524552564265		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.01949524552564265 | validation: 0.03173955400792447]
	TIME [epoch: 26.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029271398977125877		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.029271398977125877 | validation: 0.02139565598170807]
	TIME [epoch: 26.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02106262472630397		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.02106262472630397 | validation: 0.021967591886637605]
	TIME [epoch: 26.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03758118209318756		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.03758118209318756 | validation: 0.05476201660763574]
	TIME [epoch: 26.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03573986061649273		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.03573986061649273 | validation: 0.03839290158225916]
	TIME [epoch: 26.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022597331465320523		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.022597331465320523 | validation: 0.017158597681294196]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016466254902900955		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.016466254902900955 | validation: 0.01959708072462005]
	TIME [epoch: 26.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026065844243135114		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.026065844243135114 | validation: 0.030743114639301954]
	TIME [epoch: 26.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02245353102617535		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.02245353102617535 | validation: 0.020105177537696358]
	TIME [epoch: 26.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013178039907007878		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.013178039907007878 | validation: 0.021641968073996085]
	TIME [epoch: 26.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02906818644658335		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.02906818644658335 | validation: 0.02454910913403554]
	TIME [epoch: 26.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019067224171958184		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.019067224171958184 | validation: 0.029912606372179393]
	TIME [epoch: 26.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018236871959167466		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.018236871959167466 | validation: 0.01701996050095648]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018357635509419677		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.018357635509419677 | validation: 0.03661334468129738]
	TIME [epoch: 26.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029021943979241505		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.029021943979241505 | validation: 0.01510388547531524]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018183498449840213		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.018183498449840213 | validation: 0.017029152800923566]
	TIME [epoch: 26.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01638158257802189		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.01638158257802189 | validation: 0.03566592720764641]
	TIME [epoch: 26.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026985708845734938		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.026985708845734938 | validation: 0.06401618466954846]
	TIME [epoch: 26.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028574278058697666		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.028574278058697666 | validation: 0.020468408593163023]
	TIME [epoch: 26.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015345271349353809		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.015345271349353809 | validation: 0.020664426360420044]
	TIME [epoch: 26.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021366386461886207		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.021366386461886207 | validation: 0.017881057421464154]
	TIME [epoch: 26.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014733114450375607		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.014733114450375607 | validation: 0.02315432024176265]
	TIME [epoch: 26.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02560630801705427		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.02560630801705427 | validation: 0.03959065361205265]
	TIME [epoch: 26.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028288456328342634		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.028288456328342634 | validation: 0.022026854018536657]
	TIME [epoch: 26.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016981755691222213		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.016981755691222213 | validation: 0.017364622642402183]
	TIME [epoch: 26.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01676368784482105		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.01676368784482105 | validation: 0.026487145700570543]
	TIME [epoch: 26.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02771896408404714		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.02771896408404714 | validation: 0.02039077478612896]
	TIME [epoch: 26.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018132532801707485		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.018132532801707485 | validation: 0.01798656049638616]
	TIME [epoch: 26.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018146614272868455		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.018146614272868455 | validation: 0.016951525620874017]
	TIME [epoch: 26.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019460983851230518		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.019460983851230518 | validation: 0.05128780488135907]
	TIME [epoch: 26.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023480722556669102		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.023480722556669102 | validation: 0.020749455636363788]
	TIME [epoch: 26.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016537622130115795		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.016537622130115795 | validation: 0.019283871307288633]
	TIME [epoch: 26.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019715651088415363		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.019715651088415363 | validation: 0.03668024076289261]
	TIME [epoch: 26.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021060062158514817		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.021060062158514817 | validation: 0.02652949810226525]
	TIME [epoch: 26.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017382848131607412		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.017382848131607412 | validation: 0.015100978977766643]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016904624150912484		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.016904624150912484 | validation: 0.02099813305949482]
	TIME [epoch: 26.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02394319436566411		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.02394319436566411 | validation: 0.037718851486136115]
	TIME [epoch: 26.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019516412320985856		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.019516412320985856 | validation: 0.017922653814304537]
	TIME [epoch: 26.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014338347402232547		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.014338347402232547 | validation: 0.017189786729004723]
	TIME [epoch: 26.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01684649728266389		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.01684649728266389 | validation: 0.040470468978847915]
	TIME [epoch: 26.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03154066486656636		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.03154066486656636 | validation: 0.027287283033925892]
	TIME [epoch: 26.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018307839190145336		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.018307839190145336 | validation: 0.020422229464803026]
	TIME [epoch: 26.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016830086574169085		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.016830086574169085 | validation: 0.017987219470022277]
	TIME [epoch: 26.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014293454882388847		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.014293454882388847 | validation: 0.02460191516515975]
	TIME [epoch: 26.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02497360622826339		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.02497360622826339 | validation: 0.02495702066277638]
	TIME [epoch: 26.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017016976832361964		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.017016976832361964 | validation: 0.021677097791408854]
	TIME [epoch: 26.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022209275212247283		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.022209275212247283 | validation: 0.01744741084717883]
	TIME [epoch: 26.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015174080993236241		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.015174080993236241 | validation: 0.020526161058395567]
	TIME [epoch: 26.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021078014878371614		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.021078014878371614 | validation: 0.018083286823800827]
	TIME [epoch: 26.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015778171294229305		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.015778171294229305 | validation: 0.017294467414956625]
	TIME [epoch: 26.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015779343679695135		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.015779343679695135 | validation: 0.017116025496367475]
	TIME [epoch: 26.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019400205719331005		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.019400205719331005 | validation: 0.02643408600694413]
	TIME [epoch: 26.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02131101673911518		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.02131101673911518 | validation: 0.03221727842449826]
	TIME [epoch: 26.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021948480890873773		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.021948480890873773 | validation: 0.01517906840495287]
	TIME [epoch: 26.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019161173505258013		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.019161173505258013 | validation: 0.027096100690845434]
	TIME [epoch: 26.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01864447364555384		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.01864447364555384 | validation: 0.014185612935392293]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012814633316143768		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.012814633316143768 | validation: 0.017882749530133402]
	TIME [epoch: 26.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020244233060529148		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.020244233060529148 | validation: 0.030555027433047326]
	TIME [epoch: 26.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017060412719827174		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.017060412719827174 | validation: 0.015242525848190986]
	TIME [epoch: 26.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01600116922319337		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.01600116922319337 | validation: 0.018822799323284893]
	TIME [epoch: 26.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016073268942243243		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.016073268942243243 | validation: 0.02737954802325187]
	TIME [epoch: 26.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020613543075739302		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.020613543075739302 | validation: 0.017101908994000433]
	TIME [epoch: 26.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015850525534468918		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.015850525534468918 | validation: 0.020292133719809147]
	TIME [epoch: 26.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018817308900753667		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.018817308900753667 | validation: 0.016758682428074004]
	TIME [epoch: 26.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014294762042577399		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.014294762042577399 | validation: 0.02599593676411842]
	TIME [epoch: 26.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021006089317378455		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.021006089317378455 | validation: 0.018496239074284257]
	TIME [epoch: 26.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01473589724087239		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.01473589724087239 | validation: 0.017489999625700643]
	TIME [epoch: 26.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015504735738531263		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.015504735738531263 | validation: 0.016510579503586347]
	TIME [epoch: 26.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0245834094835615		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.0245834094835615 | validation: 0.015883724034747658]
	TIME [epoch: 26.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013745537304193116		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.013745537304193116 | validation: 0.01333686444866294]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014784490730772757		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.014784490730772757 | validation: 0.014772163218515322]
	TIME [epoch: 26.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018190125858759468		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.018190125858759468 | validation: 0.05321696681614835]
	TIME [epoch: 26.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023457778717005006		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.023457778717005006 | validation: 0.01578393021751495]
	TIME [epoch: 26.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013563999416149016		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.013563999416149016 | validation: 0.016307732180999235]
	TIME [epoch: 26.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016906740242568656		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.016906740242568656 | validation: 0.0200332749107544]
	TIME [epoch: 26.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015517344321980688		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.015517344321980688 | validation: 0.02919837958839199]
	TIME [epoch: 26.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017709422075024867		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.017709422075024867 | validation: 0.016706079822016096]
	TIME [epoch: 26.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019275630327658534		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.019275630327658534 | validation: 0.01928882298452105]
	TIME [epoch: 26.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014790201025309958		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.014790201025309958 | validation: 0.017201782614484542]
	TIME [epoch: 26.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01681222875253685		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.01681222875253685 | validation: 0.021031856913255487]
	TIME [epoch: 26.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017321778990576464		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.017321778990576464 | validation: 0.02057241358796443]
	TIME [epoch: 26.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01435588860501165		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.01435588860501165 | validation: 0.018613712930916716]
	TIME [epoch: 26.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017836365464681207		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.017836365464681207 | validation: 0.02630744318081532]
	TIME [epoch: 26.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015244184570328853		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.015244184570328853 | validation: 0.016122214186865914]
	TIME [epoch: 26.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013851475930460343		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.013851475930460343 | validation: 0.021322753474129454]
	TIME [epoch: 26.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015164998082271407		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.015164998082271407 | validation: 0.021532316656894653]
	TIME [epoch: 26.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01610333998051038		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.01610333998051038 | validation: 0.029833551707092246]
	TIME [epoch: 26.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015071717732293387		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.015071717732293387 | validation: 0.015895760281502247]
	TIME [epoch: 26.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01831744159718241		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.01831744159718241 | validation: 0.01920014423224792]
	TIME [epoch: 26.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016529426054084365		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.016529426054084365 | validation: 0.014623951364337757]
	TIME [epoch: 26.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012556624079588522		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.012556624079588522 | validation: 0.0226880868232755]
	TIME [epoch: 26 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020228682814885268		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.020228682814885268 | validation: 0.02815588761534231]
	TIME [epoch: 26.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017804370193180874		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.017804370193180874 | validation: 0.015192491075246867]
	TIME [epoch: 26.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01426952189455108		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.01426952189455108 | validation: 0.018004261553770703]
	TIME [epoch: 26.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018360729955265408		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.018360729955265408 | validation: 0.01615490115885588]
	TIME [epoch: 26.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011752961605561947		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.011752961605561947 | validation: 0.013281837950118522]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013779656505354176		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.013779656505354176 | validation: 0.019936647438472926]
	TIME [epoch: 26.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016878054947163778		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.016878054947163778 | validation: 0.01750615767043049]
	TIME [epoch: 26.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0126702043287831		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.0126702043287831 | validation: 0.01451258773523268]
	TIME [epoch: 26.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01627264292043184		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.01627264292043184 | validation: 0.020223749378481835]
	TIME [epoch: 26 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017193814263341545		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.017193814263341545 | validation: 0.02271496931039467]
	TIME [epoch: 26.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014030462593804681		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.014030462593804681 | validation: 0.014997965640629815]
	TIME [epoch: 26.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011097112945694597		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.011097112945694597 | validation: 0.020967311557891295]
	TIME [epoch: 26.1 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023139570148729395		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.023139570148729395 | validation: 0.015070546362260438]
	TIME [epoch: 26.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011902084447495919		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.011902084447495919 | validation: 0.01828304166732165]
	TIME [epoch: 26 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013395885788685485		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.013395885788685485 | validation: 0.022832792685246905]
	TIME [epoch: 26 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020165293967380697		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.020165293967380697 | validation: 0.01850450180446668]
	TIME [epoch: 26 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013236072874958333		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.013236072874958333 | validation: 0.01747205380600845]
	TIME [epoch: 26.1 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014010531505244044		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.014010531505244044 | validation: 0.01893983610639926]
	TIME [epoch: 26.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012860190390973511		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.012860190390973511 | validation: 0.012707587554586314]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012425407220278802		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.012425407220278802 | validation: 0.022063162456844896]
	TIME [epoch: 26.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01881653965401489		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.01881653965401489 | validation: 0.023899043067667478]
	TIME [epoch: 26.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013422266445617542		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.013422266445617542 | validation: 0.01493151041244796]
	TIME [epoch: 26.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01465238332999105		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.01465238332999105 | validation: 0.015960310550823767]
	TIME [epoch: 26.1 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015338994838764646		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.015338994838764646 | validation: 0.021710874203137512]
	TIME [epoch: 26.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011448375045476047		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.011448375045476047 | validation: 0.015835127887836233]
	TIME [epoch: 26 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013969109458763087		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.013969109458763087 | validation: 0.021229802554425893]
	TIME [epoch: 26 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01774682182531555		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.01774682182531555 | validation: 0.011495748467748227]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011265001488352796		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.011265001488352796 | validation: 0.016162142879957854]
	TIME [epoch: 26.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0157410823117812		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.0157410823117812 | validation: 0.02613347727117423]
	TIME [epoch: 26.1 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01591728455770556		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.01591728455770556 | validation: 0.02055505751705229]
	TIME [epoch: 26.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013063859250476758		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.013063859250476758 | validation: 0.01661334908277969]
	TIME [epoch: 26.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016761382547361484		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.016761382547361484 | validation: 0.014146078120632368]
	TIME [epoch: 26.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01224312783593327		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.01224312783593327 | validation: 0.01578874832257834]
	TIME [epoch: 26.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011110305694677077		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.011110305694677077 | validation: 0.013907812473308554]
	TIME [epoch: 26.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017211873329826682		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.017211873329826682 | validation: 0.02943213026824936]
	TIME [epoch: 26.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707701610053312		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.01707701610053312 | validation: 0.013022785497204386]
	TIME [epoch: 26.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01224700634256692		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.01224700634256692 | validation: 0.0129310197906141]
	TIME [epoch: 26.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011529910897118426		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.011529910897118426 | validation: 0.014396705341263666]
	TIME [epoch: 26.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014843561680210022		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.014843561680210022 | validation: 0.023813831848693335]
	TIME [epoch: 26.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01532125186419313		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.01532125186419313 | validation: 0.015643822138942144]
	TIME [epoch: 26.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011569869771045228		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.011569869771045228 | validation: 0.015610345627004694]
	TIME [epoch: 26.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012592882057325179		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.012592882057325179 | validation: 0.017750837701290583]
	TIME [epoch: 26.1 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016469470261838856		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.016469470261838856 | validation: 0.015359779478000874]
	TIME [epoch: 26.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012997137004674121		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.012997137004674121 | validation: 0.01521263840929751]
	TIME [epoch: 26.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0157232262739335		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.0157232262739335 | validation: 0.015526708317534875]
	TIME [epoch: 26.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01188502452112282		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.01188502452112282 | validation: 0.01572621594875468]
	TIME [epoch: 26.1 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01444481338301179		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.01444481338301179 | validation: 0.01661413950842112]
	TIME [epoch: 26.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01272199734974365		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.01272199734974365 | validation: 0.013580155412019224]
	TIME [epoch: 26.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013732060278349204		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.013732060278349204 | validation: 0.018119846293475222]
	TIME [epoch: 26.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012679085222547301		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.012679085222547301 | validation: 0.012682610948511943]
	TIME [epoch: 26.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013951023525196837		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.013951023525196837 | validation: 0.017760835006610878]
	TIME [epoch: 26.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012217945983111034		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.012217945983111034 | validation: 0.019435871567778935]
	TIME [epoch: 26.1 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017074542011670076		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.017074542011670076 | validation: 0.0125488137564797]
	TIME [epoch: 26.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010656119408197785		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.010656119408197785 | validation: 0.013872963489830508]
	TIME [epoch: 26.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01166019806546505		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.01166019806546505 | validation: 0.015824132485649494]
	TIME [epoch: 26.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014694460301024061		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.014694460301024061 | validation: 0.014755309441065952]
	TIME [epoch: 26.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015758401177620465		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.015758401177620465 | validation: 0.0123344519611699]
	TIME [epoch: 26.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01171546980273985		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.01171546980273985 | validation: 0.021544430270525383]
	TIME [epoch: 26.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012939730537062481		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.012939730537062481 | validation: 0.016217089665313374]
	TIME [epoch: 26.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012296438586310427		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.012296438586310427 | validation: 0.012340748035006389]
	TIME [epoch: 26.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01454353391715214		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.01454353391715214 | validation: 0.02494422325561097]
	TIME [epoch: 26.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013978129220740567		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.013978129220740567 | validation: 0.0160569422808974]
	TIME [epoch: 26.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015047463755832784		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.015047463755832784 | validation: 0.016009697832303896]
	TIME [epoch: 26.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010081593631838364		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.010081593631838364 | validation: 0.011316796440674324]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010117929561962937		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.010117929561962937 | validation: 0.024625868680292238]
	TIME [epoch: 26.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0156450434660357		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.0156450434660357 | validation: 0.014885136915193047]
	TIME [epoch: 26.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016366932950501206		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.016366932950501206 | validation: 0.013839752821822779]
	TIME [epoch: 26.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010175137278785001		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.010175137278785001 | validation: 0.012201183307629294]
	TIME [epoch: 26.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011760397840301126		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.011760397840301126 | validation: 0.012788788584641515]
	TIME [epoch: 26.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014755144235276282		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.014755144235276282 | validation: 0.015245243918971979]
	TIME [epoch: 26.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012898036158517116		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.012898036158517116 | validation: 0.0141015816984085]
	TIME [epoch: 26.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013427642208416173		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.013427642208416173 | validation: 0.02097947841655483]
	TIME [epoch: 26.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011594754737623666		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.011594754737623666 | validation: 0.01294631066095682]
	TIME [epoch: 26.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011079239628186036		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.011079239628186036 | validation: 0.020472359343765817]
	TIME [epoch: 26.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012587941951201144		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.012587941951201144 | validation: 0.012541290295922332]
	TIME [epoch: 26.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010888435368341342		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.010888435368341342 | validation: 0.012919855166837153]
	TIME [epoch: 26.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013474542434842375		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.013474542434842375 | validation: 0.014216880976097471]
	TIME [epoch: 26.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013410666607291493		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.013410666607291493 | validation: 0.013756763312690268]
	TIME [epoch: 26.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011270311609646023		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.011270311609646023 | validation: 0.013655072551844687]
	TIME [epoch: 26.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013847022496210466		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.013847022496210466 | validation: 0.01662097915291532]
	TIME [epoch: 26.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012063078454383026		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.012063078454383026 | validation: 0.01133353566925787]
	TIME [epoch: 26.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010702685883466942		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.010702685883466942 | validation: 0.014405195014663084]
	TIME [epoch: 26.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01153509677238313		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.01153509677238313 | validation: 0.013559702971373018]
	TIME [epoch: 26.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01158361022423741		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.01158361022423741 | validation: 0.01326067515361248]
	TIME [epoch: 26.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01116879990683476		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.01116879990683476 | validation: 0.013458876301092132]
	TIME [epoch: 26.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010199191378082808		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.010199191378082808 | validation: 0.013158127996103969]
	TIME [epoch: 26.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012244866119868287		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.012244866119868287 | validation: 0.016472394724465128]
	TIME [epoch: 26.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01257718424687111		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.01257718424687111 | validation: 0.01306066749784807]
	TIME [epoch: 26.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015269807115084084		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.015269807115084084 | validation: 0.011628474132430533]
	TIME [epoch: 26.1 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010865730091302511		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.010865730091302511 | validation: 0.013443025258939416]
	TIME [epoch: 26.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012640222537587217		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.012640222537587217 | validation: 0.01234848758307065]
	TIME [epoch: 26.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010453210825307408		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.010453210825307408 | validation: 0.01649761316139462]
	TIME [epoch: 26.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01360648265232063		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.01360648265232063 | validation: 0.015437055771727198]
	TIME [epoch: 26.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011739999344785933		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.011739999344785933 | validation: 0.012495488669648773]
	TIME [epoch: 26.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01144485775001554		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.01144485775001554 | validation: 0.015665095192830436]
	TIME [epoch: 26.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012541845165939484		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.012541845165939484 | validation: 0.012880410351641818]
	TIME [epoch: 26.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009652603857551791		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.009652603857551791 | validation: 0.012976351469683008]
	TIME [epoch: 26.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012103997184130298		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.012103997184130298 | validation: 0.025381172396169112]
	TIME [epoch: 26.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013218785470787475		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.013218785470787475 | validation: 0.011097211475484852]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_729.pth
	Model improved!!!
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010860157799024451		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.010860157799024451 | validation: 0.012012025976607651]
	TIME [epoch: 26.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010902928884213922		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.010902928884213922 | validation: 0.012301918698449574]
	TIME [epoch: 26.1 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011383332254303099		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.011383332254303099 | validation: 0.017581602639415557]
	TIME [epoch: 26.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012230530904973013		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.012230530904973013 | validation: 0.014871797975403982]
	TIME [epoch: 26.1 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01026838127573474		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.01026838127573474 | validation: 0.013970099284222541]
	TIME [epoch: 26.1 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012102640223163538		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.012102640223163538 | validation: 0.012484685434021685]
	TIME [epoch: 26.1 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011178459908006522		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.011178459908006522 | validation: 0.013567190060966006]
	TIME [epoch: 26.1 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010820521527631317		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.010820521527631317 | validation: 0.014910419345213501]
	TIME [epoch: 26.1 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0134338234396597		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0134338234396597 | validation: 0.01356235478295266]
	TIME [epoch: 26.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009739326772479805		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.009739326772479805 | validation: 0.011507060485001528]
	TIME [epoch: 26.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01262791348327209		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.01262791348327209 | validation: 0.013495937421081855]
	TIME [epoch: 26.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010464749955404938		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.010464749955404938 | validation: 0.013983350832354219]
	TIME [epoch: 26.1 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012120659151173274		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.012120659151173274 | validation: 0.011497966656526108]
	TIME [epoch: 26.1 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010625920128044506		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.010625920128044506 | validation: 0.013202501915361872]
	TIME [epoch: 26.1 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010521005416789507		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.010521005416789507 | validation: 0.011578317144217757]
	TIME [epoch: 26.1 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010386423852830829		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.010386423852830829 | validation: 0.018402593569324055]
	TIME [epoch: 26.1 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011724094596892672		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.011724094596892672 | validation: 0.012403517506231997]
	TIME [epoch: 26.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009852499499666477		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.009852499499666477 | validation: 0.01009608920793463]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01132020491221409		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.01132020491221409 | validation: 0.013127124141933148]
	TIME [epoch: 26.1 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010770757330867194		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.010770757330867194 | validation: 0.012283631004085626]
	TIME [epoch: 26.1 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011887673614677923		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.011887673614677923 | validation: 0.012494925020812307]
	TIME [epoch: 26.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010857568655057633		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.010857568655057633 | validation: 0.011223713678444044]
	TIME [epoch: 26.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010287990732676233		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.010287990732676233 | validation: 0.012093565228690671]
	TIME [epoch: 26.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010761376831750609		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.010761376831750609 | validation: 0.01037535717640572]
	TIME [epoch: 26.1 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008987067676923486		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.008987067676923486 | validation: 0.01841935182899685]
	TIME [epoch: 26.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010523459066767235		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.010523459066767235 | validation: 0.014693331564509932]
	TIME [epoch: 26.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011358720110475274		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.011358720110475274 | validation: 0.014350712740120859]
	TIME [epoch: 26.1 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010953219676895082		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.010953219676895082 | validation: 0.014629412170408756]
	TIME [epoch: 26.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010890303142810781		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.010890303142810781 | validation: 0.015755309496959802]
	TIME [epoch: 26.1 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011455537763798432		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.011455537763798432 | validation: 0.02426543811004922]
	TIME [epoch: 26.1 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010450874409376138		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.010450874409376138 | validation: 0.01227782860592376]
	TIME [epoch: 26.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009325551755925459		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.009325551755925459 | validation: 0.012159746025340114]
	TIME [epoch: 26.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010208902966315964		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.010208902966315964 | validation: 0.010691685762255905]
	TIME [epoch: 26.1 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010124853765825528		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.010124853765825528 | validation: 0.010950913564323995]
	TIME [epoch: 26.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012731828547389055		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.012731828547389055 | validation: 0.011179466365625345]
	TIME [epoch: 26.1 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010007499776353651		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.010007499776353651 | validation: 0.014257227377911165]
	TIME [epoch: 26.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009342476392736183		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.009342476392736183 | validation: 0.015977064737633512]
	TIME [epoch: 26.1 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012309269115527767		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.012309269115527767 | validation: 0.015258041344722029]
	TIME [epoch: 26.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011090762427519517		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.011090762427519517 | validation: 0.013683656730294055]
	TIME [epoch: 26.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008534509003129064		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.008534509003129064 | validation: 0.010511641856896162]
	TIME [epoch: 26.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009648142084965367		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.009648142084965367 | validation: 0.012421548789028918]
	TIME [epoch: 26.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011671009545355753		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.011671009545355753 | validation: 0.010503785771332508]
	TIME [epoch: 26.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009945563532028152		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.009945563532028152 | validation: 0.009782852720016564]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009092811723203881		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.009092811723203881 | validation: 0.013253868500662794]
	TIME [epoch: 26.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010370897156068606		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.010370897156068606 | validation: 0.016284560657580032]
	TIME [epoch: 26.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009814221660288188		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.009814221660288188 | validation: 0.012016630536822611]
	TIME [epoch: 26.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008614919048699064		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.008614919048699064 | validation: 0.01179139655177507]
	TIME [epoch: 26.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009988625560410136		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.009988625560410136 | validation: 0.011019849711397914]
	TIME [epoch: 26.1 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010765421386211544		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.010765421386211544 | validation: 0.012000650474584224]
	TIME [epoch: 26.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010873985073404993		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.010873985073404993 | validation: 0.010330106717563577]
	TIME [epoch: 26.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009107118256307629		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.009107118256307629 | validation: 0.009114824826238183]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010146918502787667		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.010146918502787667 | validation: 0.018853919313703667]
	TIME [epoch: 26.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010979834815198965		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.010979834815198965 | validation: 0.010315754963520777]
	TIME [epoch: 26.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008743235333414891		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.008743235333414891 | validation: 0.012140644468586334]
	TIME [epoch: 26.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009459483337811337		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.009459483337811337 | validation: 0.014381015440403724]
	TIME [epoch: 26.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010963443296855545		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.010963443296855545 | validation: 0.012082630182314792]
	TIME [epoch: 26.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008707771880162241		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.008707771880162241 | validation: 0.0108628812665765]
	TIME [epoch: 26.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00811719332457413		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.00811719332457413 | validation: 0.0118642924129095]
	TIME [epoch: 26.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012105118866711789		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.012105118866711789 | validation: 0.01200987914728359]
	TIME [epoch: 26.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011211208118859804		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.011211208118859804 | validation: 0.010770574738404225]
	TIME [epoch: 26.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009622225550067894		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.009622225550067894 | validation: 0.012433944808814613]
	TIME [epoch: 26.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00910153696098789		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.00910153696098789 | validation: 0.010465236699523426]
	TIME [epoch: 26.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008118519947598504		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.008118519947598504 | validation: 0.010500810455833785]
	TIME [epoch: 26.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010579740954614952		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.010579740954614952 | validation: 0.01211477753454147]
	TIME [epoch: 26.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010345137712054059		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.010345137712054059 | validation: 0.011319655767668527]
	TIME [epoch: 26.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009027733277844546		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.009027733277844546 | validation: 0.011538231090291536]
	TIME [epoch: 26.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008678503003254862		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.008678503003254862 | validation: 0.011803646232977165]
	TIME [epoch: 26.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010392789891607608		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.010392789891607608 | validation: 0.020948935512375134]
	TIME [epoch: 26.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011491992011178486		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.011491992011178486 | validation: 0.01035733427751889]
	TIME [epoch: 26.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008417906009340127		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.008417906009340127 | validation: 0.010997712296322222]
	TIME [epoch: 26.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00982199924432715		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.00982199924432715 | validation: 0.013061585847965726]
	TIME [epoch: 26.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010054019474816354		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.010054019474816354 | validation: 0.010034249413283607]
	TIME [epoch: 26.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008247401304337916		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.008247401304337916 | validation: 0.013623165314634285]
	TIME [epoch: 26.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009575354229979365		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.009575354229979365 | validation: 0.012281306903916205]
	TIME [epoch: 26.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009068243373241615		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.009068243373241615 | validation: 0.011152685198364197]
	TIME [epoch: 26.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009775104374481619		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.009775104374481619 | validation: 0.010364350913906476]
	TIME [epoch: 26.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008741278165603276		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.008741278165603276 | validation: 0.012561323066078735]
	TIME [epoch: 26.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009031465807468886		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.009031465807468886 | validation: 0.013132708051091601]
	TIME [epoch: 26.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00912704113985014		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.00912704113985014 | validation: 0.010397484406169543]
	TIME [epoch: 26.1 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011127486751885213		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.011127486751885213 | validation: 0.013504282146597994]
	TIME [epoch: 26.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010144952228453267		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.010144952228453267 | validation: 0.011729401643830592]
	TIME [epoch: 26.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008835771181754475		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.008835771181754475 | validation: 0.01136048554773638]
	TIME [epoch: 26.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008176900586867306		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.008176900586867306 | validation: 0.010014580026082028]
	TIME [epoch: 26.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010101873880233037		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.010101873880233037 | validation: 0.013281130513887233]
	TIME [epoch: 26.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00845101455571781		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.00845101455571781 | validation: 0.009622433195924734]
	TIME [epoch: 26.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009933876283021349		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.009933876283021349 | validation: 0.010353227841873654]
	TIME [epoch: 26.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008606468241757167		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.008606468241757167 | validation: 0.015363790824156611]
	TIME [epoch: 26.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008909440517347027		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.008909440517347027 | validation: 0.013592177782741001]
	TIME [epoch: 26.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008487574097305252		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.008487574097305252 | validation: 0.011109062304865708]
	TIME [epoch: 26.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008985042393258815		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.008985042393258815 | validation: 0.01098705995922898]
	TIME [epoch: 26.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008451502665852938		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.008451502665852938 | validation: 0.012208094844222906]
	TIME [epoch: 26.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009457050361955772		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.009457050361955772 | validation: 0.010039768867372532]
	TIME [epoch: 26.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00964677106460797		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.00964677106460797 | validation: 0.014739020912136237]
	TIME [epoch: 26.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008406678257968916		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.008406678257968916 | validation: 0.010835908694632475]
	TIME [epoch: 26.1 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01034931973562934		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.01034931973562934 | validation: 0.012526050030812851]
	TIME [epoch: 26.1 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009405189509030859		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.009405189509030859 | validation: 0.00927318831813416]
	TIME [epoch: 26.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007872133367192056		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.007872133367192056 | validation: 0.010135325081253511]
	TIME [epoch: 26.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008866488875105685		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.008866488875105685 | validation: 0.009795237494902723]
	TIME [epoch: 26.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009052603865041343		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.009052603865041343 | validation: 0.009780566071008863]
	TIME [epoch: 26.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008958210601252042		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.008958210601252042 | validation: 0.01046481633485435]
	TIME [epoch: 26.1 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009632433023228006		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.009632433023228006 | validation: 0.008835162578357363]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008268726437063726		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.008268726437063726 | validation: 0.011033422428235344]
	TIME [epoch: 26.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008299239213624538		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.008299239213624538 | validation: 0.009663997022034488]
	TIME [epoch: 26.1 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008671277785739805		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.008671277785739805 | validation: 0.011509516905867743]
	TIME [epoch: 26.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008209981396085997		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.008209981396085997 | validation: 0.009142328722110892]
	TIME [epoch: 26.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008948337992917561		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.008948337992917561 | validation: 0.008872190379245658]
	TIME [epoch: 26.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071523723832176735		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0071523723832176735 | validation: 0.009537441992776952]
	TIME [epoch: 26.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009448115222269182		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.009448115222269182 | validation: 0.015413750908870691]
	TIME [epoch: 26.1 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008859138115372998		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.008859138115372998 | validation: 0.00893047469654301]
	TIME [epoch: 26.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007115597674997937		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.007115597674997937 | validation: 0.010364149543674992]
	TIME [epoch: 26.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009962427416738227		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.009962427416738227 | validation: 0.009535806005261478]
	TIME [epoch: 26.1 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008352050167692556		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.008352050167692556 | validation: 0.009897029482076086]
	TIME [epoch: 26.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009115773349842904		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.009115773349842904 | validation: 0.008571049545270926]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0082041070372099		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.0082041070372099 | validation: 0.011228127894179447]
	TIME [epoch: 26.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009661984649846599		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.009661984649846599 | validation: 0.010571860909024127]
	TIME [epoch: 26.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00791367252046085		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.00791367252046085 | validation: 0.01031947142154465]
	TIME [epoch: 26.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008678773519866373		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.008678773519866373 | validation: 0.00952343771637101]
	TIME [epoch: 26.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008074888390847544		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.008074888390847544 | validation: 0.011230141141656092]
	TIME [epoch: 26.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007553793264275623		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.007553793264275623 | validation: 0.014107546214696745]
	TIME [epoch: 26.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009014027010923426		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.009014027010923426 | validation: 0.01030503385769544]
	TIME [epoch: 26.1 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009155298725801515		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.009155298725801515 | validation: 0.009332184083798213]
	TIME [epoch: 26.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008332517999465444		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.008332517999465444 | validation: 0.012928327265749755]
	TIME [epoch: 26.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008469266189421256		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.008469266189421256 | validation: 0.008884931681552282]
	TIME [epoch: 26.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009023098722339654		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.009023098722339654 | validation: 0.008886602581276304]
	TIME [epoch: 26.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007562425131052457		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.007562425131052457 | validation: 0.009312637323135862]
	TIME [epoch: 26.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007485981631462089		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.007485981631462089 | validation: 0.010126462579055641]
	TIME [epoch: 26.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008613287585335524		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.008613287585335524 | validation: 0.01007326883419695]
	TIME [epoch: 26.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00810484053938053		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.00810484053938053 | validation: 0.01069199775456137]
	TIME [epoch: 26.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009213519224063578		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.009213519224063578 | validation: 0.009075651409301643]
	TIME [epoch: 26.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007308456116684279		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.007308456116684279 | validation: 0.009087868179831406]
	TIME [epoch: 26.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007585740691253412		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.007585740691253412 | validation: 0.009909707215008582]
	TIME [epoch: 26.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008259011938393828		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.008259011938393828 | validation: 0.012556649729182544]
	TIME [epoch: 26.1 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009001198679643054		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.009001198679643054 | validation: 0.008244510826631416]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007495482702737675		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.007495482702737675 | validation: 0.00870903195746098]
	TIME [epoch: 26.1 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00814414106956894		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.00814414106956894 | validation: 0.010355906500932474]
	TIME [epoch: 26.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008532119280618238		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.008532119280618238 | validation: 0.00953216367789109]
	TIME [epoch: 26.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007266943909088235		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.007266943909088235 | validation: 0.009956331374608189]
	TIME [epoch: 26.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007738001055665276		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.007738001055665276 | validation: 0.010073693565169566]
	TIME [epoch: 26.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008139713690375189		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.008139713690375189 | validation: 0.008080256835522284]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007644620845276041		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.007644620845276041 | validation: 0.00899667825498152]
	TIME [epoch: 26.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008635040757374543		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.008635040757374543 | validation: 0.013455239540240135]
	TIME [epoch: 26.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008230012394683857		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.008230012394683857 | validation: 0.009550584723328537]
	TIME [epoch: 26.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0089294829793269		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.0089294829793269 | validation: 0.010925987699529998]
	TIME [epoch: 26.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007495074751689544		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.007495074751689544 | validation: 0.008838802042566981]
	TIME [epoch: 26.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007866249252753185		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.007866249252753185 | validation: 0.009887627165972454]
	TIME [epoch: 26.1 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007280360075290465		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.007280360075290465 | validation: 0.008116873795892062]
	TIME [epoch: 26.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007180933798745729		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.007180933798745729 | validation: 0.009568133746916613]
	TIME [epoch: 26.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00786795897947126		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.00786795897947126 | validation: 0.009144065606751725]
	TIME [epoch: 26.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008938773513598557		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.008938773513598557 | validation: 0.009226199996419831]
	TIME [epoch: 26.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007724926760865161		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.007724926760865161 | validation: 0.012582513384894734]
	TIME [epoch: 26.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00814356281095305		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.00814356281095305 | validation: 0.00963161883384241]
	TIME [epoch: 26.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007241749453683005		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.007241749453683005 | validation: 0.009939777260341573]
	TIME [epoch: 26.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008331534913214872		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.008331534913214872 | validation: 0.009942669265962704]
	TIME [epoch: 26.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007128161804629558		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.007128161804629558 | validation: 0.009467115465960484]
	TIME [epoch: 26.1 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008108840763917647		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.008108840763917647 | validation: 0.00864212848747561]
	TIME [epoch: 26.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008288419277328542		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.008288419277328542 | validation: 0.008218693051043073]
	TIME [epoch: 26.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007043617483527042		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.007043617483527042 | validation: 0.009262730515721072]
	TIME [epoch: 26.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007556365634986767		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.007556365634986767 | validation: 0.008741296897150615]
	TIME [epoch: 26.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0077722818190535305		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.0077722818190535305 | validation: 0.00969379760894053]
	TIME [epoch: 26.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006580685845569303		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.006580685845569303 | validation: 0.00813062910197292]
	TIME [epoch: 26.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007631938454649557		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.007631938454649557 | validation: 0.012224921118398253]
	TIME [epoch: 26.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008223221650697476		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.008223221650697476 | validation: 0.00970657943263238]
	TIME [epoch: 26.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00752122637498066		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.00752122637498066 | validation: 0.013361616415437138]
	TIME [epoch: 26.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00817128256521589		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.00817128256521589 | validation: 0.010501645993349124]
	TIME [epoch: 26.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008846115153405793		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.008846115153405793 | validation: 0.011887089311290086]
	TIME [epoch: 26.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007710499873909482		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.007710499873909482 | validation: 0.011676073970162315]
	TIME [epoch: 26.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007569293503785162		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.007569293503785162 | validation: 0.008059989978566594]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007473438354453165		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.007473438354453165 | validation: 0.00962529688616506]
	TIME [epoch: 26.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007754531261725791		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.007754531261725791 | validation: 0.01121504818111867]
	TIME [epoch: 26.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008011080201521012		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.008011080201521012 | validation: 0.009586378766685496]
	TIME [epoch: 26.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006998075272819622		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.006998075272819622 | validation: 0.008594145133516362]
	TIME [epoch: 26.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006846030354745037		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.006846030354745037 | validation: 0.01033996854345341]
	TIME [epoch: 26.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072672898071524505		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.0072672898071524505 | validation: 0.00987374017337667]
	TIME [epoch: 26.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008412313671230777		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.008412313671230777 | validation: 0.008237283126271144]
	TIME [epoch: 26.1 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068935809027000465		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.0068935809027000465 | validation: 0.00807325337995778]
	TIME [epoch: 26.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007214054226808589		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.007214054226808589 | validation: 0.01033282132601825]
	TIME [epoch: 26.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007799543107107681		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.007799543107107681 | validation: 0.00705173078456999]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008264311039091239		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.008264311039091239 | validation: 0.00840731716612016]
	TIME [epoch: 26.1 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072824176480286015		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.0072824176480286015 | validation: 0.010230641151350785]
	TIME [epoch: 26.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007431525095195514		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.007431525095195514 | validation: 0.0088382680263356]
	TIME [epoch: 26.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007325526043902797		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.007325526043902797 | validation: 0.009342538955344228]
	TIME [epoch: 26.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007488172352587925		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.007488172352587925 | validation: 0.009524862117679488]
	TIME [epoch: 26.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006835948204443947		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.006835948204443947 | validation: 0.008019430625051017]
	TIME [epoch: 26.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068760116609749315		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0068760116609749315 | validation: 0.009538849725441136]
	TIME [epoch: 26.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00812941116154284		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.00812941116154284 | validation: 0.008167902459129121]
	TIME [epoch: 26.1 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007675890818325617		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.007675890818325617 | validation: 0.007748372347703767]
	TIME [epoch: 26.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006569792318503126		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.006569792318503126 | validation: 0.007367692406937706]
	TIME [epoch: 26.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00649799659674457		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.00649799659674457 | validation: 0.009053179694199097]
	TIME [epoch: 26.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008558186786987047		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.008558186786987047 | validation: 0.009089218255564536]
	TIME [epoch: 26.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007210294176830608		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.007210294176830608 | validation: 0.009149858049122848]
	TIME [epoch: 26.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006843686104993642		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.006843686104993642 | validation: 0.008921688372392534]
	TIME [epoch: 26.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064696508373928326		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0064696508373928326 | validation: 0.007972163624225707]
	TIME [epoch: 26.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00642626327582113		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.00642626327582113 | validation: 0.008533055076300571]
	TIME [epoch: 26.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006547932696773269		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.006547932696773269 | validation: 0.008289328403950198]
	TIME [epoch: 26.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0072371776221951765		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0072371776221951765 | validation: 0.008715969432759207]
	TIME [epoch: 26.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007605919532373471		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.007605919532373471 | validation: 0.010003871590937671]
	TIME [epoch: 26.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007704625658349345		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.007704625658349345 | validation: 0.008296445595172322]
	TIME [epoch: 26.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0071354002716042236		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0071354002716042236 | validation: 0.0074865498762404035]
	TIME [epoch: 26.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006589799501829927		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.006589799501829927 | validation: 0.007638197580542861]
	TIME [epoch: 26.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060668744950360485		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0060668744950360485 | validation: 0.008978989726337448]
	TIME [epoch: 26.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006961735855689246		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.006961735855689246 | validation: 0.008195657035998501]
	TIME [epoch: 26.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006748811004055661		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.006748811004055661 | validation: 0.008035575147676423]
	TIME [epoch: 26.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007449272787158407		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.007449272787158407 | validation: 0.007642103364678033]
	TIME [epoch: 26.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006601803457188974		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.006601803457188974 | validation: 0.01228549678697707]
	TIME [epoch: 26.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006529314787622744		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.006529314787622744 | validation: 0.008585703265849272]
	TIME [epoch: 26.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006749531424054343		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.006749531424054343 | validation: 0.008306892079170387]
	TIME [epoch: 26.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006642238161731535		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.006642238161731535 | validation: 0.01006003864329726]
	TIME [epoch: 26.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007218708927247127		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.007218708927247127 | validation: 0.006969534716280364]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_937.pth
	Model improved!!!
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006414064322674072		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.006414064322674072 | validation: 0.009601450849944383]
	TIME [epoch: 26.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006422890646836926		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.006422890646836926 | validation: 0.007632638561552049]
	TIME [epoch: 26.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006981029515309132		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.006981029515309132 | validation: 0.007826281283038767]
	TIME [epoch: 26.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00711695526648273		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.00711695526648273 | validation: 0.008365590430765199]
	TIME [epoch: 26.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006413135544794098		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.006413135544794098 | validation: 0.007818752515189827]
	TIME [epoch: 26.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00655764844409109		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.00655764844409109 | validation: 0.009319580950642285]
	TIME [epoch: 26.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00634229385797159		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.00634229385797159 | validation: 0.009410276409545818]
	TIME [epoch: 26.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007139978414652202		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.007139978414652202 | validation: 0.00787759965478216]
	TIME [epoch: 26.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007134220900334763		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.007134220900334763 | validation: 0.009930567609428701]
	TIME [epoch: 26.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006452275675529142		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.006452275675529142 | validation: 0.0125568897357125]
	TIME [epoch: 26.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068080202956647354		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.0068080202956647354 | validation: 0.008004552862209077]
	TIME [epoch: 26.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006558268827316033		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.006558268827316033 | validation: 0.007788756126354483]
	TIME [epoch: 26.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006575164513582043		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.006575164513582043 | validation: 0.00925643884718887]
	TIME [epoch: 26.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00746029610205924		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.00746029610205924 | validation: 0.0076611471068757315]
	TIME [epoch: 26.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006218652224437552		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.006218652224437552 | validation: 0.009667173823554238]
	TIME [epoch: 26.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00610555290875738		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.00610555290875738 | validation: 0.00797163897682035]
	TIME [epoch: 26.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006328445049232469		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.006328445049232469 | validation: 0.007864706968706458]
	TIME [epoch: 26.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00638578064151275		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.00638578064151275 | validation: 0.007728338431049805]
	TIME [epoch: 26.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062770110390478065		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0062770110390478065 | validation: 0.0070929899866648775]
	TIME [epoch: 26.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006814442571971128		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.006814442571971128 | validation: 0.006666398760469077]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007531436164406139		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.007531436164406139 | validation: 0.007415845471539544]
	TIME [epoch: 26.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005836129231093563		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.005836129231093563 | validation: 0.008076942151708214]
	TIME [epoch: 26.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006590428749242929		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.006590428749242929 | validation: 0.008051334609942132]
	TIME [epoch: 26.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006617520487797721		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.006617520487797721 | validation: 0.00789598320699431]
	TIME [epoch: 26.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006373456585705749		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.006373456585705749 | validation: 0.008035197641763404]
	TIME [epoch: 26.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005905430519734202		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.005905430519734202 | validation: 0.006682531061790994]
	TIME [epoch: 26.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065121772419594005		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.0065121772419594005 | validation: 0.010661224410921119]
	TIME [epoch: 26.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006965049189576788		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.006965049189576788 | validation: 0.007437572233102337]
	TIME [epoch: 26.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006467681050184826		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.006467681050184826 | validation: 0.008292622839196727]
	TIME [epoch: 26.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006477364111130174		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.006477364111130174 | validation: 0.007963939456541596]
	TIME [epoch: 26.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006418359639701755		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.006418359639701755 | validation: 0.009970454328247197]
	TIME [epoch: 26.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0065192045579135684		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.0065192045579135684 | validation: 0.008040803698718044]
	TIME [epoch: 26.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006292897131692841		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.006292897131692841 | validation: 0.0078656635882937]
	TIME [epoch: 26.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006038256107340685		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.006038256107340685 | validation: 0.007634752304268871]
	TIME [epoch: 26 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006682648764808651		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.006682648764808651 | validation: 0.00767291354474107]
	TIME [epoch: 26.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00666725272410969		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.00666725272410969 | validation: 0.009033844787601197]
	TIME [epoch: 26.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006009096201514104		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.006009096201514104 | validation: 0.005846435198249244]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005450264144003333		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.005450264144003333 | validation: 0.007843639325152679]
	TIME [epoch: 26.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006666512386760518		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.006666512386760518 | validation: 0.009136398728359196]
	TIME [epoch: 26.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006360697668860625		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.006360697668860625 | validation: 0.007189493412691822]
	TIME [epoch: 26.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006724175390710246		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.006724175390710246 | validation: 0.007888370263006542]
	TIME [epoch: 26.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057469574104785015		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0057469574104785015 | validation: 0.0071236702294327995]
	TIME [epoch: 26.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005668921122692232		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.005668921122692232 | validation: 0.007433486407881554]
	TIME [epoch: 26.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006463604072159223		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.006463604072159223 | validation: 0.007858653361730727]
	TIME [epoch: 26.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005487834413771401		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.005487834413771401 | validation: 0.00659672956037745]
	TIME [epoch: 26.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005936511693709076		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.005936511693709076 | validation: 0.00641528456331562]
	TIME [epoch: 26.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006037927662533843		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.006037927662533843 | validation: 0.007189793814122608]
	TIME [epoch: 26.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005374394803619374		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.005374394803619374 | validation: 0.006448574396831686]
	TIME [epoch: 26.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006026474791830215		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.006026474791830215 | validation: 0.007585224265599831]
	TIME [epoch: 26.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006226785593113325		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.006226785593113325 | validation: 0.007621372882647314]
	TIME [epoch: 26 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005906410997001976		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.005906410997001976 | validation: 0.006371355892443944]
	TIME [epoch: 26 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005924272021833464		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.005924272021833464 | validation: 0.00788379684351821]
	TIME [epoch: 26.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006232803890543211		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.006232803890543211 | validation: 0.008129350445579171]
	TIME [epoch: 26.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006336795108636637		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.006336795108636637 | validation: 0.007606295187974529]
	TIME [epoch: 26.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006301019764784539		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.006301019764784539 | validation: 0.007028771873268367]
	TIME [epoch: 26.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00578933258882548		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.00578933258882548 | validation: 0.0064717183339047645]
	TIME [epoch: 26.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006024124611186342		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.006024124611186342 | validation: 0.006455555518570126]
	TIME [epoch: 26.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006290989313577211		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.006290989313577211 | validation: 0.00855085192941702]
	TIME [epoch: 26.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005771097528354761		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.005771097528354761 | validation: 0.007727399788609028]
	TIME [epoch: 26 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005855111810844195		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.005855111810844195 | validation: 0.0065915920938087215]
	TIME [epoch: 26 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005817193736779314		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.005817193736779314 | validation: 0.007127738649316449]
	TIME [epoch: 26.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005618091005905351		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.005618091005905351 | validation: 0.004691448055851308]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005861252305514276		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.005861252305514276 | validation: 0.006408215359097192]
	TIME [epoch: 26 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006198868331958192		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.006198868331958192 | validation: 0.007152924117196584]
	TIME [epoch: 439 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006011538368146522		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.006011538368146522 | validation: 0.007870638866973628]
	TIME [epoch: 55.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006002112146324113		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.006002112146324113 | validation: 0.006597721497443831]
	TIME [epoch: 55.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006004124206002974		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.006004124206002974 | validation: 0.008558696973717639]
	TIME [epoch: 55.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006350436693680013		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.006350436693680013 | validation: 0.00710640032654217]
	TIME [epoch: 55.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006089380792780623		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.006089380792780623 | validation: 0.00715783876103525]
	TIME [epoch: 55.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005870778262325372		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.005870778262325372 | validation: 0.006807644090382743]
	TIME [epoch: 55.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051744692827013295		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0051744692827013295 | validation: 0.00819395690201671]
	TIME [epoch: 55.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062109952304980915		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.0062109952304980915 | validation: 0.006165078169102225]
	TIME [epoch: 55.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005495085466191512		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.005495085466191512 | validation: 0.006945585084884982]
	TIME [epoch: 55.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005364576333805541		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.005364576333805541 | validation: 0.006076376073961479]
	TIME [epoch: 55.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005809175998867389		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.005809175998867389 | validation: 0.008795356597641456]
	TIME [epoch: 55.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006092194072726813		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.006092194072726813 | validation: 0.0074089664074365215]
	TIME [epoch: 55.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0061639677426996945		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0061639677426996945 | validation: 0.008128719195146438]
	TIME [epoch: 55.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005813370106967057		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.005813370106967057 | validation: 0.006139397844879069]
	TIME [epoch: 55.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005504000757657438		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.005504000757657438 | validation: 0.006923824859243878]
	TIME [epoch: 55.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005971011439084708		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.005971011439084708 | validation: 0.006424747271407317]
	TIME [epoch: 55.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005211292882734751		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.005211292882734751 | validation: 0.006249238209661434]
	TIME [epoch: 55.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005513520251581458		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.005513520251581458 | validation: 0.007561571970556498]
	TIME [epoch: 55.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005702682275506961		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.005702682275506961 | validation: 0.006595060686001776]
	TIME [epoch: 55.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056604289798904885		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0056604289798904885 | validation: 0.007774245751360157]
	TIME [epoch: 55.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005572539874098876		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.005572539874098876 | validation: 0.0063020871901359014]
	TIME [epoch: 55.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005744643285615868		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.005744643285615868 | validation: 0.006969667537063382]
	TIME [epoch: 55.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005207930760173167		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.005207930760173167 | validation: 0.006871199139513386]
	TIME [epoch: 55.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005263997514216878		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.005263997514216878 | validation: 0.0065289927742622645]
	TIME [epoch: 55.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005924880339392605		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.005924880339392605 | validation: 0.007033896928405345]
	TIME [epoch: 55.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005404080981442858		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.005404080981442858 | validation: 0.006634314133121872]
	TIME [epoch: 55.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005862148638181663		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.005862148638181663 | validation: 0.0071455384923387495]
	TIME [epoch: 55.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005504423037007266		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.005504423037007266 | validation: 0.006472912173341762]
	TIME [epoch: 55.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005868493216170097		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.005868493216170097 | validation: 0.0061454774112702495]
	TIME [epoch: 55.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005304742670676181		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.005304742670676181 | validation: 0.006451657195991667]
	TIME [epoch: 55.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058083246970778565		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0058083246970778565 | validation: 0.006061681925363748]
	TIME [epoch: 55.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005721061575934503		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.005721061575934503 | validation: 0.005478794600336808]
	TIME [epoch: 55.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005568482751604115		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.005568482751604115 | validation: 0.007323054135206366]
	TIME [epoch: 55.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005382021008225536		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.005382021008225536 | validation: 0.006327557009904761]
	TIME [epoch: 55.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056386020678692115		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0056386020678692115 | validation: 0.006332262599288981]
	TIME [epoch: 55.2 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005219542538214219		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.005219542538214219 | validation: 0.00649793402091851]
	TIME [epoch: 55.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056556724591585		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.0056556724591585 | validation: 0.007069126334412682]
	TIME [epoch: 55.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005897399803879431		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.005897399803879431 | validation: 0.006702168609670793]
	TIME [epoch: 55.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005404428594750885		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.005404428594750885 | validation: 0.0057711317374028615]
	TIME [epoch: 55.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050981714654099435		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0050981714654099435 | validation: 0.008500546183667785]
	TIME [epoch: 55.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00598973942755951		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.00598973942755951 | validation: 0.006742480083261143]
	TIME [epoch: 55.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005193678663131238		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.005193678663131238 | validation: 0.006429765309178772]
	TIME [epoch: 55.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005914262647899521		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.005914262647899521 | validation: 0.0061031784818478185]
	TIME [epoch: 55.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048769278285923584		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.0048769278285923584 | validation: 0.007773766171382349]
	TIME [epoch: 55.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005197622322117169		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.005197622322117169 | validation: 0.006968432240297781]
	TIME [epoch: 55.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007297202199021694		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.007297202199021694 | validation: 0.0075126517160060795]
	TIME [epoch: 55.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00561952661673008		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.00561952661673008 | validation: 0.006216295727694766]
	TIME [epoch: 55.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0046958825906984955		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.0046958825906984955 | validation: 0.006501312086268161]
	TIME [epoch: 55.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006019303403300153		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.006019303403300153 | validation: 0.00915283388420535]
	TIME [epoch: 55.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00573973527795498		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.00573973527795498 | validation: 0.00688418217444452]
	TIME [epoch: 55.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005539419361863135		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.005539419361863135 | validation: 0.0059249143419825845]
	TIME [epoch: 55.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005509432675673615		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.005509432675673615 | validation: 0.006176057125311425]
	TIME [epoch: 55.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005500126399489825		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.005500126399489825 | validation: 0.007310036659270924]
	TIME [epoch: 55.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005614812890685439		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.005614812890685439 | validation: 0.006009409486128917]
	TIME [epoch: 55.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00486067897780951		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.00486067897780951 | validation: 0.00691684814954551]
	TIME [epoch: 55.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052434211924899795		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.0052434211924899795 | validation: 0.006379651453080382]
	TIME [epoch: 55.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004987345828547556		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.004987345828547556 | validation: 0.0066249205354343334]
	TIME [epoch: 55.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005186779819464579		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.005186779819464579 | validation: 0.006656534519867078]
	TIME [epoch: 55.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005537659627921691		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.005537659627921691 | validation: 0.006870329488010157]
	TIME [epoch: 55.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005363748920312071		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.005363748920312071 | validation: 0.006112513518716823]
	TIME [epoch: 55.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005304108066953629		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.005304108066953629 | validation: 0.006104873259219684]
	TIME [epoch: 55.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005200667050641015		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.005200667050641015 | validation: 0.007302309370007627]
	TIME [epoch: 55.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052597843191334315		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0052597843191334315 | validation: 0.005780628837061218]
	TIME [epoch: 55.2 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050826475769319025		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0050826475769319025 | validation: 0.007710103452552187]
	TIME [epoch: 55.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0055054695206079515		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.0055054695206079515 | validation: 0.008323807438854346]
	TIME [epoch: 55.2 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005139064540038313		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.005139064540038313 | validation: 0.005897795233781349]
	TIME [epoch: 55.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005138853857132114		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.005138853857132114 | validation: 0.005874606587882929]
	TIME [epoch: 55.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005417590319204191		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.005417590319204191 | validation: 0.005932550883288111]
	TIME [epoch: 55.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004963350937290994		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.004963350937290994 | validation: 0.006740918945956539]
	TIME [epoch: 55.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005135374587336441		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.005135374587336441 | validation: 0.005423377598439265]
	TIME [epoch: 55.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050925592136060165		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0050925592136060165 | validation: 0.006645360168759624]
	TIME [epoch: 55.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005058931930354362		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.005058931930354362 | validation: 0.0071069829238728744]
	TIME [epoch: 55.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00564328540322254		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.00564328540322254 | validation: 0.006744237407412078]
	TIME [epoch: 55.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0049402260355729715		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0049402260355729715 | validation: 0.005846603680983458]
	TIME [epoch: 55.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004816783158842198		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.004816783158842198 | validation: 0.005365292874570261]
	TIME [epoch: 55.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005361493947568107		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.005361493947568107 | validation: 0.006710102469038375]
	TIME [epoch: 55.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005366300014550268		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.005366300014550268 | validation: 0.0057848297650648925]
	TIME [epoch: 55.2 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005036683744001139		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.005036683744001139 | validation: 0.006586550174734233]
	TIME [epoch: 55.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005453921171912821		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.005453921171912821 | validation: 0.006268438508207755]
	TIME [epoch: 55.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004741055500232839		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.004741055500232839 | validation: 0.006581217773383779]
	TIME [epoch: 55.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005431948647804781		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.005431948647804781 | validation: 0.005645629284349965]
	TIME [epoch: 55.2 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004709799184455119		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.004709799184455119 | validation: 0.0063052300962109864]
	TIME [epoch: 55.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00568421377108882		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.00568421377108882 | validation: 0.006161417984561046]
	TIME [epoch: 55.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004996685079454929		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.004996685079454929 | validation: 0.005614724906047187]
	TIME [epoch: 55.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005280915554977017		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.005280915554977017 | validation: 0.0059806954527055235]
	TIME [epoch: 55.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005139578543385226		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.005139578543385226 | validation: 0.005836749314195972]
	TIME [epoch: 55.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005104051491912842		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.005104051491912842 | validation: 0.00614125914241133]
	TIME [epoch: 55.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051437058594043395		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.0051437058594043395 | validation: 0.0054306807757056805]
	TIME [epoch: 55.2 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004582839661366986		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.004582839661366986 | validation: 0.006292661240217261]
	TIME [epoch: 55.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005013092400622584		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.005013092400622584 | validation: 0.007114030355370966]
	TIME [epoch: 55.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005436127288421058		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.005436127288421058 | validation: 0.00632291017646929]
	TIME [epoch: 55.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004960835447423551		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.004960835447423551 | validation: 0.0053164097193170124]
	TIME [epoch: 55.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004952655916549715		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.004952655916549715 | validation: 0.005984181018281763]
	TIME [epoch: 55.1 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004736043014900009		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.004736043014900009 | validation: 0.006679099929292413]
	TIME [epoch: 55.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005280328130649955		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.005280328130649955 | validation: 0.006004528487007629]
	TIME [epoch: 55.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005126860317422205		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.005126860317422205 | validation: 0.005395253511369736]
	TIME [epoch: 55.2 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005408074736370897		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.005408074736370897 | validation: 0.006752812183124096]
	TIME [epoch: 55.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004751788259711002		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.004751788259711002 | validation: 0.005634231869863801]
	TIME [epoch: 55.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004986705149350424		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.004986705149350424 | validation: 0.005583975386398661]
	TIME [epoch: 55.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1a_3_v_mmd1_20250611_071933/states/model_phi1_1a_saddle_v1a_3_v_mmd1_1100.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 25135.818 seconds.
