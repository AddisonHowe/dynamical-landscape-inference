Args:
Namespace(name='model_phi1_1a_saddle_v1b_0_v_mmd1', outdir='out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1', training_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1b_0/training', validation_data='data/training_data/saddle_v1/data_phi1_1a_saddle_v1b_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05700302496552467, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4290769327

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.232551094562755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.232551094562755 | validation: 6.930075528913349]
	TIME [epoch: 375 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.713584903181945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.713584903181945 | validation: 6.864481561377792]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.336046739195492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.336046739195492 | validation: 6.550677538527562]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.052369923571858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.052369923571858 | validation: 5.646746242398585]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3925569071025485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3925569071025485 | validation: 5.776442379818379]
	TIME [epoch: 6.21 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.890415940301947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.890415940301947 | validation: 5.451908488739669]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.711787499975896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.711787499975896 | validation: 4.776589778220885]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.36117302965261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.36117302965261 | validation: 4.734121141526648]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.085559340038781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085559340038781 | validation: 4.428435051367147]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.015020872622447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015020872622447 | validation: 4.383897050563852]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.930308091573231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.930308091573231 | validation: 4.2054528165369645]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8389889120516467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8389889120516467 | validation: 4.166332784544112]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.74973863112967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.74973863112967 | validation: 3.8809076680712216]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4545177716553783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4545177716553783 | validation: 3.695748444459901]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3802665372732235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3802665372732235 | validation: 3.3218314553867065]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.131598579200802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.131598579200802 | validation: 3.0782725985392387]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.909450111903038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.909450111903038 | validation: 2.97227613715672]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.780423845574438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.780423845574438 | validation: 2.755111489035968]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8387190202315686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8387190202315686 | validation: 2.9854703884312124]
	TIME [epoch: 6.2 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8076343873831346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8076343873831346 | validation: 2.626715663268998]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5352024183460156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5352024183460156 | validation: 2.545724964551003]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4221811935016624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4221811935016624 | validation: 2.8398314053123848]
	TIME [epoch: 6.19 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.476620695433799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.476620695433799 | validation: 2.45845338921534]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.371696185875278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.371696185875278 | validation: 2.295457316460671]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3081061974591126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3081061974591126 | validation: 2.2605252935598648]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0999707123884503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0999707123884503 | validation: 2.293436951985586]
	TIME [epoch: 6.2 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1246557958489993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1246557958489993 | validation: 1.8608107363326392]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.270184908760325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.270184908760325 | validation: 3.235969878941261]
	TIME [epoch: 6.2 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4485224657213167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4485224657213167 | validation: 2.0950139382637825]
	TIME [epoch: 6.19 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7764563029976632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7764563029976632 | validation: 1.6177465089833671]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.970812987530024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.970812987530024 | validation: 2.4582527941816132]
	TIME [epoch: 6.19 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2125925566199887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2125925566199887 | validation: 1.6256877792895805]
	TIME [epoch: 6.19 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4398869982963034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4398869982963034 | validation: 1.3560159465230968]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4902111458573075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4902111458573075 | validation: 1.4526339436767048]
	TIME [epoch: 6.2 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5041170707869476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5041170707869476 | validation: 1.117776071622607]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3421146429725588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3421146429725588 | validation: 1.7964948925784816]
	TIME [epoch: 6.22 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5920964321370956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5920964321370956 | validation: 1.080451935442091]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4016876875504403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4016876875504403 | validation: 1.1117332570628737]
	TIME [epoch: 6.2 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2030793631742538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2030793631742538 | validation: 1.2219249369456473]
	TIME [epoch: 6.19 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0355474716160797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0355474716160797 | validation: 1.4630008627041886]
	TIME [epoch: 6.19 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3655641305205923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3655641305205923 | validation: 0.9092246588455031]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8354669975881976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8354669975881976 | validation: 0.9208233439102783]
	TIME [epoch: 6.21 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1526371916278328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1526371916278328 | validation: 1.6460258571366495]
	TIME [epoch: 6.18 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3315550417601414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3315550417601414 | validation: 0.9166018987307483]
	TIME [epoch: 6.2 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9665173533366238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9665173533366238 | validation: 1.3864534192327382]
	TIME [epoch: 6.19 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0640188563588941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0640188563588941 | validation: 0.8624427095345757]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9656170055560087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9656170055560087 | validation: 0.874798892021883]
	TIME [epoch: 6.2 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.858211128591815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.858211128591815 | validation: 1.2592677753330332]
	TIME [epoch: 6.19 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0716084571968807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0716084571968807 | validation: 0.8383261521998285]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.006214928534079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006214928534079 | validation: 1.0273385153666474]
	TIME [epoch: 6.19 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7920337524537384		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.7920337524537384 | validation: 0.7191747153971564]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9809239696608492		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.9809239696608492 | validation: 0.9194203186886792]
	TIME [epoch: 6.2 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6976390870630711		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.6976390870630711 | validation: 0.528063261190867]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8262193312680508		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.8262193312680508 | validation: 0.7296365654367374]
	TIME [epoch: 6.19 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8053994527081628		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.8053994527081628 | validation: 0.6020043266790436]
	TIME [epoch: 6.19 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7143228025866576		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.7143228025866576 | validation: 0.6332044349669546]
	TIME [epoch: 6.21 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8960271969028153		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.8960271969028153 | validation: 0.8493156486849714]
	TIME [epoch: 6.19 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6412490664796306		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.6412490664796306 | validation: 0.9185806467863467]
	TIME [epoch: 6.19 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0816752194788983		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.0816752194788983 | validation: 0.6108720243890771]
	TIME [epoch: 6.19 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8858218097503923		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.8858218097503923 | validation: 0.8992837337802253]
	TIME [epoch: 6.19 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.808875218019508		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.808875218019508 | validation: 0.7085169417707193]
	TIME [epoch: 6.21 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5957378576155108		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.5957378576155108 | validation: 0.8812923372014944]
	TIME [epoch: 6.19 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8858452937402578		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.8858452937402578 | validation: 0.604075614587721]
	TIME [epoch: 6.19 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6074429968119194		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.6074429968119194 | validation: 0.6935610317293374]
	TIME [epoch: 6.19 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8941360560472665		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.8941360560472665 | validation: 0.5924161490507698]
	TIME [epoch: 6.2 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49353514733289844		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.49353514733289844 | validation: 0.49797050637046103]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8713727324612903		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.8713727324612903 | validation: 0.5823952567650459]
	TIME [epoch: 6.19 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5904087692260969		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5904087692260969 | validation: 0.5927784739746009]
	TIME [epoch: 6.2 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.520281599050064		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.520281599050064 | validation: 1.2021555888813555]
	TIME [epoch: 6.19 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8391841031361895		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.8391841031361895 | validation: 0.44351438224900863]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.510933832433251		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.510933832433251 | validation: 0.507700854937822]
	TIME [epoch: 6.2 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7228762965957927		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.7228762965957927 | validation: 0.7065462210842615]
	TIME [epoch: 6.21 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5165403688393204		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5165403688393204 | validation: 0.790753644911028]
	TIME [epoch: 6.19 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.636648871337518		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.636648871337518 | validation: 0.9587535665674682]
	TIME [epoch: 6.19 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6329417096127015		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6329417096127015 | validation: 0.34788487862320805]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5620636258386748		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5620636258386748 | validation: 0.8556772326591249]
	TIME [epoch: 6.19 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6553453506806509		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.6553453506806509 | validation: 0.454222924878539]
	TIME [epoch: 6.18 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5125689528604369		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5125689528604369 | validation: 0.4609245917154071]
	TIME [epoch: 6.18 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3642396419552896		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.3642396419552896 | validation: 0.3966225602202067]
	TIME [epoch: 6.19 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6536107639093709		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.6536107639093709 | validation: 0.7737286816607238]
	TIME [epoch: 6.2 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5354481973539938		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.5354481973539938 | validation: 0.49489737465449746]
	TIME [epoch: 6.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5127665761068964		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.5127665761068964 | validation: 0.3247317359441181]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4506276147109773		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.4506276147109773 | validation: 0.47161613474634373]
	TIME [epoch: 6.19 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49307277210715017		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.49307277210715017 | validation: 0.4019926303470419]
	TIME [epoch: 6.19 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4744082536620724		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4744082536620724 | validation: 0.38248247137780333]
	TIME [epoch: 6.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4454602880441945		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.4454602880441945 | validation: 0.954982115868237]
	TIME [epoch: 6.19 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6459449009418331		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.6459449009418331 | validation: 0.4163970914123135]
	TIME [epoch: 6.19 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37729785731025517		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.37729785731025517 | validation: 0.7371517356303685]
	TIME [epoch: 6.19 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5514778597867567		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.5514778597867567 | validation: 0.41181614270682876]
	TIME [epoch: 6.2 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5383301575933607		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.5383301575933607 | validation: 0.625356536831148]
	TIME [epoch: 6.2 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5050341448429102		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.5050341448429102 | validation: 0.43485866911608084]
	TIME [epoch: 6.18 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44060006463811197		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.44060006463811197 | validation: 0.6808310523407641]
	TIME [epoch: 6.22 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47265720114422755		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.47265720114422755 | validation: 0.38468858452920857]
	TIME [epoch: 6.19 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42530407647610635		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.42530407647610635 | validation: 0.41057365339617125]
	TIME [epoch: 6.21 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40429430609356337		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.40429430609356337 | validation: 0.6329330734988059]
	TIME [epoch: 6.21 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5281183118234984		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.5281183118234984 | validation: 0.398625437195294]
	TIME [epoch: 6.19 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3211134836734576		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.3211134836734576 | validation: 0.3401622322742145]
	TIME [epoch: 6.2 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6000130064328779		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.6000130064328779 | validation: 0.5445680735310897]
	TIME [epoch: 6.19 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4849846593679824		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.4849846593679824 | validation: 0.4278641488760692]
	TIME [epoch: 6.2 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3322930286657537		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3322930286657537 | validation: 0.29912429334777724]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4886124476848408		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.4886124476848408 | validation: 0.4540771180615922]
	TIME [epoch: 6.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4325651778153292		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.4325651778153292 | validation: 0.3472741469256917]
	TIME [epoch: 6.2 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101724872670882		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.3101724872670882 | validation: 0.43785688696685665]
	TIME [epoch: 6.2 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4387873826342662		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.4387873826342662 | validation: 0.466387242488746]
	TIME [epoch: 6.2 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33918030220874973		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.33918030220874973 | validation: 0.28775554967464134]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40604374048812586		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.40604374048812586 | validation: 0.544433600190026]
	TIME [epoch: 6.19 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4388708144482393		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.4388708144482393 | validation: 0.6692606101991763]
	TIME [epoch: 6.19 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45379376632651525		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.45379376632651525 | validation: 0.4342987797049426]
	TIME [epoch: 6.2 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3316779082741973		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.3316779082741973 | validation: 0.2840910927056035]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2740241718115459		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2740241718115459 | validation: 0.43461352916430496]
	TIME [epoch: 6.21 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.576875565383536		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.576875565383536 | validation: 0.6982674168834198]
	TIME [epoch: 6.21 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4635814903185541		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.4635814903185541 | validation: 0.2925075219557125]
	TIME [epoch: 6.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.340617209151643		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.340617209151643 | validation: 0.6876118344245925]
	TIME [epoch: 6.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4328678868342149		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.4328678868342149 | validation: 0.34228639129534555]
	TIME [epoch: 6.2 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.312342428087665		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.312342428087665 | validation: 0.299551268717999]
	TIME [epoch: 6.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4280703402142454		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.4280703402142454 | validation: 0.4187525778315212]
	TIME [epoch: 6.2 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3006362457528598		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.3006362457528598 | validation: 0.29133998014230383]
	TIME [epoch: 6.2 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31084917164045445		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.31084917164045445 | validation: 0.7181770024277665]
	TIME [epoch: 6.22 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5641470143643162		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.5641470143643162 | validation: 0.47114507818827567]
	TIME [epoch: 6.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39561507967723636		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.39561507967723636 | validation: 0.2723570589694608]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2565761499155559		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2565761499155559 | validation: 0.34137394805796795]
	TIME [epoch: 6.21 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.312370312195552		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.312370312195552 | validation: 0.24182002804573932]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28115131179497715		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.28115131179497715 | validation: 0.26118936305421425]
	TIME [epoch: 6.21 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35495531858607254		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.35495531858607254 | validation: 0.3017226086672652]
	TIME [epoch: 6.21 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2950334672456966		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.2950334672456966 | validation: 0.25659690496046106]
	TIME [epoch: 6.21 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3760244265268737		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.3760244265268737 | validation: 0.3384410803188963]
	TIME [epoch: 6.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3352098056463605		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.3352098056463605 | validation: 0.30171976481722695]
	TIME [epoch: 6.21 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29536304445605555		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.29536304445605555 | validation: 0.2478149412922636]
	TIME [epoch: 6.21 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2551567370054667		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2551567370054667 | validation: 0.3492961355723627]
	TIME [epoch: 6.2 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36723264356675295		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.36723264356675295 | validation: 0.5069276356106129]
	TIME [epoch: 6.2 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33011544776120544		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.33011544776120544 | validation: 0.22499206409559544]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2526722286464198		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.2526722286464198 | validation: 0.24530072053634697]
	TIME [epoch: 6.21 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3685325005081235		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3685325005081235 | validation: 0.4761858643272664]
	TIME [epoch: 6.2 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38955199345956876		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.38955199345956876 | validation: 0.42886212262604917]
	TIME [epoch: 6.2 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2791844944590064		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.2791844944590064 | validation: 0.2188236136359848]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2365196600634965		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2365196600634965 | validation: 0.3681523210451701]
	TIME [epoch: 6.21 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3524490900873486		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.3524490900873486 | validation: 0.35298362036032915]
	TIME [epoch: 6.21 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30832043099129597		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.30832043099129597 | validation: 0.3249848972813806]
	TIME [epoch: 6.21 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28858216917573226		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.28858216917573226 | validation: 0.25559292444047366]
	TIME [epoch: 6.21 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19472745242854242		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.19472745242854242 | validation: 0.2655242671103098]
	TIME [epoch: 6.2 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38662105691659854		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.38662105691659854 | validation: 0.4094947863480365]
	TIME [epoch: 6.2 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27159749523660165		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.27159749523660165 | validation: 0.2554961322007896]
	TIME [epoch: 6.21 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22249951834444276		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.22249951834444276 | validation: 0.42237865210829995]
	TIME [epoch: 6.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2954439237543729		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2954439237543729 | validation: 0.41165981128335316]
	TIME [epoch: 6.21 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3081844682715038		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.3081844682715038 | validation: 0.2744542646151755]
	TIME [epoch: 6.21 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26884848469302935		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.26884848469302935 | validation: 0.31398731067880736]
	TIME [epoch: 6.21 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853946250602381		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2853946250602381 | validation: 0.2832719105505835]
	TIME [epoch: 6.2 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665267001002144		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.2665267001002144 | validation: 0.21665164899867914]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2311689596171828		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.2311689596171828 | validation: 0.351424734219415]
	TIME [epoch: 6.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3114831735833084		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.3114831735833084 | validation: 0.27516536709818296]
	TIME [epoch: 6.19 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24040257158763323		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.24040257158763323 | validation: 0.24329236892841116]
	TIME [epoch: 6.2 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.232185823337112		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.232185823337112 | validation: 0.4375346600438297]
	TIME [epoch: 6.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35001503367645587		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.35001503367645587 | validation: 0.19868145760674427]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19278540547643622		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.19278540547643622 | validation: 0.230188163552783]
	TIME [epoch: 6.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33447476522634706		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.33447476522634706 | validation: 0.2826357938345801]
	TIME [epoch: 6.19 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24090637706217552		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.24090637706217552 | validation: 0.28332278926163346]
	TIME [epoch: 6.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25316666826873707		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.25316666826873707 | validation: 0.23331310488370993]
	TIME [epoch: 6.2 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23394696998568926		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.23394696998568926 | validation: 0.4782818857235183]
	TIME [epoch: 6.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2905389006712403		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2905389006712403 | validation: 0.20160857733543305]
	TIME [epoch: 6.2 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17637737311062357		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.17637737311062357 | validation: 0.279903636779857]
	TIME [epoch: 6.19 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30342571476877556		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.30342571476877556 | validation: 0.21682477528506394]
	TIME [epoch: 6.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24322671170613475		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.24322671170613475 | validation: 0.2749171866435365]
	TIME [epoch: 6.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2533921604193494		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.2533921604193494 | validation: 0.25022729519745457]
	TIME [epoch: 6.21 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18744733175246597		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.18744733175246597 | validation: 0.20521746332911608]
	TIME [epoch: 6.2 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28652038370096095		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.28652038370096095 | validation: 0.22227953654314592]
	TIME [epoch: 6.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1899866607502068		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.1899866607502068 | validation: 0.2264053304108352]
	TIME [epoch: 6.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2301757959955945		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.2301757959955945 | validation: 0.272888902674349]
	TIME [epoch: 6.2 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2412321790645452		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.2412321790645452 | validation: 0.20485265795551802]
	TIME [epoch: 6.2 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1858148982469284		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.1858148982469284 | validation: 0.27056407553268436]
	TIME [epoch: 6.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22778364533033224		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.22778364533033224 | validation: 0.25895352273027983]
	TIME [epoch: 6.2 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23570093488368854		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.23570093488368854 | validation: 0.35452597827287735]
	TIME [epoch: 6.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298039529403897		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.298039529403897 | validation: 0.20119800602304405]
	TIME [epoch: 6.2 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1897437028385569		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.1897437028385569 | validation: 0.21396581100809892]
	TIME [epoch: 6.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20799494566196614		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.20799494566196614 | validation: 0.2129157418843991]
	TIME [epoch: 6.21 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19930956501900782		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.19930956501900782 | validation: 0.27653369813321405]
	TIME [epoch: 6.21 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19244523739681407		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.19244523739681407 | validation: 0.27859167182247163]
	TIME [epoch: 6.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2338125066692513		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.2338125066692513 | validation: 0.22036905618471472]
	TIME [epoch: 6.2 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20683996921202594		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.20683996921202594 | validation: 0.20232995056416125]
	TIME [epoch: 6.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22432992937516574		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.22432992937516574 | validation: 0.25011924361431037]
	TIME [epoch: 6.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17343724003487468		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.17343724003487468 | validation: 0.2023599025848502]
	TIME [epoch: 6.21 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1605423213969684		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.1605423213969684 | validation: 0.1966945709159482]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29968148723379556		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.29968148723379556 | validation: 0.1866138464749546]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16071335796472463		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.16071335796472463 | validation: 0.28329122343427043]
	TIME [epoch: 6.19 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18756567265552504		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.18756567265552504 | validation: 0.19726652677166168]
	TIME [epoch: 6.21 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.211742668046996		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.211742668046996 | validation: 0.17686901829404192]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2267244029135287		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.2267244029135287 | validation: 0.2304843039543301]
	TIME [epoch: 6.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17340585638875683		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.17340585638875683 | validation: 0.29098759856987266]
	TIME [epoch: 6.21 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29950341295272065		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.29950341295272065 | validation: 0.22644307153589352]
	TIME [epoch: 6.19 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18038146682498338		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.18038146682498338 | validation: 0.2561334291142263]
	TIME [epoch: 6.21 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17496545336504565		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.17496545336504565 | validation: 0.20503325668614314]
	TIME [epoch: 6.21 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19211234451004877		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.19211234451004877 | validation: 0.3233971454215141]
	TIME [epoch: 6.18 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25830543597549543		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.25830543597549543 | validation: 0.2780379506910169]
	TIME [epoch: 6.21 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20463365706984485		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.20463365706984485 | validation: 0.19421321442034767]
	TIME [epoch: 6.21 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15217601442067458		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.15217601442067458 | validation: 0.1470985494529457]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17378471514672233		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.17378471514672233 | validation: 0.18573407746688975]
	TIME [epoch: 6.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16640995735779335		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.16640995735779335 | validation: 0.18313044826896252]
	TIME [epoch: 6.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16462128282672983		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.16462128282672983 | validation: 0.23900055038048562]
	TIME [epoch: 6.21 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20929518169846087		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.20929518169846087 | validation: 0.177146147258247]
	TIME [epoch: 6.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14294348397764906		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.14294348397764906 | validation: 0.18623896525703587]
	TIME [epoch: 6.19 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16073228803526232		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.16073228803526232 | validation: 0.2538637894727831]
	TIME [epoch: 6.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19110071339903528		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.19110071339903528 | validation: 0.1678738858393249]
	TIME [epoch: 397 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13471600885470136		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.13471600885470136 | validation: 0.2818010241306169]
	TIME [epoch: 12.2 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17719120285626064		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.17719120285626064 | validation: 0.15152147346630865]
	TIME [epoch: 12.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14937393538812768		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.14937393538812768 | validation: 0.18857864998762777]
	TIME [epoch: 12.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25453720446187655		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.25453720446187655 | validation: 0.23842328553024716]
	TIME [epoch: 12.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16548991563009777		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.16548991563009777 | validation: 0.172688966737866]
	TIME [epoch: 12.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1413394606220632		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.1413394606220632 | validation: 0.1488607512513335]
	TIME [epoch: 12.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15889270308103356		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.15889270308103356 | validation: 0.1768783779640109]
	TIME [epoch: 12.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1844599728866424		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.1844599728866424 | validation: 0.158300284752464]
	TIME [epoch: 12.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1624212492611312		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1624212492611312 | validation: 0.17383623860502614]
	TIME [epoch: 12.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1705980513045228		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.1705980513045228 | validation: 0.1922852798627446]
	TIME [epoch: 12.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1517471001957439		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1517471001957439 | validation: 0.20120160831252248]
	TIME [epoch: 12.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14896880218681374		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.14896880218681374 | validation: 0.18771044050757668]
	TIME [epoch: 12.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18665805721178938		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.18665805721178938 | validation: 0.15910956390367953]
	TIME [epoch: 12.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13014412563294675		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.13014412563294675 | validation: 0.18546160953492494]
	TIME [epoch: 12.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12966495205554346		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.12966495205554346 | validation: 0.14626451935148935]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14336531593698995		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.14336531593698995 | validation: 0.23141059899704214]
	TIME [epoch: 12.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1521577699817664		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.1521577699817664 | validation: 0.17740826268736942]
	TIME [epoch: 12.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1988814668600173		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.1988814668600173 | validation: 0.21808322859862972]
	TIME [epoch: 12.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14384527274971418		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.14384527274971418 | validation: 0.14736858678639775]
	TIME [epoch: 12.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13120491730723108		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.13120491730723108 | validation: 0.17298211621938117]
	TIME [epoch: 12.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14449951175276632		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.14449951175276632 | validation: 0.14004096083583234]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1457987460343383		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.1457987460343383 | validation: 0.13989619603212727]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15213788567328007		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.15213788567328007 | validation: 0.17124216684857335]
	TIME [epoch: 12.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12320575913764818		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.12320575913764818 | validation: 0.13296231846973056]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12564828381935084		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.12564828381935084 | validation: 0.21393791538728874]
	TIME [epoch: 12.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14821082070830843		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.14821082070830843 | validation: 0.1580095101165349]
	TIME [epoch: 12.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11423405724050215		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.11423405724050215 | validation: 0.14917768331961553]
	TIME [epoch: 12.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1841089188516335		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.1841089188516335 | validation: 0.20001747030003156]
	TIME [epoch: 12.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1421386116144126		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.1421386116144126 | validation: 0.12050538529214408]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11745357274972254		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.11745357274972254 | validation: 0.13231674817756955]
	TIME [epoch: 12.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13573780265045932		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.13573780265045932 | validation: 0.1643832099743523]
	TIME [epoch: 12.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12584452750000058		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.12584452750000058 | validation: 0.17864168745104425]
	TIME [epoch: 12.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1524179296817001		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.1524179296817001 | validation: 0.1631467334110229]
	TIME [epoch: 12.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17309847796957673		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.17309847796957673 | validation: 0.1946349146446753]
	TIME [epoch: 12.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11776342095116867		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.11776342095116867 | validation: 0.11495038727688307]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12103986426140673		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.12103986426140673 | validation: 0.14285133752654772]
	TIME [epoch: 12.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1515018445321354		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1515018445321354 | validation: 0.13575601290901124]
	TIME [epoch: 12.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1277969248874602		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.1277969248874602 | validation: 0.1276892286315501]
	TIME [epoch: 12.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11293058379358221		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.11293058379358221 | validation: 0.3461627736843217]
	TIME [epoch: 12.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19643656091371464		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.19643656091371464 | validation: 0.1256230378464336]
	TIME [epoch: 12.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16112676663295106		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.16112676663295106 | validation: 0.16867946997377564]
	TIME [epoch: 12.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11767054312809287		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.11767054312809287 | validation: 0.11821974065913521]
	TIME [epoch: 12.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11902766564178566		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.11902766564178566 | validation: 0.15395660385610715]
	TIME [epoch: 12.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11286600976258483		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.11286600976258483 | validation: 0.17101594456242128]
	TIME [epoch: 12.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16092936092313126		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.16092936092313126 | validation: 0.1479082170979888]
	TIME [epoch: 12.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10702664451588434		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.10702664451588434 | validation: 0.1265849606287056]
	TIME [epoch: 12.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09477004896031228		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.09477004896031228 | validation: 0.24979894155595814]
	TIME [epoch: 12.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16947388918685924		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.16947388918685924 | validation: 0.13636688506300354]
	TIME [epoch: 12.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11268101649916004		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.11268101649916004 | validation: 0.2124160857806412]
	TIME [epoch: 12.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16297406444736753		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.16297406444736753 | validation: 0.17064524649206683]
	TIME [epoch: 12.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10666292349811264		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.10666292349811264 | validation: 0.10318653609861325]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10735876928763909		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.10735876928763909 | validation: 0.16358840405193562]
	TIME [epoch: 12.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11667184840272384		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.11667184840272384 | validation: 0.1605314048247728]
	TIME [epoch: 12.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1239106384626551		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.1239106384626551 | validation: 0.14899374568472323]
	TIME [epoch: 12.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11296487066347863		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.11296487066347863 | validation: 0.12027253277150823]
	TIME [epoch: 12.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1170835069823419		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.1170835069823419 | validation: 0.1624784848249175]
	TIME [epoch: 12.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.138631281039161		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.138631281039161 | validation: 0.12889446499885177]
	TIME [epoch: 12.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10375843650584347		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.10375843650584347 | validation: 0.11720084049649936]
	TIME [epoch: 12.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09268878188023288		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.09268878188023288 | validation: 0.2528091693433736]
	TIME [epoch: 12.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17323461827881345		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.17323461827881345 | validation: 0.12698971359079367]
	TIME [epoch: 12.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12772245331785542		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.12772245331785542 | validation: 0.1277058791313918]
	TIME [epoch: 12.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09901960867087305		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.09901960867087305 | validation: 0.1053865339372834]
	TIME [epoch: 12.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09874441411699746		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09874441411699746 | validation: 0.12081345638191456]
	TIME [epoch: 12.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09092398846595817		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.09092398846595817 | validation: 0.12315948655436283]
	TIME [epoch: 12.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12064275939809183		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.12064275939809183 | validation: 0.10773590833595978]
	TIME [epoch: 12.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09219172169610285		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.09219172169610285 | validation: 0.16664717415861086]
	TIME [epoch: 12.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13166103850897576		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.13166103850897576 | validation: 0.1381320359158679]
	TIME [epoch: 12.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09671165394752929		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.09671165394752929 | validation: 0.09574244102297061]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15292304641136964		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.15292304641136964 | validation: 0.20791777553314097]
	TIME [epoch: 12.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14789405566386926		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.14789405566386926 | validation: 0.10701695452406529]
	TIME [epoch: 12.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08716236975882574		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.08716236975882574 | validation: 0.12161882205505892]
	TIME [epoch: 12.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09377102529280128		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.09377102529280128 | validation: 0.0894463329703177]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09626572882108556		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.09626572882108556 | validation: 0.14825843553241214]
	TIME [epoch: 12.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1073871390597019		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.1073871390597019 | validation: 0.09022980437048214]
	TIME [epoch: 12.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07845422564521028		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.07845422564521028 | validation: 0.17288285502024076]
	TIME [epoch: 12.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11911504228327364		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.11911504228327364 | validation: 0.14596648236007326]
	TIME [epoch: 12.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14487231064437192		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.14487231064437192 | validation: 0.09545551904135446]
	TIME [epoch: 12.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07986828575601798		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.07986828575601798 | validation: 0.11116615168730755]
	TIME [epoch: 12.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.096238181509141		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.096238181509141 | validation: 0.09926394624932008]
	TIME [epoch: 12.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10526064464482854		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.10526064464482854 | validation: 0.16323944184720152]
	TIME [epoch: 12.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12508666509920838		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.12508666509920838 | validation: 0.16383207992891805]
	TIME [epoch: 12.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0881903122768452		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.0881903122768452 | validation: 0.08506801351006527]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10484479945904215		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.10484479945904215 | validation: 0.09865367625713778]
	TIME [epoch: 12.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09247297147419403		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.09247297147419403 | validation: 0.12746616307581016]
	TIME [epoch: 12.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10626688291794818		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.10626688291794818 | validation: 0.08803314465173565]
	TIME [epoch: 12.2 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12107798810401756		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.12107798810401756 | validation: 0.12110766573472201]
	TIME [epoch: 12.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0856716862234987		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.0856716862234987 | validation: 0.10532727973658718]
	TIME [epoch: 12.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09117128262817813		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.09117128262817813 | validation: 0.09677446080529278]
	TIME [epoch: 12.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11000884645701445		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.11000884645701445 | validation: 0.09012562419981618]
	TIME [epoch: 12.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09154390636528636		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.09154390636528636 | validation: 0.10838597928700093]
	TIME [epoch: 12.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07438145928398103		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.07438145928398103 | validation: 0.08204151118254413]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11342983482006957		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.11342983482006957 | validation: 0.12829915105058412]
	TIME [epoch: 12.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0960002938033823		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.0960002938033823 | validation: 0.11825800838629556]
	TIME [epoch: 12.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10103709626263455		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.10103709626263455 | validation: 0.1306778405378368]
	TIME [epoch: 12.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08323355896270693		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.08323355896270693 | validation: 0.11028960940662795]
	TIME [epoch: 12.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07575472939374447		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.07575472939374447 | validation: 0.10033960519119625]
	TIME [epoch: 12.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0966918059565062		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.0966918059565062 | validation: 0.1113471016205583]
	TIME [epoch: 12.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08837202892717638		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.08837202892717638 | validation: 0.1613399670054294]
	TIME [epoch: 12.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11327073656685277		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.11327073656685277 | validation: 0.09740748351918975]
	TIME [epoch: 12.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08859252964745376		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.08859252964745376 | validation: 0.09148108360884158]
	TIME [epoch: 12.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07381602330663262		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.07381602330663262 | validation: 0.15238340140029658]
	TIME [epoch: 12.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14655157569179136		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.14655157569179136 | validation: 0.10038581113296452]
	TIME [epoch: 12.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07870420924953267		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.07870420924953267 | validation: 0.08526503528049652]
	TIME [epoch: 12.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.088772761938797		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.088772761938797 | validation: 0.07974972317446025]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08431582732652117		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.08431582732652117 | validation: 0.09992709371927355]
	TIME [epoch: 12.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08369010383501846		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.08369010383501846 | validation: 0.09725763589284098]
	TIME [epoch: 12.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08210706305376352		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.08210706305376352 | validation: 0.09839451204633831]
	TIME [epoch: 12.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12241383015574087		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.12241383015574087 | validation: 0.12984179341750957]
	TIME [epoch: 12.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09134801152679621		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.09134801152679621 | validation: 0.09382853391822579]
	TIME [epoch: 12.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09164811068725999		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.09164811068725999 | validation: 0.07289749107386398]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640493219120773		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.0640493219120773 | validation: 0.09985541098777191]
	TIME [epoch: 12.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16386695313554075		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.16386695313554075 | validation: 0.12515093041264488]
	TIME [epoch: 12.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07922797042075072		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.07922797042075072 | validation: 0.07844141486612294]
	TIME [epoch: 12.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08651962761284328		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.08651962761284328 | validation: 0.09969486590851964]
	TIME [epoch: 12.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08559810680814285		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.08559810680814285 | validation: 0.1100710249485514]
	TIME [epoch: 12.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08614532568616654		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.08614532568616654 | validation: 0.12689427840727546]
	TIME [epoch: 12.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0795629670110659		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.0795629670110659 | validation: 0.09173036501843435]
	TIME [epoch: 12.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07861245051553295		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.07861245051553295 | validation: 0.09498383019195865]
	TIME [epoch: 12.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09527753457983165		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.09527753457983165 | validation: 0.0908345044588465]
	TIME [epoch: 12.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07755786669946294		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.07755786669946294 | validation: 0.0790214800372756]
	TIME [epoch: 12.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07533957252177859		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07533957252177859 | validation: 0.11149674867993203]
	TIME [epoch: 12.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09374916690795708		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.09374916690795708 | validation: 0.07018724609446295]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0658111034933802		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.0658111034933802 | validation: 0.15114037703946365]
	TIME [epoch: 12.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08967935556346408		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.08967935556346408 | validation: 0.1189145468931872]
	TIME [epoch: 12.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08035294129396879		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.08035294129396879 | validation: 0.07090879808843231]
	TIME [epoch: 12.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0806904485537214		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.0806904485537214 | validation: 0.08076846726772408]
	TIME [epoch: 12.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0705413359102895		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.0705413359102895 | validation: 0.0665649203877133]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055757246472469456		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.055757246472469456 | validation: 0.08191398242058848]
	TIME [epoch: 12.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08683996278981797		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.08683996278981797 | validation: 0.06898382590105606]
	TIME [epoch: 12.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07189581574519496		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.07189581574519496 | validation: 0.14685282433257685]
	TIME [epoch: 12.2 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0993298927399714		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.0993298927399714 | validation: 0.0704127146243104]
	TIME [epoch: 12.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07545323308915818		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.07545323308915818 | validation: 0.08959461449552422]
	TIME [epoch: 12.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0775465856106502		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.0775465856106502 | validation: 0.07999364172012294]
	TIME [epoch: 12.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08886523082690281		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.08886523082690281 | validation: 0.09259747357038656]
	TIME [epoch: 12.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06497110907978353		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.06497110907978353 | validation: 0.0714326788292602]
	TIME [epoch: 12.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19387254379938548		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.19387254379938548 | validation: 0.19049870105178518]
	TIME [epoch: 12.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10147281280464551		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.10147281280464551 | validation: 0.06323751008705368]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06515319246420981		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.06515319246420981 | validation: 0.07778607490018613]
	TIME [epoch: 12.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06027044766880311		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.06027044766880311 | validation: 0.11651372918304105]
	TIME [epoch: 12.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07735330496249872		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.07735330496249872 | validation: 0.07122153082879248]
	TIME [epoch: 12.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06530541887055379		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.06530541887055379 | validation: 0.08777720136476166]
	TIME [epoch: 12.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061753920145743324		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.061753920145743324 | validation: 0.08827648634770321]
	TIME [epoch: 12.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11342394627574179		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.11342394627574179 | validation: 0.09874635284769319]
	TIME [epoch: 12.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06567852183879948		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.06567852183879948 | validation: 0.06927286511455705]
	TIME [epoch: 12.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06207988331892093		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.06207988331892093 | validation: 0.09514869662737503]
	TIME [epoch: 12.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059465885735462964		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.059465885735462964 | validation: 0.07428935987620325]
	TIME [epoch: 12.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06904618284920243		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.06904618284920243 | validation: 0.08226937798258235]
	TIME [epoch: 12.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05733335178418091		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.05733335178418091 | validation: 0.06899145794297495]
	TIME [epoch: 12.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09144255053541821		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.09144255053541821 | validation: 0.08755127629091276]
	TIME [epoch: 12.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0672402954256825		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.0672402954256825 | validation: 0.08905375445129537]
	TIME [epoch: 12.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07075949288780256		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07075949288780256 | validation: 0.0798030821918046]
	TIME [epoch: 12.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.070255932044787		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.070255932044787 | validation: 0.12216935797416748]
	TIME [epoch: 12.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08586155088106165		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.08586155088106165 | validation: 0.0573266980027521]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05157160648276773		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.05157160648276773 | validation: 0.05255930624818474]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056171105553594836		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.056171105553594836 | validation: 0.09733961972640523]
	TIME [epoch: 12.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09309838191744602		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.09309838191744602 | validation: 0.07886143730855288]
	TIME [epoch: 12.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06612052866402061		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.06612052866402061 | validation: 0.05644757881957027]
	TIME [epoch: 12.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07062898622529472		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.07062898622529472 | validation: 0.10928229774758676]
	TIME [epoch: 12.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06704887244745954		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.06704887244745954 | validation: 0.08759384726976475]
	TIME [epoch: 12.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06966394522072963		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.06966394522072963 | validation: 0.06714532063506834]
	TIME [epoch: 12.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05341442528856084		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.05341442528856084 | validation: 0.05463680689424591]
	TIME [epoch: 12.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08106622214070577		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.08106622214070577 | validation: 0.06640393558364391]
	TIME [epoch: 12.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059961221498720164		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.059961221498720164 | validation: 0.07305448291084826]
	TIME [epoch: 12.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05683424223305918		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.05683424223305918 | validation: 0.0661616062755643]
	TIME [epoch: 12.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04932969360054618		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.04932969360054618 | validation: 0.09403155263136323]
	TIME [epoch: 12.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07508823729573491		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.07508823729573491 | validation: 0.06602310961178107]
	TIME [epoch: 12.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10114641041298013		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.10114641041298013 | validation: 0.07204376701627857]
	TIME [epoch: 12.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057256532511136514		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.057256532511136514 | validation: 0.05975405535457462]
	TIME [epoch: 12.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0512764119033843		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.0512764119033843 | validation: 0.0881030921830964]
	TIME [epoch: 12.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07164752994649905		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.07164752994649905 | validation: 0.07480107751556025]
	TIME [epoch: 12.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057726468661553065		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.057726468661553065 | validation: 0.05058757183513663]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05304175968489827		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.05304175968489827 | validation: 0.0707309883819516]
	TIME [epoch: 12.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06451443101863116		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.06451443101863116 | validation: 0.09119069326022498]
	TIME [epoch: 12.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08040677438039284		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.08040677438039284 | validation: 0.0720988413830145]
	TIME [epoch: 12.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057087776335027166		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.057087776335027166 | validation: 0.058032517843165835]
	TIME [epoch: 12.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06739823999133086		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.06739823999133086 | validation: 0.052361347625108745]
	TIME [epoch: 12.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04250184913875754		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.04250184913875754 | validation: 0.061026862942986995]
	TIME [epoch: 12.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05788018325653281		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.05788018325653281 | validation: 0.0651412666611429]
	TIME [epoch: 12.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05108587885115574		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.05108587885115574 | validation: 0.06881529720864071]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055585025502854345		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.055585025502854345 | validation: 0.06521141540910402]
	TIME [epoch: 12.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07507683925239694		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.07507683925239694 | validation: 0.07794411742216673]
	TIME [epoch: 12.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08591720199814913		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.08591720199814913 | validation: 0.07318289243401879]
	TIME [epoch: 12.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048915818518049464		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.048915818518049464 | validation: 0.046610244526419695]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04372495188001834		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.04372495188001834 | validation: 0.04920882740480345]
	TIME [epoch: 12.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06337075038229373		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.06337075038229373 | validation: 0.06670779950491143]
	TIME [epoch: 12.2 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05087979064413953		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.05087979064413953 | validation: 0.07091441424495086]
	TIME [epoch: 12.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0605925333962849		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.0605925333962849 | validation: 0.06085051070358366]
	TIME [epoch: 12.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05606640733350998		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.05606640733350998 | validation: 0.05662123456308894]
	TIME [epoch: 12.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0499434246327116		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.0499434246327116 | validation: 0.05424202937481555]
	TIME [epoch: 12.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07889950394937754		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.07889950394937754 | validation: 0.09269490265464153]
	TIME [epoch: 12.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05987926750886381		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.05987926750886381 | validation: 0.05709333777253943]
	TIME [epoch: 12.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044855796741143254		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.044855796741143254 | validation: 0.04805114513631033]
	TIME [epoch: 12.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048352012983496194		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.048352012983496194 | validation: 0.056561876908797]
	TIME [epoch: 12.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05269540174357305		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.05269540174357305 | validation: 0.0653987173434279]
	TIME [epoch: 12.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06730153348935076		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.06730153348935076 | validation: 0.06936753951483189]
	TIME [epoch: 12.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04672370174974022		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04672370174974022 | validation: 0.04809713464673266]
	TIME [epoch: 12.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05114801191341854		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.05114801191341854 | validation: 0.06616304366856741]
	TIME [epoch: 12.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04981457666489399		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.04981457666489399 | validation: 0.0775225922205434]
	TIME [epoch: 12.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06205882073169857		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.06205882073169857 | validation: 0.08303905394428662]
	TIME [epoch: 12.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047652846776736195		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.047652846776736195 | validation: 0.06051643796486465]
	TIME [epoch: 12.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04371212108452362		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.04371212108452362 | validation: 0.08504112733514547]
	TIME [epoch: 12.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06066610280878091		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.06066610280878091 | validation: 0.07589274707246643]
	TIME [epoch: 12.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05417725415190695		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.05417725415190695 | validation: 0.04536521116813509]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05685884530870172		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.05685884530870172 | validation: 0.06761318810119395]
	TIME [epoch: 12.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0491505425255825		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.0491505425255825 | validation: 0.10421164201964325]
	TIME [epoch: 12.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055761338412540634		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.055761338412540634 | validation: 0.05369339759413991]
	TIME [epoch: 12.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05793514600136233		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.05793514600136233 | validation: 0.0413374402364729]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056809356674945945		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.056809356674945945 | validation: 0.06416473201403859]
	TIME [epoch: 12.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0394749590566854		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.0394749590566854 | validation: 0.06705870520246315]
	TIME [epoch: 12.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050987672142240266		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.050987672142240266 | validation: 0.05895114398668343]
	TIME [epoch: 12.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0505255947874208		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.0505255947874208 | validation: 0.05150639545069867]
	TIME [epoch: 12.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04134717901087488		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.04134717901087488 | validation: 0.11826475855346491]
	TIME [epoch: 12.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07208224662346162		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.07208224662346162 | validation: 0.05453816914898512]
	TIME [epoch: 12.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04829971761966796		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.04829971761966796 | validation: 0.04729483959227285]
	TIME [epoch: 12.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03386237007981936		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.03386237007981936 | validation: 0.0441621436228104]
	TIME [epoch: 12.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061219326490695336		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.061219326490695336 | validation: 0.06839617628970077]
	TIME [epoch: 12.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048174721392063566		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.048174721392063566 | validation: 0.06075752946547789]
	TIME [epoch: 12.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04130548689305788		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.04130548689305788 | validation: 0.054855327425624184]
	TIME [epoch: 12.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04904659307075051		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.04904659307075051 | validation: 0.05853000279606723]
	TIME [epoch: 12.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05001553691278918		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.05001553691278918 | validation: 0.03687181205874218]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037390135826353185		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.037390135826353185 | validation: 0.04749435373746511]
	TIME [epoch: 12.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046927939812624275		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.046927939812624275 | validation: 0.051747434779062726]
	TIME [epoch: 12.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03645288953139736		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.03645288953139736 | validation: 0.049608908630286906]
	TIME [epoch: 12.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061800049245264846		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.061800049245264846 | validation: 0.038204951144620095]
	TIME [epoch: 12.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04029406107155712		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.04029406107155712 | validation: 0.05402577358927305]
	TIME [epoch: 12.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04678827649330408		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.04678827649330408 | validation: 0.04985339993088554]
	TIME [epoch: 12.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04474087722608791		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.04474087722608791 | validation: 0.03793549990280773]
	TIME [epoch: 12.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037128487081933156		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.037128487081933156 | validation: 0.042381013027832454]
	TIME [epoch: 12.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061646544511894656		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.061646544511894656 | validation: 0.041242811878825905]
	TIME [epoch: 12.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047964690891289866		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.047964690891289866 | validation: 0.04780831462729425]
	TIME [epoch: 12.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03962802290348502		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.03962802290348502 | validation: 0.04230499641838062]
	TIME [epoch: 12.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04061798944094842		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.04061798944094842 | validation: 0.04741714253031924]
	TIME [epoch: 12.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04137426120553359		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.04137426120553359 | validation: 0.042037341670114026]
	TIME [epoch: 12.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03581960133660824		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.03581960133660824 | validation: 0.04130514173927785]
	TIME [epoch: 12.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04160456800443373		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.04160456800443373 | validation: 0.07256905527779671]
	TIME [epoch: 12.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07716173681339947		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.07716173681339947 | validation: 0.05076026811012915]
	TIME [epoch: 12.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04039456580860756		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.04039456580860756 | validation: 0.031197924981426397]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03945314674542487		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.03945314674542487 | validation: 0.04322035925009353]
	TIME [epoch: 12.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03204050354302187		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.03204050354302187 | validation: 0.03645083396362371]
	TIME [epoch: 12.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041255898314210025		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.041255898314210025 | validation: 0.05052182202224956]
	TIME [epoch: 12.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039557339512605955		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.039557339512605955 | validation: 0.062472799192210285]
	TIME [epoch: 12.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044995015041368644		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.044995015041368644 | validation: 0.0572950605538714]
	TIME [epoch: 12.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039781772891724645		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.039781772891724645 | validation: 0.05217497980614185]
	TIME [epoch: 12.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035432031135451576		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.035432031135451576 | validation: 0.06714931874415349]
	TIME [epoch: 12.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04905173756679381		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04905173756679381 | validation: 0.038858355173310224]
	TIME [epoch: 12.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03430732364667252		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.03430732364667252 | validation: 0.05880986610516002]
	TIME [epoch: 12.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0348881253915511		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0348881253915511 | validation: 0.03575273315629574]
	TIME [epoch: 12.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04124089374261522		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.04124089374261522 | validation: 0.047910317343559605]
	TIME [epoch: 12.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05036043555866228		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.05036043555866228 | validation: 0.04221962631968091]
	TIME [epoch: 12.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03573255912261019		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.03573255912261019 | validation: 0.07968645835057625]
	TIME [epoch: 12.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04352341761544526		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.04352341761544526 | validation: 0.04786388340942172]
	TIME [epoch: 12.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03195059870843686		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03195059870843686 | validation: 0.04539243219175157]
	TIME [epoch: 12.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030161635657241885		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.030161635657241885 | validation: 0.04642386626053913]
	TIME [epoch: 12.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03894859204531845		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.03894859204531845 | validation: 0.08366762728806013]
	TIME [epoch: 12.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0647460061768805		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.0647460061768805 | validation: 0.034477775867835035]
	TIME [epoch: 12.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0369390846620665		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.0369390846620665 | validation: 0.03829698444707648]
	TIME [epoch: 12.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025378822879980452		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.025378822879980452 | validation: 0.03434502422796305]
	TIME [epoch: 12.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03620149580546979		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03620149580546979 | validation: 0.04367176713770592]
	TIME [epoch: 12.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03803304404897676		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.03803304404897676 | validation: 0.03520098440264884]
	TIME [epoch: 12.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04633076971928548		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.04633076971928548 | validation: 0.0746352013285937]
	TIME [epoch: 12.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053945147960619826		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.053945147960619826 | validation: 0.05851600516382863]
	TIME [epoch: 12.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03134969560309769		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.03134969560309769 | validation: 0.03916294638734423]
	TIME [epoch: 12.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032553128259017156		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.032553128259017156 | validation: 0.05914935503696956]
	TIME [epoch: 12.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04634276482963867		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.04634276482963867 | validation: 0.03653973201732928]
	TIME [epoch: 12.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02782063862959267		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.02782063862959267 | validation: 0.04444827486277493]
	TIME [epoch: 12.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03398340588759649		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.03398340588759649 | validation: 0.05433270245719923]
	TIME [epoch: 12.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04853279522993987		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.04853279522993987 | validation: 0.039026293294510045]
	TIME [epoch: 12.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04189878851107978		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.04189878851107978 | validation: 0.03469374312134061]
	TIME [epoch: 12.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02627354062852995		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.02627354062852995 | validation: 0.0550658535019079]
	TIME [epoch: 12.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04024777868985027		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.04024777868985027 | validation: 0.044510978193235906]
	TIME [epoch: 12.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03531457111472089		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.03531457111472089 | validation: 0.052939093351970515]
	TIME [epoch: 12.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037666033326395024		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.037666033326395024 | validation: 0.05421871449765882]
	TIME [epoch: 12.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0433890454841587		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.0433890454841587 | validation: 0.0393767066087697]
	TIME [epoch: 12.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02574233342399903		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.02574233342399903 | validation: 0.04664663951883568]
	TIME [epoch: 12.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03870877450144513		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.03870877450144513 | validation: 0.034875758790206005]
	TIME [epoch: 12.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03632443287504833		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.03632443287504833 | validation: 0.03835565432068771]
	TIME [epoch: 12.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02868515241021804		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.02868515241021804 | validation: 0.0532074161222859]
	TIME [epoch: 12.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03926587161680827		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.03926587161680827 | validation: 0.02882715436569331]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027431996241335875		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.027431996241335875 | validation: 0.08460044760473133]
	TIME [epoch: 12.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052978611146817886		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.052978611146817886 | validation: 0.041563804580946426]
	TIME [epoch: 12.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02968383361661777		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.02968383361661777 | validation: 0.028639453688698503]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030481022340044198		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.030481022340044198 | validation: 0.03725352715656847]
	TIME [epoch: 12.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0399531236689181		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0399531236689181 | validation: 0.0443421166078181]
	TIME [epoch: 12.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030335701069017267		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.030335701069017267 | validation: 0.03763180823926707]
	TIME [epoch: 12.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037279027988022864		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.037279027988022864 | validation: 0.04364690322902354]
	TIME [epoch: 12.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02889800098589995		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.02889800098589995 | validation: 0.04406249965003258]
	TIME [epoch: 12.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03095910652539325		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.03095910652539325 | validation: 0.06217552315261868]
	TIME [epoch: 12.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03022997347670787		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03022997347670787 | validation: 0.03321293510509988]
	TIME [epoch: 12.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04009686002196711		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.04009686002196711 | validation: 0.05927601174297286]
	TIME [epoch: 12.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033635106226952045		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.033635106226952045 | validation: 0.035082214436133255]
	TIME [epoch: 12.2 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030766798069638626		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.030766798069638626 | validation: 0.031626009004514934]
	TIME [epoch: 12.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026320670062546757		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.026320670062546757 | validation: 0.04430106554561987]
	TIME [epoch: 12.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034493127522944804		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.034493127522944804 | validation: 0.06596664761332084]
	TIME [epoch: 12.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0356613974462794		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.0356613974462794 | validation: 0.04259768493971054]
	TIME [epoch: 12.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031148411763166666		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.031148411763166666 | validation: 0.06127720550573241]
	TIME [epoch: 12.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03412696714060827		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.03412696714060827 | validation: 0.03288099886298562]
	TIME [epoch: 12.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025157458916944535		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.025157458916944535 | validation: 0.04572974764381924]
	TIME [epoch: 12.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035764683594564285		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.035764683594564285 | validation: 0.030361961533087074]
	TIME [epoch: 12.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020732198602885818		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.020732198602885818 | validation: 0.03301556490722189]
	TIME [epoch: 12.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028253124756586537		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.028253124756586537 | validation: 0.030264563375373622]
	TIME [epoch: 419 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048512735233540646		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.048512735233540646 | validation: 0.04670917734333674]
	TIME [epoch: 26.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03217270899325984		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03217270899325984 | validation: 0.03395826911914347]
	TIME [epoch: 26.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02656931407685693		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.02656931407685693 | validation: 0.036105667947164036]
	TIME [epoch: 26.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03281089339460049		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.03281089339460049 | validation: 0.03568084040898402]
	TIME [epoch: 26.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03393544646282996		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.03393544646282996 | validation: 0.036340475535315934]
	TIME [epoch: 26.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030555032626834115		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.030555032626834115 | validation: 0.03853843655671789]
	TIME [epoch: 26.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02142042043990994		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.02142042043990994 | validation: 0.030963259856221375]
	TIME [epoch: 26.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02724323107848427		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.02724323107848427 | validation: 0.05121376530994626]
	TIME [epoch: 26.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038354865954218144		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.038354865954218144 | validation: 0.03707634072708118]
	TIME [epoch: 26.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027851306926846196		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.027851306926846196 | validation: 0.030356000102110484]
	TIME [epoch: 26.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022281890551881834		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.022281890551881834 | validation: 0.044815436543083016]
	TIME [epoch: 26.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03780631133212781		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.03780631133212781 | validation: 0.07306136038129914]
	TIME [epoch: 26.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03467663804701263		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.03467663804701263 | validation: 0.03575913613323432]
	TIME [epoch: 26.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022383289050445957		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.022383289050445957 | validation: 0.038532161042960034]
	TIME [epoch: 26.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03191797998300422		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.03191797998300422 | validation: 0.04329868137257899]
	TIME [epoch: 26.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02386906423449278		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.02386906423449278 | validation: 0.03829552419867708]
	TIME [epoch: 26.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027034233275221885		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.027034233275221885 | validation: 0.0318171620456211]
	TIME [epoch: 26.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0384335191644265		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.0384335191644265 | validation: 0.03448654791723678]
	TIME [epoch: 26.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03297303902301948		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.03297303902301948 | validation: 0.028812627904945973]
	TIME [epoch: 26.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027964301026576533		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.027964301026576533 | validation: 0.02679466602451601]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018930742169672374		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.018930742169672374 | validation: 0.03281511860288277]
	TIME [epoch: 26.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034304023296288365		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.034304023296288365 | validation: 0.04358746674199167]
	TIME [epoch: 26.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13538341001317278		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.13538341001317278 | validation: 0.08663001188110292]
	TIME [epoch: 26.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04235286568980913		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.04235286568980913 | validation: 0.028569607654005343]
	TIME [epoch: 26.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022890031558831448		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.022890031558831448 | validation: 0.034781249383057766]
	TIME [epoch: 26.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021347696087701505		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.021347696087701505 | validation: 0.03111790497222739]
	TIME [epoch: 26.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023981131390940148		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.023981131390940148 | validation: 0.03141965996234887]
	TIME [epoch: 26.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023163192056931403		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.023163192056931403 | validation: 0.02882117151013117]
	TIME [epoch: 26.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024119604964030917		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.024119604964030917 | validation: 0.036564761869157136]
	TIME [epoch: 26.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0278206868347385		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.0278206868347385 | validation: 0.03890132369167381]
	TIME [epoch: 26.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02854523880406344		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.02854523880406344 | validation: 0.028657492432641103]
	TIME [epoch: 26.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03781779121836934		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.03781779121836934 | validation: 0.03485567644993655]
	TIME [epoch: 26.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0246952978194742		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.0246952978194742 | validation: 0.03321348454413134]
	TIME [epoch: 26.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022962252230955757		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.022962252230955757 | validation: 0.032199391086296714]
	TIME [epoch: 26.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028561915848574745		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.028561915848574745 | validation: 0.031199271269829347]
	TIME [epoch: 26.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02399355613785019		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.02399355613785019 | validation: 0.036481439693300824]
	TIME [epoch: 26.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02943024006139995		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.02943024006139995 | validation: 0.027915435460778376]
	TIME [epoch: 26.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022021764805480314		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.022021764805480314 | validation: 0.03118932495813838]
	TIME [epoch: 26.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024495486256569603		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.024495486256569603 | validation: 0.0328213848061128]
	TIME [epoch: 26.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03230290973974417		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.03230290973974417 | validation: 0.025190193473334302]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023619328447549572		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.023619328447549572 | validation: 0.028322189346929903]
	TIME [epoch: 26.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027060771910869915		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.027060771910869915 | validation: 0.04447515115322742]
	TIME [epoch: 26.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026294422492567443		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.026294422492567443 | validation: 0.030504535755941954]
	TIME [epoch: 26.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023589773375843677		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.023589773375843677 | validation: 0.03947058981320056]
	TIME [epoch: 26.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026132786218285284		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.026132786218285284 | validation: 0.030193152843450095]
	TIME [epoch: 26.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022585163865882142		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.022585163865882142 | validation: 0.05086833187083479]
	TIME [epoch: 26.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034016603893082815		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.034016603893082815 | validation: 0.03165443882949459]
	TIME [epoch: 26.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026671473748399885		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.026671473748399885 | validation: 0.036430011733284674]
	TIME [epoch: 26.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022433578282216437		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.022433578282216437 | validation: 0.02877293935415359]
	TIME [epoch: 26.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02223275475290133		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.02223275475290133 | validation: 0.0294335293476548]
	TIME [epoch: 26.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025173548256707215		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.025173548256707215 | validation: 0.029691251146082294]
	TIME [epoch: 26.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02424852551835519		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.02424852551835519 | validation: 0.03415446048775237]
	TIME [epoch: 26.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025347389185155517		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.025347389185155517 | validation: 0.02480965065070685]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029450525999011615		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.029450525999011615 | validation: 0.03810248930447949]
	TIME [epoch: 26.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023683496704302604		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.023683496704302604 | validation: 0.033090925716552116]
	TIME [epoch: 26.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02779005389218736		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.02779005389218736 | validation: 0.031001257592008842]
	TIME [epoch: 26.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022739661832740628		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.022739661832740628 | validation: 0.03992951163067328]
	TIME [epoch: 26.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03025652131710039		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.03025652131710039 | validation: 0.02499475110435726]
	TIME [epoch: 26.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019520072723861425		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.019520072723861425 | validation: 0.051126679975214626]
	TIME [epoch: 26.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029462911640058556		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.029462911640058556 | validation: 0.033120366119947224]
	TIME [epoch: 26.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026412963033841018		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.026412963033841018 | validation: 0.02775588770458578]
	TIME [epoch: 26.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019586530220205175		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.019586530220205175 | validation: 0.030879832244176186]
	TIME [epoch: 26.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024995375479729035		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.024995375479729035 | validation: 0.028617248212299815]
	TIME [epoch: 26.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022629063203126097		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.022629063203126097 | validation: 0.03307166947946415]
	TIME [epoch: 26.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024681962595464714		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.024681962595464714 | validation: 0.02777376434130934]
	TIME [epoch: 26.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026729422852890178		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.026729422852890178 | validation: 0.029413005401801744]
	TIME [epoch: 26.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019900549950662868		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.019900549950662868 | validation: 0.02670700580546783]
	TIME [epoch: 26.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02502138444476075		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.02502138444476075 | validation: 0.02826368679007149]
	TIME [epoch: 26.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02135846556857361		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.02135846556857361 | validation: 0.027517757792278927]
	TIME [epoch: 26.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02185970895725707		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.02185970895725707 | validation: 0.033174591898829146]
	TIME [epoch: 26.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025409076767311765		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.025409076767311765 | validation: 0.03171925535730222]
	TIME [epoch: 26.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020221155042943157		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.020221155042943157 | validation: 0.029214215936510756]
	TIME [epoch: 26.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023855160532388498		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.023855160532388498 | validation: 0.02620300240143633]
	TIME [epoch: 26.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021488104327863442		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.021488104327863442 | validation: 0.029773060722842214]
	TIME [epoch: 26.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02668045393110205		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.02668045393110205 | validation: 0.1713963566242037]
	TIME [epoch: 26.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.087855427119846		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.087855427119846 | validation: 0.04361961596654731]
	TIME [epoch: 26.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027747469034908767		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.027747469034908767 | validation: 0.026109492431892718]
	TIME [epoch: 26.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017363949881704602		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.017363949881704602 | validation: 0.02623916470733819]
	TIME [epoch: 26.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01634787841279755		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.01634787841279755 | validation: 0.02354363606781576]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017735137882751235		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.017735137882751235 | validation: 0.028264001865066266]
	TIME [epoch: 26.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027187492718386587		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.027187492718386587 | validation: 0.03318785081285524]
	TIME [epoch: 26.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026814153945590644		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.026814153945590644 | validation: 0.028231346236819506]
	TIME [epoch: 26.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026045291379111032		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.026045291379111032 | validation: 0.023930480180295026]
	TIME [epoch: 26.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01875589411911617		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.01875589411911617 | validation: 0.02570015192411887]
	TIME [epoch: 26.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020839236188774483		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.020839236188774483 | validation: 0.03403280639617682]
	TIME [epoch: 26.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020766199439164714		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.020766199439164714 | validation: 0.029963439828410758]
	TIME [epoch: 26.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022499391077187767		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.022499391077187767 | validation: 0.02833561166259526]
	TIME [epoch: 26.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025216265151023112		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.025216265151023112 | validation: 0.026833334486092625]
	TIME [epoch: 26.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019972400842148873		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.019972400842148873 | validation: 0.024924388348975625]
	TIME [epoch: 26.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01895331834903691		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.01895331834903691 | validation: 0.02983327761945626]
	TIME [epoch: 26.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02251913118059688		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.02251913118059688 | validation: 0.03397519708871599]
	TIME [epoch: 26.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023767594529357387		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.023767594529357387 | validation: 0.027331720059850368]
	TIME [epoch: 26.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0242813626072838		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.0242813626072838 | validation: 0.026774005437600206]
	TIME [epoch: 26.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02689448952619204		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.02689448952619204 | validation: 0.026881609671329035]
	TIME [epoch: 26.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022011444489402024		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.022011444489402024 | validation: 0.030397640627312653]
	TIME [epoch: 26.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02169281472591773		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.02169281472591773 | validation: 0.02795952330873548]
	TIME [epoch: 26.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019553453406116646		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.019553453406116646 | validation: 0.02796959442699512]
	TIME [epoch: 26.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019333366932614615		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.019333366932614615 | validation: 0.023708186791180606]
	TIME [epoch: 26.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017607473311965428		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.017607473311965428 | validation: 0.03538959778226131]
	TIME [epoch: 26.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02540304045879926		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.02540304045879926 | validation: 0.026365831188057205]
	TIME [epoch: 26.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018299604166073426		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.018299604166073426 | validation: 0.026010408166778073]
	TIME [epoch: 26.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020553845103207322		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.020553845103207322 | validation: 0.032007581462869084]
	TIME [epoch: 26.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02109678089973379		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.02109678089973379 | validation: 0.02899972021676227]
	TIME [epoch: 26.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02123800352237209		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.02123800352237209 | validation: 0.02528349206951834]
	TIME [epoch: 26.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023989047260983828		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.023989047260983828 | validation: 0.034324514655293535]
	TIME [epoch: 26.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021095425508312848		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.021095425508312848 | validation: 0.02606377022578676]
	TIME [epoch: 26.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604808253595481		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.01604808253595481 | validation: 0.029289344868788234]
	TIME [epoch: 26.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020898589160708116		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.020898589160708116 | validation: 0.027872437937604053]
	TIME [epoch: 26.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019407833826824615		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.019407833826824615 | validation: 0.032754923954402076]
	TIME [epoch: 26.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02062422247341973		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.02062422247341973 | validation: 0.025270480312595753]
	TIME [epoch: 26.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016987001989223482		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.016987001989223482 | validation: 0.02713595825187761]
	TIME [epoch: 26.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01848321251675918		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.01848321251675918 | validation: 0.041231519889378426]
	TIME [epoch: 26.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022297848978584476		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.022297848978584476 | validation: 0.029569136333778848]
	TIME [epoch: 26.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016875000488311684		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.016875000488311684 | validation: 0.03227796821513938]
	TIME [epoch: 26.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023814631568335555		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.023814631568335555 | validation: 0.030317247441844296]
	TIME [epoch: 26.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022221730256862512		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.022221730256862512 | validation: 0.024367730775054004]
	TIME [epoch: 26.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019210251197278268		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.019210251197278268 | validation: 0.030632424100660263]
	TIME [epoch: 26.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020910260940754807		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.020910260940754807 | validation: 0.02141134789467039]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028010442837643718		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.028010442837643718 | validation: 0.03486455384809846]
	TIME [epoch: 26.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022351845927874144		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.022351845927874144 | validation: 0.030679819357278162]
	TIME [epoch: 26.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02029907465017977		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.02029907465017977 | validation: 0.025694824199883396]
	TIME [epoch: 26.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02251761218948573		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.02251761218948573 | validation: 0.029762079358743358]
	TIME [epoch: 26.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019264076680525606		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.019264076680525606 | validation: 0.02733085549213643]
	TIME [epoch: 26.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015951896519376448		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.015951896519376448 | validation: 0.024335814963016407]
	TIME [epoch: 26.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019065586477966235		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.019065586477966235 | validation: 0.028262979425870423]
	TIME [epoch: 26.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02059378065895507		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.02059378065895507 | validation: 0.025974083589023672]
	TIME [epoch: 26.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017897239654674386		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.017897239654674386 | validation: 0.0275213287554366]
	TIME [epoch: 26.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01850981239110647		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.01850981239110647 | validation: 0.02627856414272359]
	TIME [epoch: 26.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02058298095013192		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.02058298095013192 | validation: 0.02188891406643676]
	TIME [epoch: 26.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01717681441369839		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.01717681441369839 | validation: 0.027092543620371595]
	TIME [epoch: 26.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016143616648695104		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.016143616648695104 | validation: 0.030432285477933065]
	TIME [epoch: 26.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0228520737160789		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0228520737160789 | validation: 0.036982938273193836]
	TIME [epoch: 26.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018382968315396386		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.018382968315396386 | validation: 0.022946996863178616]
	TIME [epoch: 26.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018931226231221898		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.018931226231221898 | validation: 0.02589527327253851]
	TIME [epoch: 26.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018058082828469744		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.018058082828469744 | validation: 0.024497505086315527]
	TIME [epoch: 26.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0182054824765586		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.0182054824765586 | validation: 0.02773363259216638]
	TIME [epoch: 26.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016938872477359614		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.016938872477359614 | validation: 0.023740499465678282]
	TIME [epoch: 26.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02647536030965734		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.02647536030965734 | validation: 0.03543192901375458]
	TIME [epoch: 26.1 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022685217784742		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.022685217784742 | validation: 0.024287943277470726]
	TIME [epoch: 26.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01712509876769629		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.01712509876769629 | validation: 0.023998729168363334]
	TIME [epoch: 26.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013928099920561609		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.013928099920561609 | validation: 0.022986615714911295]
	TIME [epoch: 26.1 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015646170690466882		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.015646170690466882 | validation: 0.025232030275978874]
	TIME [epoch: 26.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0179994080436134		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.0179994080436134 | validation: 0.0226420954037527]
	TIME [epoch: 26.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01753618734362059		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.01753618734362059 | validation: 0.0295825025865952]
	TIME [epoch: 26.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02076218265007946		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.02076218265007946 | validation: 0.027093134717939636]
	TIME [epoch: 26.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019143329186791437		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.019143329186791437 | validation: 0.02678159819610049]
	TIME [epoch: 26.1 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016596459515365515		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.016596459515365515 | validation: 0.024644913842846117]
	TIME [epoch: 26.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016588683030928628		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.016588683030928628 | validation: 0.03104628427046388]
	TIME [epoch: 26.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018393980320189643		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.018393980320189643 | validation: 0.026208250622586988]
	TIME [epoch: 26.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01586973270490558		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.01586973270490558 | validation: 0.10454669214497378]
	TIME [epoch: 26.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03503092877284976		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.03503092877284976 | validation: 0.023789188864320767]
	TIME [epoch: 26.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016364292037423407		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.016364292037423407 | validation: 0.025738712355399374]
	TIME [epoch: 26.1 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02064413427527146		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.02064413427527146 | validation: 0.028531693565411152]
	TIME [epoch: 26.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01778158578214426		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.01778158578214426 | validation: 0.024018369136882975]
	TIME [epoch: 26.1 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016477988113131288		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.016477988113131288 | validation: 0.02338155149923951]
	TIME [epoch: 26.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016943737462660077		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.016943737462660077 | validation: 0.02219829501015905]
	TIME [epoch: 26.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0188711524557418		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.0188711524557418 | validation: 0.024515548387651825]
	TIME [epoch: 26.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018587674508631743		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.018587674508631743 | validation: 0.02527905289066997]
	TIME [epoch: 26.1 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029572674901251667		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.029572674901251667 | validation: 0.04044090350458082]
	TIME [epoch: 26.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021793720489293445		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.021793720489293445 | validation: 0.02251649711595239]
	TIME [epoch: 26.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015071559470935917		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.015071559470935917 | validation: 0.023090270094889433]
	TIME [epoch: 26.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015202262092891581		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.015202262092891581 | validation: 0.025179056413148035]
	TIME [epoch: 26.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014898829945398137		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.014898829945398137 | validation: 0.025740723436909148]
	TIME [epoch: 26.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01566316396622417		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.01566316396622417 | validation: 0.03036410341950946]
	TIME [epoch: 26.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01794722113030199		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.01794722113030199 | validation: 0.0350554686174806]
	TIME [epoch: 26.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01809332356040117		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.01809332356040117 | validation: 0.021570155144911125]
	TIME [epoch: 26.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013393084840730325		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.013393084840730325 | validation: 0.02410002736077968]
	TIME [epoch: 26.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015212024020471032		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.015212024020471032 | validation: 0.02719177044712712]
	TIME [epoch: 26.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020294780614691043		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.020294780614691043 | validation: 0.027188930497739933]
	TIME [epoch: 26.1 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018506128503259393		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.018506128503259393 | validation: 0.023225498524939804]
	TIME [epoch: 26.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014974258072506838		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.014974258072506838 | validation: 0.020954367463839198]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015530093606364466		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.015530093606364466 | validation: 0.023147621813962294]
	TIME [epoch: 26.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01581896735902492		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.01581896735902492 | validation: 0.02527868863089391]
	TIME [epoch: 26.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019651384124766108		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.019651384124766108 | validation: 0.027033849594552538]
	TIME [epoch: 26.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018080655736706874		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.018080655736706874 | validation: 0.02381478193150631]
	TIME [epoch: 26.1 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01552248192010105		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.01552248192010105 | validation: 0.023935048355202895]
	TIME [epoch: 26.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018261633710548307		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.018261633710548307 | validation: 0.031015380285891486]
	TIME [epoch: 26.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01820042382782516		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.01820042382782516 | validation: 0.027581884761526457]
	TIME [epoch: 26.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014332746903612947		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.014332746903612947 | validation: 0.02446848081905685]
	TIME [epoch: 26.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015578680139180675		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.015578680139180675 | validation: 0.02491989930509137]
	TIME [epoch: 26.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018668982359980348		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.018668982359980348 | validation: 0.02821298005722553]
	TIME [epoch: 26.1 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019173916693750463		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.019173916693750463 | validation: 0.026834905059180902]
	TIME [epoch: 26.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016375074546539192		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.016375074546539192 | validation: 0.022956331985994245]
	TIME [epoch: 26.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013369489555754413		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.013369489555754413 | validation: 0.024066280516306538]
	TIME [epoch: 26.1 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013867345596285722		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.013867345596285722 | validation: 0.026540389200941823]
	TIME [epoch: 26.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018756316542874255		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.018756316542874255 | validation: 0.023157694611491426]
	TIME [epoch: 26.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014780703018427788		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.014780703018427788 | validation: 0.030515416259712925]
	TIME [epoch: 26.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020254092578892928		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.020254092578892928 | validation: 0.025392466595218727]
	TIME [epoch: 26.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014261848361227432		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.014261848361227432 | validation: 0.023566665276605014]
	TIME [epoch: 26.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01693600389867583		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.01693600389867583 | validation: 0.022877623823492396]
	TIME [epoch: 26.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016350454367119224		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.016350454367119224 | validation: 0.021370288593501808]
	TIME [epoch: 26 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014759349924174627		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.014759349924174627 | validation: 0.02463386450330797]
	TIME [epoch: 26.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019107119044331003		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.019107119044331003 | validation: 0.02024691910454431]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013087398692420375		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.013087398692420375 | validation: 0.025478170864788707]
	TIME [epoch: 26.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01897239036864972		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.01897239036864972 | validation: 0.02439944967575995]
	TIME [epoch: 26.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01721490405941016		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.01721490405941016 | validation: 0.023920955762828516]
	TIME [epoch: 26.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015806045044253828		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.015806045044253828 | validation: 0.02437582457032681]
	TIME [epoch: 26.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015570915200779762		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.015570915200779762 | validation: 0.024251032886401215]
	TIME [epoch: 26 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015783481814914672		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.015783481814914672 | validation: 0.02407545490955025]
	TIME [epoch: 26.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015306862797535434		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.015306862797535434 | validation: 0.026873439190788682]
	TIME [epoch: 26.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01821562172510162		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.01821562172510162 | validation: 0.022559839542698676]
	TIME [epoch: 26.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013682874353162343		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.013682874353162343 | validation: 0.02477521076532111]
	TIME [epoch: 26.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017441263770536012		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.017441263770536012 | validation: 0.026163122653435924]
	TIME [epoch: 26.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016248334022583494		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.016248334022583494 | validation: 0.023850866840727722]
	TIME [epoch: 26.1 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013892117010804658		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.013892117010804658 | validation: 0.020346077625648527]
	TIME [epoch: 26.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016088515066548643		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.016088515066548643 | validation: 0.020003304937544725]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014555826007213007		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.014555826007213007 | validation: 0.025275101633114472]
	TIME [epoch: 26.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015225641640070902		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.015225641640070902 | validation: 0.021535523795789772]
	TIME [epoch: 26.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013087357668449635		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.013087357668449635 | validation: 0.023147082961400268]
	TIME [epoch: 26 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015453528935066246		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.015453528935066246 | validation: 0.025637506725883278]
	TIME [epoch: 26.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01552356144385337		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.01552356144385337 | validation: 0.02472302834352525]
	TIME [epoch: 26.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014417939765388674		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.014417939765388674 | validation: 0.026245888462773374]
	TIME [epoch: 26.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01650261238635706		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.01650261238635706 | validation: 0.024374863570678456]
	TIME [epoch: 26 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016859578810728058		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.016859578810728058 | validation: 0.027344712150006306]
	TIME [epoch: 26.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017961426674371547		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.017961426674371547 | validation: 0.023234993636942744]
	TIME [epoch: 26.1 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014024127799391113		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.014024127799391113 | validation: 0.025684741914290897]
	TIME [epoch: 26.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014678912286507598		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.014678912286507598 | validation: 0.023917713922359897]
	TIME [epoch: 26 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015846963307195094		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.015846963307195094 | validation: 0.026282794479549114]
	TIME [epoch: 26.1 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016038888088753325		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.016038888088753325 | validation: 0.023400222155361743]
	TIME [epoch: 26.1 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016700739020965336		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.016700739020965336 | validation: 0.020835868650198187]
	TIME [epoch: 26.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016096928513919175		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.016096928513919175 | validation: 0.021132535745190355]
	TIME [epoch: 26.1 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012638843314411823		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.012638843314411823 | validation: 0.022774738121898938]
	TIME [epoch: 26.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015549106993647081		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.015549106993647081 | validation: 0.02902710359181508]
	TIME [epoch: 26.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016028541483534116		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.016028541483534116 | validation: 0.020532651967499576]
	TIME [epoch: 26.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014144371093556162		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.014144371093556162 | validation: 0.020936498801775873]
	TIME [epoch: 26.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015500951882812357		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.015500951882812357 | validation: 0.025502680453233048]
	TIME [epoch: 26.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014131800361496973		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.014131800361496973 | validation: 0.02153135909669726]
	TIME [epoch: 26.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030090623031069432		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.030090623031069432 | validation: 0.04300712894537337]
	TIME [epoch: 26.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017608285436625962		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.017608285436625962 | validation: 0.021186477171703407]
	TIME [epoch: 26.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013598268536634593		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.013598268536634593 | validation: 0.020436024776774975]
	TIME [epoch: 26.1 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013606795194075628		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.013606795194075628 | validation: 0.020789897582296313]
	TIME [epoch: 26.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01234348764303339		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.01234348764303339 | validation: 0.018169666624782192]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013458086644027763		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.013458086644027763 | validation: 0.024539439278766895]
	TIME [epoch: 26.1 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01483787014435282		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.01483787014435282 | validation: 0.022234109133793534]
	TIME [epoch: 26.1 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015169886555930078		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.015169886555930078 | validation: 0.021417670139569925]
	TIME [epoch: 26.1 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0142459209421806		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0142459209421806 | validation: 0.020571382319848973]
	TIME [epoch: 26.1 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02129515099240017		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.02129515099240017 | validation: 0.024099484683240812]
	TIME [epoch: 26.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01318609737701587		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.01318609737701587 | validation: 0.021585235727997743]
	TIME [epoch: 26.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012885718648676325		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.012885718648676325 | validation: 0.024146238875637882]
	TIME [epoch: 26.1 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014502717669285162		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.014502717669285162 | validation: 0.022702472087220087]
	TIME [epoch: 26.1 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021692074855384724		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.021692074855384724 | validation: 0.045902940731556624]
	TIME [epoch: 26 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02156065277535826		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.02156065277535826 | validation: 0.021249730755428544]
	TIME [epoch: 26.1 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012686344798372641		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.012686344798372641 | validation: 0.021132741462728172]
	TIME [epoch: 26 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0136133695351858		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.0136133695351858 | validation: 0.02288122161392011]
	TIME [epoch: 26.1 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012034757021206235		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.012034757021206235 | validation: 0.02160893056075064]
	TIME [epoch: 26 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013561434910319158		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.013561434910319158 | validation: 0.021758970840393334]
	TIME [epoch: 26.1 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012377294046407722		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.012377294046407722 | validation: 0.024910603069070576]
	TIME [epoch: 26 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014745729903653694		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.014745729903653694 | validation: 0.027485579477991788]
	TIME [epoch: 26.1 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014656329580720277		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.014656329580720277 | validation: 0.021193679502605343]
	TIME [epoch: 26 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013333413387796112		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.013333413387796112 | validation: 0.020565051465746064]
	TIME [epoch: 26.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013065496369544268		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.013065496369544268 | validation: 0.020855657665219827]
	TIME [epoch: 26.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014096673726407254		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.014096673726407254 | validation: 0.021650629425018518]
	TIME [epoch: 26.1 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015483465541119769		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.015483465541119769 | validation: 0.02434719351498156]
	TIME [epoch: 26.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013195472081789747		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.013195472081789747 | validation: 0.020636014720407735]
	TIME [epoch: 26.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015217822647140403		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.015217822647140403 | validation: 0.02361954986242911]
	TIME [epoch: 26 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014286426645149602		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.014286426645149602 | validation: 0.022403930255860918]
	TIME [epoch: 26 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019161492388808814		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.019161492388808814 | validation: 0.025068316541848136]
	TIME [epoch: 26 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016853582272006933		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.016853582272006933 | validation: 0.02071969003427048]
	TIME [epoch: 26 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01726267443349707		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.01726267443349707 | validation: 0.029360590482093885]
	TIME [epoch: 26 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022581951170788282		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.022581951170788282 | validation: 0.02251804378457789]
	TIME [epoch: 26 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012942445010222871		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.012942445010222871 | validation: 0.019144308837290645]
	TIME [epoch: 26 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011774872011686706		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.011774872011686706 | validation: 0.018923198123622875]
	TIME [epoch: 26 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011250132260165009		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.011250132260165009 | validation: 0.020411619004747663]
	TIME [epoch: 26 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01488557296912564		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.01488557296912564 | validation: 0.026058492094498512]
	TIME [epoch: 26 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01603882222748624		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.01603882222748624 | validation: 0.021087488657267114]
	TIME [epoch: 26 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012955004188860239		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.012955004188860239 | validation: 0.02137863564110662]
	TIME [epoch: 26.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012293548856785345		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.012293548856785345 | validation: 0.020239154683864648]
	TIME [epoch: 26 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013233013690978507		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.013233013690978507 | validation: 0.021096831503704658]
	TIME [epoch: 26.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014549459788243367		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.014549459788243367 | validation: 0.022324499581470345]
	TIME [epoch: 26 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012947093933743459		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.012947093933743459 | validation: 0.019972895024286043]
	TIME [epoch: 26 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01286708288797831		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.01286708288797831 | validation: 0.021796268970413885]
	TIME [epoch: 26 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01197647107788605		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.01197647107788605 | validation: 0.024134861134105456]
	TIME [epoch: 26.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014941463547879242		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.014941463547879242 | validation: 0.021601870528220442]
	TIME [epoch: 26.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014460316063083603		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.014460316063083603 | validation: 0.0202838174933849]
	TIME [epoch: 26.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01386829828332099		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.01386829828332099 | validation: 0.02128618558232144]
	TIME [epoch: 26.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01289586065072338		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.01289586065072338 | validation: 0.020896151465634408]
	TIME [epoch: 26 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01370002266388318		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.01370002266388318 | validation: 0.020979124415341534]
	TIME [epoch: 26.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012688584033149842		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.012688584033149842 | validation: 0.022745176855277202]
	TIME [epoch: 26 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014532667230571169		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.014532667230571169 | validation: 0.020097722056518358]
	TIME [epoch: 26.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011297054574317334		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.011297054574317334 | validation: 0.021838259863254054]
	TIME [epoch: 26 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013756220491428188		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.013756220491428188 | validation: 0.02271870271022819]
	TIME [epoch: 26.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012470272825471713		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.012470272825471713 | validation: 0.020426145009725376]
	TIME [epoch: 26.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01333790741117118		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.01333790741117118 | validation: 0.020898615844898886]
	TIME [epoch: 26.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01419905754733874		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.01419905754733874 | validation: 0.019503309546118498]
	TIME [epoch: 26 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012655945161423696		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.012655945161423696 | validation: 0.02148251396919615]
	TIME [epoch: 26.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013155796267063901		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.013155796267063901 | validation: 0.021077039385643336]
	TIME [epoch: 26 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012485569152901256		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.012485569152901256 | validation: 0.020717952907703358]
	TIME [epoch: 26.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013384699818103085		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.013384699818103085 | validation: 0.0198916926533671]
	TIME [epoch: 26.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013698402910274016		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.013698402910274016 | validation: 0.0206216450507578]
	TIME [epoch: 26.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01331157604152284		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.01331157604152284 | validation: 0.02114906628764849]
	TIME [epoch: 26.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012279401516858302		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.012279401516858302 | validation: 0.0200816340288352]
	TIME [epoch: 26.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011018200784990752		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.011018200784990752 | validation: 0.022132697832320063]
	TIME [epoch: 26.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013591800907395512		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.013591800907395512 | validation: 0.020993967153417913]
	TIME [epoch: 26.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012360994688583668		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.012360994688583668 | validation: 0.019042898721735906]
	TIME [epoch: 26.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01278747647112431		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.01278747647112431 | validation: 0.021347785761030115]
	TIME [epoch: 26.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013322122981460287		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.013322122981460287 | validation: 0.02153892749494875]
	TIME [epoch: 26 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012408556910163883		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.012408556910163883 | validation: 0.023100703851364618]
	TIME [epoch: 26.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013199052790266567		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.013199052790266567 | validation: 0.020502241099910785]
	TIME [epoch: 26 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01241338752370534		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.01241338752370534 | validation: 0.020337605105425913]
	TIME [epoch: 26.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012529952165364443		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.012529952165364443 | validation: 0.02060431654211626]
	TIME [epoch: 26.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013040929036436059		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.013040929036436059 | validation: 0.01811555564665555]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01139424278402054		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.01139424278402054 | validation: 0.02536796027727018]
	TIME [epoch: 26.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014776175392073747		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.014776175392073747 | validation: 0.020612062567946576]
	TIME [epoch: 26 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011680855089074006		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.011680855089074006 | validation: 0.021458552111770085]
	TIME [epoch: 26.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012651997932431117		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.012651997932431117 | validation: 0.022855398125482866]
	TIME [epoch: 26 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014497129209393179		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.014497129209393179 | validation: 0.01938651327448742]
	TIME [epoch: 26 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011646916409519831		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.011646916409519831 | validation: 0.02692515512342884]
	TIME [epoch: 26 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01376547202961188		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.01376547202961188 | validation: 0.019847354668979505]
	TIME [epoch: 26.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011658311208886779		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.011658311208886779 | validation: 0.020639430412753693]
	TIME [epoch: 26 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011369220812895493		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.011369220812895493 | validation: 0.019903433306625466]
	TIME [epoch: 26 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011661132368517053		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.011661132368517053 | validation: 0.01964203363847075]
	TIME [epoch: 26 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011141391030289566		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.011141391030289566 | validation: 0.019380134829746282]
	TIME [epoch: 26 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012934330256589183		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.012934330256589183 | validation: 0.033695676162475256]
	TIME [epoch: 26 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018112500413186648		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.018112500413186648 | validation: 0.04429325699558517]
	TIME [epoch: 26.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023478588863299482		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.023478588863299482 | validation: 0.017946077574580076]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014747264892617289		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.014747264892617289 | validation: 0.024673520297299148]
	TIME [epoch: 26 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011618868444154789		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.011618868444154789 | validation: 0.01838426193341151]
	TIME [epoch: 26 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027038181998312504		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.027038181998312504 | validation: 0.10606597856339783]
	TIME [epoch: 26.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055301572924478626		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.055301572924478626 | validation: 0.041690525173169304]
	TIME [epoch: 26.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018637220715385018		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.018637220715385018 | validation: 0.02401066966389754]
	TIME [epoch: 26.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01284099274375107		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.01284099274375107 | validation: 0.02101611743073635]
	TIME [epoch: 26 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012030280142715762		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.012030280142715762 | validation: 0.021561812781601283]
	TIME [epoch: 26.1 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011331303198758982		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.011331303198758982 | validation: 0.020413168213400976]
	TIME [epoch: 26 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010929142307067463		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.010929142307067463 | validation: 0.020388099977138155]
	TIME [epoch: 26.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012000655177426189		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.012000655177426189 | validation: 0.018587773767424032]
	TIME [epoch: 26 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012392243246094448		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.012392243246094448 | validation: 0.020966604554689294]
	TIME [epoch: 26.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010997670181132886		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.010997670181132886 | validation: 0.018598810435525498]
	TIME [epoch: 26 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011080105622185336		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.011080105622185336 | validation: 0.02083960977797504]
	TIME [epoch: 26.1 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010768182739870173		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.010768182739870173 | validation: 0.021678091086794936]
	TIME [epoch: 26.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012296775364930382		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.012296775364930382 | validation: 0.0203493457724469]
	TIME [epoch: 26.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010708625570446996		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.010708625570446996 | validation: 0.019671407392248888]
	TIME [epoch: 26 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010782708291505759		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.010782708291505759 | validation: 0.018908812217173353]
	TIME [epoch: 26.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01202342370119917		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.01202342370119917 | validation: 0.022162136607055058]
	TIME [epoch: 26.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01269451347207727		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.01269451347207727 | validation: 0.019624306156205723]
	TIME [epoch: 26 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012546475366831344		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.012546475366831344 | validation: 0.0201486593354028]
	TIME [epoch: 26.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010895673077989903		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.010895673077989903 | validation: 0.020857067298216497]
	TIME [epoch: 26 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010743846278099836		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.010743846278099836 | validation: 0.02032902716287428]
	TIME [epoch: 26 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012517212524731725		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.012517212524731725 | validation: 0.020919814223418827]
	TIME [epoch: 26 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011529969644679842		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.011529969644679842 | validation: 0.018370137118759244]
	TIME [epoch: 26 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011255966709430333		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.011255966709430333 | validation: 0.019756208769166228]
	TIME [epoch: 26 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012204894539425755		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.012204894539425755 | validation: 0.022420872756395503]
	TIME [epoch: 26 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012195093829324623		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.012195093829324623 | validation: 0.019306511299588793]
	TIME [epoch: 26 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010882034457387786		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.010882034457387786 | validation: 0.020387374430884248]
	TIME [epoch: 26 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012366284994061371		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.012366284994061371 | validation: 0.019814353262320603]
	TIME [epoch: 26 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011243544398933686		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.011243544398933686 | validation: 0.01979603478567809]
	TIME [epoch: 26 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012340849515737677		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.012340849515737677 | validation: 0.020999223578975358]
	TIME [epoch: 26 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010978964472926334		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.010978964472926334 | validation: 0.021148531775525687]
	TIME [epoch: 26 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012009787788652516		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.012009787788652516 | validation: 0.018743088789114484]
	TIME [epoch: 26 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011290324722246968		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.011290324722246968 | validation: 0.021877218448076725]
	TIME [epoch: 26 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011495527800431435		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.011495527800431435 | validation: 0.01985328700917538]
	TIME [epoch: 26 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0129925850767782		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0129925850767782 | validation: 0.02040268499880758]
	TIME [epoch: 26.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01224068230732913		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.01224068230732913 | validation: 0.018541546491290875]
	TIME [epoch: 26 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01053043619552325		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.01053043619552325 | validation: 0.018511845749484807]
	TIME [epoch: 26 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0104064934251251		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0104064934251251 | validation: 0.017871625909992077]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011270174818444712		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.011270174818444712 | validation: 0.01990009476411682]
	TIME [epoch: 26 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011636641727762		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.011636641727762 | validation: 0.023025825070614396]
	TIME [epoch: 26.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011127109605504004		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.011127109605504004 | validation: 0.01972760439627723]
	TIME [epoch: 26.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0115391109112787		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0115391109112787 | validation: 0.019296832685604892]
	TIME [epoch: 26 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013206751738555782		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.013206751738555782 | validation: 0.018688980496836545]
	TIME [epoch: 26.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01178155510761567		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.01178155510761567 | validation: 0.020690947302001262]
	TIME [epoch: 26 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011517949803176946		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.011517949803176946 | validation: 0.021010247360353687]
	TIME [epoch: 26 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012035734837382702		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.012035734837382702 | validation: 0.018653293238457432]
	TIME [epoch: 26 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011995592344460213		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.011995592344460213 | validation: 0.018714811225466893]
	TIME [epoch: 26 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0108332468039282		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.0108332468039282 | validation: 0.025102939423965652]
	TIME [epoch: 26.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0137950149248314		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.0137950149248314 | validation: 0.021322098192515918]
	TIME [epoch: 26.1 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011666539186372081		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.011666539186372081 | validation: 0.019898103786297208]
	TIME [epoch: 26.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012013045565528752		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.012013045565528752 | validation: 0.018599903122953412]
	TIME [epoch: 26.1 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011097175822682803		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.011097175822682803 | validation: 0.01798303433075671]
	TIME [epoch: 26 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011381013065545266		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.011381013065545266 | validation: 0.020178194054452425]
	TIME [epoch: 26 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012269712538695465		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.012269712538695465 | validation: 0.020347802173019423]
	TIME [epoch: 26.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011020087072314045		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.011020087072314045 | validation: 0.018806697276151668]
	TIME [epoch: 26.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011678199171280488		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.011678199171280488 | validation: 0.021215955089736983]
	TIME [epoch: 26.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011464317439036486		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.011464317439036486 | validation: 0.018307823830502237]
	TIME [epoch: 26 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011223541925277681		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.011223541925277681 | validation: 0.020081432737611742]
	TIME [epoch: 26.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01094784418850183		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.01094784418850183 | validation: 0.01895836450151842]
	TIME [epoch: 26.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0110871433638899		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0110871433638899 | validation: 0.018138619378652444]
	TIME [epoch: 26.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01038263149495606		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.01038263149495606 | validation: 0.018101743991504445]
	TIME [epoch: 26.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012607253768036106		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.012607253768036106 | validation: 0.019570520681100097]
	TIME [epoch: 26 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011610610996181374		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.011610610996181374 | validation: 0.020298516730805714]
	TIME [epoch: 26 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011376859563871375		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.011376859563871375 | validation: 0.01993633928988553]
	TIME [epoch: 26 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010875749327660566		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.010875749327660566 | validation: 0.019965103049528796]
	TIME [epoch: 26 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01140840846281849		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.01140840846281849 | validation: 0.01798974571878255]
	TIME [epoch: 26 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01135719904749225		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.01135719904749225 | validation: 0.019314559215217936]
	TIME [epoch: 26 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011415852267249643		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.011415852267249643 | validation: 0.018445335245154487]
	TIME [epoch: 26.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010902105491417726		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.010902105491417726 | validation: 0.020023583206798364]
	TIME [epoch: 26.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01091460772969525		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.01091460772969525 | validation: 0.018055125411644175]
	TIME [epoch: 26.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010236859523692342		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.010236859523692342 | validation: 0.018078849972984038]
	TIME [epoch: 26 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010792491476076398		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.010792491476076398 | validation: 0.02077152084404462]
	TIME [epoch: 26 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011275185952092613		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.011275185952092613 | validation: 0.01899810740704321]
	TIME [epoch: 26 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011307826483601196		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.011307826483601196 | validation: 0.01932581289300528]
	TIME [epoch: 26.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010360808255740529		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.010360808255740529 | validation: 0.017647730666378862]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011280457982152789		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.011280457982152789 | validation: 0.017997465225471923]
	TIME [epoch: 26 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010386517735811911		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.010386517735811911 | validation: 0.018156688737451642]
	TIME [epoch: 26 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011122487085701838		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.011122487085701838 | validation: 0.020225450298601583]
	TIME [epoch: 26.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01188482371810083		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.01188482371810083 | validation: 0.02092705874372885]
	TIME [epoch: 26 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011497229653065436		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.011497229653065436 | validation: 0.01815923250491936]
	TIME [epoch: 26.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01064637478240673		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.01064637478240673 | validation: 0.018956864035369298]
	TIME [epoch: 26 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014041672727032325		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.014041672727032325 | validation: 0.020085254426478634]
	TIME [epoch: 26 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010692723694423487		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.010692723694423487 | validation: 0.018147585286378685]
	TIME [epoch: 26.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010664662091660499		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.010664662091660499 | validation: 0.018399569615336554]
	TIME [epoch: 26.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010377993332768362		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.010377993332768362 | validation: 0.019992517124117905]
	TIME [epoch: 26 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010717790955941123		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.010717790955941123 | validation: 0.01697568923568896]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010067424881880957		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.010067424881880957 | validation: 0.019613951147600336]
	TIME [epoch: 26.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01027624489361197		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.01027624489361197 | validation: 0.019003568492309917]
	TIME [epoch: 26 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011103204664278556		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.011103204664278556 | validation: 0.019533292337021592]
	TIME [epoch: 26 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010420398366602942		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.010420398366602942 | validation: 0.019837776213092947]
	TIME [epoch: 26 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010901434936618128		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.010901434936618128 | validation: 0.01824814475800579]
	TIME [epoch: 26 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010613433716350072		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.010613433716350072 | validation: 0.01764164656661694]
	TIME [epoch: 26 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010427271896118247		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.010427271896118247 | validation: 0.0189419298497678]
	TIME [epoch: 26 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0105918387664201		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.0105918387664201 | validation: 0.018024721323990814]
	TIME [epoch: 26 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010141531130943869		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.010141531130943869 | validation: 0.019357798458373578]
	TIME [epoch: 26 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01401679041559735		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.01401679041559735 | validation: 0.032143562045482466]
	TIME [epoch: 26 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015628247516853908		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.015628247516853908 | validation: 0.019402010755366465]
	TIME [epoch: 26 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010601031148404981		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.010601031148404981 | validation: 0.018587258377743627]
	TIME [epoch: 26 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010737259874255986		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.010737259874255986 | validation: 0.01819290708379976]
	TIME [epoch: 26 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01052273989209625		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.01052273989209625 | validation: 0.01916032846630998]
	TIME [epoch: 26 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010882240404885531		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.010882240404885531 | validation: 0.01914084547004858]
	TIME [epoch: 26 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01027077250109963		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.01027077250109963 | validation: 0.021104891350152935]
	TIME [epoch: 26 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01103401090069053		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.01103401090069053 | validation: 0.017596138899448197]
	TIME [epoch: 26 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010695212285663239		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.010695212285663239 | validation: 0.01827378336718051]
	TIME [epoch: 26 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009981207718413196		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.009981207718413196 | validation: 0.017907709341404066]
	TIME [epoch: 26.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010035400970818687		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.010035400970818687 | validation: 0.01877001531999054]
	TIME [epoch: 26 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010066490264822309		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.010066490264822309 | validation: 0.019410038088189568]
	TIME [epoch: 26 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010654730145422777		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.010654730145422777 | validation: 0.018987020092037288]
	TIME [epoch: 26.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010705716070957222		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.010705716070957222 | validation: 0.017082480371635095]
	TIME [epoch: 26 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011712222144322423		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.011712222144322423 | validation: 0.018494818043683144]
	TIME [epoch: 26 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00950155181821744		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.00950155181821744 | validation: 0.017540887432606032]
	TIME [epoch: 26.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009543642707821157		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.009543642707821157 | validation: 0.019574149744944792]
	TIME [epoch: 26 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010568523398979355		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.010568523398979355 | validation: 0.018028166888473144]
	TIME [epoch: 26 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009371612878948268		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.009371612878948268 | validation: 0.018559003037616824]
	TIME [epoch: 26 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009922224246783575		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.009922224246783575 | validation: 0.019817627713472803]
	TIME [epoch: 26 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010353948438017232		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.010353948438017232 | validation: 0.018431567799190075]
	TIME [epoch: 26 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009981378504899593		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.009981378504899593 | validation: 0.019538718225899587]
	TIME [epoch: 26 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00979786611689234		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.00979786611689234 | validation: 0.018326841249457215]
	TIME [epoch: 26 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010383042889466305		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.010383042889466305 | validation: 0.01826800465078786]
	TIME [epoch: 26 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010801672346283257		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.010801672346283257 | validation: 0.017924828928543707]
	TIME [epoch: 26.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010294851264065588		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.010294851264065588 | validation: 0.017951693204732642]
	TIME [epoch: 26.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009831806686833724		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.009831806686833724 | validation: 0.01734815566755253]
	TIME [epoch: 26 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01011939080447316		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.01011939080447316 | validation: 0.018668904357267264]
	TIME [epoch: 26.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009919388623171224		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.009919388623171224 | validation: 0.018824276725129062]
	TIME [epoch: 26 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010027577502906466		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.010027577502906466 | validation: 0.019084250388225782]
	TIME [epoch: 26 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011007069073665212		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.011007069073665212 | validation: 0.018447533565541192]
	TIME [epoch: 26 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009361687081468556		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.009361687081468556 | validation: 0.019424183384365024]
	TIME [epoch: 26.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010119648202574558		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.010119648202574558 | validation: 0.01859243231042834]
	TIME [epoch: 26 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009648752191999451		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.009648752191999451 | validation: 0.018726538842784603]
	TIME [epoch: 26 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010560407241894328		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.010560407241894328 | validation: 0.020472168420670053]
	TIME [epoch: 26 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010116136648885506		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.010116136648885506 | validation: 0.019480230973759425]
	TIME [epoch: 26.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010213050457101592		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.010213050457101592 | validation: 0.019085626851117764]
	TIME [epoch: 26 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010004636437984408		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.010004636437984408 | validation: 0.018428637898081076]
	TIME [epoch: 26 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010312508924026926		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.010312508924026926 | validation: 0.017610121691262776]
	TIME [epoch: 26 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009864660495687847		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.009864660495687847 | validation: 0.020377832436813445]
	TIME [epoch: 26.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009801440575120743		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.009801440575120743 | validation: 0.01890897332658823]
	TIME [epoch: 26.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010219783984632263		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.010219783984632263 | validation: 0.018925823684529414]
	TIME [epoch: 26.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009958446511952004		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.009958446511952004 | validation: 0.01811367219276879]
	TIME [epoch: 26 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009780460352319009		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.009780460352319009 | validation: 0.018052477594132123]
	TIME [epoch: 26.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00961983976507549		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.00961983976507549 | validation: 0.017282798066616346]
	TIME [epoch: 26 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010484864212569767		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.010484864212569767 | validation: 0.01799692304800904]
	TIME [epoch: 26.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009847735942947319		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.009847735942947319 | validation: 0.018372222391091937]
	TIME [epoch: 26.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009574590642281338		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.009574590642281338 | validation: 0.019092673270753875]
	TIME [epoch: 26 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01073442494833729		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.01073442494833729 | validation: 0.01674136592590432]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010996858589479963		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.010996858589479963 | validation: 0.01829646190050968]
	TIME [epoch: 26.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009862064176563823		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.009862064176563823 | validation: 0.01866047049016676]
	TIME [epoch: 26 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009417106617266871		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.009417106617266871 | validation: 0.01713698502485824]
	TIME [epoch: 26 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01009175792781326		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.01009175792781326 | validation: 0.02253042321305994]
	TIME [epoch: 26 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010584977079790947		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.010584977079790947 | validation: 0.019582026513237556]
	TIME [epoch: 26 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009334336867565844		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.009334336867565844 | validation: 0.018262126035704]
	TIME [epoch: 26.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009293964566477141		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.009293964566477141 | validation: 0.017191425696113843]
	TIME [epoch: 26 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009869005816658546		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.009869005816658546 | validation: 0.01882793456733878]
	TIME [epoch: 26.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009267928751395375		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.009267928751395375 | validation: 0.019307238282022438]
	TIME [epoch: 26.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009300792028743841		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.009300792028743841 | validation: 0.02065068814618879]
	TIME [epoch: 26.1 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010010281940999248		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.010010281940999248 | validation: 0.01726592136927381]
	TIME [epoch: 26.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009912669008858751		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.009912669008858751 | validation: 0.01929769786967904]
	TIME [epoch: 26.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009854054184979217		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.009854054184979217 | validation: 0.017803842330334132]
	TIME [epoch: 26 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009235011073980656		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.009235011073980656 | validation: 0.017403184265463726]
	TIME [epoch: 26 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009772529391218178		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.009772529391218178 | validation: 0.016763878808282208]
	TIME [epoch: 26.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009433762467455972		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.009433762467455972 | validation: 0.017621577785492084]
	TIME [epoch: 26.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010235869392188186		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.010235869392188186 | validation: 0.019689812859871522]
	TIME [epoch: 26.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009984208942222792		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.009984208942222792 | validation: 0.0166506655822819]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00913109101534445		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.00913109101534445 | validation: 0.019584445038713615]
	TIME [epoch: 26.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009281967503217721		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.009281967503217721 | validation: 0.01822145211137914]
	TIME [epoch: 26.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009507162732355565		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.009507162732355565 | validation: 0.018175630950055723]
	TIME [epoch: 26.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009874827565066415		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.009874827565066415 | validation: 0.018311219430259432]
	TIME [epoch: 26.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009332919722881031		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.009332919722881031 | validation: 0.01698143520639135]
	TIME [epoch: 26.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009090351481574208		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.009090351481574208 | validation: 0.017543897368070046]
	TIME [epoch: 26.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010206084829535913		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.010206084829535913 | validation: 0.019542095985997616]
	TIME [epoch: 26 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011155162180733015		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.011155162180733015 | validation: 0.016752438837189156]
	TIME [epoch: 26 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009838870450646058		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.009838870450646058 | validation: 0.018073661920406645]
	TIME [epoch: 26 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00924889542697962		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.00924889542697962 | validation: 0.018451842541738143]
	TIME [epoch: 26.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010360179821102787		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.010360179821102787 | validation: 0.019256415862931017]
	TIME [epoch: 26.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009418133577025362		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.009418133577025362 | validation: 0.01855941535531293]
	TIME [epoch: 26.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010555866729875856		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.010555866729875856 | validation: 0.02075157923513018]
	TIME [epoch: 26.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010389646853717039		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.010389646853717039 | validation: 0.017584870508817515]
	TIME [epoch: 26.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009174622657508137		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.009174622657508137 | validation: 0.017761265706900967]
	TIME [epoch: 26.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009435209091221983		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.009435209091221983 | validation: 0.01911910532314879]
	TIME [epoch: 26.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009092670598279904		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.009092670598279904 | validation: 0.016866039472813753]
	TIME [epoch: 26 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008660802532173861		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.008660802532173861 | validation: 0.018224992760150258]
	TIME [epoch: 26.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010112671406272283		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.010112671406272283 | validation: 0.017206752428265533]
	TIME [epoch: 26.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009517037704859746		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.009517037704859746 | validation: 0.016837523397294073]
	TIME [epoch: 26.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009332912229959562		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.009332912229959562 | validation: 0.017467792889707505]
	TIME [epoch: 26.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009800462204757086		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.009800462204757086 | validation: 0.019179803518336546]
	TIME [epoch: 403 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010018228203978186		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.010018228203978186 | validation: 0.018044904177101712]
	TIME [epoch: 55.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009021971618629073		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.009021971618629073 | validation: 0.018057969887720682]
	TIME [epoch: 55.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009015769083083044		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.009015769083083044 | validation: 0.017829560231527864]
	TIME [epoch: 55.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00918606764806694		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.00918606764806694 | validation: 0.017771415030254315]
	TIME [epoch: 55.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009188141792103763		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.009188141792103763 | validation: 0.017680188955120392]
	TIME [epoch: 55.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0090426325653961		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0090426325653961 | validation: 0.018428426821459905]
	TIME [epoch: 55.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009407664361621972		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.009407664361621972 | validation: 0.017425789094582762]
	TIME [epoch: 55.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009629965078186939		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.009629965078186939 | validation: 0.018526803070309548]
	TIME [epoch: 55.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009034763675303255		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.009034763675303255 | validation: 0.018680667827122743]
	TIME [epoch: 55.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009058093237286485		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.009058093237286485 | validation: 0.01870151453841686]
	TIME [epoch: 55.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009259926489964456		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.009259926489964456 | validation: 0.01895461162964217]
	TIME [epoch: 55.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009514478530548657		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.009514478530548657 | validation: 0.01777565812643891]
	TIME [epoch: 55.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011396933993444271		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.011396933993444271 | validation: 0.016774575298288522]
	TIME [epoch: 55.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00906336473181781		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.00906336473181781 | validation: 0.017772264894503575]
	TIME [epoch: 55.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009155145983378278		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.009155145983378278 | validation: 0.017396431131965595]
	TIME [epoch: 55.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01062843192131632		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.01062843192131632 | validation: 0.018864416409617987]
	TIME [epoch: 55.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008683598401841377		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.008683598401841377 | validation: 0.016972060478563148]
	TIME [epoch: 55.2 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008661452469599923		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.008661452469599923 | validation: 0.01775429074611763]
	TIME [epoch: 55.2 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009005661779305583		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.009005661779305583 | validation: 0.018529834118438667]
	TIME [epoch: 55.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009067786153238963		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.009067786153238963 | validation: 0.01758940868814257]
	TIME [epoch: 55.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009093211437540305		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.009093211437540305 | validation: 0.01970066039621864]
	TIME [epoch: 55.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015157812004028005		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.015157812004028005 | validation: 0.05674215176935721]
	TIME [epoch: 55.2 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029322567198889267		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.029322567198889267 | validation: 0.0309514178477265]
	TIME [epoch: 55.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011634291699721256		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.011634291699721256 | validation: 0.018184838601370705]
	TIME [epoch: 55.2 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009155413762859017		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.009155413762859017 | validation: 0.01841685769552455]
	TIME [epoch: 55.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008689241064776351		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.008689241064776351 | validation: 0.017160852746008443]
	TIME [epoch: 55.2 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008657307436092391		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.008657307436092391 | validation: 0.01715762161839656]
	TIME [epoch: 55.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00883712565217613		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.00883712565217613 | validation: 0.017246153377862367]
	TIME [epoch: 55.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008872051249670592		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.008872051249670592 | validation: 0.018103419093912387]
	TIME [epoch: 55.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00851762526948531		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.00851762526948531 | validation: 0.01877550860512966]
	TIME [epoch: 55.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009762158563018703		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.009762158563018703 | validation: 0.017880402425007547]
	TIME [epoch: 55.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008925385058423666		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.008925385058423666 | validation: 0.01676588494593795]
	TIME [epoch: 55.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00883885624294932		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.00883885624294932 | validation: 0.019615962178190612]
	TIME [epoch: 55.2 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009966770137777536		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.009966770137777536 | validation: 0.018168315711614653]
	TIME [epoch: 55.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009192493078552084		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.009192493078552084 | validation: 0.015507819984958426]
	TIME [epoch: 55.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_1036.pth
	Model improved!!!
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008820619183741728		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.008820619183741728 | validation: 0.017647140941027997]
	TIME [epoch: 55.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008844164983370314		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.008844164983370314 | validation: 0.017079166883562754]
	TIME [epoch: 55.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008855215859384294		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.008855215859384294 | validation: 0.016672949245937613]
	TIME [epoch: 55.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008422882292798587		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.008422882292798587 | validation: 0.016290283918848986]
	TIME [epoch: 55.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008360341499834334		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.008360341499834334 | validation: 0.016642473296423342]
	TIME [epoch: 55.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009735016011879555		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.009735016011879555 | validation: 0.020335759677113836]
	TIME [epoch: 55.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00960632297333313		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.00960632297333313 | validation: 0.017117728241885167]
	TIME [epoch: 55.2 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008970078076872123		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.008970078076872123 | validation: 0.016751582610766053]
	TIME [epoch: 55.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009101529697144296		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.009101529697144296 | validation: 0.018393717889619327]
	TIME [epoch: 55.2 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008974289270814192		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.008974289270814192 | validation: 0.016863286014044947]
	TIME [epoch: 55.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008513934834170352		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.008513934834170352 | validation: 0.01803856412967342]
	TIME [epoch: 55.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00895695874828625		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.00895695874828625 | validation: 0.018414507357051774]
	TIME [epoch: 55.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009288081299341304		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.009288081299341304 | validation: 0.018547714124793814]
	TIME [epoch: 55.2 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009753562014243275		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.009753562014243275 | validation: 0.017510687893887728]
	TIME [epoch: 55.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009673583597477251		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.009673583597477251 | validation: 0.01746809753717944]
	TIME [epoch: 55.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00882066931344648		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.00882066931344648 | validation: 0.017675316573825105]
	TIME [epoch: 55.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009029543884486423		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.009029543884486423 | validation: 0.016701769414111105]
	TIME [epoch: 55.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008784038716144714		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.008784038716144714 | validation: 0.018146118561128306]
	TIME [epoch: 55.2 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009028480935175784		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.009028480935175784 | validation: 0.018553489006049563]
	TIME [epoch: 55.2 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009057962826780237		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.009057962826780237 | validation: 0.019492227202863005]
	TIME [epoch: 55.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009351837625698344		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.009351837625698344 | validation: 0.01762463226593582]
	TIME [epoch: 55.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008547545554851125		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.008547545554851125 | validation: 0.017687609069492922]
	TIME [epoch: 55.2 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0087349894579575		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.0087349894579575 | validation: 0.01820876472707205]
	TIME [epoch: 55.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009191962571661818		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.009191962571661818 | validation: 0.017667146340775017]
	TIME [epoch: 55.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009426220257034865		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.009426220257034865 | validation: 0.017213913267702743]
	TIME [epoch: 55.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008755729207768749		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.008755729207768749 | validation: 0.017643156169582473]
	TIME [epoch: 55.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009021938174794866		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.009021938174794866 | validation: 0.017416392594004456]
	TIME [epoch: 55.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008634493319030321		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.008634493319030321 | validation: 0.01726134510048888]
	TIME [epoch: 55.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008954811963215131		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.008954811963215131 | validation: 0.017503779914507835]
	TIME [epoch: 55.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008397281662143907		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.008397281662143907 | validation: 0.01748027531099077]
	TIME [epoch: 55.2 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008564675580743322		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.008564675580743322 | validation: 0.016787131092670805]
	TIME [epoch: 55.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008911507499198052		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.008911507499198052 | validation: 0.016312556321539676]
	TIME [epoch: 55.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008492229274125861		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.008492229274125861 | validation: 0.017438910360247112]
	TIME [epoch: 55.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009329611640344482		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.009329611640344482 | validation: 0.022316507124017664]
	TIME [epoch: 55.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009685504439656607		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.009685504439656607 | validation: 0.01794961562405622]
	TIME [epoch: 55.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009083462734774169		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.009083462734774169 | validation: 0.015960812795169945]
	TIME [epoch: 55.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008123901520873496		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.008123901520873496 | validation: 0.018724295648204152]
	TIME [epoch: 55.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008884097090969772		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.008884097090969772 | validation: 0.016084141219579715]
	TIME [epoch: 55.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008939609203279353		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.008939609203279353 | validation: 0.018160183378477936]
	TIME [epoch: 55.2 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008877722643476183		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.008877722643476183 | validation: 0.01617687505914025]
	TIME [epoch: 55.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008678033454073388		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.008678033454073388 | validation: 0.01628942022231098]
	TIME [epoch: 55.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008733444736724836		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.008733444736724836 | validation: 0.01844326848201101]
	TIME [epoch: 55.2 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008535614249998285		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.008535614249998285 | validation: 0.016793667987876314]
	TIME [epoch: 55.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00833858204493216		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.00833858204493216 | validation: 0.01735577163494329]
	TIME [epoch: 55.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008969163559450948		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.008969163559450948 | validation: 0.01833396504240573]
	TIME [epoch: 55.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008934185366576353		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.008934185366576353 | validation: 0.016223116068965208]
	TIME [epoch: 55.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00864854002514064		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.00864854002514064 | validation: 0.017847800512387085]
	TIME [epoch: 55.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009119001214321645		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.009119001214321645 | validation: 0.016414172924551773]
	TIME [epoch: 55.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008827031219893861		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.008827031219893861 | validation: 0.018301743037797996]
	TIME [epoch: 55.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00866853510501232		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.00866853510501232 | validation: 0.018435630838924216]
	TIME [epoch: 55.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008621435487311577		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.008621435487311577 | validation: 0.017339887515588986]
	TIME [epoch: 55.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008499114462199005		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.008499114462199005 | validation: 0.015419399332052084]
	TIME [epoch: 55.3 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_1088.pth
	Model improved!!!
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009515580756195451		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.009515580756195451 | validation: 0.016757782688457883]
	TIME [epoch: 55.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008767764077323482		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.008767764077323482 | validation: 0.017007139071438415]
	TIME [epoch: 55.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009291129574152516		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.009291129574152516 | validation: 0.01721318486574065]
	TIME [epoch: 55.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00833688833018376		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.00833688833018376 | validation: 0.01705097614573067]
	TIME [epoch: 55.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009026882461294222		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.009026882461294222 | validation: 0.017739423246923146]
	TIME [epoch: 55.2 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008816141901203702		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.008816141901203702 | validation: 0.016571943834283665]
	TIME [epoch: 55.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008685186686482459		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.008685186686482459 | validation: 0.017228429663593184]
	TIME [epoch: 55.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00835540656595279		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.00835540656595279 | validation: 0.01607162153906734]
	TIME [epoch: 55.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008997964062774859		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.008997964062774859 | validation: 0.018347501437329848]
	TIME [epoch: 55.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009180675814705455		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.009180675814705455 | validation: 0.016578144874160188]
	TIME [epoch: 55.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008318559080488126		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.008318559080488126 | validation: 0.016731303619209437]
	TIME [epoch: 55.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00828736930522312		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.00828736930522312 | validation: 0.017753731908049065]
	TIME [epoch: 55.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013128044409332637		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.013128044409332637 | validation: 0.0319508634677116]
	TIME [epoch: 55.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012147297007939165		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.012147297007939165 | validation: 0.018576852028743505]
	TIME [epoch: 55.2 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007986163139244445		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.007986163139244445 | validation: 0.01647307634767515]
	TIME [epoch: 55.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008102826386909517		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.008102826386909517 | validation: 0.017858443153379693]
	TIME [epoch: 55.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008743290284969996		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.008743290284969996 | validation: 0.01893748205218249]
	TIME [epoch: 55.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008092425916229332		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.008092425916229332 | validation: 0.015857698979434284]
	TIME [epoch: 55.2 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00803399842862325		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.00803399842862325 | validation: 0.018399093351941447]
	TIME [epoch: 55.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008565752014438318		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.008565752014438318 | validation: 0.016642617110132967]
	TIME [epoch: 55.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007804383052207911		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.007804383052207911 | validation: 0.01697603590730584]
	TIME [epoch: 55.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008211905917728041		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.008211905917728041 | validation: 0.01744988697902759]
	TIME [epoch: 55.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00858158843262195		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.00858158843262195 | validation: 0.03592740125796341]
	TIME [epoch: 55.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01748357454845437		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.01748357454845437 | validation: 0.023973001641387527]
	TIME [epoch: 55.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009685943449167497		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.009685943449167497 | validation: 0.018138635488726303]
	TIME [epoch: 55.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008163457781412215		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.008163457781412215 | validation: 0.017503656912470345]
	TIME [epoch: 55.2 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008556842944721949		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.008556842944721949 | validation: 0.01834516955638178]
	TIME [epoch: 55.2 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008099351342331387		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.008099351342331387 | validation: 0.01651512349507965]
	TIME [epoch: 55.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008768044976827483		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.008768044976827483 | validation: 0.016948523087485484]
	TIME [epoch: 55.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008200033939392887		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.008200033939392887 | validation: 0.016503373538266152]
	TIME [epoch: 55.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008177722005686321		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.008177722005686321 | validation: 0.01655737314468811]
	TIME [epoch: 55.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008344476414178646		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.008344476414178646 | validation: 0.017465933456356827]
	TIME [epoch: 55.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008495142942896123		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.008495142942896123 | validation: 0.017232894989338512]
	TIME [epoch: 55.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008597675536062584		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.008597675536062584 | validation: 0.015760322227347245]
	TIME [epoch: 55.2 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008654204847502608		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.008654204847502608 | validation: 0.017774816149741746]
	TIME [epoch: 55.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008388842350884919		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.008388842350884919 | validation: 0.016252503071897014]
	TIME [epoch: 55.2 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009129425797407654		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.009129425797407654 | validation: 0.017109239243424505]
	TIME [epoch: 55.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008423093250228499		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.008423093250228499 | validation: 0.016501422487679062]
	TIME [epoch: 55.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008889011169459438		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.008889011169459438 | validation: 0.016849640425497782]
	TIME [epoch: 55.2 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007890524280593206		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.007890524280593206 | validation: 0.01702146284846062]
	TIME [epoch: 55.2 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00817984970789629		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.00817984970789629 | validation: 0.017325674150627628]
	TIME [epoch: 55.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00885425264618491		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.00885425264618491 | validation: 0.01615686462953264]
	TIME [epoch: 55.2 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008190418221295515		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.008190418221295515 | validation: 0.016275630655602166]
	TIME [epoch: 55.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008314317292275553		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.008314317292275553 | validation: 0.016845990320393897]
	TIME [epoch: 55.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008300290419130473		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.008300290419130473 | validation: 0.015754962531220926]
	TIME [epoch: 55.2 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008759849828913898		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.008759849828913898 | validation: 0.017494478025853813]
	TIME [epoch: 55.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008560561568128709		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.008560561568128709 | validation: 0.0188848353554837]
	TIME [epoch: 55.2 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008440537675465277		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.008440537675465277 | validation: 0.01708446402368003]
	TIME [epoch: 55.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008050003151595018		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.008050003151595018 | validation: 0.015687532197396943]
	TIME [epoch: 55.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008302631558673501		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.008302631558673501 | validation: 0.017045401495010423]
	TIME [epoch: 55.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008305510239785494		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.008305510239785494 | validation: 0.01624324335203758]
	TIME [epoch: 55.2 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00805088582312733		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.00805088582312733 | validation: 0.01694228439348651]
	TIME [epoch: 55.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008506569014400904		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.008506569014400904 | validation: 0.016774643503353743]
	TIME [epoch: 55.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007756285383224376		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.007756285383224376 | validation: 0.017143128759488216]
	TIME [epoch: 55.2 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008256256017987755		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.008256256017987755 | validation: 0.017191949851477744]
	TIME [epoch: 55.2 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008209016435656686		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.008209016435656686 | validation: 0.016390984068649805]
	TIME [epoch: 55.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008856126684228013		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.008856126684228013 | validation: 0.01585482522814948]
	TIME [epoch: 55.2 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008253701646416495		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.008253701646416495 | validation: 0.01585035658071827]
	TIME [epoch: 55.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008302310482855249		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.008302310482855249 | validation: 0.016641375499234293]
	TIME [epoch: 55.2 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008122658001533561		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.008122658001533561 | validation: 0.016693330332351763]
	TIME [epoch: 55.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008691864204571863		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.008691864204571863 | validation: 0.01687345469886777]
	TIME [epoch: 55.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00966414819432262		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.00966414819432262 | validation: 0.0163279211044364]
	TIME [epoch: 55.2 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008628972477466		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.008628972477466 | validation: 0.017008797744148735]
	TIME [epoch: 55.2 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00850938201306914		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.00850938201306914 | validation: 0.015751636030764724]
	TIME [epoch: 55.2 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007832963689680006		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.007832963689680006 | validation: 0.016328256138465527]
	TIME [epoch: 55.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007908521845047534		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.007908521845047534 | validation: 0.01771847555304197]
	TIME [epoch: 55.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008150686195424785		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.008150686195424785 | validation: 0.018744501097586123]
	TIME [epoch: 55.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008304687790668876		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.008304687790668876 | validation: 0.016436647363656305]
	TIME [epoch: 55.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008435472663806622		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.008435472663806622 | validation: 0.015580567639204766]
	TIME [epoch: 55.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008016456499651024		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.008016456499651024 | validation: 0.016024975717327233]
	TIME [epoch: 55.2 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008370254796381083		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.008370254796381083 | validation: 0.017327353724630238]
	TIME [epoch: 55.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008020601327712606		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.008020601327712606 | validation: 0.016229550984226376]
	TIME [epoch: 55.2 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00845100020090547		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.00845100020090547 | validation: 0.016183447608901648]
	TIME [epoch: 55.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008067147256537826		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.008067147256537826 | validation: 0.017232898345110367]
	TIME [epoch: 55.2 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008205861737779358		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.008205861737779358 | validation: 0.015881722207707387]
	TIME [epoch: 55.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007504670554763076		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.007504670554763076 | validation: 0.017519732228118964]
	TIME [epoch: 55.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008650503272346441		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.008650503272346441 | validation: 0.016732815590546776]
	TIME [epoch: 55.2 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008759705642089975		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.008759705642089975 | validation: 0.01962197442557998]
	TIME [epoch: 55.2 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008666339773670349		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.008666339773670349 | validation: 0.0172198552614481]
	TIME [epoch: 55.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008114759220411638		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.008114759220411638 | validation: 0.016883882705625602]
	TIME [epoch: 55.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008348949924719636		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.008348949924719636 | validation: 0.016021866268720826]
	TIME [epoch: 55.2 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008064318704082609		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.008064318704082609 | validation: 0.01613194145803976]
	TIME [epoch: 55.2 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008112673011404177		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.008112673011404177 | validation: 0.015960846292413434]
	TIME [epoch: 55.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008183408054104361		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.008183408054104361 | validation: 0.016513800038432825]
	TIME [epoch: 55.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00802981353066029		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.00802981353066029 | validation: 0.016692237681013095]
	TIME [epoch: 55.2 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008327992049030956		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.008327992049030956 | validation: 0.01691643096160074]
	TIME [epoch: 55.2 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007910056478929133		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.007910056478929133 | validation: 0.01831071883688104]
	TIME [epoch: 55.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00803186161975169		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.00803186161975169 | validation: 0.018413641981481106]
	TIME [epoch: 55.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008372482277008951		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.008372482277008951 | validation: 0.01603784470834303]
	TIME [epoch: 55.2 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00820595222298114		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.00820595222298114 | validation: 0.016085153034497493]
	TIME [epoch: 55.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008429932491233354		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.008429932491233354 | validation: 0.016167057792938696]
	TIME [epoch: 55.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008268614961228394		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.008268614961228394 | validation: 0.01731769885827478]
	TIME [epoch: 55.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008589056561636107		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.008589056561636107 | validation: 0.018218143369559424]
	TIME [epoch: 55.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008238858715147766		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.008238858715147766 | validation: 0.015964092638389393]
	TIME [epoch: 55.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008590020099230508		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.008590020099230508 | validation: 0.017726763771173326]
	TIME [epoch: 55.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009234133403057825		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.009234133403057825 | validation: 0.016194767271205175]
	TIME [epoch: 55.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008025415293370131		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.008025415293370131 | validation: 0.01640272461921359]
	TIME [epoch: 55.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010429765657267099		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.010429765657267099 | validation: 0.019774928237437647]
	TIME [epoch: 55.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0080595177637593		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.0080595177637593 | validation: 0.017543722785571203]
	TIME [epoch: 55.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007961399635806252		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.007961399635806252 | validation: 0.015814309924552343]
	TIME [epoch: 55.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007731771905206882		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.007731771905206882 | validation: 0.016430688445612563]
	TIME [epoch: 55.2 sec]
	Saving model to: out/model_training/model_phi1_1a_saddle_v1b_0_v_mmd1_20250610_204037/states/model_phi1_1a_saddle_v1b_0_v_mmd1_1189.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 29984.470 seconds.
