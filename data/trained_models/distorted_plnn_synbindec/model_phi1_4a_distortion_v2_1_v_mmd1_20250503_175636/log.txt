Args:
Namespace(name='model_phi1_4a_distortion_v2_1_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1223870958

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.639164534150621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.639164534150621 | validation: 5.048411125949952]
	TIME [epoch: 165 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.348978789577435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.348978789577435 | validation: 5.922913791072499]
	TIME [epoch: 0.813 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.3456629342806785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3456629342806785 | validation: 5.728950613715236]
	TIME [epoch: 0.706 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.665315124558429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.665315124558429 | validation: 5.4047504258475065]
	TIME [epoch: 0.705 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.271977770080911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.271977770080911 | validation: 5.020229405480309]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.124837971171472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.124837971171472 | validation: 4.299043548892948]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.902030165799566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902030165799566 | validation: 3.7163308348371373]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.677369836693761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.677369836693761 | validation: 3.202700036771068]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.247813899124484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.247813899124484 | validation: 3.6589993416909596]
	TIME [epoch: 0.7 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.331154124002793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.331154124002793 | validation: 5.605472723545687]
	TIME [epoch: 0.697 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.792673460433105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792673460433105 | validation: 5.468842131438007]
	TIME [epoch: 0.698 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.274779967823395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.274779967823395 | validation: 5.349110382518699]
	TIME [epoch: 0.701 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.224848701097365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.224848701097365 | validation: 5.509980882247176]
	TIME [epoch: 0.705 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9386097593224405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9386097593224405 | validation: 5.520088999824866]
	TIME [epoch: 0.706 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8498150816767254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8498150816767254 | validation: 5.107067891955153]
	TIME [epoch: 0.71 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.829627591483296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.829627591483296 | validation: 5.4994271845121725]
	TIME [epoch: 0.704 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.920614436973942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.920614436973942 | validation: 4.985787586413949]
	TIME [epoch: 0.704 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6579397773956677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6579397773956677 | validation: 5.046846073258383]
	TIME [epoch: 0.704 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4588128205082405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4588128205082405 | validation: 5.334446799547386]
	TIME [epoch: 0.699 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.585599296106874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.585599296106874 | validation: 4.680304999956903]
	TIME [epoch: 0.7 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021923474443571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.021923474443571 | validation: 4.6222071189506835]
	TIME [epoch: 0.699 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3679708444195358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3679708444195358 | validation: 4.55186579450467]
	TIME [epoch: 0.699 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.351892797582582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.351892797582582 | validation: 2.9496742233371847]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.741715492873029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.741715492873029 | validation: 2.57264352260369]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.189198849251431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.189198849251431 | validation: 3.3136167300843753]
	TIME [epoch: 0.705 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9564404090435454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9564404090435454 | validation: 2.403349225004175]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.384034237961435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.384034237961435 | validation: 2.3301249829152137]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3776430216047197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3776430216047197 | validation: 2.368975207107803]
	TIME [epoch: 0.705 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.235795794541129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.235795794541129 | validation: 1.9478150586723666]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.091391214821152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.091391214821152 | validation: 2.017920759693877]
	TIME [epoch: 0.706 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0463848180529243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0463848180529243 | validation: 2.1735381342183024]
	TIME [epoch: 0.708 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.596201212631158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.596201212631158 | validation: 2.8044156349985006]
	TIME [epoch: 0.707 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.43182782766583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.43182782766583 | validation: 1.8816834245814698]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.00649742979571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.00649742979571 | validation: 1.7395001830758403]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9405244157150265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9405244157150265 | validation: 1.9797309527248594]
	TIME [epoch: 0.702 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0111051070181025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0111051070181025 | validation: 1.7191099909390184]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.085493395949796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.085493395949796 | validation: 2.104663727632658]
	TIME [epoch: 0.701 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9692858640724038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9692858640724038 | validation: 1.5963203906796366]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8728056755630715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8728056755630715 | validation: 1.8451991590557482]
	TIME [epoch: 0.703 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.851043553160078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.851043553160078 | validation: 1.6029563743428277]
	TIME [epoch: 0.7 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9691425242015521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9691425242015521 | validation: 2.3357150924136865]
	TIME [epoch: 0.7 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0524944370223825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0524944370223825 | validation: 1.6396370113823915]
	TIME [epoch: 0.698 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8873821174215573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8873821174215573 | validation: 1.7013763222722607]
	TIME [epoch: 0.7 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.740476037702647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.740476037702647 | validation: 1.4690780292294168]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7174589160210179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7174589160210179 | validation: 1.862086828875117]
	TIME [epoch: 0.7 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7548788959780024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7548788959780024 | validation: 1.7129051939032323]
	TIME [epoch: 0.699 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0577839116947763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0577839116947763 | validation: 2.156617193043637]
	TIME [epoch: 0.699 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8605790526021475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8605790526021475 | validation: 1.5093039773591175]
	TIME [epoch: 0.698 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7166471430035324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7166471430035324 | validation: 1.598094368349048]
	TIME [epoch: 0.701 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6340957769601676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6340957769601676 | validation: 1.456754360573066]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5917555194374806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5917555194374806 | validation: 1.636909143495096]
	TIME [epoch: 0.706 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5484512906163435		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.5484512906163435 | validation: 1.4421129524815384]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.648961909761619		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.648961909761619 | validation: 3.4401195006286716]
	TIME [epoch: 0.706 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4811058295141497		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.4811058295141497 | validation: 1.780221030811784]
	TIME [epoch: 0.704 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9165554201896993		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.9165554201896993 | validation: 1.5105755310651419]
	TIME [epoch: 0.708 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5578328904946352		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.5578328904946352 | validation: 1.9318148280298033]
	TIME [epoch: 0.702 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7112813643462428		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.7112813643462428 | validation: 1.4680673804557438]
	TIME [epoch: 0.702 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.554336974218512		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.554336974218512 | validation: 1.6130516624863664]
	TIME [epoch: 0.702 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.47916458067655		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.47916458067655 | validation: 1.6502241525206445]
	TIME [epoch: 0.7 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4407606382355602		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.4407606382355602 | validation: 1.5100486127782144]
	TIME [epoch: 0.703 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.407718584042494		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.407718584042494 | validation: 1.8154811463240472]
	TIME [epoch: 0.7 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4417243772707884		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.4417243772707884 | validation: 1.6283320302278783]
	TIME [epoch: 0.701 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.694762887991522		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.694762887991522 | validation: 2.29952956610819]
	TIME [epoch: 0.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6291487515416077		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.6291487515416077 | validation: 1.542164066709518]
	TIME [epoch: 0.698 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4729599371452746		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.4729599371452746 | validation: 1.615715500243368]
	TIME [epoch: 0.699 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3627760886325546		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3627760886325546 | validation: 1.5406954325251494]
	TIME [epoch: 0.698 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3272376841749394		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.3272376841749394 | validation: 1.569736526300579]
	TIME [epoch: 0.698 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3014482894588566		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3014482894588566 | validation: 1.665627427075811]
	TIME [epoch: 0.697 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2796809743639677		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.2796809743639677 | validation: 1.4910346009958584]
	TIME [epoch: 0.696 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356264074898156		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.356264074898156 | validation: 2.855729194438289]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9292519662354233		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.9292519662354233 | validation: 1.608526376319407]
	TIME [epoch: 0.699 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6497316109275755		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.6497316109275755 | validation: 1.4899136095331134]
	TIME [epoch: 0.702 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2969484897822747		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2969484897822747 | validation: 1.9531009140948652]
	TIME [epoch: 0.698 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.453110503637031		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.453110503637031 | validation: 1.4760826231126214]
	TIME [epoch: 0.698 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3124931294135949		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.3124931294135949 | validation: 1.4920414567588274]
	TIME [epoch: 0.697 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2852420466366146		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2852420466366146 | validation: 1.7015372981566828]
	TIME [epoch: 0.699 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2921960418220346		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.2921960418220346 | validation: 1.49091760136747]
	TIME [epoch: 0.697 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262328325636944		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.262328325636944 | validation: 1.6140831024453945]
	TIME [epoch: 0.707 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2389073752622883		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.2389073752622883 | validation: 1.5205355881413012]
	TIME [epoch: 0.697 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2222167269858515		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2222167269858515 | validation: 1.5796029170186943]
	TIME [epoch: 0.698 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2106048950940713		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.2106048950940713 | validation: 1.48599913699284]
	TIME [epoch: 0.698 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2202696797695998		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2202696797695998 | validation: 1.8181347769986234]
	TIME [epoch: 0.7 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2879524424513913		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.2879524424513913 | validation: 1.6844259686298302]
	TIME [epoch: 0.698 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6500259479590178		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.6500259479590178 | validation: 1.6931134983189193]
	TIME [epoch: 0.698 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2561138663529219		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2561138663529219 | validation: 1.4465083935212015]
	TIME [epoch: 0.696 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2123661946955568		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2123661946955568 | validation: 1.4276453006814696]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073267173504374		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.2073267173504374 | validation: 1.5880949587359288]
	TIME [epoch: 0.709 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2035547555136297		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2035547555136297 | validation: 1.4474952261666187]
	TIME [epoch: 0.705 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2158877648938853		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2158877648938853 | validation: 1.6476362836789655]
	TIME [epoch: 0.703 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2116941828330667		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.2116941828330667 | validation: 1.4639263316875153]
	TIME [epoch: 0.704 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2688745897380267		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.2688745897380267 | validation: 1.967502762460415]
	TIME [epoch: 0.704 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.382650656558491		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.382650656558491 | validation: 1.4654602941173387]
	TIME [epoch: 0.704 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4077561563808023		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.4077561563808023 | validation: 1.4625327876599017]
	TIME [epoch: 0.704 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1846101528623545		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.1846101528623545 | validation: 1.702072176995776]
	TIME [epoch: 0.703 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.263760951305464		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.263760951305464 | validation: 1.4140245675608973]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2288388763938032		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2288388763938032 | validation: 1.502431815494481]
	TIME [epoch: 0.706 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1997547093578884		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.1997547093578884 | validation: 1.5794249461131544]
	TIME [epoch: 0.706 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966571160587753		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.1966571160587753 | validation: 1.4680449405432834]
	TIME [epoch: 0.706 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1928491710234834		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.1928491710234834 | validation: 1.5819576782982612]
	TIME [epoch: 0.705 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1932343297047703		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.1932343297047703 | validation: 1.44892439579532]
	TIME [epoch: 0.706 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1994357304944876		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1994357304944876 | validation: 1.6839856904733048]
	TIME [epoch: 0.701 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2184167240587416		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2184167240587416 | validation: 1.430575033837058]
	TIME [epoch: 0.699 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2983323499732413		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.2983323499732413 | validation: 1.8407013794560836]
	TIME [epoch: 0.698 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.285177694298701		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.285177694298701 | validation: 1.4260975394251811]
	TIME [epoch: 0.7 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2588274844069332		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.2588274844069332 | validation: 1.4918085280165743]
	TIME [epoch: 0.699 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17584457407234		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.17584457407234 | validation: 1.5539641704614127]
	TIME [epoch: 0.698 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019117350510065		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.2019117350510065 | validation: 1.399624457743977]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2079005485931944		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2079005485931944 | validation: 1.5675354813108018]
	TIME [epoch: 0.706 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1754277055031295		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1754277055031295 | validation: 1.4504824884260465]
	TIME [epoch: 0.7 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1841051716187476		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1841051716187476 | validation: 1.5634265450851974]
	TIME [epoch: 0.699 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1793690690401026		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.1793690690401026 | validation: 1.3807438405789778]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2169210074360501		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2169210074360501 | validation: 1.7581097531197245]
	TIME [epoch: 0.7 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2652773164615163		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.2652773164615163 | validation: 1.4118131082479366]
	TIME [epoch: 0.706 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31639719064075		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.31639719064075 | validation: 1.5827577757547093]
	TIME [epoch: 0.704 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.193623927546658		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.193623927546658 | validation: 1.4882959845785866]
	TIME [epoch: 0.705 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1766894854424956		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1766894854424956 | validation: 1.4821877931823737]
	TIME [epoch: 0.708 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1773168868475397		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.1773168868475397 | validation: 1.5868523956064442]
	TIME [epoch: 0.705 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179382494494152		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.179382494494152 | validation: 1.448736693518276]
	TIME [epoch: 0.703 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179960586566878		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.179960586566878 | validation: 1.5731001859301017]
	TIME [epoch: 0.704 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1836422160238487		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1836422160238487 | validation: 1.4193537216194398]
	TIME [epoch: 0.704 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2232543948102066		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.2232543948102066 | validation: 1.8054586212662125]
	TIME [epoch: 0.704 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2719606142633288		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2719606142633288 | validation: 1.45058248711531]
	TIME [epoch: 0.704 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3562622251864072		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.3562622251864072 | validation: 1.5052330107394036]
	TIME [epoch: 0.704 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.170782312896565		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.170782312896565 | validation: 1.6318428254806001]
	TIME [epoch: 0.702 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.214111202070886		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.214111202070886 | validation: 1.3734785528334799]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2141668776981618		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2141668776981618 | validation: 1.4929786967646739]
	TIME [epoch: 0.706 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1812886193230592		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1812886193230592 | validation: 1.5293560683138185]
	TIME [epoch: 0.709 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1858722409382025		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1858722409382025 | validation: 1.471745949895133]
	TIME [epoch: 0.708 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1804745943475807		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1804745943475807 | validation: 1.5529918241168978]
	TIME [epoch: 0.706 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.173483328803385		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.173483328803385 | validation: 1.468679895236366]
	TIME [epoch: 0.705 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1600791392923477		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.1600791392923477 | validation: 1.5351642222280653]
	TIME [epoch: 0.704 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1693158574031273		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1693158574031273 | validation: 1.409137052273579]
	TIME [epoch: 0.706 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1913271417830975		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1913271417830975 | validation: 1.822007301480698]
	TIME [epoch: 0.706 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.277708148521065		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.277708148521065 | validation: 1.4347321434540738]
	TIME [epoch: 0.707 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3951202946604122		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.3951202946604122 | validation: 1.4403797092092356]
	TIME [epoch: 0.705 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1935409488961417		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.1935409488961417 | validation: 1.6592868347731242]
	TIME [epoch: 0.705 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.239581201676889		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.239581201676889 | validation: 1.4074710366676575]
	TIME [epoch: 0.703 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2057053620224962		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2057053620224962 | validation: 1.4499515866007162]
	TIME [epoch: 0.707 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1784855874066358		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1784855874066358 | validation: 1.5763923115066651]
	TIME [epoch: 0.708 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1890779581739346		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1890779581739346 | validation: 1.403751502677394]
	TIME [epoch: 0.707 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1848457547468305		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1848457547468305 | validation: 1.4877003411252443]
	TIME [epoch: 0.706 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690308109478331		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1690308109478331 | validation: 1.460367971340258]
	TIME [epoch: 0.705 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1703341859955978		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1703341859955978 | validation: 1.4933026495345663]
	TIME [epoch: 0.704 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1658260082802345		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1658260082802345 | validation: 1.5225236317226987]
	TIME [epoch: 0.705 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625336721737314		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1625336721737314 | validation: 1.4666576216049925]
	TIME [epoch: 0.705 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1472582140712304		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1472582140712304 | validation: 1.4423111609597599]
	TIME [epoch: 0.705 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.158721412009775		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.158721412009775 | validation: 1.4771448869259005]
	TIME [epoch: 0.703 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1546613675868165		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1546613675868165 | validation: 1.519476237335869]
	TIME [epoch: 0.704 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1691033886963587		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1691033886963587 | validation: 1.436130181088008]
	TIME [epoch: 0.705 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2654771235801077		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.2654771235801077 | validation: 2.454711805262512]
	TIME [epoch: 0.706 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.65987585937613		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.65987585937613 | validation: 1.3471349179028882]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3051016271074911		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3051016271074911 | validation: 1.3601959904854835]
	TIME [epoch: 0.708 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2078683453802963		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.2078683453802963 | validation: 1.5931050745614168]
	TIME [epoch: 0.707 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2230261053666749		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2230261053666749 | validation: 1.4384995725341359]
	TIME [epoch: 0.705 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1736332593258711		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.1736332593258711 | validation: 1.4238656068703885]
	TIME [epoch: 0.706 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171913166333124		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.171913166333124 | validation: 1.4924199743578113]
	TIME [epoch: 0.715 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1650497988936501		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1650497988936501 | validation: 1.497358218730362]
	TIME [epoch: 0.705 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679118623719698		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1679118623719698 | validation: 1.4526298817605112]
	TIME [epoch: 0.704 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.167067555470256		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.167067555470256 | validation: 1.4752697932423176]
	TIME [epoch: 0.705 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1644633453037272		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.1644633453037272 | validation: 1.4771522346605093]
	TIME [epoch: 0.705 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1482432560151592		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1482432560151592 | validation: 1.506862077012832]
	TIME [epoch: 0.705 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615900105018777		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1615900105018777 | validation: 1.466602270678076]
	TIME [epoch: 0.704 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1541443241783356		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1541443241783356 | validation: 1.5105844306531644]
	TIME [epoch: 0.705 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1570473855174948		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1570473855174948 | validation: 1.396530582502353]
	TIME [epoch: 0.707 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911924442546054		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1911924442546054 | validation: 1.8400468700259758]
	TIME [epoch: 0.705 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2776747354264952		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2776747354264952 | validation: 1.3841647185075034]
	TIME [epoch: 0.705 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.306172103799031		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.306172103799031 | validation: 1.4145843067783026]
	TIME [epoch: 0.706 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1539497404305203		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1539497404305203 | validation: 1.5798409125287811]
	TIME [epoch: 0.706 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1902678724028153		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1902678724028153 | validation: 1.3910159503214787]
	TIME [epoch: 0.706 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1821083342890173		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1821083342890173 | validation: 1.489085189728435]
	TIME [epoch: 0.705 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574874950981053		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1574874950981053 | validation: 1.4999996857604412]
	TIME [epoch: 0.706 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1607672811856182		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1607672811856182 | validation: 1.4481810340044488]
	TIME [epoch: 0.703 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1618316229716728		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.1618316229716728 | validation: 1.592252589675145]
	TIME [epoch: 0.704 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1618002031954826		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1618002031954826 | validation: 1.3934420293372694]
	TIME [epoch: 0.705 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1783986146296617		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.1783986146296617 | validation: 1.6134509818940388]
	TIME [epoch: 0.707 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1880731182234496		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1880731182234496 | validation: 1.3776987898867499]
	TIME [epoch: 0.705 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2118557957235856		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.2118557957235856 | validation: 1.5922217043965743]
	TIME [epoch: 0.704 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1718573576280213		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1718573576280213 | validation: 1.3789552931666984]
	TIME [epoch: 0.705 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1724798964291632		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1724798964291632 | validation: 1.5158807267486283]
	TIME [epoch: 0.707 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1496580714964089		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1496580714964089 | validation: 1.4018752197599782]
	TIME [epoch: 0.704 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1529667382947788		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1529667382947788 | validation: 1.5512853565401468]
	TIME [epoch: 0.706 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1670021456810609		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1670021456810609 | validation: 1.3660918383589438]
	TIME [epoch: 0.707 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1934022474504613		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.1934022474504613 | validation: 1.7911465905196935]
	TIME [epoch: 0.707 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2582155260597792		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2582155260597792 | validation: 1.377581932951166]
	TIME [epoch: 0.705 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2743630962961447		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.2743630962961447 | validation: 1.4090940366631242]
	TIME [epoch: 0.706 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1499241642982696		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1499241642982696 | validation: 1.607118347479492]
	TIME [epoch: 0.705 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1873535808519744		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1873535808519744 | validation: 1.389111909465849]
	TIME [epoch: 0.707 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1743218136129019		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1743218136129019 | validation: 1.4859941834427748]
	TIME [epoch: 0.707 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1387092682197326		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1387092682197326 | validation: 1.4717771429874362]
	TIME [epoch: 0.706 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1432027721065472		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1432027721065472 | validation: 1.406110363677243]
	TIME [epoch: 0.704 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1537994954804767		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1537994954804767 | validation: 1.4930780824270684]
	TIME [epoch: 0.705 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1412973033910294		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1412973033910294 | validation: 1.4075017276213353]
	TIME [epoch: 0.704 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140540748392131		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.140540748392131 | validation: 1.4768384413191697]
	TIME [epoch: 0.705 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1461178294846393		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1461178294846393 | validation: 1.3717099972923024]
	TIME [epoch: 0.705 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498498877031968		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.1498498877031968 | validation: 1.8131575122975925]
	TIME [epoch: 0.704 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2838245046993262		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2838245046993262 | validation: 1.3864725154529667]
	TIME [epoch: 0.703 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3814834677959535		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.3814834677959535 | validation: 1.3263137014754576]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1683254857836733		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.1683254857836733 | validation: 1.7697938598056888]
	TIME [epoch: 0.707 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2428111915953794		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2428111915953794 | validation: 1.3205808726110755]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1704865830491509		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1704865830491509 | validation: 1.4071595196938729]
	TIME [epoch: 0.704 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1462896286089213		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.1462896286089213 | validation: 1.5256298588555373]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1661471868099749		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1661471868099749 | validation: 1.3901642913458074]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1605912610676599		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1605912610676599 | validation: 1.4167418783612709]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1333822546027772		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1333822546027772 | validation: 1.494742229796615]
	TIME [epoch: 1.37 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1383925211321633		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.1383925211321633 | validation: 1.3871286790674557]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1516669791099905		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.1516669791099905 | validation: 1.5075478935898925]
	TIME [epoch: 1.37 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1407445401982956		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1407445401982956 | validation: 1.4093797442234945]
	TIME [epoch: 1.37 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127767124117374		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.127767124117374 | validation: 1.4839169774218217]
	TIME [epoch: 1.37 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134359577864632		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.134359577864632 | validation: 1.3831971539312928]
	TIME [epoch: 1.37 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1649350957006417		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1649350957006417 | validation: 1.6800270893311862]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1856276550436424		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1856276550436424 | validation: 1.322587440113567]
	TIME [epoch: 1.37 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2582607054174033		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2582607054174033 | validation: 1.4230759859695268]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1296139168096553		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1296139168096553 | validation: 1.5015708581247744]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.158763627933603		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.158763627933603 | validation: 1.381093361011416]
	TIME [epoch: 1.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.163094926708313		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.163094926708313 | validation: 1.506123380343726]
	TIME [epoch: 1.37 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144865298641453		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.144865298641453 | validation: 1.3694796345679483]
	TIME [epoch: 1.37 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336883115453775		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1336883115453775 | validation: 1.4691100976195373]
	TIME [epoch: 1.37 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1294833862798181		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1294833862798181 | validation: 1.3249280268043166]
	TIME [epoch: 1.37 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1235696505717996		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.1235696505717996 | validation: 1.5404809369704933]
	TIME [epoch: 1.37 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1335119689694058		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1335119689694058 | validation: 1.305468068193754]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1903768367895804		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1903768367895804 | validation: 1.6323245196998533]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1808844764915798		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.1808844764915798 | validation: 1.3089963776298625]
	TIME [epoch: 1.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1808543793419295		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1808543793419295 | validation: 1.4557550790982725]
	TIME [epoch: 1.37 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121755091980871		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.121755091980871 | validation: 1.4197031372057864]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1113358339067134		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1113358339067134 | validation: 1.3744602021552599]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1089102782119467		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1089102782119467 | validation: 1.4535593292137936]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064226694540602		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1064226694540602 | validation: 1.319821191316965]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254274966069528		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1254274966069528 | validation: 1.7275419800165697]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2174703098709632		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.2174703098709632 | validation: 1.2891857654561667]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3104977461255651		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.3104977461255651 | validation: 1.3371463490185622]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1154620851891444		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1154620851891444 | validation: 1.5598663500848264]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1611979830222574		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1611979830222574 | validation: 1.3140807022465122]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164498922760735		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.164498922760735 | validation: 1.4598203578244626]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031567252900125		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1031567252900125 | validation: 1.4315722999266407]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107458136804977		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.107458136804977 | validation: 1.3334615069058449]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1177852864091171		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.1177852864091171 | validation: 1.482072065010051]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131905995447201		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.131905995447201 | validation: 1.3184417619738313]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1330015955239419		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1330015955239419 | validation: 1.5961797051500026]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1705038867338171		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.1705038867338171 | validation: 1.2746310444889128]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1893716653125297		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1893716653125297 | validation: 1.4070232585664186]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10170636216731		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.10170636216731 | validation: 1.4458557611770884]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1044971550834368		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1044971550834368 | validation: 1.2956095499231914]
	TIME [epoch: 1.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1328157918972321		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1328157918972321 | validation: 1.517885186193593]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1335808229264495		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1335808229264495 | validation: 1.262602190122204]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1341266621633783		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1341266621633783 | validation: 1.4582287580883821]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1037181952525879		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1037181952525879 | validation: 1.296451720554451]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1029984723380812		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.1029984723380812 | validation: 1.3897909954397443]
	TIME [epoch: 1.37 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091395110825795		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.091395110825795 | validation: 1.2968299495401807]
	TIME [epoch: 1.37 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102581339611755		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.102581339611755 | validation: 1.50693448491569]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1051376670953401		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1051376670953401 | validation: 1.2767909807568767]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2223183339557768		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.2223183339557768 | validation: 1.504239169125652]
	TIME [epoch: 1.37 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1293965450020937		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1293965450020937 | validation: 1.2515438188449772]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0777277227295476		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0777277227295476 | validation: 1.3408158436334077]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.076270936379529		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.076270936379529 | validation: 1.3248723414014976]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0631259369686594		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.0631259369686594 | validation: 1.2821050044278313]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068095498792176		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.068095498792176 | validation: 1.4522144547036469]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0841818091071225		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.0841818091071225 | validation: 1.2514837143685]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2529444659871078		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.2529444659871078 | validation: 1.5485794389049925]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1440101609505302		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.1440101609505302 | validation: 1.221101086226376]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0954692065986604		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0954692065986604 | validation: 1.3786804948631586]
	TIME [epoch: 1.37 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0627690578014515		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.0627690578014515 | validation: 1.2730243061583146]
	TIME [epoch: 1.37 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0618072945866757		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.0618072945866757 | validation: 1.4001739471852284]
	TIME [epoch: 1.37 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.081327911526723		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.081327911526723 | validation: 1.2267673552624847]
	TIME [epoch: 1.37 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1277159603071574		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1277159603071574 | validation: 1.5565919001739428]
	TIME [epoch: 1.37 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1406424239808408		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1406424239808408 | validation: 1.2184525681524834]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1725804500527717		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.1725804500527717 | validation: 1.3219600094644675]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0583325615902786		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.0583325615902786 | validation: 1.343086563516266]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0463733382746607		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0463733382746607 | validation: 1.2576818844589077]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0734831473667517		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0734831473667517 | validation: 1.4860597134621871]
	TIME [epoch: 1.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1085907830817365		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.1085907830817365 | validation: 1.1852465150771003]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610758664061636		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.1610758664061636 | validation: 1.3730041447773371]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.072493978099047		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.072493978099047 | validation: 1.2714681623650401]
	TIME [epoch: 1.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508234625103787		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.0508234625103787 | validation: 1.246542246221684]
	TIME [epoch: 1.37 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0410192348563663		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.0410192348563663 | validation: 1.2791657950596809]
	TIME [epoch: 1.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025101871457982		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.025101871457982 | validation: 1.2052899602737963]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0383182221083123		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0383182221083123 | validation: 1.3816743309215775]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061904913103714		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.061904913103714 | validation: 1.2448805312397253]
	TIME [epoch: 1.37 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.272094444288343		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.272094444288343 | validation: 1.6173814766277346]
	TIME [epoch: 1.37 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1783760915673287		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.1783760915673287 | validation: 1.149365467254684]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0830547421564485		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.0830547421564485 | validation: 1.2682198969196958]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0296960041221994		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0296960041221994 | validation: 1.2568155127677103]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031025166953907		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.031025166953907 | validation: 1.200510376704726]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0399210400161507		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0399210400161507 | validation: 1.3996134355864682]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0633386877477444		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0633386877477444 | validation: 1.1876095735519667]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.212187781921145		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.212187781921145 | validation: 1.3866351078996588]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0448764355582159		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0448764355582159 | validation: 1.1628502893506363]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0312522265993591		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.0312522265993591 | validation: 1.2725988786695115]
	TIME [epoch: 1.37 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006059124410784		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.006059124410784 | validation: 1.1814380559489128]
	TIME [epoch: 1.37 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0109244903160768		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.0109244903160768 | validation: 1.3866139780180724]
	TIME [epoch: 1.37 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0578563451909595		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.0578563451909595 | validation: 1.1782808986922892]
	TIME [epoch: 1.37 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.227625446276404		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.227625446276404 | validation: 1.317231457975128]
	TIME [epoch: 1.37 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03147422379633		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.03147422379633 | validation: 1.1674429183709962]
	TIME [epoch: 1.37 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000922691347963		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.000922691347963 | validation: 1.205600317452153]
	TIME [epoch: 1.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0037563648273713		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0037563648273713 | validation: 1.1551624846228363]
	TIME [epoch: 1.37 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002080791281487		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0002080791281487 | validation: 1.3399697977726146]
	TIME [epoch: 1.37 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589803852151933		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0589803852151933 | validation: 1.1509910700624897]
	TIME [epoch: 1.37 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.22601518469921		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.22601518469921 | validation: 1.3178407574468431]
	TIME [epoch: 1.37 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0470555334008267		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0470555334008267 | validation: 1.0973969499487712]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013419543351525		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.013419543351525 | validation: 1.2105615342689107]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976736669986821		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.9976736669986821 | validation: 1.1155170775944316]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0016112200896217		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0016112200896217 | validation: 1.2734818483469441]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0212086998951697		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.0212086998951697 | validation: 1.1273761799641413]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1467055533437396		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.1467055533437396 | validation: 1.5192645097548096]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1418297035552105		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1418297035552105 | validation: 1.083537911811704]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099280995770076		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.099280995770076 | validation: 1.1913398712730696]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.999616510919121		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.999616510919121 | validation: 1.1846179487620196]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9956375584559496		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.9956375584559496 | validation: 1.1124868719799939]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929256429713001		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.9929256429713001 | validation: 1.2453897463703798]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192439069187738		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.0192439069187738 | validation: 1.1093832951777454]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0830437060809681		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.0830437060809681 | validation: 1.4058784540812368]
	TIME [epoch: 1.38 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0988450808603631		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0988450808603631 | validation: 1.0311172935790742]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0757987211560691		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.0757987211560691 | validation: 1.2150353763999937]
	TIME [epoch: 1.38 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9879616486424769		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.9879616486424769 | validation: 1.1263456442378919]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646669127361839		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.9646669127361839 | validation: 1.1029733283497039]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9742436921238468		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.9742436921238468 | validation: 1.1978955021386402]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788858956062473		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.9788858956062473 | validation: 1.0635358406077366]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0517432676356473		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.0517432676356473 | validation: 1.4853450742308374]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1578563367544996		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1578563367544996 | validation: 1.0555989876930618]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1424938622273006		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.1424938622273006 | validation: 1.1410937983790927]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0012802649337287		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.0012802649337287 | validation: 1.1843222907915918]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9907692667205709		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.9907692667205709 | validation: 1.10138872528914]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0517148114986472		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.0517148114986472 | validation: 1.2738244556717648]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0354499501067054		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.0354499501067054 | validation: 1.0353541057806999]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246309097819049		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0246309097819049 | validation: 1.1590453374707044]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782675488911244		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.9782675488911244 | validation: 1.049025441490245]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779820631263904		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.9779820631263904 | validation: 1.113252787872084]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9649059068869101		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.9649059068869101 | validation: 1.0341009374148733]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9820511610747158		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.9820511610747158 | validation: 1.3226125689425947]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609276027666814		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.0609276027666814 | validation: 1.059522896497962]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1764226106092115		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.1764226106092115 | validation: 1.1949491444619718]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9797039248359372		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.9797039248359372 | validation: 1.0819316180716063]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9573900814628268		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.9573900814628268 | validation: 1.046510751219244]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9526902138883928		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.9526902138883928 | validation: 1.1355361246785556]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9647956818473694		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.9647956818473694 | validation: 1.0128396859228253]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130963848830936		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0130963848830936 | validation: 1.3801204274811014]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1055430623753733		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.1055430623753733 | validation: 1.0311582015903937]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1058758336766306		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.1058758336766306 | validation: 1.113298794245809]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9622391501722998		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.9622391501722998 | validation: 1.0889244629976755]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9456471238366596		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.9456471238366596 | validation: 1.0348405547267718]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9805267579362499		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9805267579362499 | validation: 1.2802023088675885]
	TIME [epoch: 1.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0313635420653002		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0313635420653002 | validation: 1.0009772398337415]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0703724718271925		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0703724718271925 | validation: 1.1510881526666963]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788107948972363		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.9788107948972363 | validation: 0.9924995740642271]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9362657911821328		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.9362657911821328 | validation: 1.0761992297889151]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951005931913478		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.951005931913478 | validation: 0.9993898403769498]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9639772117065561		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.9639772117065561 | validation: 1.2526114002010225]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019829758933353		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.019829758933353 | validation: 1.0396562945238466]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1152971017411688		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.1152971017411688 | validation: 1.1534044039784566]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9770659100573085		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9770659100573085 | validation: 0.9962040859087451]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380418003992784		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9380418003992784 | validation: 1.0343141208170838]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9368100031939532		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.9368100031939532 | validation: 0.9959288670285311]
	TIME [epoch: 1.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938953037741424		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.938953037741424 | validation: 1.09697960963816]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662968666360948		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.9662968666360948 | validation: 0.9899398025536934]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0394185742472386		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0394185742472386 | validation: 1.3068752441093694]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0961658947466542		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.0961658947466542 | validation: 0.9568833117577431]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.058850668659963		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.058850668659963 | validation: 1.0490195412010455]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388207131504603		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.9388207131504603 | validation: 1.0252450175090748]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298898514717624		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.9298898514717624 | validation: 0.9690926927045578]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413669516522012		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.9413669516522012 | validation: 1.149482972567517]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9816635592737534		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.9816635592737534 | validation: 0.9776376819938393]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0549659515673901		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.0549659515673901 | validation: 1.1501575019823487]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9921834999853409		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9921834999853409 | validation: 0.9649350348742154]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671940384504363		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.9671940384504363 | validation: 1.0839127276047396]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9383623324083822		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.9383623324083822 | validation: 0.9534000030951643]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492299394519779		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.9492299394519779 | validation: 1.1266248485328931]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535879619709209		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.9535879619709209 | validation: 0.9356178523902973]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9712940067886321		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.9712940067886321 | validation: 1.1772531310765564]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9860151170929318		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9860151170929318 | validation: 0.9357882944986268]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0009330839175923		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.0009330839175923 | validation: 1.1457338380364603]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9676036376021233		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9676036376021233 | validation: 0.9537087466166482]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9537342253805957		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.9537342253805957 | validation: 1.109936961344344]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617869439529085		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.9617869439529085 | validation: 0.9244120654366835]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9794322162381591		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.9794322162381591 | validation: 1.1153188642665663]
	TIME [epoch: 1.37 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671167214778754		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.9671167214778754 | validation: 0.9371393465145698]
	TIME [epoch: 1.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.962905507919596		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.962905507919596 | validation: 1.1008139355834101]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666931081308897		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.9666931081308897 | validation: 0.940463820615065]
	TIME [epoch: 1.37 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975774591396444		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.9975774591396444 | validation: 1.1160142006010045]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9626166867138858		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.9626166867138858 | validation: 0.9309422082268057]
	TIME [epoch: 1.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9511374644098691		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.9511374644098691 | validation: 1.0384698101704828]
	TIME [epoch: 1.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9406725859696177		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.9406725859696177 | validation: 0.9233942900759757]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367084409410771		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9367084409410771 | validation: 1.059976525968892]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9344644587757676		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9344644587757676 | validation: 0.9046503972888278]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.997075206973545		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.997075206973545 | validation: 1.1828424292028268]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0132673637221803		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0132673637221803 | validation: 0.9245063271129311]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9817209153986309		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9817209153986309 | validation: 1.0221700237245646]
	TIME [epoch: 1.38 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9261861839225908		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9261861839225908 | validation: 0.8983427131765658]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096804558725475		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.9096804558725475 | validation: 0.9788516819800912]
	TIME [epoch: 1.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9035667692555372		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9035667692555372 | validation: 0.9139750084973326]
	TIME [epoch: 1.37 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163723277667896		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9163723277667896 | validation: 1.0436628314571799]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.943733564514349		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.943733564514349 | validation: 0.943634306127536]
	TIME [epoch: 1.37 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0577657499675766		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.0577657499675766 | validation: 1.1667292720412814]
	TIME [epoch: 1.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9947912988659191		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.9947912988659191 | validation: 0.9034531776394008]
	TIME [epoch: 1.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366644501682471		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9366644501682471 | validation: 0.9701777841586371]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904054107483		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.904054107483 | validation: 0.8965902172439875]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093683780116933		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.9093683780116933 | validation: 0.991605282863818]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907808459626097		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.907808459626097 | validation: 0.8855770390416222]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9392592949785453		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.9392592949785453 | validation: 1.1418757692588313]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0023830527949027		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0023830527949027 | validation: 0.9076776970292433]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0458367374016704		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.0458367374016704 | validation: 1.0768568306409791]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444034757407636		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9444034757407636 | validation: 0.901923767180755]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014703655365799		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9014703655365799 | validation: 0.9337476015270525]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956041613428737		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.8956041613428737 | validation: 0.9116965208024894]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901301813087019		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.8901301813087019 | validation: 0.9145190047299245]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887671952610885		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.887671952610885 | validation: 0.8935118525702341]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943948241837133		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.8943948241837133 | validation: 0.969760087584864]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892555578716006		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.892555578716006 | validation: 0.8633501222620013]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9881293900349524		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.9881293900349524 | validation: 1.4255762576993782]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1829839566567992		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1829839566567992 | validation: 0.8769470731949675]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0101916411647407		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.0101916411647407 | validation: 0.925746438902947]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899515220065868		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.8899515220065868 | validation: 0.9641985487182372]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025603691254364		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.9025603691254364 | validation: 0.8473722011864734]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9246283802642518		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9246283802642518 | validation: 1.0680264317091]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578854330644816		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.9578854330644816 | validation: 0.8249316477581101]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9644184543158693		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.9644184543158693 | validation: 0.9564753481906053]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114463088281416		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.9114463088281416 | validation: 0.8415455638767355]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025706627151981		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.9025706627151981 | validation: 0.9419443546086669]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894395110844155		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.894395110844155 | validation: 0.8284568134820269]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9076992425385624		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9076992425385624 | validation: 1.047109615056856]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9493548109269145		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.9493548109269145 | validation: 0.8710261278430931]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9951458968465652		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.9951458968465652 | validation: 1.03451010374354]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9326509972096517		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9326509972096517 | validation: 0.8298426413424287]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021848254168626		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9021848254168626 | validation: 0.9446458832024544]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8978293623989165		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.8978293623989165 | validation: 0.8162336663525669]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137707965988695		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9137707965988695 | validation: 1.0456105225668737]
	TIME [epoch: 1.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9345686758804692		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.9345686758804692 | validation: 0.8562576151135848]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9491124010056917		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.9491124010056917 | validation: 0.9698338766166041]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065693538716205		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9065693538716205 | validation: 0.828447279430198]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092536918059273		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9092536918059273 | validation: 0.9732964770035366]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111372325868741		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.9111372325868741 | validation: 0.8317929284554332]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152351795203713		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9152351795203713 | validation: 0.9765498535262174]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096090061836367		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.9096090061836367 | validation: 0.8435612793803161]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9385583496210943		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9385583496210943 | validation: 1.044521665248217]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9356251090055037		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.9356251090055037 | validation: 0.8130273948089066]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298773311382809		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9298773311382809 | validation: 0.9563985588683903]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991013628064956		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.8991013628064956 | validation: 0.8333595415571445]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027466631367321		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9027466631367321 | validation: 0.9464387829892896]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087243858279554		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9087243858279554 | validation: 0.7982820737201609]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248149111328912		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9248149111328912 | validation: 0.9794958616021898]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9179807723882266		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.9179807723882266 | validation: 0.8387350919449332]
	TIME [epoch: 1.37 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927583765996099		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.927583765996099 | validation: 0.9853726610008524]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9158802209434643		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.9158802209434643 | validation: 0.799361069070972]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9185179434235262		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9185179434235262 | validation: 0.948990883069795]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012236433669355		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9012236433669355 | validation: 0.779640105010692]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972924991850179		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.8972924991850179 | validation: 0.9665729186411082]
	TIME [epoch: 1.37 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8967249598852952		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.8967249598852952 | validation: 0.7918312066829191]
	TIME [epoch: 1.37 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091945268585789		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.9091945268585789 | validation: 0.9698045495958475]
	TIME [epoch: 1.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046867409872346		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9046867409872346 | validation: 0.7920317422227373]
	TIME [epoch: 1.37 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256156698106451		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.9256156698106451 | validation: 0.9566880508232307]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9019312446241605		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9019312446241605 | validation: 0.7912037855545195]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853747712315775		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.8853747712315775 | validation: 0.9238810961119475]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938938932010789		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.8938938932010789 | validation: 0.8088306201440294]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.914521655852902		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.914521655852902 | validation: 0.9708912494555166]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085264167091077		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9085264167091077 | validation: 0.7859852049467275]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066780803662923		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.9066780803662923 | validation: 0.9143280399921085]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885586456365667		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.8885586456365667 | validation: 0.7722482065516298]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879186752193525		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.879186752193525 | validation: 0.9225044965898971]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973298258266155		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.8973298258266155 | validation: 0.7841705815272326]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258920944900375		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.9258920944900375 | validation: 0.9530237948420984]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057964616066354		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9057964616066354 | validation: 0.7722797897350555]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907158305148174		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.907158305148174 | validation: 0.870554076242028]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840308525478986		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.8840308525478986 | validation: 0.7741902163313181]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707453722753188		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.8707453722753188 | validation: 0.8865692080404234]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.875498498936964		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.875498498936964 | validation: 0.7499253682697273]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886081832888917		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.8886081832888917 | validation: 0.9570104930661674]
	TIME [epoch: 1.37 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9004002351926614		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.9004002351926614 | validation: 0.7807433924503526]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927779393100913		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.927779393100913 | validation: 0.9445909784879686]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990423283714083		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.8990423283714083 | validation: 0.7448948767055478]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794893397045133		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.8794893397045133 | validation: 0.8580769584081658]
	TIME [epoch: 1.37 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651576475636733		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.8651576475636733 | validation: 0.7544770037147673]
	TIME [epoch: 1.37 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669848247669191		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.8669848247669191 | validation: 0.8946276322974893]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760878056637429		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.8760878056637429 | validation: 0.7447001647818859]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105433690733165		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.9105433690733165 | validation: 0.9929289969124714]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9276150730149273		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.9276150730149273 | validation: 0.7288240057190607]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152541401856595		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.9152541401856595 | validation: 0.8898701862897106]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8816927413921698		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.8816927413921698 | validation: 0.7568903047519222]
	TIME [epoch: 1.38 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714371833799359		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.8714371833799359 | validation: 0.8247670940250615]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857369866758745		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.857369866758745 | validation: 0.7415442891920613]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710004535525571		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.8710004535525571 | validation: 0.8816462248410857]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800083973733303		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.8800083973733303 | validation: 0.7373592605281994]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058035031432523		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.9058035031432523 | validation: 0.9364429839280904]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9126942382371936		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.9126942382371936 | validation: 0.738476855030463]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907933956345513		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.8907933956345513 | validation: 0.8509208817294107]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666464968142112		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.8666464968142112 | validation: 0.7416394307906364]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8555978792027068		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.8555978792027068 | validation: 0.865016740233814]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864567915784363		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.864567915784363 | validation: 0.742961514473104]
	TIME [epoch: 1.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871209235240746		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.871209235240746 | validation: 0.9255209257315411]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890719150322062		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.8890719150322062 | validation: 0.7549427952709893]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057630197090045		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.9057630197090045 | validation: 0.9125848499053257]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8895831680538316		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.8895831680538316 | validation: 0.7324415552342103]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725343996027509		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.8725343996027509 | validation: 0.8391692837768766]
	TIME [epoch: 1.37 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667356057830622		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.8667356057830622 | validation: 0.7545421768046817]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8679175880398823		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.8679175880398823 | validation: 0.8675711399670833]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744253047627177		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.8744253047627177 | validation: 0.7394612203803576]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736958222898341		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.8736958222898341 | validation: 0.8929216163893717]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886076702764644		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.886076702764644 | validation: 0.710154368232011]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8822351342018252		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.8822351342018252 | validation: 0.8681080473492632]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654225681486588		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.8654225681486588 | validation: 0.7107867406364113]
	TIME [epoch: 1.38 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635378323544773		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.8635378323544773 | validation: 0.8608053504655238]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862469327431202		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.862469327431202 | validation: 0.7107576922230867]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879563697074969		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.879563697074969 | validation: 0.8619433156178862]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8762565865118407		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.8762565865118407 | validation: 0.6979545750024556]
	TIME [epoch: 176 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671166476443918		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.8671166476443918 | validation: 0.8464855562525845]
	TIME [epoch: 2.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8715439538329739		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.8715439538329739 | validation: 0.7147946092151899]
	TIME [epoch: 2.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626764204947099		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.8626764204947099 | validation: 0.832626808607866]
	TIME [epoch: 2.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85717152651907		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.85717152651907 | validation: 0.7170160649670487]
	TIME [epoch: 2.71 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8643920138333954		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.8643920138333954 | validation: 0.8455353198263151]
	TIME [epoch: 2.71 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672183306020383		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.8672183306020383 | validation: 0.7099791432815836]
	TIME [epoch: 2.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691109474881278		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.8691109474881278 | validation: 0.86493297282026]
	TIME [epoch: 2.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772598888534725		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.8772598888534725 | validation: 0.6976033498223839]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731578774467096		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.8731578774467096 | validation: 0.8081530842353815]
	TIME [epoch: 2.72 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526691614000603		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.8526691614000603 | validation: 0.695397426180295]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547132879985745		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.8547132879985745 | validation: 0.8113138385204639]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633697278680412		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.8633697278680412 | validation: 0.6833101056268265]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8697203582453844		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.8697203582453844 | validation: 0.8380532675819554]
	TIME [epoch: 2.71 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8618105630312415		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.8618105630312415 | validation: 0.7140147226831345]
	TIME [epoch: 2.72 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775173408007595		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8775173408007595 | validation: 0.8591278092661006]
	TIME [epoch: 2.71 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757421659834145		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8757421659834145 | validation: 0.7011616528446201]
	TIME [epoch: 2.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658732648172162		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8658732648172162 | validation: 0.812495650999106]
	TIME [epoch: 2.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8548850940079337		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.8548850940079337 | validation: 0.7014552472290629]
	TIME [epoch: 2.72 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591429082851828		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.8591429082851828 | validation: 0.8255888093497623]
	TIME [epoch: 2.72 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8702277766624072		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.8702277766624072 | validation: 0.6975873408683219]
	TIME [epoch: 2.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741514862868701		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.8741514862868701 | validation: 0.8316353308700433]
	TIME [epoch: 2.72 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573989465607759		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.8573989465607759 | validation: 0.6907859246261464]
	TIME [epoch: 2.71 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481245828398452		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.8481245828398452 | validation: 0.7677783211843593]
	TIME [epoch: 2.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359237889596582		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.8359237889596582 | validation: 0.6904961323650665]
	TIME [epoch: 2.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297225879548532		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8297225879548532 | validation: 0.8410126567865073]
	TIME [epoch: 2.72 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565109140322272		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.8565109140322272 | validation: 0.681623901911255]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8872791335242078		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.8872791335242078 | validation: 0.8648433480072442]
	TIME [epoch: 2.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771429988047286		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.8771429988047286 | validation: 0.6726166792222616]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8601835434815545		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.8601835434815545 | validation: 0.8001438416660163]
	TIME [epoch: 2.71 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432567664623761		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.8432567664623761 | validation: 0.685554585753496]
	TIME [epoch: 2.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358741074154469		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.8358741074154469 | validation: 0.7884745667697513]
	TIME [epoch: 2.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449095379189615		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.8449095379189615 | validation: 0.6926273117932904]
	TIME [epoch: 2.72 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468233107248956		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.8468233107248956 | validation: 0.8323128134982536]
	TIME [epoch: 2.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721533159038759		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.8721533159038759 | validation: 0.6793202234366424]
	TIME [epoch: 2.71 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710230372830319		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.8710230372830319 | validation: 0.8268976171243383]
	TIME [epoch: 2.72 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550578312796345		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.8550578312796345 | validation: 0.6889749465528332]
	TIME [epoch: 2.72 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609129348855453		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.8609129348855453 | validation: 0.762353778135803]
	TIME [epoch: 2.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358614723030802		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.8358614723030802 | validation: 0.6700775471248245]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499158448690542		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.8499158448690542 | validation: 0.8258587507676216]
	TIME [epoch: 2.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573068376037923		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.8573068376037923 | validation: 0.6864224763039274]
	TIME [epoch: 2.72 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681943462644847		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.8681943462644847 | validation: 0.8187451003972314]
	TIME [epoch: 2.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543623032751038		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.8543623032751038 | validation: 0.6821173801944457]
	TIME [epoch: 2.72 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596407814525648		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8596407814525648 | validation: 0.769086935962608]
	TIME [epoch: 2.71 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510229593959201		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.8510229593959201 | validation: 0.6716798330641156]
	TIME [epoch: 2.71 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392007658029104		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.8392007658029104 | validation: 0.7763648873954719]
	TIME [epoch: 2.71 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400808614458666		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.8400808614458666 | validation: 0.6615270769106818]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508111821513324		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.8508111821513324 | validation: 0.83966346038413]
	TIME [epoch: 2.71 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605938462400315		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.8605938462400315 | validation: 0.6473768256248953]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868932841834687		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.868932841834687 | validation: 0.8001463991651016]
	TIME [epoch: 2.71 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674708465456378		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.8674708465456378 | validation: 0.6727733639930885]
	TIME [epoch: 2.72 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514827526395112		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.8514827526395112 | validation: 0.768460180269917]
	TIME [epoch: 2.72 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426441529344085		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.8426441529344085 | validation: 0.6518756919934366]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420097459905412		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.8420097459905412 | validation: 0.7760792469429403]
	TIME [epoch: 2.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837949704453365		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.837949704453365 | validation: 0.6536974892834643]
	TIME [epoch: 2.72 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8429042401001323		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.8429042401001323 | validation: 0.7919996910481529]
	TIME [epoch: 2.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517846205758014		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.8517846205758014 | validation: 0.6774332273316457]
	TIME [epoch: 2.72 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594223955712211		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.8594223955712211 | validation: 0.8258398012866266]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613367018722585		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.8613367018722585 | validation: 0.6658102068507203]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547617192269894		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.8547617192269894 | validation: 0.7689534615355855]
	TIME [epoch: 2.72 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340430394761127		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.8340430394761127 | validation: 0.6609971857395358]
	TIME [epoch: 2.72 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267562010013063		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.8267562010013063 | validation: 0.7330629844174419]
	TIME [epoch: 2.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8230858290651039		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.8230858290651039 | validation: 0.6655507581030484]
	TIME [epoch: 2.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352505846698535		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.8352505846698535 | validation: 0.8100179526696079]
	TIME [epoch: 2.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479306364123355		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.8479306364123355 | validation: 0.6529667833154499]
	TIME [epoch: 2.71 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633579288477691		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.8633579288477691 | validation: 0.798783509048663]
	TIME [epoch: 2.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597095361917292		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.8597095361917292 | validation: 0.6617842463259361]
	TIME [epoch: 2.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437456219500937		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.8437456219500937 | validation: 0.7549603019829317]
	TIME [epoch: 2.72 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832704599673437		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.832704599673437 | validation: 0.6718381578583726]
	TIME [epoch: 2.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355702658860037		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8355702658860037 | validation: 0.7591813590254264]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833365397579478		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.833365397579478 | validation: 0.6519648312812768]
	TIME [epoch: 2.72 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525510675461607		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.8525510675461607 | validation: 0.8298478613932727]
	TIME [epoch: 2.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854209163005735		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.854209163005735 | validation: 0.6675691629537801]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490884244551837		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.8490884244551837 | validation: 0.7694629518478693]
	TIME [epoch: 2.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391994706308614		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.8391994706308614 | validation: 0.6708185424251097]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288694894681606		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.8288694894681606 | validation: 0.7453673611858435]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8369676081146118		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.8369676081146118 | validation: 0.6416019190325083]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8302924684230815		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.8302924684230815 | validation: 0.7532035330501355]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820791112200403		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.820791112200403 | validation: 0.6460914628872492]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310890873705398		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.8310890873705398 | validation: 0.7727729057996954]
	TIME [epoch: 2.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379269425323916		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.8379269425323916 | validation: 0.6419923890546714]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845228961421483		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.845228961421483 | validation: 0.7854144611606749]
	TIME [epoch: 2.71 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493443412225221		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.8493443412225221 | validation: 0.649038274025136]
	TIME [epoch: 2.72 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476436725359027		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.8476436725359027 | validation: 0.7417422880960379]
	TIME [epoch: 2.71 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318457027164788		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.8318457027164788 | validation: 0.6413667091454578]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8214051420906654		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.8214051420906654 | validation: 0.7341783939657347]
	TIME [epoch: 2.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202094232333628		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.8202094232333628 | validation: 0.6332561972462458]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190044005126649		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.8190044005126649 | validation: 0.7758676240978336]
	TIME [epoch: 2.71 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375387384399272		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.8375387384399272 | validation: 0.655248234900558]
	TIME [epoch: 2.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849169864934528		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.849169864934528 | validation: 0.8185902559751348]
	TIME [epoch: 2.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637792749391039		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.8637792749391039 | validation: 0.6487238915832025]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467344543605994		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.8467344543605994 | validation: 0.7407675292438132]
	TIME [epoch: 2.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229111871782755		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.8229111871782755 | validation: 0.6396648570005976]
	TIME [epoch: 2.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249535174207684		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.8249535174207684 | validation: 0.7324986666319986]
	TIME [epoch: 2.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243782177744805		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.8243782177744805 | validation: 0.6480814824310328]
	TIME [epoch: 2.72 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234632180442688		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.8234632180442688 | validation: 0.7581465414523529]
	TIME [epoch: 2.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280367765119757		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.8280367765119757 | validation: 0.6588604437219536]
	TIME [epoch: 2.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448672630324692		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.8448672630324692 | validation: 0.7842417993944486]
	TIME [epoch: 2.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489795040455889		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.8489795040455889 | validation: 0.6436195209015367]
	TIME [epoch: 2.72 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487321083723314		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.8487321083723314 | validation: 0.748248048444115]
	TIME [epoch: 2.71 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365088317109957		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.8365088317109957 | validation: 0.650446353094233]
	TIME [epoch: 2.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830308232809184		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.830308232809184 | validation: 0.7145991684835057]
	TIME [epoch: 2.71 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284722149805589		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.8284722149805589 | validation: 0.6517679851015346]
	TIME [epoch: 2.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283645496661851		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.8283645496661851 | validation: 0.7525852234034107]
	TIME [epoch: 2.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387788266224374		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.8387788266224374 | validation: 0.6473323903075552]
	TIME [epoch: 2.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391496923381996		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.8391496923381996 | validation: 0.7638678948411178]
	TIME [epoch: 2.71 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400105568905235		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.8400105568905235 | validation: 0.6517969544437275]
	TIME [epoch: 2.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8407189466305343		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.8407189466305343 | validation: 0.7613357654198776]
	TIME [epoch: 2.71 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309332761476023		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.8309332761476023 | validation: 0.6413750881379245]
	TIME [epoch: 2.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332301228405311		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.8332301228405311 | validation: 0.7546753202628325]
	TIME [epoch: 2.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135506017484716		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.8135506017484716 | validation: 0.6365285346614953]
	TIME [epoch: 2.71 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300365412184572		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.8300365412184572 | validation: 0.733249977727464]
	TIME [epoch: 2.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239823461468783		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.8239823461468783 | validation: 0.6277157527117745]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342259097889974		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.8342259097889974 | validation: 0.7528972382389274]
	TIME [epoch: 2.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332486326445802		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.8332486326445802 | validation: 0.6290838144761225]
	TIME [epoch: 2.72 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246204851394517		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.8246204851394517 | validation: 0.739170204034057]
	TIME [epoch: 2.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8256786373537722		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.8256786373537722 | validation: 0.642062796114707]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176834214727877		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.8176834214727877 | validation: 0.7471388204080441]
	TIME [epoch: 2.72 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240978338685011		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.8240978338685011 | validation: 0.6536599861719299]
	TIME [epoch: 2.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287245660947187		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.8287245660947187 | validation: 0.7783818967956121]
	TIME [epoch: 2.72 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8307223608333298		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.8307223608333298 | validation: 0.6274131060002244]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347957709335756		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.8347957709335756 | validation: 0.7345667844960312]
	TIME [epoch: 2.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180170581599095		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.8180170581599095 | validation: 0.6398232841096227]
	TIME [epoch: 2.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107106406035735		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.8107106406035735 | validation: 0.7285842707103716]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8213341142836851		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.8213341142836851 | validation: 0.6405768376609539]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250765466452435		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.8250765466452435 | validation: 0.7460750078776144]
	TIME [epoch: 2.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364020755060332		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.8364020755060332 | validation: 0.6387002366444751]
	TIME [epoch: 2.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344515410725128		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.8344515410725128 | validation: 0.7569065328466673]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324139308788827		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.8324139308788827 | validation: 0.6448137958973563]
	TIME [epoch: 2.72 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120266623002218		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.8120266623002218 | validation: 0.6817474215215464]
	TIME [epoch: 2.72 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078252247769454		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.8078252247769454 | validation: 0.6614958909163606]
	TIME [epoch: 2.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067580647779647		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.8067580647779647 | validation: 0.6749425076551431]
	TIME [epoch: 2.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104929894292002		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.8104929894292002 | validation: 0.6438035582261016]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102840149162528		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.8102840149162528 | validation: 0.6910068574534587]
	TIME [epoch: 2.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139108036041174		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.8139108036041174 | validation: 0.6212952345245912]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8319476040540167		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.8319476040540167 | validation: 0.8308095368412065]
	TIME [epoch: 2.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732802315281748		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.8732802315281748 | validation: 0.6352622108641617]
	TIME [epoch: 2.71 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589141955255797		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.8589141955255797 | validation: 0.747729027795086]
	TIME [epoch: 2.71 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183832143160686		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.8183832143160686 | validation: 0.6557693117500619]
	TIME [epoch: 2.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102783327824262		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.8102783327824262 | validation: 0.6719364924132203]
	TIME [epoch: 2.71 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114609736728247		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.8114609736728247 | validation: 0.6499851498348992]
	TIME [epoch: 2.71 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050150168422998		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.8050150168422998 | validation: 0.6891349705818427]
	TIME [epoch: 2.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114024508607639		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.8114024508607639 | validation: 0.6284147096248622]
	TIME [epoch: 2.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8129589045991591		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.8129589045991591 | validation: 0.7555927853853321]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8254642436167767		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.8254642436167767 | validation: 0.6243731986060257]
	TIME [epoch: 2.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607377738865105		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.8607377738865105 | validation: 0.8193552735372045]
	TIME [epoch: 2.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581488810291998		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.8581488810291998 | validation: 0.6185888266586369]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180619587735072		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.8180619587735072 | validation: 0.6768528568842395]
	TIME [epoch: 2.71 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101254766076309		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.8101254766076309 | validation: 0.6487704425148676]
	TIME [epoch: 2.72 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051539107294294		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.8051539107294294 | validation: 0.6459612961940039]
	TIME [epoch: 2.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103827375048431		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.8103827375048431 | validation: 0.6478732953281465]
	TIME [epoch: 2.72 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016931915559661		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.8016931915559661 | validation: 0.6777471474292405]
	TIME [epoch: 2.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025407021729041		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.8025407021729041 | validation: 0.6095603115862032]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8100664151532283		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.8100664151532283 | validation: 0.7047936200805384]
	TIME [epoch: 2.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268111132278082		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.8268111132278082 | validation: 0.6348834950884634]
	TIME [epoch: 2.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613870844704536		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.8613870844704536 | validation: 0.8013734672255336]
	TIME [epoch: 2.73 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564696701248876		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.8564696701248876 | validation: 0.6312024515520535]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207309232344343		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.8207309232344343 | validation: 0.6684350672938595]
	TIME [epoch: 2.72 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084794469109886		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.8084794469109886 | validation: 0.6465368330783938]
	TIME [epoch: 2.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011559240094603		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.8011559240094603 | validation: 0.6429747513170452]
	TIME [epoch: 2.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031366423149144		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.8031366423149144 | validation: 0.6812223832136266]
	TIME [epoch: 2.72 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011570393717261		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.8011570393717261 | validation: 0.6278250935981213]
	TIME [epoch: 2.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167597067154339		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.8167597067154339 | validation: 0.7659819181346444]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838132482866096		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.838132482866096 | validation: 0.6213533217232197]
	TIME [epoch: 2.73 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649153028592966		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.8649153028592966 | validation: 0.7248888451435076]
	TIME [epoch: 2.72 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142223007509293		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.8142223007509293 | validation: 0.6433435431009507]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8054912482005133		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.8054912482005133 | validation: 0.656037599448725]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801566016827405		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.801566016827405 | validation: 0.6506224271251293]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081591891501043		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.8081591891501043 | validation: 0.6745192035766506]
	TIME [epoch: 2.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8073029008701391		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.8073029008701391 | validation: 0.6439314111864048]
	TIME [epoch: 2.73 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999650334552407		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.7999650334552407 | validation: 0.684537663647595]
	TIME [epoch: 2.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164261525438079		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.8164261525438079 | validation: 0.6109568606994693]
	TIME [epoch: 2.72 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394029886241821		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.8394029886241821 | validation: 0.8000306698218458]
	TIME [epoch: 2.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642091594505068		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.8642091594505068 | validation: 0.6103152306208185]
	TIME [epoch: 2.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350254407938488		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.8350254407938488 | validation: 0.6748499355498087]
	TIME [epoch: 2.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135258390997615		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.8135258390997615 | validation: 0.6583927489249708]
	TIME [epoch: 2.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075908460755148		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.8075908460755148 | validation: 0.624133917228731]
	TIME [epoch: 2.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040632593447012		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.8040632593447012 | validation: 0.6954790794379507]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130280501292577		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.8130280501292577 | validation: 0.6241695662983363]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809878547903611		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.809878547903611 | validation: 0.7506527944431759]
	TIME [epoch: 2.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8258386961350729		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.8258386961350729 | validation: 0.621935901815438]
	TIME [epoch: 2.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8184057417598762		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.8184057417598762 | validation: 0.7029522096306391]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064309228485463		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.8064309228485463 | validation: 0.6145238143067717]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81927105364354		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.81927105364354 | validation: 0.6909613143221094]
	TIME [epoch: 2.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091408286670942		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.8091408286670942 | validation: 0.6043593245048697]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8053874327744651		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.8053874327744651 | validation: 0.6890896978147479]
	TIME [epoch: 2.71 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056246646526249		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.8056246646526249 | validation: 0.6028960230572942]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087692935425679		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.8087692935425679 | validation: 0.734200418275994]
	TIME [epoch: 2.71 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177840168609126		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.8177840168609126 | validation: 0.6024796092095581]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819774712769842		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.819774712769842 | validation: 0.7094972822141359]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117031175116765		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.8117031175116765 | validation: 0.613855717016218]
	TIME [epoch: 2.73 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809377020552416		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.809377020552416 | validation: 0.6892202796016862]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011102043661609		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.8011102043661609 | validation: 0.6239065455644529]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016847403347674		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.8016847403347674 | validation: 0.6675439450424681]
	TIME [epoch: 2.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799040384285104		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.799040384285104 | validation: 0.6277148092854619]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030481019874287		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.8030481019874287 | validation: 0.6749144777191823]
	TIME [epoch: 2.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006740675217827		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.8006740675217827 | validation: 0.6181338799608121]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804473803154511		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.804473803154511 | validation: 0.7023101762394325]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086162914866677		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.8086162914866677 | validation: 0.6163360117290534]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853252853849462		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.853252853849462 | validation: 0.7831516651383142]
	TIME [epoch: 2.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845214481612951		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.845214481612951 | validation: 0.6114339993645518]
	TIME [epoch: 2.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078735289695678		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.8078735289695678 | validation: 0.6528118107532649]
	TIME [epoch: 2.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943938471004245		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.7943938471004245 | validation: 0.6700662896910455]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024011814711358		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.8024011814711358 | validation: 0.6065879903141922]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090011162501628		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.8090011162501628 | validation: 0.7052805844283588]
	TIME [epoch: 2.72 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176990634754487		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.8176990634754487 | validation: 0.617034118231424]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136316104263314		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.8136316104263314 | validation: 0.6826560311271688]
	TIME [epoch: 2.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074586900141304		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.8074586900141304 | validation: 0.6318993687540918]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034545804121472		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.8034545804121472 | validation: 0.6719744783507555]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069448809153861		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.8069448809153861 | validation: 0.6054639139036855]
	TIME [epoch: 2.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044105437402965		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.8044105437402965 | validation: 0.7124865097554602]
	TIME [epoch: 2.72 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079929544132455		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.8079929544132455 | validation: 0.6140007857333629]
	TIME [epoch: 2.72 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145742175253641		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.8145742175253641 | validation: 0.7010187602300277]
	TIME [epoch: 2.72 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206575947968564		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.8206575947968564 | validation: 0.6120624756674172]
	TIME [epoch: 2.72 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811459051406543		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.811459051406543 | validation: 0.6816224648617691]
	TIME [epoch: 2.72 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039959237881044		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.8039959237881044 | validation: 0.6280056080951834]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990353287425043		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.7990353287425043 | validation: 0.6702914052222729]
	TIME [epoch: 2.72 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955496493778875		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.7955496493778875 | validation: 0.6257693151401549]
	TIME [epoch: 2.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996116207358241		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.7996116207358241 | validation: 0.6711109209381819]
	TIME [epoch: 2.72 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792850803345221		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.792850803345221 | validation: 0.6250478807216846]
	TIME [epoch: 2.73 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82095452710203		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.82095452710203 | validation: 0.727006797675142]
	TIME [epoch: 2.72 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229128197682727		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.8229128197682727 | validation: 0.6143071763713959]
	TIME [epoch: 2.72 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811853497016231		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.811853497016231 | validation: 0.6572810863572323]
	TIME [epoch: 2.72 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924526205963173		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.7924526205963173 | validation: 0.6195715023932297]
	TIME [epoch: 2.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030282291240028		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.8030282291240028 | validation: 0.6257936804178551]
	TIME [epoch: 2.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970285905636376		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.7970285905636376 | validation: 0.6606978956139686]
	TIME [epoch: 2.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796009748908063		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.796009748908063 | validation: 0.6179120225632992]
	TIME [epoch: 2.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928679290624991		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.7928679290624991 | validation: 0.6761786329211453]
	TIME [epoch: 2.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804825604456997		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.804825604456997 | validation: 0.6147665067313437]
	TIME [epoch: 2.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253338001969698		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.8253338001969698 | validation: 0.7681692003408234]
	TIME [epoch: 2.72 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835958564732411		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.835958564732411 | validation: 0.6217916138278188]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034133124040443		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.8034133124040443 | validation: 0.6546557693467772]
	TIME [epoch: 2.72 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013880897194896		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.8013880897194896 | validation: 0.6270264403438706]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7931160164203304		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.7931160164203304 | validation: 0.6275915679675877]
	TIME [epoch: 2.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952377685071733		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.7952377685071733 | validation: 0.6498905005458427]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796821422618618		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.796821422618618 | validation: 0.5976960886505858]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003956051177127		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.8003956051177127 | validation: 0.7137762812218104]
	TIME [epoch: 2.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011492133766447		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.8011492133766447 | validation: 0.5987537970242878]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103332792010275		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.8103332792010275 | validation: 0.6907349519498278]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161401267858763		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.8161401267858763 | validation: 0.6122810876286268]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065070464577991		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.8065070464577991 | validation: 0.6628434584301857]
	TIME [epoch: 2.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952880498407526		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.7952880498407526 | validation: 0.6103302328773446]
	TIME [epoch: 2.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972715018621723		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.7972715018621723 | validation: 0.6717032787098757]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006417242117828		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.8006417242117828 | validation: 0.6091038075879495]
	TIME [epoch: 2.72 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962859944048256		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.7962859944048256 | validation: 0.6822186296092092]
	TIME [epoch: 2.73 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985207827403327		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.7985207827403327 | validation: 0.5823307363163167]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025245629142873		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.8025245629142873 | validation: 0.6970080112050401]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131924872460476		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.8131924872460476 | validation: 0.6067486079506793]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084668891721621		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.8084668891721621 | validation: 0.6680782859209309]
	TIME [epoch: 2.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057508545600098		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.8057508545600098 | validation: 0.6438250725078696]
	TIME [epoch: 2.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890871949216615		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.7890871949216615 | validation: 0.6303916761544155]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918261649102027		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.7918261649102027 | validation: 0.6534266470705813]
	TIME [epoch: 2.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876797008112593		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.7876797008112593 | validation: 0.6182249358548509]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970325143812597		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.7970325143812597 | validation: 0.6713517126684123]
	TIME [epoch: 2.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021293291709095		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.8021293291709095 | validation: 0.621842421837712]
	TIME [epoch: 2.72 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179623474215925		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.8179623474215925 | validation: 0.7158566989377316]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270810104478784		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.8270810104478784 | validation: 0.5971281017055291]
	TIME [epoch: 2.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012799245707458		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.8012799245707458 | validation: 0.6326821320667606]
	TIME [epoch: 2.74 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936991813966057		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.7936991813966057 | validation: 0.6223714733179456]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949614164376249		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.7949614164376249 | validation: 0.627521157016878]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789993925015769		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.789993925015769 | validation: 0.6685171875793223]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881116597586504		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.7881116597586504 | validation: 0.61657182580171]
	TIME [epoch: 2.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914914060826364		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.7914914060826364 | validation: 0.6758015327807576]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957161871159325		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.7957161871159325 | validation: 0.590414518704998]
	TIME [epoch: 2.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064453646817128		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.8064453646817128 | validation: 0.7364611994871849]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82341928199889		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.82341928199889 | validation: 0.6056560522490937]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029237246338026		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.8029237246338026 | validation: 0.6517216258207428]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917331427020199		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.7917331427020199 | validation: 0.6382726847349899]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966087123274902		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.7966087123274902 | validation: 0.6097417301379104]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863236915056979		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.7863236915056979 | validation: 0.6501283551857524]
	TIME [epoch: 2.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864151585711736		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.7864151585711736 | validation: 0.6154779682901579]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969788872325474		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.7969788872325474 | validation: 0.6833857046937923]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014490757264268		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.8014490757264268 | validation: 0.5825717320582549]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091084269178253		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.8091084269178253 | validation: 0.6779821082889179]
	TIME [epoch: 2.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055142354042929		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.8055142354042929 | validation: 0.6008920687271317]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902424075501172		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.7902424075501172 | validation: 0.6588966738332522]
	TIME [epoch: 2.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808236491847018		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.7808236491847018 | validation: 0.6308650616328382]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900683881620438		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.7900683881620438 | validation: 0.6276942748529986]
	TIME [epoch: 2.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890367938824697		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.7890367938824697 | validation: 0.6381243624833421]
	TIME [epoch: 2.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836487943853271		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.7836487943853271 | validation: 0.6259048731906597]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782127971610017		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.782127971610017 | validation: 0.6284307225063089]
	TIME [epoch: 2.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807462202951141		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.7807462202951141 | validation: 0.6006357240155629]
	TIME [epoch: 2.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947136669048044		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.7947136669048044 | validation: 0.6605812720258879]
	TIME [epoch: 2.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008690276415356		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.8008690276415356 | validation: 0.6044083244584868]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322855197907799		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.8322855197907799 | validation: 0.7561380049435077]
	TIME [epoch: 2.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827167954426496		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.827167954426496 | validation: 0.6048699898909946]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921707999573867		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.7921707999573867 | validation: 0.6226353065388294]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869783568163851		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.7869783568163851 | validation: 0.6460041650130602]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843366239663939		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.7843366239663939 | validation: 0.6075139520482564]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936346660117951		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.7936346660117951 | validation: 0.6350029603005715]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78200251903701		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.78200251903701 | validation: 0.6198915268711804]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941981228118158		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.7941981228118158 | validation: 0.6498891763420973]
	TIME [epoch: 2.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918312181255022		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.7918312181255022 | validation: 0.6026250364286084]
	TIME [epoch: 2.72 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926405740188471		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.7926405740188471 | validation: 0.6771402599481873]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030002680625503		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.8030002680625503 | validation: 0.588176798310954]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092411749313194		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.8092411749313194 | validation: 0.6826467423293947]
	TIME [epoch: 2.73 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7970827452056127		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.7970827452056127 | validation: 0.5992498368786414]
	TIME [epoch: 2.71 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884254097734552		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.7884254097734552 | validation: 0.628235877758657]
	TIME [epoch: 2.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871885529192322		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.7871885529192322 | validation: 0.6043582966611627]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875816422465183		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.7875816422465183 | validation: 0.640275195517977]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791937826197338		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.7791937826197338 | validation: 0.6222799058194597]
	TIME [epoch: 2.72 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786039411847712		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.7786039411847712 | validation: 0.6463719911131979]
	TIME [epoch: 2.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877637731408453		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.7877637731408453 | validation: 0.6008055401791852]
	TIME [epoch: 2.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881154118192276		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.7881154118192276 | validation: 0.6962078029180327]
	TIME [epoch: 2.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802080044661717		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.802080044661717 | validation: 0.6038549123307466]
	TIME [epoch: 2.71 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956153718721405		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.7956153718721405 | validation: 0.6730105913665314]
	TIME [epoch: 2.72 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952278841335156		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.7952278841335156 | validation: 0.6123003700233065]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787122509280767		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.787122509280767 | validation: 0.6577548300454702]
	TIME [epoch: 2.72 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869987341660079		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.7869987341660079 | validation: 0.6016129962354205]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780312988913293		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.780312988913293 | validation: 0.6428843286271113]
	TIME [epoch: 2.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823345615044889		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.7823345615044889 | validation: 0.6013806982893574]
	TIME [epoch: 2.72 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784430997069478		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.784430997069478 | validation: 0.6737166849683748]
	TIME [epoch: 2.72 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814064775095909		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.7814064775095909 | validation: 0.5891327531640883]
	TIME [epoch: 2.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010207352399529		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.8010207352399529 | validation: 0.6961072223178006]
	TIME [epoch: 2.72 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906888223707548		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.7906888223707548 | validation: 0.5956841021329983]
	TIME [epoch: 2.72 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898199318162997		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.7898199318162997 | validation: 0.628246957662929]
	TIME [epoch: 2.72 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781751643845775		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.781751643845775 | validation: 0.6133551968587652]
	TIME [epoch: 2.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766093432050272		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.7766093432050272 | validation: 0.6041783687252605]
	TIME [epoch: 2.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790534489146296		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.7790534489146296 | validation: 0.6197687894167091]
	TIME [epoch: 2.72 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791242634254179		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.791242634254179 | validation: 0.6191067086019726]
	TIME [epoch: 2.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871413940286833		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.7871413940286833 | validation: 0.6632987360964444]
	TIME [epoch: 2.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918604435272896		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.7918604435272896 | validation: 0.5909157424758538]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961158947327229		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.7961158947327229 | validation: 0.690352251544375]
	TIME [epoch: 2.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984993595495451		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.7984993595495451 | validation: 0.5930257804463469]
	TIME [epoch: 2.72 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887481830374148		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.7887481830374148 | validation: 0.6304747629274794]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837108162432184		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.7837108162432184 | validation: 0.6071698323361249]
	TIME [epoch: 2.72 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920038388202815		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.7920038388202815 | validation: 0.5945747405200864]
	TIME [epoch: 2.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786517962887562		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.786517962887562 | validation: 0.6266419567170803]
	TIME [epoch: 2.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856522108734572		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.7856522108734572 | validation: 0.6147089038220171]
	TIME [epoch: 2.72 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841751080431427		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.7841751080431427 | validation: 0.6409855494987708]
	TIME [epoch: 2.72 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777783948293264		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.777783948293264 | validation: 0.6086786677684666]
	TIME [epoch: 2.72 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848038642707869		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.7848038642707869 | validation: 0.6705371355113133]
	TIME [epoch: 2.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941387496916761		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.7941387496916761 | validation: 0.5871472522542288]
	TIME [epoch: 2.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056557944496757		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.8056557944496757 | validation: 0.6945299483991292]
	TIME [epoch: 2.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79048335897621		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.79048335897621 | validation: 0.59654987111665]
	TIME [epoch: 2.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722486070202308		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.7722486070202308 | validation: 0.6071613162727124]
	TIME [epoch: 2.72 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831448418428275		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.7831448418428275 | validation: 0.6219766335757516]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775421710680265		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.7775421710680265 | validation: 0.6029711140845933]
	TIME [epoch: 2.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765338245238406		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.7765338245238406 | validation: 0.6276350885367395]
	TIME [epoch: 2.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835939565589948		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.7835939565589948 | validation: 0.6020408656584457]
	TIME [epoch: 2.73 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885616279758523		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.7885616279758523 | validation: 0.6683113885895587]
	TIME [epoch: 2.71 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797231008171485		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.797231008171485 | validation: 0.5844189059026413]
	TIME [epoch: 2.72 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960896722120044		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.7960896722120044 | validation: 0.6507772999000405]
	TIME [epoch: 2.72 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898242938728136		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.7898242938728136 | validation: 0.594467598604032]
	TIME [epoch: 2.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786713385478728		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.7786713385478728 | validation: 0.6290654794273319]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746537495476093		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.7746537495476093 | validation: 0.6299776760753281]
	TIME [epoch: 2.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805341504989983		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.7805341504989983 | validation: 0.5868770267851868]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_175636/states/model_phi1_4a_distortion_v2_1_v_mmd1_847.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2083.292 seconds.
