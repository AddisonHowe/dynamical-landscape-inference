Args:
Namespace(name='model_phi1_4a_distortion1_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3816390619

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6353294162779095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6353294162779095 | validation: 5.459739681699609]
	TIME [epoch: 128 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.994801610992964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.994801610992964 | validation: 5.596271017927445]
	TIME [epoch: 0.484 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.732671018659407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.732671018659407 | validation: 6.80996473000827]
	TIME [epoch: 0.689 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.329130342048138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.329130342048138 | validation: 6.486485442398838]
	TIME [epoch: 0.476 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.966917741081843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.966917741081843 | validation: 5.4058238085975745]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.765543553186144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.765543553186144 | validation: 5.249153164046857]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.549895439969972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549895439969972 | validation: 5.164324476017223]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.476273630151946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.476273630151946 | validation: 5.221795709296398]
	TIME [epoch: 0.477 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.318719294146793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.318719294146793 | validation: 5.160303639411397]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.166338473312676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.166338473312676 | validation: 5.373264879993478]
	TIME [epoch: 0.475 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.209154795263101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.209154795263101 | validation: 5.098412153921291]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.692504723697398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.692504723697398 | validation: 5.7806173168054835]
	TIME [epoch: 0.475 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.850953274462131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.850953274462131 | validation: 5.658930119166811]
	TIME [epoch: 0.474 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.337138107118388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.337138107118388 | validation: 5.803084392943515]
	TIME [epoch: 0.473 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.517440285468105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517440285468105 | validation: 5.016379189576455]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7916349534500187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7916349534500187 | validation: 4.971386424361278]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7508070541924803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7508070541924803 | validation: 5.113383517798266]
	TIME [epoch: 0.475 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7480753190161433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7480753190161433 | validation: 5.011934912111922]
	TIME [epoch: 0.476 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.498524258961305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.498524258961305 | validation: 5.072631993493955]
	TIME [epoch: 0.475 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5246327346441593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5246327346441593 | validation: 5.089344258494727]
	TIME [epoch: 0.474 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.297805543771574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.297805543771574 | validation: 4.910513942312332]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2370867782495014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2370867782495014 | validation: 5.141423047825662]
	TIME [epoch: 0.477 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2428404719090804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2428404719090804 | validation: 4.922652007633257]
	TIME [epoch: 0.475 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.347257986897364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.347257986897364 | validation: 4.9775509098335275]
	TIME [epoch: 0.474 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.128634565429985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.128634565429985 | validation: 4.818326782310011]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.109687573166015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.109687573166015 | validation: 4.905617453122949]
	TIME [epoch: 0.477 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.118877372237898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.118877372237898 | validation: 4.8937878579748775]
	TIME [epoch: 0.475 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2719197931872457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2719197931872457 | validation: 4.900839061963139]
	TIME [epoch: 0.475 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.013001127724716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.013001127724716 | validation: 4.7997779055433325]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.052209259014588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.052209259014588 | validation: 4.823347001417042]
	TIME [epoch: 0.476 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9632400283727267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9632400283727267 | validation: 4.817679200519175]
	TIME [epoch: 0.476 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0395846817698096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0395846817698096 | validation: 4.8364750633327676]
	TIME [epoch: 0.475 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0241804768271487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0241804768271487 | validation: 4.746639186454941]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1584046056228203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1584046056228203 | validation: 4.724353278939561]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8365788149689104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8365788149689104 | validation: 4.726472682543667]
	TIME [epoch: 0.476 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.889910921824044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.889910921824044 | validation: 4.5445809604534615]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.920176487491874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.920176487491874 | validation: 4.8530096078115195]
	TIME [epoch: 0.477 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1000201067749322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1000201067749322 | validation: 4.592368191836639]
	TIME [epoch: 0.476 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.088721527072351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.088721527072351 | validation: 4.6014388121308]
	TIME [epoch: 0.475 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.728685218387003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.728685218387003 | validation: 4.736562881462192]
	TIME [epoch: 0.475 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.94363791116134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.94363791116134 | validation: 4.492611712215208]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.990536380341622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.990536380341622 | validation: 4.551244821447455]
	TIME [epoch: 0.477 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.714746264115362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.714746264115362 | validation: 4.727136551034761]
	TIME [epoch: 0.476 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9461633952242496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9461633952242496 | validation: 4.432406889609328]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8997671237192013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8997671237192013 | validation: 4.47784898389789]
	TIME [epoch: 0.478 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6642956592737392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6642956592737392 | validation: 4.579013788564247]
	TIME [epoch: 0.476 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7764455783715203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7764455783715203 | validation: 4.378765783442479]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.837551254451014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.837551254451014 | validation: 4.4421570105564365]
	TIME [epoch: 0.476 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.614749139868415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.614749139868415 | validation: 4.425751899415485]
	TIME [epoch: 0.476 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5966615259906893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5966615259906893 | validation: 4.2818160988676786]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.693166989863361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.693166989863361 | validation: 4.483240792291122]
	TIME [epoch: 0.476 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7537950127062745		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.7537950127062745 | validation: 4.239420435342483]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.708334622140891		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.708334622140891 | validation: 4.31779749548416]
	TIME [epoch: 0.476 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5393545882851045		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.5393545882851045 | validation: 4.206190260035812]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4964022012490314		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.4964022012490314 | validation: 4.154010168519778]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.476719275192355		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.476719275192355 | validation: 4.116502169105626]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4673840623099035		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.4673840623099035 | validation: 4.106333636266919]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.471665639515595		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.471665639515595 | validation: 4.039322281232167]
	TIME [epoch: 0.477 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.516635516329191		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.516635516329191 | validation: 4.217178677810232]
	TIME [epoch: 0.475 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.623756203810294		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.623756203810294 | validation: 4.211868249548466]
	TIME [epoch: 0.475 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0053604060549004		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.0053604060549004 | validation: 4.091899973983515]
	TIME [epoch: 0.475 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.430271835481924		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.430271835481924 | validation: 4.102522521447322]
	TIME [epoch: 0.475 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.485270474301006		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.485270474301006 | validation: 3.992661740699746]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5968757316792557		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.5968757316792557 | validation: 4.007263018918498]
	TIME [epoch: 0.475 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.417499881142062		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.417499881142062 | validation: 3.9300065761340557]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3635985430765016		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.3635985430765016 | validation: 3.855449466512086]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3490337991420907		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.3490337991420907 | validation: 3.8751434612341664]
	TIME [epoch: 0.476 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3638634840988497		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.3638634840988497 | validation: 3.770664883919834]
	TIME [epoch: 0.479 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4044093893032055		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.4044093893032055 | validation: 3.9020390524133415]
	TIME [epoch: 0.476 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4684563536082864		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.4684563536082864 | validation: 3.7597413206821066]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5315376656064377		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.5315376656064377 | validation: 3.7773271036254252]
	TIME [epoch: 0.476 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.309846005694859		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.309846005694859 | validation: 3.6797597828989286]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2783383336799496		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.2783383336799496 | validation: 3.586352302581917]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.27179191985781		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.27179191985781 | validation: 3.584161056585003]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2678531004939755		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.2678531004939755 | validation: 3.5148140890166895]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3301010095491086		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.3301010095491086 | validation: 3.7443948716820667]
	TIME [epoch: 0.475 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4016520600185887		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.4016520600185887 | validation: 3.5835654010658033]
	TIME [epoch: 0.476 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.465823056566792		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.465823056566792 | validation: 3.4927530744780526]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.251599455355099		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.251599455355099 | validation: 3.506473758123577]
	TIME [epoch: 0.476 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.255996400443744		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.255996400443744 | validation: 3.3680280409614514]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.243828440012233		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.243828440012233 | validation: 3.3499418793295392]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.189371832449359		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.189371832449359 | validation: 3.236708291747508]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.160275684149573		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.160275684149573 | validation: 3.199933757927161]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.152189515608405		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.152189515608405 | validation: 3.1091994131003506]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.140498554079845		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.140498554079845 | validation: 3.3040095809750127]
	TIME [epoch: 0.476 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2409753805779		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.2409753805779 | validation: 3.4142317315655313]
	TIME [epoch: 0.475 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5421765466798294		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.5421765466798294 | validation: 3.222434980214037]
	TIME [epoch: 0.475 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1462447162703517		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.1462447162703517 | validation: 3.328747221315558]
	TIME [epoch: 0.475 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2246182931365968		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.2246182931365968 | validation: 3.201062823673926]
	TIME [epoch: 0.475 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1860052213541725		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.1860052213541725 | validation: 3.0352356451923]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.091155910041863		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.091155910041863 | validation: 2.9466164354500375]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0914134470602113		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.0914134470602113 | validation: 2.896020331763311]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.083955512298242		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.083955512298242 | validation: 3.0427169980783164]
	TIME [epoch: 0.476 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1215382280100137		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.1215382280100137 | validation: 2.9198409405852424]
	TIME [epoch: 0.475 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.239624699328137		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.239624699328137 | validation: 2.9892269963302365]
	TIME [epoch: 0.475 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.11484852011463		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.11484852011463 | validation: 2.799051924732629]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.026482467300922		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.026482467300922 | validation: 2.673010473028996]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9440190720312993		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.9440190720312993 | validation: 2.6388984152068073]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9360672117920223		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.9360672117920223 | validation: 2.5817274283192306]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.063983701441425		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.063983701441425 | validation: 3.6224120931569477]
	TIME [epoch: 0.474 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4488994657645886		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.4488994657645886 | validation: 2.9040718469435083]
	TIME [epoch: 0.474 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.145530127604969		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.145530127604969 | validation: 2.394867070875024]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8296827205793158		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8296827205793158 | validation: 2.368536649644429]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8864332049858186		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.8864332049858186 | validation: 3.0117882808786884]
	TIME [epoch: 0.474 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1944703214962393		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.1944703214962393 | validation: 2.2972862790400077]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8696734602204037		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.8696734602204037 | validation: 2.1693034921362737]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7584647060311693		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.7584647060311693 | validation: 1.7987831355147188]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5429120272454524		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5429120272454524 | validation: 1.9560664858598416]
	TIME [epoch: 0.476 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7600599063463298		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.7600599063463298 | validation: 3.7756644398289225]
	TIME [epoch: 0.474 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6368807681098043		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.6368807681098043 | validation: 2.6395340063295327]
	TIME [epoch: 0.474 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1744303419966506		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.1744303419966506 | validation: 1.819065065082799]
	TIME [epoch: 0.475 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5957447907095974		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5957447907095974 | validation: 2.474436304516104]
	TIME [epoch: 0.475 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.094644444709348		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.094644444709348 | validation: 1.2825333730508595]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217047537138882		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.217047537138882 | validation: 1.5008343422610908]
	TIME [epoch: 0.476 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3798942819611741		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.3798942819611741 | validation: 1.3283290420944787]
	TIME [epoch: 0.476 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3008407619039417		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.3008407619039417 | validation: 1.3351268032617498]
	TIME [epoch: 0.476 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2416007150827328		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.2416007150827328 | validation: 0.8744320699654788]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024130327794894		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1024130327794894 | validation: 0.9901728623939796]
	TIME [epoch: 0.475 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087945874238853		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.087945874238853 | validation: 0.8165368589551383]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1008819133928065		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1008819133928065 | validation: 1.220486136140596]
	TIME [epoch: 0.476 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183532116175085		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.183532116175085 | validation: 0.9520822640470705]
	TIME [epoch: 0.474 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0795170527212954		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.0795170527212954 | validation: 1.0578962979165139]
	TIME [epoch: 0.474 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0636017057441722		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.0636017057441722 | validation: 0.9944690536270149]
	TIME [epoch: 0.474 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0169526998813845		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.0169526998813845 | validation: 0.9128982972138694]
	TIME [epoch: 0.476 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0517237149335856		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0517237149335856 | validation: 0.8743815550210163]
	TIME [epoch: 0.474 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662135283999796		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9662135283999796 | validation: 0.944920378015897]
	TIME [epoch: 0.474 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9205498124204968		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9205498124204968 | validation: 0.7192576799598402]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9561497839512515		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9561497839512515 | validation: 1.1417310158563154]
	TIME [epoch: 0.475 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0913165786901284		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0913165786901284 | validation: 0.7285857400586289]
	TIME [epoch: 0.474 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311828439649159		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.9311828439649159 | validation: 0.8062117317162072]
	TIME [epoch: 0.474 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602411276904846		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8602411276904846 | validation: 0.5865282806895289]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.83626504736779		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.83626504736779 | validation: 0.8468767477287132]
	TIME [epoch: 0.476 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404262292110058		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8404262292110058 | validation: 0.6098006471516144]
	TIME [epoch: 0.474 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128949691320076		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8128949691320076 | validation: 0.8315662857805187]
	TIME [epoch: 0.474 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074614596485522		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9074614596485522 | validation: 0.6364450389613119]
	TIME [epoch: 0.475 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266536165883561		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9266536165883561 | validation: 0.9762517728148445]
	TIME [epoch: 0.475 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826226464549342		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8826226464549342 | validation: 0.5877489273206071]
	TIME [epoch: 0.474 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741202514370489		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7741202514370489 | validation: 0.6971098542968653]
	TIME [epoch: 0.474 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503544237352884		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7503544237352884 | validation: 0.906647558862733]
	TIME [epoch: 0.474 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476373297357597		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8476373297357597 | validation: 0.9001149600849886]
	TIME [epoch: 0.475 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9972003906641312		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.9972003906641312 | validation: 0.6586601228600459]
	TIME [epoch: 0.475 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822124338834482		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.822124338834482 | validation: 0.6914849064749159]
	TIME [epoch: 0.474 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741793148883013		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.741793148883013 | validation: 0.5813982305172223]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825232321584292		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6825232321584292 | validation: 0.6335941153908259]
	TIME [epoch: 0.475 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299383626314437		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7299383626314437 | validation: 0.8781970929845727]
	TIME [epoch: 0.48 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893520236968702		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7893520236968702 | validation: 0.6479456801574052]
	TIME [epoch: 0.474 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753087695753785		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.753087695753785 | validation: 0.8131487027688284]
	TIME [epoch: 0.474 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286824250488952		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8286824250488952 | validation: 0.49698637224307385]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047678038826009		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8047678038826009 | validation: 0.7988185859494561]
	TIME [epoch: 0.475 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131536018769962		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7131536018769962 | validation: 0.3922865044066701]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884332070314301		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6884332070314301 | validation: 0.7396017039627675]
	TIME [epoch: 0.474 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736382076171341		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.736382076171341 | validation: 0.7598193356382794]
	TIME [epoch: 0.476 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111442729401388		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7111442729401388 | validation: 0.8401059343910371]
	TIME [epoch: 0.474 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181553266060305		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7181553266060305 | validation: 0.7172515812353151]
	TIME [epoch: 0.474 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398405403876066		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7398405403876066 | validation: 0.6292454388619778]
	TIME [epoch: 0.474 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138580882944362		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7138580882944362 | validation: 0.7869545977704576]
	TIME [epoch: 0.475 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910314234343263		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6910314234343263 | validation: 0.44405343341996323]
	TIME [epoch: 0.475 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015578809577397		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7015578809577397 | validation: 0.7132630706908284]
	TIME [epoch: 0.474 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072111823445861		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7072111823445861 | validation: 0.45427674250477024]
	TIME [epoch: 0.474 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729529372980832		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7729529372980832 | validation: 1.0479003143103054]
	TIME [epoch: 0.474 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457551120835248		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8457551120835248 | validation: 0.4292982156627117]
	TIME [epoch: 0.475 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5806299557833114		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5806299557833114 | validation: 0.4948874256436849]
	TIME [epoch: 0.475 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984265294238659		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.5984265294238659 | validation: 0.5276668106499655]
	TIME [epoch: 0.474 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889734893793918		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5889734893793918 | validation: 0.647872367807441]
	TIME [epoch: 0.474 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974904363008675		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5974904363008675 | validation: 0.4685893545352814]
	TIME [epoch: 0.474 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793473513914825		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6793473513914825 | validation: 0.5660074986268727]
	TIME [epoch: 0.475 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568215582852618		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.568215582852618 | validation: 0.6674581961555028]
	TIME [epoch: 0.474 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5781875248965247		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.5781875248965247 | validation: 0.4113629572381958]
	TIME [epoch: 0.473 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5313283813379388		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.5313283813379388 | validation: 0.5334751018639879]
	TIME [epoch: 0.474 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025488047969457		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5025488047969457 | validation: 0.34550058380642046]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509501470987206		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.509501470987206 | validation: 0.5581870104356459]
	TIME [epoch: 0.475 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394083100040927		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5394083100040927 | validation: 0.8593735277736478]
	TIME [epoch: 0.475 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8405661985280983		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.8405661985280983 | validation: 0.717227834713639]
	TIME [epoch: 0.474 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9773060026332004		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9773060026332004 | validation: 1.027349540140228]
	TIME [epoch: 0.474 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767039658109863		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.767039658109863 | validation: 0.5400116560464009]
	TIME [epoch: 0.474 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786260944425532		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.5786260944425532 | validation: 0.5977005989001977]
	TIME [epoch: 0.473 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629876271435862		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6629876271435862 | validation: 0.6624208182045334]
	TIME [epoch: 0.474 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5524877699759211		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5524877699759211 | validation: 0.3814309243822684]
	TIME [epoch: 0.474 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4660867929522013		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.4660867929522013 | validation: 0.5525261311508792]
	TIME [epoch: 0.474 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189031504444274		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.5189031504444274 | validation: 0.8091613245580986]
	TIME [epoch: 0.474 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725261985410892		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.725261985410892 | validation: 0.8663949905879984]
	TIME [epoch: 0.474 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918953748853401		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6918953748853401 | validation: 0.34986077739281296]
	TIME [epoch: 0.474 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5549491692427853		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.5549491692427853 | validation: 0.4815450400098204]
	TIME [epoch: 0.474 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48851215581984136		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.48851215581984136 | validation: 0.617181351942742]
	TIME [epoch: 0.474 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635554683553456		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.5635554683553456 | validation: 0.6093219409615721]
	TIME [epoch: 0.474 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5486345463151345		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5486345463151345 | validation: 0.5785725547712478]
	TIME [epoch: 0.474 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356539542897924		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6356539542897924 | validation: 0.6965385907274837]
	TIME [epoch: 0.474 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877418855723472		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.5877418855723472 | validation: 0.33423776401430777]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4831924561865581		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.4831924561865581 | validation: 0.5517821711565319]
	TIME [epoch: 0.475 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4886152414245929		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.4886152414245929 | validation: 0.38753095775393614]
	TIME [epoch: 0.475 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5329393191321424		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5329393191321424 | validation: 0.4887019564551445]
	TIME [epoch: 0.475 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4420952661805827		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.4420952661805827 | validation: 0.4772286206861549]
	TIME [epoch: 0.476 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4073722395639811		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.4073722395639811 | validation: 0.2797041071735046]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4246572806878187		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.4246572806878187 | validation: 0.5954214503994585]
	TIME [epoch: 0.474 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4648335556415878		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.4648335556415878 | validation: 0.28333200256179203]
	TIME [epoch: 0.475 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.562493373006597		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.562493373006597 | validation: 0.5731444686815466]
	TIME [epoch: 0.476 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5570884303795602		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.5570884303795602 | validation: 0.6008974338324157]
	TIME [epoch: 0.476 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6947624312382764		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6947624312382764 | validation: 0.7307726980326277]
	TIME [epoch: 0.475 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398814553449211		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7398814553449211 | validation: 0.4024012752735616]
	TIME [epoch: 0.475 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3845895822279702		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.3845895822279702 | validation: 0.5318439860808533]
	TIME [epoch: 0.476 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45690777741127336		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.45690777741127336 | validation: 0.5687639396781808]
	TIME [epoch: 135 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49001681791644486		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.49001681791644486 | validation: 0.36174056797997206]
	TIME [epoch: 0.938 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4171981534047521		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.4171981534047521 | validation: 0.5721010810063043]
	TIME [epoch: 0.93 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4403422322101892		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.4403422322101892 | validation: 0.23538130542924862]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5012909650601192		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5012909650601192 | validation: 0.4945067726556083]
	TIME [epoch: 0.928 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41870775756323453		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.41870775756323453 | validation: 0.2976085518287094]
	TIME [epoch: 0.925 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4058204531401555		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.4058204531401555 | validation: 0.7822613011090541]
	TIME [epoch: 0.926 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5362441293584518		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5362441293584518 | validation: 0.44278246144613775]
	TIME [epoch: 0.926 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029045751241989		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.4029045751241989 | validation: 0.41987167728679164]
	TIME [epoch: 0.927 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4988786087585301		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.4988786087585301 | validation: 0.5187839710818137]
	TIME [epoch: 0.929 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5043485440967213		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5043485440967213 | validation: 0.46959728221981556]
	TIME [epoch: 0.93 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42848969523780844		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.42848969523780844 | validation: 0.40219287583576957]
	TIME [epoch: 0.93 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3536937819370427		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.3536937819370427 | validation: 0.2752830349093469]
	TIME [epoch: 0.929 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3690572532184814		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.3690572532184814 | validation: 0.9753130536847903]
	TIME [epoch: 0.929 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793416938259568		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8793416938259568 | validation: 1.030580002785374]
	TIME [epoch: 0.943 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205270618269512		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.8205270618269512 | validation: 0.8618903442977051]
	TIME [epoch: 0.93 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609424493060355		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6609424493060355 | validation: 0.36774125988799733]
	TIME [epoch: 0.929 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34479602921840086		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.34479602921840086 | validation: 0.35810984200523166]
	TIME [epoch: 0.934 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5515247029093452		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.5515247029093452 | validation: 0.43779981197124657]
	TIME [epoch: 0.929 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4820627028545214		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.4820627028545214 | validation: 0.3483822841726753]
	TIME [epoch: 0.929 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098743647182468		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.3098743647182468 | validation: 0.30069015117862147]
	TIME [epoch: 0.929 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3909475898228884		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.3909475898228884 | validation: 0.41894015390145783]
	TIME [epoch: 0.929 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193228132503444		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.3193228132503444 | validation: 0.22955092053336967]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32764329582361357		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.32764329582361357 | validation: 0.39599632392313305]
	TIME [epoch: 0.926 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3468819997226067		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.3468819997226067 | validation: 0.28567665969812195]
	TIME [epoch: 0.925 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870363183983886		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.3870363183983886 | validation: 0.44561450208036124]
	TIME [epoch: 0.925 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701005480127763		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.3701005480127763 | validation: 0.405357617426082]
	TIME [epoch: 0.924 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3854275812094561		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.3854275812094561 | validation: 0.3786517486434766]
	TIME [epoch: 0.926 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3241201053114471		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.3241201053114471 | validation: 0.48246257075085786]
	TIME [epoch: 0.925 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3762104089594313		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.3762104089594313 | validation: 0.3542054458563735]
	TIME [epoch: 0.925 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4690005360951078		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.4690005360951078 | validation: 0.38785335019558664]
	TIME [epoch: 0.924 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3377246138642626		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.3377246138642626 | validation: 0.24989702520321036]
	TIME [epoch: 0.934 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24965939785317745		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.24965939785317745 | validation: 0.25351145056145435]
	TIME [epoch: 0.925 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22836622264372872		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.22836622264372872 | validation: 0.23835080394838803]
	TIME [epoch: 0.926 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225846440642467		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.225846440642467 | validation: 0.20955348429817]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26715758531999684		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.26715758531999684 | validation: 0.4543718321961989]
	TIME [epoch: 0.929 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33096832499636336		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.33096832499636336 | validation: 0.33717453509816725]
	TIME [epoch: 0.928 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4146189474304899		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.4146189474304899 | validation: 0.41638692785755504]
	TIME [epoch: 0.927 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37224020829739923		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.37224020829739923 | validation: 0.27633335424017774]
	TIME [epoch: 0.927 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328495624321737		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.328495624321737 | validation: 0.2737910299201343]
	TIME [epoch: 0.928 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26739397700640494		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.26739397700640494 | validation: 0.2777042222645662]
	TIME [epoch: 0.927 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23841712022964978		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.23841712022964978 | validation: 0.21847787457107468]
	TIME [epoch: 0.93 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22339427298736772		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.22339427298736772 | validation: 0.40661383785281874]
	TIME [epoch: 0.924 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26652783532885344		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.26652783532885344 | validation: 0.2701997463908405]
	TIME [epoch: 0.925 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35711029862465715		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.35711029862465715 | validation: 0.6341867643538706]
	TIME [epoch: 0.925 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4584596290536779		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.4584596290536779 | validation: 0.1617062643426564]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066436755806479		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.3066436755806479 | validation: 0.233479501147181]
	TIME [epoch: 0.93 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19909186345135488		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.19909186345135488 | validation: 0.169100616642791]
	TIME [epoch: 0.934 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17969325465358496		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.17969325465358496 | validation: 0.2699318204227544]
	TIME [epoch: 0.93 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22843416730233443		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.22843416730233443 | validation: 0.36418480453934565]
	TIME [epoch: 0.929 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36437024752287345		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.36437024752287345 | validation: 0.4987171308005384]
	TIME [epoch: 0.929 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3926788194983133		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.3926788194983133 | validation: 0.31953138953401605]
	TIME [epoch: 0.929 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2737894383613989		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.2737894383613989 | validation: 0.22427419093274273]
	TIME [epoch: 0.929 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20069440592085896		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.20069440592085896 | validation: 0.15340356587407264]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2148558873381709		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.2148558873381709 | validation: 0.30648915908619595]
	TIME [epoch: 0.926 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25281768727930043		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.25281768727930043 | validation: 0.13859685525936002]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098735763717367		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.3098735763717367 | validation: 0.39837883346194]
	TIME [epoch: 0.928 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26982241957030334		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.26982241957030334 | validation: 0.15612836252275278]
	TIME [epoch: 0.928 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19678562381047268		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.19678562381047268 | validation: 0.3140822768069409]
	TIME [epoch: 0.928 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21094311758066847		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.21094311758066847 | validation: 0.1854543114971167]
	TIME [epoch: 0.928 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23476925756959957		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.23476925756959957 | validation: 0.4049227885132908]
	TIME [epoch: 0.928 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716284344182144		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.2716284344182144 | validation: 0.29453590962628723]
	TIME [epoch: 0.928 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28439058738137485		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.28439058738137485 | validation: 0.33190777108794295]
	TIME [epoch: 0.95 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339197259241351		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.3339197259241351 | validation: 0.22355745254406012]
	TIME [epoch: 0.928 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034637480852418		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.2034637480852418 | validation: 0.22605412610456077]
	TIME [epoch: 0.928 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18008310722387677		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.18008310722387677 | validation: 0.19712037146615013]
	TIME [epoch: 0.925 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17382676340601777		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.17382676340601777 | validation: 0.20941763121010606]
	TIME [epoch: 0.929 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18799288107101084		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.18799288107101084 | validation: 0.2501595426380414]
	TIME [epoch: 0.929 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21773282927409984		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.21773282927409984 | validation: 0.28824058220080356]
	TIME [epoch: 0.929 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23477810573640348		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.23477810573640348 | validation: 0.30919842630943495]
	TIME [epoch: 0.929 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069718933634602		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3069718933634602 | validation: 0.2626662243205548]
	TIME [epoch: 0.929 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3263345201709387		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3263345201709387 | validation: 0.22069981901958965]
	TIME [epoch: 0.929 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19370526294113535		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.19370526294113535 | validation: 0.11360438697732983]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1723414322752346		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.1723414322752346 | validation: 0.26028780055524486]
	TIME [epoch: 0.928 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19005660228359852		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.19005660228359852 | validation: 0.13841825843820113]
	TIME [epoch: 0.928 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20243109279236138		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.20243109279236138 | validation: 0.31398832466279436]
	TIME [epoch: 0.928 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20729395794417527		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.20729395794417527 | validation: 0.1141406195490674]
	TIME [epoch: 0.928 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071365553767722		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.2071365553767722 | validation: 0.2572716170756983]
	TIME [epoch: 0.928 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031694468030679		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.2031694468030679 | validation: 0.12265240757768657]
	TIME [epoch: 0.933 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20345294025604552		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.20345294025604552 | validation: 0.3129274251262928]
	TIME [epoch: 0.929 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20581598275044616		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.20581598275044616 | validation: 0.1500587166668526]
	TIME [epoch: 0.928 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16712614907238021		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.16712614907238021 | validation: 0.273380361379192]
	TIME [epoch: 0.927 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18564607383974657		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.18564607383974657 | validation: 0.34676075681815555]
	TIME [epoch: 0.929 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3010131433759643		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.3010131433759643 | validation: 0.3626740917669081]
	TIME [epoch: 0.928 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187585668309329		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.3187585668309329 | validation: 0.21158365146755825]
	TIME [epoch: 0.927 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16870867154370836		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.16870867154370836 | validation: 0.14988977863317002]
	TIME [epoch: 0.929 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421594918981857		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.1421594918981857 | validation: 0.16600303365915792]
	TIME [epoch: 0.927 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614800635726501		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.1614800635726501 | validation: 0.13815154971368124]
	TIME [epoch: 0.928 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2103636536748404		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.2103636536748404 | validation: 0.16896741115809852]
	TIME [epoch: 0.927 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18281008068189294		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.18281008068189294 | validation: 0.13286850713444973]
	TIME [epoch: 0.928 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1395793802259955		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.1395793802259955 | validation: 0.13478351231267716]
	TIME [epoch: 0.928 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12545009526144482		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.12545009526144482 | validation: 0.19128328597718036]
	TIME [epoch: 0.928 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15489211030784042		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.15489211030784042 | validation: 0.2645131904263247]
	TIME [epoch: 0.928 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21037127366394665		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.21037127366394665 | validation: 0.4040989361046455]
	TIME [epoch: 0.928 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3282042954189504		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.3282042954189504 | validation: 0.2830499764081525]
	TIME [epoch: 0.928 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23971892837264427		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.23971892837264427 | validation: 0.196656460325231]
	TIME [epoch: 0.928 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16151980740378974		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.16151980740378974 | validation: 0.09531329860987997]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16804559827293236		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.16804559827293236 | validation: 0.252838933254079]
	TIME [epoch: 0.949 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18017380935902677		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.18017380935902677 | validation: 0.11711805940058512]
	TIME [epoch: 0.927 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16730460765074737		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.16730460765074737 | validation: 0.21553643672727804]
	TIME [epoch: 0.926 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14934387194033374		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.14934387194033374 | validation: 0.09231158714298375]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12376214475094408		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.12376214475094408 | validation: 0.17603331625598612]
	TIME [epoch: 0.929 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12692883777518138		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.12692883777518138 | validation: 0.09058245859018098]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15540254100302078		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.15540254100302078 | validation: 0.2569943809196668]
	TIME [epoch: 0.927 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18345345536291421		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.18345345536291421 | validation: 0.13457394459074337]
	TIME [epoch: 0.927 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15611371631251397		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.15611371631251397 | validation: 0.2900111764040136]
	TIME [epoch: 0.928 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22804010189386464		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.22804010189386464 | validation: 0.3497688777468708]
	TIME [epoch: 0.929 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26044727003240636		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.26044727003240636 | validation: 0.22766715781942892]
	TIME [epoch: 0.927 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17163287475338915		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.17163287475338915 | validation: 0.15748090770503784]
	TIME [epoch: 0.931 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499909697857466		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.1499909697857466 | validation: 0.1553256374236956]
	TIME [epoch: 0.927 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885959644561941		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.1885959644561941 | validation: 0.21158165222064904]
	TIME [epoch: 0.929 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19816393498732243		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.19816393498732243 | validation: 0.14852181591328018]
	TIME [epoch: 0.926 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17439107856478173		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.17439107856478173 | validation: 0.18380388778932427]
	TIME [epoch: 0.93 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14091315284754022		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.14091315284754022 | validation: 0.11870510125175027]
	TIME [epoch: 0.927 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086019746251076		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.1086019746251076 | validation: 0.10876002848185334]
	TIME [epoch: 0.928 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.110371472710783		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.110371472710783 | validation: 0.16953129043915413]
	TIME [epoch: 0.927 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178310170699039		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.12178310170699039 | validation: 0.1670046259458754]
	TIME [epoch: 0.926 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14705346418286905		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.14705346418286905 | validation: 0.23734718476595604]
	TIME [epoch: 0.928 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754891085581437		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.1754891085581437 | validation: 0.2025272992215312]
	TIME [epoch: 0.928 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19364233069268885		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.19364233069268885 | validation: 0.17545545011434238]
	TIME [epoch: 0.928 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13351784979452314		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.13351784979452314 | validation: 0.09710687348089314]
	TIME [epoch: 0.928 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245143224091063		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.11245143224091063 | validation: 0.14788765404734835]
	TIME [epoch: 0.926 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289654904319457		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.1289654904319457 | validation: 0.08654911819054584]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17898992549731485		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.17898992549731485 | validation: 0.1809407991964061]
	TIME [epoch: 0.929 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17481924168230534		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.17481924168230534 | validation: 0.08168338767438486]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12643218808424853		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.12643218808424853 | validation: 0.13574261416945277]
	TIME [epoch: 0.929 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902389899528054		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.09902389899528054 | validation: 0.10789501967188575]
	TIME [epoch: 0.928 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10565403899138458		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.10565403899138458 | validation: 0.24681231932984815]
	TIME [epoch: 0.927 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15858826610199045		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.15858826610199045 | validation: 0.21596424822381755]
	TIME [epoch: 0.952 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19126159042254176		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.19126159042254176 | validation: 0.20312417649654751]
	TIME [epoch: 0.928 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17382344544191242		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.17382344544191242 | validation: 0.14339254247097644]
	TIME [epoch: 0.927 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1515508763629165		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.1515508763629165 | validation: 0.16576057306889358]
	TIME [epoch: 0.927 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2350114056968711		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.2350114056968711 | validation: 0.20325730222197508]
	TIME [epoch: 0.928 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16507378765251082		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.16507378765251082 | validation: 0.13309897511889307]
	TIME [epoch: 0.927 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11317644348556062		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.11317644348556062 | validation: 0.11468280969461749]
	TIME [epoch: 0.928 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1054897973361529		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.1054897973361529 | validation: 0.15032225380618108]
	TIME [epoch: 0.927 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1076563656978685		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.1076563656978685 | validation: 0.10449856515866936]
	TIME [epoch: 0.927 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10358186613543208		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.10358186613543208 | validation: 0.15689010977642603]
	TIME [epoch: 0.927 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13219909439741825		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.13219909439741825 | validation: 0.15751710032761335]
	TIME [epoch: 0.927 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15341393882735163		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.15341393882735163 | validation: 0.22384405658817186]
	TIME [epoch: 0.933 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15514749356882115		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.15514749356882115 | validation: 0.1626382497245539]
	TIME [epoch: 0.927 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15096481211780874		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.15096481211780874 | validation: 0.1555491759915929]
	TIME [epoch: 0.926 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12434648472652972		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.12434648472652972 | validation: 0.06612517743345764]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14958936410040502		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.14958936410040502 | validation: 0.1483735289440467]
	TIME [epoch: 0.929 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12214642198644017		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.12214642198644017 | validation: 0.10357317406291347]
	TIME [epoch: 0.937 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419847922361743		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.1419847922361743 | validation: 0.1643408704116081]
	TIME [epoch: 0.926 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16385100444126827		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.16385100444126827 | validation: 0.159064871859022]
	TIME [epoch: 0.927 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12425285702032365		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.12425285702032365 | validation: 0.18432805605869312]
	TIME [epoch: 0.926 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13967977689736186		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.13967977689736186 | validation: 0.1595625470622981]
	TIME [epoch: 0.926 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12533898371285893		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.12533898371285893 | validation: 0.11304524694178296]
	TIME [epoch: 0.928 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.113464567690984		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.113464567690984 | validation: 0.10251402195781384]
	TIME [epoch: 0.926 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10596729651810334		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.10596729651810334 | validation: 0.12843257184438844]
	TIME [epoch: 0.928 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10857726986866237		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.10857726986866237 | validation: 0.11323825560471477]
	TIME [epoch: 0.927 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11668305981808913		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.11668305981808913 | validation: 0.13990551775609003]
	TIME [epoch: 0.929 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12322533222924019		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.12322533222924019 | validation: 0.11447532480582599]
	TIME [epoch: 0.927 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758507481991698		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.11758507481991698 | validation: 0.13077099171089634]
	TIME [epoch: 0.928 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12482002564154902		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.12482002564154902 | validation: 0.1388192016377021]
	TIME [epoch: 0.926 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13016011656446214		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.13016011656446214 | validation: 0.1337275092142807]
	TIME [epoch: 0.928 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989278800694626		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.11989278800694626 | validation: 0.1464406883671567]
	TIME [epoch: 0.926 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12243286576730482		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.12243286576730482 | validation: 0.09494745727722854]
	TIME [epoch: 0.929 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744410983337524		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.08744410983337524 | validation: 0.060855729588050717]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08449335827318331		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.08449335827318331 | validation: 0.10665348254486454]
	TIME [epoch: 0.927 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09837229848371523		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.09837229848371523 | validation: 0.0730986416027432]
	TIME [epoch: 0.927 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032080704807314		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.11032080704807314 | validation: 0.22498886435482254]
	TIME [epoch: 0.928 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15608100253484433		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.15608100253484433 | validation: 0.19932074362319852]
	TIME [epoch: 0.928 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17509917712637763		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.17509917712637763 | validation: 0.17151733170445937]
	TIME [epoch: 0.927 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12980930585569384		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.12980930585569384 | validation: 0.1065813993057829]
	TIME [epoch: 0.928 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110474608754436		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.1110474608754436 | validation: 0.0888117802838189]
	TIME [epoch: 0.927 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09272971706258573		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.09272971706258573 | validation: 0.11264575077830652]
	TIME [epoch: 0.929 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10369220062280697		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.10369220062280697 | validation: 0.0906552987326116]
	TIME [epoch: 0.927 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178020863717892		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.12178020863717892 | validation: 0.12761046825539957]
	TIME [epoch: 0.928 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11569355614960618		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.11569355614960618 | validation: 0.11451310718866477]
	TIME [epoch: 0.932 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248671829814479		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.10248671829814479 | validation: 0.1087422286846783]
	TIME [epoch: 0.928 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218402163709431		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.10218402163709431 | validation: 0.16749383826278705]
	TIME [epoch: 0.925 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304207465392838		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.11304207465392838 | validation: 0.12244388105236959]
	TIME [epoch: 0.926 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1079678104759708		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.1079678104759708 | validation: 0.12873886905743898]
	TIME [epoch: 0.925 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990404313022395		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.09990404313022395 | validation: 0.09593590932492012]
	TIME [epoch: 0.927 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08983727991100274		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.08983727991100274 | validation: 0.09732461582020516]
	TIME [epoch: 0.928 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350937098209646		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.07350937098209646 | validation: 0.06337731242019787]
	TIME [epoch: 0.928 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122525705838526		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.07122525705838526 | validation: 0.11027804988040134]
	TIME [epoch: 0.926 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08309008050787485		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.08309008050787485 | validation: 0.09806149026452284]
	TIME [epoch: 0.926 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13551045013207064		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.13551045013207064 | validation: 0.1581063163836033]
	TIME [epoch: 0.924 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1354389555561892		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.1354389555561892 | validation: 0.07912206898716147]
	TIME [epoch: 0.926 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652651714033145		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.12652651714033145 | validation: 0.1433106786134097]
	TIME [epoch: 0.924 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982284371845221		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.09982284371845221 | validation: 0.10274276675636003]
	TIME [epoch: 0.926 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08642475266097333		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.08642475266097333 | validation: 0.11000082366408628]
	TIME [epoch: 0.926 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0999154382189027		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.0999154382189027 | validation: 0.11427881215966403]
	TIME [epoch: 0.925 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10812109134099292		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.10812109134099292 | validation: 0.11306166539876604]
	TIME [epoch: 0.926 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764665054111718		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.11764665054111718 | validation: 0.1156743322482118]
	TIME [epoch: 0.925 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10390437990952968		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.10390437990952968 | validation: 0.11850675192104501]
	TIME [epoch: 0.926 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10561357755967528		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.10561357755967528 | validation: 0.1568219398043248]
	TIME [epoch: 0.925 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135724663646375		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.135724663646375 | validation: 0.11946657068101313]
	TIME [epoch: 0.925 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900411902646039		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.0900411902646039 | validation: 0.09168122048390184]
	TIME [epoch: 0.924 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07931501040776263		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.07931501040776263 | validation: 0.09791397564481938]
	TIME [epoch: 0.924 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07424226248724265		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.07424226248724265 | validation: 0.0795417648807937]
	TIME [epoch: 0.924 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06803684428338665		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.06803684428338665 | validation: 0.070810758573203]
	TIME [epoch: 0.923 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825605849494538		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.06825605849494538 | validation: 0.07258175563863593]
	TIME [epoch: 0.925 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285425818537269		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.06285425818537269 | validation: 0.06277038442805696]
	TIME [epoch: 0.925 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401414198361792		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.09401414198361792 | validation: 0.09627822685889487]
	TIME [epoch: 0.925 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08303540822232183		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.08303540822232183 | validation: 0.06526291727770782]
	TIME [epoch: 0.925 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379092389286951		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.06379092389286951 | validation: 0.07833018096169646]
	TIME [epoch: 0.925 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916616302954854		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.0916616302954854 | validation: 0.1259996594546385]
	TIME [epoch: 0.927 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496122323675075		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.1496122323675075 | validation: 0.2721428874658751]
	TIME [epoch: 0.931 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2220607146531765		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2220607146531765 | validation: 0.20081246248434148]
	TIME [epoch: 0.927 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14786669668483818		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.14786669668483818 | validation: 0.12033784559073082]
	TIME [epoch: 0.925 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11146381934573572		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.11146381934573572 | validation: 0.0645371310020457]
	TIME [epoch: 0.925 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153121349144385		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.06153121349144385 | validation: 0.0698129712314793]
	TIME [epoch: 0.925 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059271559762615576		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.059271559762615576 | validation: 0.07426747120288295]
	TIME [epoch: 0.925 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360891569918899		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.06360891569918899 | validation: 0.09030987339220342]
	TIME [epoch: 0.926 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07432538184293173		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.07432538184293173 | validation: 0.10270164799609281]
	TIME [epoch: 0.926 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08982094710969171		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.08982094710969171 | validation: 0.1381463673942549]
	TIME [epoch: 0.927 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841578774428695		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.11841578774428695 | validation: 0.12125060179466525]
	TIME [epoch: 0.926 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310934551329706		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.11310934551329706 | validation: 0.09928967124686494]
	TIME [epoch: 0.926 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08221595453197093		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.08221595453197093 | validation: 0.07319777705232436]
	TIME [epoch: 0.925 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651720100439176		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.0651720100439176 | validation: 0.05450970094771404]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698347771190916		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.06698347771190916 | validation: 0.08682894721682675]
	TIME [epoch: 0.926 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07986417574085357		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.07986417574085357 | validation: 0.05779607331912935]
	TIME [epoch: 0.925 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09098489291755463		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.09098489291755463 | validation: 0.10282871853928471]
	TIME [epoch: 0.924 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288606923584352		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.08288606923584352 | validation: 0.07471198738315976]
	TIME [epoch: 0.924 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074402384567502		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.07074402384567502 | validation: 0.10212060040471811]
	TIME [epoch: 0.924 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599436907397619		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.07599436907397619 | validation: 0.09359482319220203]
	TIME [epoch: 0.925 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746135234923208		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.0746135234923208 | validation: 0.10222461905564306]
	TIME [epoch: 1.48 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853383552597212		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.07853383552597212 | validation: 0.09210664976438161]
	TIME [epoch: 0.926 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08108747420083759		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.08108747420083759 | validation: 0.11376974139575047]
	TIME [epoch: 0.924 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263973269430233		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.09263973269430233 | validation: 0.11464662607825718]
	TIME [epoch: 0.926 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131153691153435		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1131153691153435 | validation: 0.1511570033542379]
	TIME [epoch: 0.924 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287349380047703		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.1287349380047703 | validation: 0.09092570072018565]
	TIME [epoch: 0.924 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08449351891712353		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.08449351891712353 | validation: 0.07341423876443195]
	TIME [epoch: 0.924 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05598051630376156		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.05598051630376156 | validation: 0.053943081436863084]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973461184885314		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.04973461184885314 | validation: 0.048642773052294766]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061786166702337594		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.061786166702337594 | validation: 0.06651386641222744]
	TIME [epoch: 0.925 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06290800861152188		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.06290800861152188 | validation: 0.0746628569653611]
	TIME [epoch: 0.924 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07002072519335986		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.07002072519335986 | validation: 0.09250934718878545]
	TIME [epoch: 0.927 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061555182794518624		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.061555182794518624 | validation: 0.07758060743150881]
	TIME [epoch: 0.924 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15899704699882797		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.15899704699882797 | validation: 0.1600629836155275]
	TIME [epoch: 0.925 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11716068340633808		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.11716068340633808 | validation: 0.094006333732288]
	TIME [epoch: 0.925 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998474068249128		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.0998474068249128 | validation: 0.06372468055262771]
	TIME [epoch: 0.925 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242310438722827		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.05242310438722827 | validation: 0.04229888117740498]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496857538562152		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.0496857538562152 | validation: 0.04657070536519137]
	TIME [epoch: 0.925 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045147925419851236		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.045147925419851236 | validation: 0.05760276600439364]
	TIME [epoch: 0.925 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04727583699910245		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.04727583699910245 | validation: 0.05153020247071053]
	TIME [epoch: 0.925 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05029670365074173		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.05029670365074173 | validation: 0.11133725650858906]
	TIME [epoch: 0.924 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07761620400685654		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.07761620400685654 | validation: 0.19233177153594794]
	TIME [epoch: 0.924 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14425689043284312		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.14425689043284312 | validation: 0.15906687174286144]
	TIME [epoch: 0.924 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1300877215406295		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1300877215406295 | validation: 0.09834394840169236]
	TIME [epoch: 0.924 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243207973767476		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.10243207973767476 | validation: 0.0512681384665922]
	TIME [epoch: 0.924 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705635580106636		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.0705635580106636 | validation: 0.06659275658771781]
	TIME [epoch: 0.925 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544401307044712		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.05544401307044712 | validation: 0.056947971212299435]
	TIME [epoch: 0.924 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585806542297375		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.05585806542297375 | validation: 0.06108505520106203]
	TIME [epoch: 0.924 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04902323202075458		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.04902323202075458 | validation: 0.06399322692924385]
	TIME [epoch: 0.924 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05393265748299574		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.05393265748299574 | validation: 0.0798642744254]
	TIME [epoch: 0.924 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690423587070841		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.06690423587070841 | validation: 0.09967871805522299]
	TIME [epoch: 0.924 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861870348277851		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.0861870348277851 | validation: 0.1153831932830483]
	TIME [epoch: 0.924 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08740708100499033		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.08740708100499033 | validation: 0.08439935408183746]
	TIME [epoch: 0.924 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07744987650676358		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.07744987650676358 | validation: 0.08091821858831896]
	TIME [epoch: 0.924 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06709988999823907		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.06709988999823907 | validation: 0.06703677758055315]
	TIME [epoch: 0.925 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829220720975183		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.05829220720975183 | validation: 0.05660231320820322]
	TIME [epoch: 0.924 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058809309432786415		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.058809309432786415 | validation: 0.0685552109587232]
	TIME [epoch: 0.925 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052472715707593		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.06052472715707593 | validation: 0.051271960161156205]
	TIME [epoch: 0.924 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06287356346339346		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.06287356346339346 | validation: 0.0747724618911599]
	TIME [epoch: 0.924 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06367470433379376		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.06367470433379376 | validation: 0.07161415861742092]
	TIME [epoch: 0.925 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07747269066653614		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.07747269066653614 | validation: 0.08675661871295826]
	TIME [epoch: 0.925 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807519176285746		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.06807519176285746 | validation: 0.05665906969455822]
	TIME [epoch: 0.926 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05110878050130274		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.05110878050130274 | validation: 0.05867485117360475]
	TIME [epoch: 0.925 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050547594509322676		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.050547594509322676 | validation: 0.06529571830055607]
	TIME [epoch: 0.929 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05201171305886355		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.05201171305886355 | validation: 0.10959454565915745]
	TIME [epoch: 0.924 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710149941854602		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.0710149941854602 | validation: 0.12078989457718309]
	TIME [epoch: 0.924 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11403440051169467		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.11403440051169467 | validation: 0.13015559260856274]
	TIME [epoch: 0.924 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09893354494853221		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.09893354494853221 | validation: 0.06696087787084849]
	TIME [epoch: 0.924 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625656801828664		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.0625656801828664 | validation: 0.046562127704148526]
	TIME [epoch: 0.925 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972299634859062		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.04972299634859062 | validation: 0.07054950058963298]
	TIME [epoch: 0.924 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062016671811640854		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.062016671811640854 | validation: 0.043045338406246074]
	TIME [epoch: 0.925 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08520646030620156		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08520646030620156 | validation: 0.08506946350504832]
	TIME [epoch: 0.924 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06780986626126204		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.06780986626126204 | validation: 0.0495551549270356]
	TIME [epoch: 0.925 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05052918172690985		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.05052918172690985 | validation: 0.0634399177376966]
	TIME [epoch: 0.924 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046395522372438165		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.046395522372438165 | validation: 0.056203170388940654]
	TIME [epoch: 0.924 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05350159437412918		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.05350159437412918 | validation: 0.08688307474949286]
	TIME [epoch: 0.925 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871586284935557		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.06871586284935557 | validation: 0.08974847577514082]
	TIME [epoch: 0.923 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09143285684694707		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.09143285684694707 | validation: 0.09962670627702626]
	TIME [epoch: 0.924 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07666810933961328		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.07666810933961328 | validation: 0.06260527669561008]
	TIME [epoch: 0.934 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055569832912892865		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.055569832912892865 | validation: 0.05616944177504454]
	TIME [epoch: 0.924 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044482682572328916		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.044482682572328916 | validation: 0.04468075195910734]
	TIME [epoch: 0.924 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042019678044045695		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.042019678044045695 | validation: 0.04659893274660686]
	TIME [epoch: 0.925 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04466170572093777		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.04466170572093777 | validation: 0.05281891019782924]
	TIME [epoch: 0.924 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219939447209886		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.05219939447209886 | validation: 0.05879911649438013]
	TIME [epoch: 0.925 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05631580339931164		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.05631580339931164 | validation: 0.062495384911538845]
	TIME [epoch: 0.924 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06376702640380442		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.06376702640380442 | validation: 0.05902275710266363]
	TIME [epoch: 0.924 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658644496765241		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.0658644496765241 | validation: 0.06279269439744128]
	TIME [epoch: 0.924 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06315196122241723		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.06315196122241723 | validation: 0.082729630581657]
	TIME [epoch: 0.924 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06552017619759067		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.06552017619759067 | validation: 0.10159464562214227]
	TIME [epoch: 0.924 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712305207370619		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.07712305207370619 | validation: 0.11039232484314546]
	TIME [epoch: 0.924 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777969662402308		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.07777969662402308 | validation: 0.05201422314499299]
	TIME [epoch: 0.924 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407132254352352		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.06407132254352352 | validation: 0.07055931889087051]
	TIME [epoch: 0.924 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053595744284678226		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.053595744284678226 | validation: 0.04438554946260335]
	TIME [epoch: 0.924 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757721240227065		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.04757721240227065 | validation: 0.053879207754302154]
	TIME [epoch: 0.924 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045854479129101886		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.045854479129101886 | validation: 0.04093988935135361]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047087712666695085		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.047087712666695085 | validation: 0.0659578492491615]
	TIME [epoch: 0.93 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04469586625502366		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.04469586625502366 | validation: 0.04180319280087119]
	TIME [epoch: 0.924 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04769897892691935		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.04769897892691935 | validation: 0.056633725818575564]
	TIME [epoch: 0.924 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537936000806713		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.04537936000806713 | validation: 0.04530016039015731]
	TIME [epoch: 0.925 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054717447221203676		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.054717447221203676 | validation: 0.057073887127283666]
	TIME [epoch: 139 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04811198937537954		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.04811198937537954 | validation: 0.052896008721116085]
	TIME [epoch: 1.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04521086938504361		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.04521086938504361 | validation: 0.06408250191213882]
	TIME [epoch: 1.82 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05406736834108252		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.05406736834108252 | validation: 0.10011973043505513]
	TIME [epoch: 1.82 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777911690093841		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.0777911690093841 | validation: 0.09765320822372679]
	TIME [epoch: 1.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09476111616488733		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.09476111616488733 | validation: 0.08936119424924992]
	TIME [epoch: 1.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185972878325898		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.07185972878325898 | validation: 0.05223155843597171]
	TIME [epoch: 1.82 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048322976559320295		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.048322976559320295 | validation: 0.03845697850793579]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813422484644478		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.03813422484644478 | validation: 0.04675875241885248]
	TIME [epoch: 1.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03946397168475908		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.03946397168475908 | validation: 0.03939605684421863]
	TIME [epoch: 1.82 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037296074264940575		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.037296074264940575 | validation: 0.05611533840318215]
	TIME [epoch: 1.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044402358471689996		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.044402358471689996 | validation: 0.04396827494076472]
	TIME [epoch: 1.82 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051002341918588566		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.051002341918588566 | validation: 0.08557858658832508]
	TIME [epoch: 1.82 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05904172995705396		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.05904172995705396 | validation: 0.05268562025403966]
	TIME [epoch: 1.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06183868930529661		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.06183868930529661 | validation: 0.08677645769218804]
	TIME [epoch: 1.83 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658197597642072		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.05658197597642072 | validation: 0.04956706518804676]
	TIME [epoch: 1.83 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062135234124515056		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.062135234124515056 | validation: 0.071761768861376]
	TIME [epoch: 1.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047757344610910454		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.047757344610910454 | validation: 0.05753378714980241]
	TIME [epoch: 1.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050050265140009306		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.050050265140009306 | validation: 0.05766697218224193]
	TIME [epoch: 1.82 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05600037190898302		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.05600037190898302 | validation: 0.054492691568128915]
	TIME [epoch: 1.82 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05906520914628569		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.05906520914628569 | validation: 0.05985471511949843]
	TIME [epoch: 1.83 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053711945161722285		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.053711945161722285 | validation: 0.054667733750196415]
	TIME [epoch: 1.82 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905569230796116		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.04905569230796116 | validation: 0.06632495627769183]
	TIME [epoch: 1.82 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047323993037291956		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.047323993037291956 | validation: 0.0469377291292895]
	TIME [epoch: 1.83 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527097185852501		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.04527097185852501 | validation: 0.05804502730403736]
	TIME [epoch: 1.82 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043327160756127726		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.043327160756127726 | validation: 0.036342752269497625]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007918036236188		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.04007918036236188 | validation: 0.063813548320225]
	TIME [epoch: 1.82 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127627371803393		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.04127627371803393 | validation: 0.043601392239887404]
	TIME [epoch: 1.83 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048605842323495824		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.048605842323495824 | validation: 0.09321078259619725]
	TIME [epoch: 1.82 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054999686229352814		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.054999686229352814 | validation: 0.05633439310623489]
	TIME [epoch: 1.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772738189555689		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.05772738189555689 | validation: 0.0703577002535657]
	TIME [epoch: 1.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04869338553333475		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.04869338553333475 | validation: 0.04951634391348593]
	TIME [epoch: 1.83 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042410093669009274		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.042410093669009274 | validation: 0.06060607207295259]
	TIME [epoch: 1.82 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422851129036566		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.0422851129036566 | validation: 0.051259212161314585]
	TIME [epoch: 1.82 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04683123545101647		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.04683123545101647 | validation: 0.05392418166283223]
	TIME [epoch: 1.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06273320294171698		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.06273320294171698 | validation: 0.05178759929435359]
	TIME [epoch: 1.82 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059986641845432176		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.059986641845432176 | validation: 0.04891353878926276]
	TIME [epoch: 1.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04208697274424259		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.04208697274424259 | validation: 0.05188751790425383]
	TIME [epoch: 1.82 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041678796758241744		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.041678796758241744 | validation: 0.06746577909225791]
	TIME [epoch: 1.82 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04718061773690483		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.04718061773690483 | validation: 0.0597727224753504]
	TIME [epoch: 1.82 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397644649932305		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.05397644649932305 | validation: 0.054854496048361026]
	TIME [epoch: 1.82 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152291518973114		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.04152291518973114 | validation: 0.04328977820022996]
	TIME [epoch: 1.82 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227885850985724		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.03227885850985724 | validation: 0.028289285488147645]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036226374652609764		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.036226374652609764 | validation: 0.057503779092455846]
	TIME [epoch: 1.83 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04386083647861213		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04386083647861213 | validation: 0.04100867525837734]
	TIME [epoch: 1.83 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05508823081765675		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.05508823081765675 | validation: 0.0842547718316568]
	TIME [epoch: 1.82 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04964353281558982		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.04964353281558982 | validation: 0.06032409005054184]
	TIME [epoch: 1.82 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04948576981214291		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.04948576981214291 | validation: 0.08425314729458082]
	TIME [epoch: 1.82 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05454925413462146		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.05454925413462146 | validation: 0.06704919051846839]
	TIME [epoch: 1.82 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815154150658086		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.05815154150658086 | validation: 0.06500859595280238]
	TIME [epoch: 1.82 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512673369016224		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.05512673369016224 | validation: 0.05045565074302591]
	TIME [epoch: 1.82 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047739053698841065		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.047739053698841065 | validation: 0.04423826729365771]
	TIME [epoch: 1.82 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115389860945522		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.04115389860945522 | validation: 0.038305889636164464]
	TIME [epoch: 1.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568220145270185		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.03568220145270185 | validation: 0.04411191865302169]
	TIME [epoch: 1.82 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404380382577507		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.03404380382577507 | validation: 0.03338499730169688]
	TIME [epoch: 1.82 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03714695598825957		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.03714695598825957 | validation: 0.05248288388581893]
	TIME [epoch: 1.82 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577692032720473		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.03577692032720473 | validation: 0.03308128129043553]
	TIME [epoch: 1.82 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913016761402131		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.03913016761402131 | validation: 0.06264293348974549]
	TIME [epoch: 1.82 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042872585675635726		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.042872585675635726 | validation: 0.04533218990025755]
	TIME [epoch: 1.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739289209739123		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.04739289209739123 | validation: 0.0726096396005994]
	TIME [epoch: 1.82 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047368832777393254		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.047368832777393254 | validation: 0.05549787009051778]
	TIME [epoch: 1.82 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692943372736057		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.04692943372736057 | validation: 0.07607734249866606]
	TIME [epoch: 1.82 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04705059442121656		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.04705059442121656 | validation: 0.04336105522305152]
	TIME [epoch: 1.82 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327774975730757		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.04327774975730757 | validation: 0.05526910266023963]
	TIME [epoch: 1.82 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04716127810676448		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.04716127810676448 | validation: 0.042127150371377445]
	TIME [epoch: 1.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048124195013509306		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.048124195013509306 | validation: 0.05505788809852043]
	TIME [epoch: 1.82 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04055800044880127		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.04055800044880127 | validation: 0.041963162568114026]
	TIME [epoch: 1.82 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035095567385542356		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.035095567385542356 | validation: 0.04760146313689582]
	TIME [epoch: 1.82 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03569324877393408		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.03569324877393408 | validation: 0.0424173321109648]
	TIME [epoch: 1.82 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03910831477280747		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.03910831477280747 | validation: 0.05620220161228856]
	TIME [epoch: 1.83 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0396131493548347		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.0396131493548347 | validation: 0.04621230263314503]
	TIME [epoch: 1.82 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042897839604465		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.042897839604465 | validation: 0.06077138719908201]
	TIME [epoch: 1.82 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424164776521146		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.0424164776521146 | validation: 0.05261373978101126]
	TIME [epoch: 1.82 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04130789282839511		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.04130789282839511 | validation: 0.05776182760960105]
	TIME [epoch: 1.82 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041089312471979736		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.041089312471979736 | validation: 0.04332340853723047]
	TIME [epoch: 1.82 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03845587138986053		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.03845587138986053 | validation: 0.044198958334355845]
	TIME [epoch: 1.82 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037955394498088		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.04037955394498088 | validation: 0.04680271802881484]
	TIME [epoch: 1.82 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597483625270986		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.04597483625270986 | validation: 0.03939792511540407]
	TIME [epoch: 1.82 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434447414695214		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.04434447414695214 | validation: 0.04532387663102053]
	TIME [epoch: 1.82 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03988911624970871		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.03988911624970871 | validation: 0.059933162051234]
	TIME [epoch: 1.82 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037172296415993		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.037172296415993 | validation: 0.03165208221796132]
	TIME [epoch: 1.82 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040313208748504914		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.040313208748504914 | validation: 0.05206284179267125]
	TIME [epoch: 1.82 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622103351801582		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.03622103351801582 | validation: 0.03340449161806567]
	TIME [epoch: 1.82 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036817285054393835		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.036817285054393835 | validation: 0.06391142127642856]
	TIME [epoch: 1.82 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852010755139853		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.03852010755139853 | validation: 0.04578994682605811]
	TIME [epoch: 1.82 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042921143707447366		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.042921143707447366 | validation: 0.07764534407536453]
	TIME [epoch: 1.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047366458133089456		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.047366458133089456 | validation: 0.05544482548674539]
	TIME [epoch: 1.82 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043105537574427755		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.043105537574427755 | validation: 0.04803664721459999]
	TIME [epoch: 1.82 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649863716245719		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.03649863716245719 | validation: 0.05107549857834834]
	TIME [epoch: 1.82 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358429353674613		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.0358429353674613 | validation: 0.02644735315050899]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776827081398332		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.03776827081398332 | validation: 0.05069235216238534]
	TIME [epoch: 1.82 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0341621200043233		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.0341621200043233 | validation: 0.03858925154032098]
	TIME [epoch: 1.82 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298398651321492		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.03298398651321492 | validation: 0.043080798788620644]
	TIME [epoch: 1.82 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04487970890190627		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.04487970890190627 | validation: 0.03343943258693382]
	TIME [epoch: 1.82 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031854646068963086		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.031854646068963086 | validation: 0.04600982549216584]
	TIME [epoch: 1.83 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03457285466240344		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.03457285466240344 | validation: 0.039438836915269684]
	TIME [epoch: 1.82 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04199240583053765		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.04199240583053765 | validation: 0.06559171204089552]
	TIME [epoch: 1.82 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046963322892091916		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.046963322892091916 | validation: 0.07030781137950197]
	TIME [epoch: 1.82 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050171834409728645		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.050171834409728645 | validation: 0.05581089366190944]
	TIME [epoch: 1.82 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04365587478748454		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.04365587478748454 | validation: 0.038446061048702344]
	TIME [epoch: 1.82 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03206463064951124		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.03206463064951124 | validation: 0.037109444127285354]
	TIME [epoch: 1.82 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028801774841121057		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.028801774841121057 | validation: 0.03416670932952703]
	TIME [epoch: 1.83 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031325281254798915		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.031325281254798915 | validation: 0.03918829791977074]
	TIME [epoch: 1.82 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031649245259117116		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.031649245259117116 | validation: 0.03484223745656204]
	TIME [epoch: 1.82 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03483375037099148		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.03483375037099148 | validation: 0.05589043536258729]
	TIME [epoch: 1.82 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03412599278095928		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.03412599278095928 | validation: 0.031574969379333316]
	TIME [epoch: 1.82 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03475198389903337		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.03475198389903337 | validation: 0.05154818482827786]
	TIME [epoch: 1.82 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03646144694058915		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.03646144694058915 | validation: 0.04429177506395481]
	TIME [epoch: 1.82 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04173943831549277		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.04173943831549277 | validation: 0.06828634246600135]
	TIME [epoch: 1.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126594633817865		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.04126594633817865 | validation: 0.03258573562195053]
	TIME [epoch: 1.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551895846730166		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.03551895846730166 | validation: 0.05675730921586793]
	TIME [epoch: 1.82 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133660600683543		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.03133660600683543 | validation: 0.033376160719283364]
	TIME [epoch: 1.82 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02731758104207823		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.02731758104207823 | validation: 0.03878482814609116]
	TIME [epoch: 1.82 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027622213453977498		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.027622213453977498 | validation: 0.03064779738452893]
	TIME [epoch: 1.82 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02829977122584289		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.02829977122584289 | validation: 0.04424496881882555]
	TIME [epoch: 1.83 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029185943733884556		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.029185943733884556 | validation: 0.03278073680611565]
	TIME [epoch: 1.82 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03078268005811223		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.03078268005811223 | validation: 0.054153163408335515]
	TIME [epoch: 1.82 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03303791791454062		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.03303791791454062 | validation: 0.05620647186716807]
	TIME [epoch: 1.83 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837330438773692		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.04837330438773692 | validation: 0.08760822798776575]
	TIME [epoch: 1.83 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754457738821731		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.06754457738821731 | validation: 0.22724964384358326]
	TIME [epoch: 1.83 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257335573007501		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.257335573007501 | validation: 0.22497149719173884]
	TIME [epoch: 1.83 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1817804213692486		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1817804213692486 | validation: 0.12656673709180474]
	TIME [epoch: 1.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058807773328952		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.09058807773328952 | validation: 0.06727322892062197]
	TIME [epoch: 1.82 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823537862266154		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.05823537862266154 | validation: 0.038150080846612956]
	TIME [epoch: 1.83 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04748419194965218		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.04748419194965218 | validation: 0.040632819102274426]
	TIME [epoch: 1.83 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604260680227088		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.04604260680227088 | validation: 0.03833670234138314]
	TIME [epoch: 1.83 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03788808930148536		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.03788808930148536 | validation: 0.040883432423449544]
	TIME [epoch: 1.82 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644962230146336		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.03644962230146336 | validation: 0.0413769752099603]
	TIME [epoch: 1.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03368341871021094		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.03368341871021094 | validation: 0.03789833602985389]
	TIME [epoch: 1.82 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359092299079925		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.03359092299079925 | validation: 0.037104567643676]
	TIME [epoch: 1.83 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031051908965787023		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.031051908965787023 | validation: 0.03605427486381242]
	TIME [epoch: 1.82 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852488613868089		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.02852488613868089 | validation: 0.030846382345541858]
	TIME [epoch: 1.82 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029730633206565625		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.029730633206565625 | validation: 0.03367389081548291]
	TIME [epoch: 1.82 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029496133233651696		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.029496133233651696 | validation: 0.03462957633101856]
	TIME [epoch: 1.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029454173551756186		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.029454173551756186 | validation: 0.03302092693351281]
	TIME [epoch: 1.82 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030949242592921797		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.030949242592921797 | validation: 0.03494696316592375]
	TIME [epoch: 1.83 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029936487243964582		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.029936487243964582 | validation: 0.037530479155150454]
	TIME [epoch: 1.83 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029383991490726968		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.029383991490726968 | validation: 0.0306125951416934]
	TIME [epoch: 1.82 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029324927727205467		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.029324927727205467 | validation: 0.03211997816700264]
	TIME [epoch: 1.83 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026967633520822227		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.026967633520822227 | validation: 0.03169823262539741]
	TIME [epoch: 1.82 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027618838832134254		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.027618838832134254 | validation: 0.02914417611522905]
	TIME [epoch: 1.82 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025661661487192466		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.025661661487192466 | validation: 0.030460873401584855]
	TIME [epoch: 1.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028366317101947987		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.028366317101947987 | validation: 0.04168450792320597]
	TIME [epoch: 1.82 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180400533536757		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.03180400533536757 | validation: 0.04205285832262939]
	TIME [epoch: 1.83 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424213802754956		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.04424213802754956 | validation: 0.06218834206088391]
	TIME [epoch: 1.83 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04790679114334186		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.04790679114334186 | validation: 0.05282748834715983]
	TIME [epoch: 1.82 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054386730155996		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.04054386730155996 | validation: 0.04140023841926164]
	TIME [epoch: 1.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03521095354424192		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.03521095354424192 | validation: 0.048606379291324164]
	TIME [epoch: 1.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363592607748961		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.03363592607748961 | validation: 0.031866964645152894]
	TIME [epoch: 1.83 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028953900856638803		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.028953900856638803 | validation: 0.04305473777436332]
	TIME [epoch: 1.83 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025353594965952252		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.025353594965952252 | validation: 0.026131166582076537]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062519754446628		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.04062519754446628 | validation: 0.03096848279553408]
	TIME [epoch: 1.83 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027351416695784864		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.027351416695784864 | validation: 0.03799618642341838]
	TIME [epoch: 1.83 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03054761233953445		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.03054761233953445 | validation: 0.03249567108642463]
	TIME [epoch: 1.83 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026559832537798884		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.026559832537798884 | validation: 0.039274158574300944]
	TIME [epoch: 1.83 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02506087077098216		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.02506087077098216 | validation: 0.03300151908855179]
	TIME [epoch: 1.83 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026582622419950033		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.026582622419950033 | validation: 0.04405078991813475]
	TIME [epoch: 1.83 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027705940531161054		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.027705940531161054 | validation: 0.0316227616385405]
	TIME [epoch: 1.83 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029665699780103427		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.029665699780103427 | validation: 0.05416422105884158]
	TIME [epoch: 1.83 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860326629433907		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.03860326629433907 | validation: 0.033723534989803204]
	TIME [epoch: 1.82 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401641377948146		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.03401641377948146 | validation: 0.05076050540869401]
	TIME [epoch: 1.83 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035307593027944145		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.035307593027944145 | validation: 0.04444329936769688]
	TIME [epoch: 1.82 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354551166745113		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.0354551166745113 | validation: 0.059317244277479864]
	TIME [epoch: 1.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039745317408145006		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.039745317408145006 | validation: 0.06961628501857181]
	TIME [epoch: 1.82 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380478081471963		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.04380478081471963 | validation: 0.034149129064884744]
	TIME [epoch: 1.83 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039274730820084376		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.039274730820084376 | validation: 0.03527938680873343]
	TIME [epoch: 1.82 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647937463209128		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.02647937463209128 | validation: 0.03191411662883136]
	TIME [epoch: 1.83 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023343998110090816		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.023343998110090816 | validation: 0.030387293745576218]
	TIME [epoch: 1.82 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023791860729698334		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.023791860729698334 | validation: 0.03856704001708708]
	TIME [epoch: 1.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02597338239389635		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.02597338239389635 | validation: 0.03152380163207399]
	TIME [epoch: 1.82 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02477359024137801		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.02477359024137801 | validation: 0.04033785267889949]
	TIME [epoch: 1.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025863546911729558		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.025863546911729558 | validation: 0.038613700378225646]
	TIME [epoch: 1.82 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027001492757113985		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.027001492757113985 | validation: 0.04290985287236895]
	TIME [epoch: 1.83 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029476663173827813		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.029476663173827813 | validation: 0.05186383329807444]
	TIME [epoch: 1.82 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159895173135284		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.03159895173135284 | validation: 0.03836759949817797]
	TIME [epoch: 1.83 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02775476298468964		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.02775476298468964 | validation: 0.03223824908211472]
	TIME [epoch: 1.83 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028026362030344636		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.028026362030344636 | validation: 0.04779000894659428]
	TIME [epoch: 1.82 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032939925603132345		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.032939925603132345 | validation: 0.03922978798960183]
	TIME [epoch: 1.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041563465800025456		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.041563465800025456 | validation: 0.04858114320972199]
	TIME [epoch: 1.82 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03709412261747343		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.03709412261747343 | validation: 0.04912957357113476]
	TIME [epoch: 1.82 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030152738347562588		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.030152738347562588 | validation: 0.02757359012196884]
	TIME [epoch: 1.82 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02656341384928396		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.02656341384928396 | validation: 0.03261361236557403]
	TIME [epoch: 1.82 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0256990709503759		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.0256990709503759 | validation: 0.033307600943580165]
	TIME [epoch: 1.83 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02345046667881347		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.02345046667881347 | validation: 0.039071621829179795]
	TIME [epoch: 1.87 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02360229406292005		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.02360229406292005 | validation: 0.02965646584085674]
	TIME [epoch: 1.82 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026681494651804707		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.026681494651804707 | validation: 0.046330868810437846]
	TIME [epoch: 1.83 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031222968919523184		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.031222968919523184 | validation: 0.03937629215733669]
	TIME [epoch: 1.82 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03425528466884146		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.03425528466884146 | validation: 0.05488190812125093]
	TIME [epoch: 1.82 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03196040113953636		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.03196040113953636 | validation: 0.024511322371421562]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027862046392083038		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.027862046392083038 | validation: 0.03873626618421129]
	TIME [epoch: 1.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02389452225940796		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.02389452225940796 | validation: 0.031392773981582016]
	TIME [epoch: 1.83 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023723443558635864		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.023723443558635864 | validation: 0.04383867665367015]
	TIME [epoch: 1.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028199355547326742		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.028199355547326742 | validation: 0.03797851324183733]
	TIME [epoch: 1.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028505347701393555		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.028505347701393555 | validation: 0.03983461179021419]
	TIME [epoch: 1.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027439658099503026		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.027439658099503026 | validation: 0.040359593703889585]
	TIME [epoch: 1.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027509361956894407		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.027509361956894407 | validation: 0.039776320059168695]
	TIME [epoch: 1.83 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030794961756914218		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.030794961756914218 | validation: 0.04805802390765625]
	TIME [epoch: 1.82 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030201272906518348		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.030201272906518348 | validation: 0.03937910734721568]
	TIME [epoch: 1.83 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033563524070656214		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.033563524070656214 | validation: 0.054184747469101305]
	TIME [epoch: 1.83 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030112554621443115		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.030112554621443115 | validation: 0.025261201928063304]
	TIME [epoch: 1.83 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028438689159651406		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.028438689159651406 | validation: 0.039356545206680506]
	TIME [epoch: 1.82 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520372588742844		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.02520372588742844 | validation: 0.02888962214216714]
	TIME [epoch: 1.82 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02275477874437288		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.02275477874437288 | validation: 0.036219557677152125]
	TIME [epoch: 1.82 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024826660500280005		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.024826660500280005 | validation: 0.03334079974873105]
	TIME [epoch: 1.82 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02300123360286172		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.02300123360286172 | validation: 0.040671034866677364]
	TIME [epoch: 1.82 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025098231475567268		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.025098231475567268 | validation: 0.030169723518606775]
	TIME [epoch: 1.82 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026021870731063636		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.026021870731063636 | validation: 0.04619246582401332]
	TIME [epoch: 1.82 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028536370574197417		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.028536370574197417 | validation: 0.028597347740234147]
	TIME [epoch: 1.82 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02968857268815589		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.02968857268815589 | validation: 0.04699571774296079]
	TIME [epoch: 1.82 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635828622948434		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.02635828622948434 | validation: 0.03264696067175454]
	TIME [epoch: 1.82 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02380848591703582		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.02380848591703582 | validation: 0.039850183995319644]
	TIME [epoch: 1.82 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023924394945634697		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.023924394945634697 | validation: 0.038504045501943585]
	TIME [epoch: 1.82 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025812246354988105		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.025812246354988105 | validation: 0.03678187324979424]
	TIME [epoch: 1.82 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0318915550608972		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.0318915550608972 | validation: 0.0770508057652555]
	TIME [epoch: 1.82 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046604105102515954		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.046604105102515954 | validation: 0.04287861981181539]
	TIME [epoch: 1.83 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03492072179577966		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.03492072179577966 | validation: 0.03304509211917045]
	TIME [epoch: 1.82 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026102691742163453		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.026102691742163453 | validation: 0.03909016342581734]
	TIME [epoch: 1.82 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025565500755668834		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.025565500755668834 | validation: 0.031323934813834435]
	TIME [epoch: 1.82 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024118323689057117		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.024118323689057117 | validation: 0.0341218268373703]
	TIME [epoch: 1.82 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02347679088834248		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.02347679088834248 | validation: 0.04467219309695362]
	TIME [epoch: 1.82 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159116426168634		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.03159116426168634 | validation: 0.03445169119455171]
	TIME [epoch: 1.82 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022948265700194866		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.022948265700194866 | validation: 0.03290712784641854]
	TIME [epoch: 1.82 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02433516466706802		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.02433516466706802 | validation: 0.029117772031955303]
	TIME [epoch: 1.82 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0246030100696627		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.0246030100696627 | validation: 0.03597033879485079]
	TIME [epoch: 1.82 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022046016920107436		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.022046016920107436 | validation: 0.026059325621877308]
	TIME [epoch: 1.82 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026018948995915718		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.026018948995915718 | validation: 0.04702398323819232]
	TIME [epoch: 1.82 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028085770529779612		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.028085770529779612 | validation: 0.02867483506761225]
	TIME [epoch: 1.83 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028470011991774662		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.028470011991774662 | validation: 0.052322842567054766]
	TIME [epoch: 1.82 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02659505920417244		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.02659505920417244 | validation: 0.033645914152189316]
	TIME [epoch: 1.83 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02803993875648146		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.02803993875648146 | validation: 0.051404903266537544]
	TIME [epoch: 1.82 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031066931646092925		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.031066931646092925 | validation: 0.04155996434751239]
	TIME [epoch: 1.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029809671269286225		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.029809671269286225 | validation: 0.03683046301533055]
	TIME [epoch: 1.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025685175164310375		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.025685175164310375 | validation: 0.03472969587692609]
	TIME [epoch: 1.83 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026488786024855576		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.026488786024855576 | validation: 0.03589453133167647]
	TIME [epoch: 1.82 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025971340586488197		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.025971340586488197 | validation: 0.03152182970959399]
	TIME [epoch: 1.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021866166778191802		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.021866166778191802 | validation: 0.04215400846023818]
	TIME [epoch: 1.82 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021920324891222708		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.021920324891222708 | validation: 0.028566706197077033]
	TIME [epoch: 1.83 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024100124395403794		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.024100124395403794 | validation: 0.03965595004551187]
	TIME [epoch: 1.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025158983010310134		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.025158983010310134 | validation: 0.03775552800824247]
	TIME [epoch: 1.83 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025758909069627977		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.025758909069627977 | validation: 0.04199768404475671]
	TIME [epoch: 1.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025179529567748195		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.025179529567748195 | validation: 0.04251412403010686]
	TIME [epoch: 1.83 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024704186515442844		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.024704186515442844 | validation: 0.03527362800289107]
	TIME [epoch: 1.83 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027278117893487747		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.027278117893487747 | validation: 0.032854172049606056]
	TIME [epoch: 1.83 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028501410544596217		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.028501410544596217 | validation: 0.04873286182358688]
	TIME [epoch: 1.82 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029139382424540682		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.029139382424540682 | validation: 0.03442121475652562]
	TIME [epoch: 1.83 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028742656849351508		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.028742656849351508 | validation: 0.046591124739221404]
	TIME [epoch: 1.83 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027934574451966978		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.027934574451966978 | validation: 0.02468340849590547]
	TIME [epoch: 1.83 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02497395195023163		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.02497395195023163 | validation: 0.04362925651866496]
	TIME [epoch: 1.83 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02336238875847377		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.02336238875847377 | validation: 0.03666199026284466]
	TIME [epoch: 1.83 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02013707835235964		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.02013707835235964 | validation: 0.03290844808641026]
	TIME [epoch: 1.82 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023641683356256955		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.023641683356256955 | validation: 0.038735422434052515]
	TIME [epoch: 1.86 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021559996521453212		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.021559996521453212 | validation: 0.03308632636411226]
	TIME [epoch: 1.83 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021596376202751552		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.021596376202751552 | validation: 0.040267162525879266]
	TIME [epoch: 1.83 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02214933216401793		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.02214933216401793 | validation: 0.034547259422719405]
	TIME [epoch: 1.83 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02480402293041224		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.02480402293041224 | validation: 0.050819979388310316]
	TIME [epoch: 1.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029043778252676483		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.029043778252676483 | validation: 0.02411515519714257]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02935175830107777		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.02935175830107777 | validation: 0.04790256735646975]
	TIME [epoch: 1.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02788125738408856		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.02788125738408856 | validation: 0.04813798598645502]
	TIME [epoch: 1.83 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03188325064668886		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.03188325064668886 | validation: 0.04367312334772521]
	TIME [epoch: 1.83 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028885179408712505		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.028885179408712505 | validation: 0.036817550401954835]
	TIME [epoch: 1.83 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02523169058370016		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.02523169058370016 | validation: 0.025591397953286223]
	TIME [epoch: 1.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021686489292519217		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.021686489292519217 | validation: 0.03887005804555236]
	TIME [epoch: 1.83 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020373760868174714		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.020373760868174714 | validation: 0.032711292705497996]
	TIME [epoch: 1.83 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021364046568864887		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.021364046568864887 | validation: 0.03204009012301078]
	TIME [epoch: 1.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02115522209346673		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.02115522209346673 | validation: 0.028255775676467223]
	TIME [epoch: 1.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02087641892348567		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.02087641892348567 | validation: 0.03145098773317395]
	TIME [epoch: 1.83 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02174255585937482		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.02174255585937482 | validation: 0.03589388692605039]
	TIME [epoch: 1.83 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021399394338809153		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.021399394338809153 | validation: 0.04286498698681285]
	TIME [epoch: 1.83 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0335961355293788		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.0335961355293788 | validation: 0.0351464413721548]
	TIME [epoch: 1.83 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025276141990068394		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.025276141990068394 | validation: 0.043642632587178015]
	TIME [epoch: 1.83 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028098447069582172		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.028098447069582172 | validation: 0.026021989131219947]
	TIME [epoch: 1.83 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0306197187365638		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.0306197187365638 | validation: 0.038324995528277186]
	TIME [epoch: 1.83 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02375546559162024		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.02375546559162024 | validation: 0.043054901162235995]
	TIME [epoch: 1.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02508666299116875		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.02508666299116875 | validation: 0.030634190910833782]
	TIME [epoch: 1.83 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02361161887743216		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.02361161887743216 | validation: 0.04457373576325551]
	TIME [epoch: 1.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02363854570850017		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.02363854570850017 | validation: 0.02860359076777279]
	TIME [epoch: 1.83 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02363458491028947		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.02363458491028947 | validation: 0.037885792896675846]
	TIME [epoch: 1.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023073734858225132		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.023073734858225132 | validation: 0.030945036424142737]
	TIME [epoch: 1.83 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021623933968074747		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.021623933968074747 | validation: 0.04787357440743262]
	TIME [epoch: 1.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037990303113432		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.037990303113432 | validation: 0.03685249230519803]
	TIME [epoch: 1.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028034336598043285		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.028034336598043285 | validation: 0.03942892641702877]
	TIME [epoch: 1.83 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02525778529879232		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.02525778529879232 | validation: 0.037233937518891626]
	TIME [epoch: 1.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02320204700919159		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.02320204700919159 | validation: 0.03613729959240412]
	TIME [epoch: 1.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0235118663776772		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.0235118663776772 | validation: 0.03711430020379044]
	TIME [epoch: 1.83 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02092455387209423		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.02092455387209423 | validation: 0.02966698007157803]
	TIME [epoch: 1.83 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02346280040714671		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.02346280040714671 | validation: 0.041776756100010415]
	TIME [epoch: 1.83 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02250124560287662		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.02250124560287662 | validation: 0.035090941017968794]
	TIME [epoch: 1.83 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02347730773580855		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.02347730773580855 | validation: 0.04074810929068249]
	TIME [epoch: 1.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023592306379414733		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.023592306379414733 | validation: 0.0400182898714733]
	TIME [epoch: 1.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023797425624811023		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.023797425624811023 | validation: 0.038610281052065]
	TIME [epoch: 1.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024111145685312155		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.024111145685312155 | validation: 0.033877461230992646]
	TIME [epoch: 1.83 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022777774694201697		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.022777774694201697 | validation: 0.038993981451126064]
	TIME [epoch: 1.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0202455165402235		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.0202455165402235 | validation: 0.027162285531341382]
	TIME [epoch: 1.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020703570225640823		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.020703570225640823 | validation: 0.04709337062055296]
	TIME [epoch: 1.83 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520363944494839		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.02520363944494839 | validation: 0.030424691205177823]
	TIME [epoch: 1.83 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02355332219358889		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.02355332219358889 | validation: 0.045199603161552]
	TIME [epoch: 1.83 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0247506735642898		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.0247506735642898 | validation: 0.03279843324319537]
	TIME [epoch: 1.83 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024704598738406018		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.024704598738406018 | validation: 0.043214967898651924]
	TIME [epoch: 1.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022040611385795588		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.022040611385795588 | validation: 0.04199794139372796]
	TIME [epoch: 1.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029843402169903537		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.029843402169903537 | validation: 0.032803703708061904]
	TIME [epoch: 1.83 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022655584233927845		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.022655584233927845 | validation: 0.03286840366907059]
	TIME [epoch: 1.83 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02167122558832562		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.02167122558832562 | validation: 0.034375128924401636]
	TIME [epoch: 1.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020663765866889977		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.020663765866889977 | validation: 0.03170175928656577]
	TIME [epoch: 1.83 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021287884675575623		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.021287884675575623 | validation: 0.03229332440323408]
	TIME [epoch: 1.83 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027012312102079817		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.027012312102079817 | validation: 0.0288709406352916]
	TIME [epoch: 1.83 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02194204128802966		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.02194204128802966 | validation: 0.04292815925919247]
	TIME [epoch: 1.83 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026160696591737444		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.026160696591737444 | validation: 0.02982496961410699]
	TIME [epoch: 1.83 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02409034772083114		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.02409034772083114 | validation: 0.03462308934190378]
	TIME [epoch: 1.83 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020404567452801527		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.020404567452801527 | validation: 0.030835115391200863]
	TIME [epoch: 1.83 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019780380018270222		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.019780380018270222 | validation: 0.026807136308289715]
	TIME [epoch: 1.83 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021391806099303325		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.021391806099303325 | validation: 0.028403946548584393]
	TIME [epoch: 1.83 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020268335086548336		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.020268335086548336 | validation: 0.03172310235703577]
	TIME [epoch: 1.83 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0214358286328572		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.0214358286328572 | validation: 0.033347688031303335]
	TIME [epoch: 1.83 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020534762784344997		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.020534762784344997 | validation: 0.029768469243018004]
	TIME [epoch: 1.83 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021209926462584204		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.021209926462584204 | validation: 0.03614504208683524]
	TIME [epoch: 1.83 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02252691517375471		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.02252691517375471 | validation: 0.03296396936003644]
	TIME [epoch: 1.83 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024776800105492023		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.024776800105492023 | validation: 0.059235290212076686]
	TIME [epoch: 1.83 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02973233604891581		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.02973233604891581 | validation: 0.03404087490395551]
	TIME [epoch: 1.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027665464387886435		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.027665464387886435 | validation: 0.034149922422367085]
	TIME [epoch: 1.85 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02176273679034668		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.02176273679034668 | validation: 0.042194247056090706]
	TIME [epoch: 1.83 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021270482017957493		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.021270482017957493 | validation: 0.02451351934049234]
	TIME [epoch: 1.83 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021963400077214673		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.021963400077214673 | validation: 0.041957101789077855]
	TIME [epoch: 1.83 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020065920190892563		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.020065920190892563 | validation: 0.03295435091163612]
	TIME [epoch: 1.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020215561879883836		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.020215561879883836 | validation: 0.03464669973894733]
	TIME [epoch: 1.83 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022698473585062683		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.022698473585062683 | validation: 0.027712653306501023]
	TIME [epoch: 1.83 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01945200158553992		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.01945200158553992 | validation: 0.03207384032187547]
	TIME [epoch: 1.83 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022464005103616697		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.022464005103616697 | validation: 0.03302127837310858]
	TIME [epoch: 1.83 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206675430152814		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.0206675430152814 | validation: 0.03233740613764235]
	TIME [epoch: 1.83 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01945742789569394		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.01945742789569394 | validation: 0.03809606686388756]
	TIME [epoch: 1.83 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021778060284741194		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.021778060284741194 | validation: 0.05164081851031747]
	TIME [epoch: 1.83 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042924295747165435		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.042924295747165435 | validation: 0.04667065567456226]
	TIME [epoch: 1.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02997908586696137		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.02997908586696137 | validation: 0.036745859842341355]
	TIME [epoch: 1.83 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02402443772771745		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.02402443772771745 | validation: 0.02724124244832331]
	TIME [epoch: 1.83 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021377541963303433		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.021377541963303433 | validation: 0.1055311769911917]
	TIME [epoch: 1.83 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09042796579236169		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.09042796579236169 | validation: 0.05847527349322865]
	TIME [epoch: 1.83 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728510830270972		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.0728510830270972 | validation: 0.02968504206822429]
	TIME [epoch: 1.83 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04404606962442025		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.04404606962442025 | validation: 0.03307868814101619]
	TIME [epoch: 1.83 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025858275188339972		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.025858275188339972 | validation: 0.034600481633114924]
	TIME [epoch: 1.83 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022312548043443736		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.022312548043443736 | validation: 0.03460242393285839]
	TIME [epoch: 1.83 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022313653923133624		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.022313653923133624 | validation: 0.032643646750275125]
	TIME [epoch: 1.83 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020452602906319492		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.020452602906319492 | validation: 0.03605231811408086]
	TIME [epoch: 1.82 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02137564928203325		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.02137564928203325 | validation: 0.03590897629547525]
	TIME [epoch: 1.83 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02000704621229928		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.02000704621229928 | validation: 0.037814978126271596]
	TIME [epoch: 1.82 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02076437278099649		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.02076437278099649 | validation: 0.03300673434746872]
	TIME [epoch: 1.82 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132083118281851		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.02132083118281851 | validation: 0.030651983896328307]
	TIME [epoch: 1.83 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021132599944212335		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.021132599944212335 | validation: 0.038953788342768185]
	TIME [epoch: 1.82 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020238334887990855		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.020238334887990855 | validation: 0.032909690272076686]
	TIME [epoch: 1.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020088730135712798		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.020088730135712798 | validation: 0.03999849955565852]
	TIME [epoch: 1.82 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021353411922752957		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.021353411922752957 | validation: 0.036861030599248945]
	TIME [epoch: 1.82 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022409720876138257		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.022409720876138257 | validation: 0.03370110052222737]
	TIME [epoch: 1.82 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020509207175208224		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.020509207175208224 | validation: 0.036446632627662616]
	TIME [epoch: 1.82 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0196267777167132		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.0196267777167132 | validation: 0.02952491385796753]
	TIME [epoch: 1.83 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020704911022961875		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.020704911022961875 | validation: 0.029124784254183734]
	TIME [epoch: 1.84 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021019633804385686		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.021019633804385686 | validation: 0.03362456114160667]
	TIME [epoch: 1.82 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020832516905905842		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.020832516905905842 | validation: 0.029088765395638774]
	TIME [epoch: 1.82 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021143445900171233		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.021143445900171233 | validation: 0.029324499814468597]
	TIME [epoch: 1.82 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019477510704173615		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.019477510704173615 | validation: 0.0321611505208933]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion1_v_mmd1_20250421_120255/states/model_phi1_4a_distortion1_v_mmd1_857.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1476.705 seconds.
