Args:
Namespace(name='model_phi1_4a_distortion_v1_0_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_0/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05595692992210388, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 938173044

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.508483146859865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.508483146859865 | validation: 7.8456220698414505]
	TIME [epoch: 160 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.397086355739012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.397086355739012 | validation: 8.13839759719073]
	TIME [epoch: 1.04 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.776297018010962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.776297018010962 | validation: 8.14451337911778]
	TIME [epoch: 0.708 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.975160748144786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.975160748144786 | validation: 8.13564082446884]
	TIME [epoch: 0.703 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.986083885169749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.986083885169749 | validation: 7.9269309695673815]
	TIME [epoch: 0.702 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.767625768051115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.767625768051115 | validation: 7.575503274221657]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.360513322838364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.360513322838364 | validation: 7.52773707202935]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.180921544431701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.180921544431701 | validation: 7.579407769734991]
	TIME [epoch: 0.707 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.149658030387152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.149658030387152 | validation: 7.603321370554522]
	TIME [epoch: 0.704 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.070003543483328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.070003543483328 | validation: 7.592553655799396]
	TIME [epoch: 0.707 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.916835370145532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.916835370145532 | validation: 7.804682719036936]
	TIME [epoch: 0.702 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.472849399819715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.472849399819715 | validation: 7.843536173587999]
	TIME [epoch: 0.704 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.1884611744450355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.1884611744450355 | validation: 7.503913922352948]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.6290301332031145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6290301332031145 | validation: 7.639859147377367]
	TIME [epoch: 0.707 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.373425199357512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.373425199357512 | validation: 7.451587269483937]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.173232044450758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.173232044450758 | validation: 7.373155961727576]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.832101459311671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.832101459311671 | validation: 7.332322800659195]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.75121023432961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.75121023432961 | validation: 7.343686631475923]
	TIME [epoch: 0.708 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.702391869635571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.702391869635571 | validation: 7.356869154912551]
	TIME [epoch: 0.706 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.610039587351808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.610039587351808 | validation: 7.358824510522844]
	TIME [epoch: 0.704 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.455180461356774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.455180461356774 | validation: 7.393453765595578]
	TIME [epoch: 0.705 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.181718006106012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.181718006106012 | validation: 7.319344198515974]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.885203600215108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.885203600215108 | validation: 7.300702707397026]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.702841794371807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.702841794371807 | validation: 7.005050045175548]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.232309900606302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.232309900606302 | validation: 7.235140324296153]
	TIME [epoch: 0.707 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.540219747767386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.540219747767386 | validation: 7.17262832008893]
	TIME [epoch: 0.708 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.403332379000569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.403332379000569 | validation: 7.177130417869811]
	TIME [epoch: 0.709 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.284447539044235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.284447539044235 | validation: 6.978950782232184]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.284209793613688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.284209793613688 | validation: 7.042586232462956]
	TIME [epoch: 0.707 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.421061238472293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.421061238472293 | validation: 6.860128018658672]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.134824331563212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.134824331563212 | validation: 6.89199638379892]
	TIME [epoch: 0.706 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.993002993331896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.993002993331896 | validation: 6.669526228197417]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.950578636066007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.950578636066007 | validation: 6.8312380554584395]
	TIME [epoch: 0.706 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.167343647185078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.167343647185078 | validation: 6.489981906046177]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.881892439049027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.881892439049027 | validation: 6.542558974261617]
	TIME [epoch: 0.707 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6982722187891595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6982722187891595 | validation: 6.349707689481694]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.592225557033237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.592225557033237 | validation: 6.537546194595045]
	TIME [epoch: 0.705 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.773402591414924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.773402591414924 | validation: 6.11470151401136]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.738580653275241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.738580653275241 | validation: 6.262535590112314]
	TIME [epoch: 0.706 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.519719164048423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.519719164048423 | validation: 6.126041292943659]
	TIME [epoch: 0.703 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.286590941210409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.286590941210409 | validation: 6.05653497981338]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.278074580450534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.278074580450534 | validation: 5.930188993914704]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.172945830809819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.172945830809819 | validation: 6.177580843152384]
	TIME [epoch: 0.709 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.339520614209127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.339520614209127 | validation: 5.792961935533314]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.452903437037358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.452903437037358 | validation: 5.953504009080219]
	TIME [epoch: 0.706 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.164377595224679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.164377595224679 | validation: 5.748762147775956]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020288323540881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020288323540881 | validation: 5.928137937948169]
	TIME [epoch: 0.708 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.12449123829853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.12449123829853 | validation: 5.731969373608141]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.268423149435657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.268423149435657 | validation: 5.815074480482625]
	TIME [epoch: 0.711 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.069648931165043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.069648931165043 | validation: 5.703121181315566]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9221862762143536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9221862762143536 | validation: 5.67198694971529]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9230156902354145		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.9230156902354145 | validation: 5.694528432300247]
	TIME [epoch: 0.706 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.904306543211467		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.904306543211467 | validation: 5.617605324252534]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9197973850131382		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.9197973850131382 | validation: 5.875526466872824]
	TIME [epoch: 0.713 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.158471115900018		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 4.158471115900018 | validation: 5.62240095580203]
	TIME [epoch: 0.704 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000590174582569		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 4.000590174582569 | validation: 5.616060894324821]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8472396139753795		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.8472396139753795 | validation: 5.644717127771274]
	TIME [epoch: 0.71 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8852906376433		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.8852906376433 | validation: 5.56800762397372]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.962366332143911		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.962366332143911 | validation: 5.628781740400897]
	TIME [epoch: 0.705 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8922530394586023		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.8922530394586023 | validation: 5.538138448479468]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8463967149929816		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.8463967149929816 | validation: 5.630260059454928]
	TIME [epoch: 0.71 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8859403136735415		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.8859403136735415 | validation: 5.508968765144383]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.850622967235056		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.850622967235056 | validation: 5.528743070076642]
	TIME [epoch: 0.71 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7919844407461194		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.7919844407461194 | validation: 5.491403836741625]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7905998169175827		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.7905998169175827 | validation: 5.553370108031045]
	TIME [epoch: 0.707 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8186909713673685		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.8186909713673685 | validation: 5.469227453747979]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8297584198470362		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.8297584198470362 | validation: 5.501351552869122]
	TIME [epoch: 0.704 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7886286768906055		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.7886286768906055 | validation: 5.418493620731081]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7723623314843446		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.7723623314843446 | validation: 5.468573490506884]
	TIME [epoch: 0.71 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7769364889509838		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.7769364889509838 | validation: 5.406673025268692]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7631766322318447		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.7631766322318447 | validation: 5.439806705687279]
	TIME [epoch: 0.713 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.76027252849774		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.76027252849774 | validation: 5.380961597117307]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.744088822037499		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.744088822037499 | validation: 5.41479165609991]
	TIME [epoch: 0.709 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.74229310358		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.74229310358 | validation: 5.368476207666157]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.738424925433801		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.738424925433801 | validation: 5.3833065441093755]
	TIME [epoch: 0.707 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7203407185219137		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.7203407185219137 | validation: 5.336792808850602]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7102436555643967		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.7102436555643967 | validation: 5.361874439788496]
	TIME [epoch: 0.709 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.719430942471939		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.719430942471939 | validation: 5.322401546976356]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.693319948856705		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.693319948856705 | validation: 5.314472695797652]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6887946098343116		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.6887946098343116 | validation: 5.279133260350343]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.67473233297949		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.67473233297949 | validation: 5.315636513284716]
	TIME [epoch: 0.708 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.671045143159589		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.671045143159589 | validation: 5.256997508038774]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6633619742437253		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.6633619742437253 | validation: 5.272678134034992]
	TIME [epoch: 0.706 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6630989596599535		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.6630989596599535 | validation: 5.246909401140817]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6605462248317187		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.6605462248317187 | validation: 5.2359953060486255]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6636521352967155		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.6636521352967155 | validation: 5.2241069437026395]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.64645388217527		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.64645388217527 | validation: 5.226213470088978]
	TIME [epoch: 0.708 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.653125404788699		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 3.653125404788699 | validation: 5.190667942485312]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.623198026487983		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.623198026487983 | validation: 5.1904979125468]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6164067279551295		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.6164067279551295 | validation: 5.156590897087179]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.608296607306629		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 3.608296607306629 | validation: 5.142465053479161]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.604657022141889		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 3.604657022141889 | validation: 5.11755439037526]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5969837076526017		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 3.5969837076526017 | validation: 5.0558578059761174]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.574936288626425		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.574936288626425 | validation: 4.87958926626841]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4401572619080465		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 3.4401572619080465 | validation: 4.569325474874609]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3843715812586654		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 3.3843715812586654 | validation: 4.847212835493076]
	TIME [epoch: 0.708 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5499433261244735		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 3.5499433261244735 | validation: 4.5714976747740526]
	TIME [epoch: 0.707 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1391341735182063		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 3.1391341735182063 | validation: 4.4978825558707625]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.014974178727207		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 3.014974178727207 | validation: 4.478699645422975]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.031535903120622		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 3.031535903120622 | validation: 4.430074901235814]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.994632441786131		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.994632441786131 | validation: 4.471828713879847]
	TIME [epoch: 0.712 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.95601229488764		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.95601229488764 | validation: 4.312828602222069]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9387763212104137		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.9387763212104137 | validation: 4.351304771431722]
	TIME [epoch: 0.718 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.919670294321312		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.919670294321312 | validation: 4.255874999281295]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.919900292684094		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.919900292684094 | validation: 4.445382834616393]
	TIME [epoch: 0.715 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0325177944983683		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 3.0325177944983683 | validation: 4.246759403332978]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1372819498694384		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 3.1372819498694384 | validation: 4.238222893614746]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8750880023714367		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.8750880023714367 | validation: 4.303779639758025]
	TIME [epoch: 0.713 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.899698662350064		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.899698662350064 | validation: 4.128839606115001]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9251092876322735		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.9251092876322735 | validation: 4.169544344892908]
	TIME [epoch: 0.714 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.836287526585666		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.836287526585666 | validation: 4.170599150146722]
	TIME [epoch: 0.714 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8238720920405873		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.8238720920405873 | validation: 4.061869043844836]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8189143841961006		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.8189143841961006 | validation: 4.155247494938168]
	TIME [epoch: 0.712 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.816077656630623		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.816077656630623 | validation: 3.9651472444393363]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.832903772336666		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.832903772336666 | validation: 4.152511129642407]
	TIME [epoch: 0.707 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.882262550818008		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.882262550818008 | validation: 3.917571726763088]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.825981337559601		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.825981337559601 | validation: 3.9069006683472605]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7186057968655315		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.7186057968655315 | validation: 3.928036388819355]
	TIME [epoch: 0.707 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7203245290184		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.7203245290184 | validation: 3.780941435007104]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8022567427260534		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.8022567427260534 | validation: 4.022568156855132]
	TIME [epoch: 0.708 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.847931887596086		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.847931887596086 | validation: 3.751683593385849]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7125970960921246		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.7125970960921246 | validation: 3.714199671968381]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6510057583843083		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.6510057583843083 | validation: 3.7457376042397046]
	TIME [epoch: 0.71 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6589963496914026		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.6589963496914026 | validation: 3.5995763374164014]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6416171844908707		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.6416171844908707 | validation: 3.6892911458724735]
	TIME [epoch: 0.711 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6528293067254265		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.6528293067254265 | validation: 3.5553872012234793]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.681234393721893		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.681234393721893 | validation: 3.601491134773687]
	TIME [epoch: 0.71 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6039244610818826		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.6039244610818826 | validation: 3.4003785139300065]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.566914407083628		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.566914407083628 | validation: 3.442437875145528]
	TIME [epoch: 0.71 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.536632134962912		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.536632134962912 | validation: 3.2338384294608247]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5221052455719635		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.5221052455719635 | validation: 3.5113196832084945]
	TIME [epoch: 0.71 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.630012326251964		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.630012326251964 | validation: 3.065100191208574]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.705854520387619		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.705854520387619 | validation: 2.922867922643342]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4960490242743685		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.4960490242743685 | validation: 3.3865225310639753]
	TIME [epoch: 0.713 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.582703083154395		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.582703083154395 | validation: 2.7534664698110256]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.385440476315852		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.385440476315852 | validation: 2.454897360877455]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.289503977685102		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.289503977685102 | validation: 2.5981069319557264]
	TIME [epoch: 0.712 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2242455227590265		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.2242455227590265 | validation: 2.156954696563518]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.418024658624737		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.418024658624737 | validation: 2.0743788030162507]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.21114423173791		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.21114423173791 | validation: 2.0145359434103045]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0080600186446795		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.0080600186446795 | validation: 1.4668148342558034]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.838993914787451		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.838993914787451 | validation: 1.9364111106768123]
	TIME [epoch: 0.709 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9228166772781263		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.9228166772781263 | validation: 1.4349424275843738]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6619039073673068		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.6619039073673068 | validation: 1.3401510423397243]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6774522916681		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.6774522916681 | validation: 1.5409083050948045]
	TIME [epoch: 0.709 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6478766987698024		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.6478766987698024 | validation: 1.2956749665014975]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5576227794143793		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.5576227794143793 | validation: 1.184291969235135]
	TIME [epoch: 0.719 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6492439975613185		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.6492439975613185 | validation: 1.4260273273486423]
	TIME [epoch: 0.713 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5067500433001717		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5067500433001717 | validation: 1.2414931734626027]
	TIME [epoch: 0.717 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.413307239851821		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.413307239851821 | validation: 1.1890475151612958]
	TIME [epoch: 0.713 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.422813143722624		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.422813143722624 | validation: 1.4594940868160415]
	TIME [epoch: 0.712 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.515895122928157		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.515895122928157 | validation: 1.4666703922437474]
	TIME [epoch: 0.712 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.488954572733345		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.488954572733345 | validation: 1.0661795662379583]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2995807936417487		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2995807936417487 | validation: 1.2331984814297972]
	TIME [epoch: 0.718 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3043587238319583		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.3043587238319583 | validation: 1.009044396226536]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4013103704678462		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.4013103704678462 | validation: 1.4538148818142804]
	TIME [epoch: 0.712 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3884520646786462		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3884520646786462 | validation: 1.2098367601552764]
	TIME [epoch: 0.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.468041665595041		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.468041665595041 | validation: 1.728818386182865]
	TIME [epoch: 0.709 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7121062418827444		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.7121062418827444 | validation: 1.1796633319687726]
	TIME [epoch: 0.71 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2472258750301835		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2472258750301835 | validation: 1.1627860529120577]
	TIME [epoch: 0.708 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3266898007605747		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.3266898007605747 | validation: 1.2115140227848546]
	TIME [epoch: 0.709 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3178670839328674		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3178670839328674 | validation: 0.9284501462826182]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1330339459713108		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1330339459713108 | validation: 0.888162940795813]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1388912463791099		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1388912463791099 | validation: 1.3061730226409036]
	TIME [epoch: 0.711 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2704219122005922		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.2704219122005922 | validation: 0.9652211170052971]
	TIME [epoch: 0.711 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2694595703974743		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2694595703974743 | validation: 1.1371625561417245]
	TIME [epoch: 0.712 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064619598623726		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.1064619598623726 | validation: 0.986130693787385]
	TIME [epoch: 0.71 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1260539521647308		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1260539521647308 | validation: 0.965601485954921]
	TIME [epoch: 0.71 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1372789477060987		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1372789477060987 | validation: 1.253920455218316]
	TIME [epoch: 0.71 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20761252637724		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.20761252637724 | validation: 0.8921773023614037]
	TIME [epoch: 0.709 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.197231881914899		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.197231881914899 | validation: 1.0665603454355266]
	TIME [epoch: 0.711 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0497425834028316		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0497425834028316 | validation: 1.3846309750273715]
	TIME [epoch: 0.71 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2350247704388035		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.2350247704388035 | validation: 1.093636961632321]
	TIME [epoch: 0.712 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.219193353796315		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.219193353796315 | validation: 0.9732763203722321]
	TIME [epoch: 0.71 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0358958193841172		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0358958193841172 | validation: 0.9488920638325701]
	TIME [epoch: 0.711 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0856279558220094		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0856279558220094 | validation: 0.9454259644006489]
	TIME [epoch: 0.711 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576025702844989		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9576025702844989 | validation: 0.7944995004520587]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200367457658644		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9200367457658644 | validation: 0.8865216314098927]
	TIME [epoch: 0.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9426908642162914		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.9426908642162914 | validation: 0.8610992244706499]
	TIME [epoch: 0.711 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302442792686014		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9302442792686014 | validation: 0.8506161458079206]
	TIME [epoch: 0.712 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9709405997921005		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9709405997921005 | validation: 1.3964634124467437]
	TIME [epoch: 0.71 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1461371927869723		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1461371927869723 | validation: 0.9667228004068961]
	TIME [epoch: 0.71 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2372104664767338		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.2372104664767338 | validation: 1.0453035140905076]
	TIME [epoch: 0.71 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072165732637774		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9072165732637774 | validation: 0.8701914703807004]
	TIME [epoch: 0.711 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996253160287302		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8996253160287302 | validation: 0.838435315563518]
	TIME [epoch: 0.71 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9170018213667603		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9170018213667603 | validation: 0.9709178931574787]
	TIME [epoch: 0.708 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721052686898366		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8721052686898366 | validation: 0.6960162970434838]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953695292304883		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7953695292304883 | validation: 0.83942117732819]
	TIME [epoch: 0.71 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350900423461933		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8350900423461933 | validation: 0.7332637676214908]
	TIME [epoch: 0.709 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9781797278546499		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9781797278546499 | validation: 1.167216986311485]
	TIME [epoch: 0.71 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9549966168721173		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9549966168721173 | validation: 0.6893357672645055]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156661124231559		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9156661124231559 | validation: 1.0108768139568827]
	TIME [epoch: 0.711 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480180924768473		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8480180924768473 | validation: 0.6812181831721631]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762059172651756		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7762059172651756 | validation: 0.7954287454733691]
	TIME [epoch: 0.71 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7644978171185675		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7644978171185675 | validation: 0.866847530311749]
	TIME [epoch: 0.709 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424448530454639		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8424448530454639 | validation: 0.9636166853704151]
	TIME [epoch: 0.714 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.942747759756804		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.942747759756804 | validation: 0.7613732501406036]
	TIME [epoch: 0.709 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367857518262563		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8367857518262563 | validation: 0.8989614413327067]
	TIME [epoch: 0.709 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8213628267142862		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8213628267142862 | validation: 0.6787600752298562]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410319516531362		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9410319516531362 | validation: 1.1058756909448941]
	TIME [epoch: 0.71 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598918391600122		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8598918391600122 | validation: 0.5935017425149697]
	TIME [epoch: 174 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148576362011748		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7148576362011748 | validation: 0.7794912025490435]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704609905250033		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6704609905250033 | validation: 0.5868005999546041]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721037713874388		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6721037713874388 | validation: 0.9630713814236401]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611164542011131		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7611164542011131 | validation: 0.7776236503493594]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451846598259356		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8451846598259356 | validation: 1.035787994191051]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774368425272655		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8774368425272655 | validation: 0.6916948242995392]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619337872167381		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7619337872167381 | validation: 0.6442466824315949]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969025858267535		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6969025858267535 | validation: 0.7365394758558473]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896048561423851		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6896048561423851 | validation: 0.7862085947989945]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675177350608752		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6675177350608752 | validation: 0.6325187072702116]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7411482197241559		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7411482197241559 | validation: 1.0162018733733416]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880074315751608		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7880074315751608 | validation: 0.5571327286093386]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775493426305124		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.775493426305124 | validation: 1.0746099504484643]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844783030843976		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.844783030843976 | validation: 0.7151406938892327]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944064279965466		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7944064279965466 | validation: 0.7483605302827075]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646330418060721		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6646330418060721 | validation: 0.6279438358147913]
	TIME [epoch: 1.39 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056147834768385		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6056147834768385 | validation: 0.6588073762693236]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.576177359103125		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.576177359103125 | validation: 0.5851960999179557]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5550494378539405		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.5550494378539405 | validation: 0.6152540744558255]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701789425035755		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5701789425035755 | validation: 0.628366139238171]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083525861650224		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6083525861650224 | validation: 0.7434940304741042]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638066585952089		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.638066585952089 | validation: 0.6714023209923328]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6193566155753341		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6193566155753341 | validation: 0.595308027936381]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5747392709965261		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.5747392709965261 | validation: 0.7898436121347272]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648311650484686		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6648311650484686 | validation: 0.7674181633632376]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2292358591072323		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.2292358591072323 | validation: 1.2570850400088334]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200959413686693		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9200959413686693 | validation: 0.7440696126434508]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5774735000440613		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.5774735000440613 | validation: 0.6633521452376681]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8995731764122408		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8995731764122408 | validation: 0.9074252212102736]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352964858436317		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7352964858436317 | validation: 0.6374155891062231]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5254856672644557		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.5254856672644557 | validation: 0.4442819958213905]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5521569732873816		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5521569732873816 | validation: 0.6837315176733395]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5234341171511576		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5234341171511576 | validation: 0.4422270190993974]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47789280024850167		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.47789280024850167 | validation: 0.5935388544398549]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617383874672368		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.4617383874672368 | validation: 0.5011023467980348]
	TIME [epoch: 1.39 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47483551859870804		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.47483551859870804 | validation: 0.6507458489387745]
	TIME [epoch: 1.39 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030376289382684		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.5030376289382684 | validation: 0.601060497676541]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5656868592932467		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.5656868592932467 | validation: 0.6698485158117189]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6005932697024945		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6005932697024945 | validation: 0.4875571230490133]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440542739596363		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6440542739596363 | validation: 0.7674766853849502]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308475405368422		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6308475405368422 | validation: 0.6828159244489626]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635753009207864		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8635753009207864 | validation: 0.9057378572453157]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021827242194009		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7021827242194009 | validation: 0.47508003287534273]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41341927102491693		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.41341927102491693 | validation: 0.4726652915756671]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439552977937809		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.5439552977937809 | validation: 0.9168579316891727]
	TIME [epoch: 1.39 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707484878046273		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.707484878046273 | validation: 0.4582234936338081]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56572242489762		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.56572242489762 | validation: 0.5498148556377773]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44916573456320563		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.44916573456320563 | validation: 0.5436580435108168]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4468536719395933		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4468536719395933 | validation: 0.5402939035295183]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47528039915145187		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.47528039915145187 | validation: 0.5798080132842758]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5277712367936154		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.5277712367936154 | validation: 0.5078514728846945]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5116140029754961		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5116140029754961 | validation: 0.6464363349139367]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5080323026101031		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5080323026101031 | validation: 0.39422590856394835]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271711237637285		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6271711237637285 | validation: 1.041427858375728]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377253141181747		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7377253141181747 | validation: 0.4254361960069081]
	TIME [epoch: 1.39 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44592879293796606		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.44592879293796606 | validation: 0.4410300080658532]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40386429838947324		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.40386429838947324 | validation: 0.5267528261700081]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38775190862743897		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.38775190862743897 | validation: 0.3488266473991284]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3843137756543766		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.3843137756543766 | validation: 0.6997475662619216]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4795959793997991		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.4795959793997991 | validation: 0.3993640692372336]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305896001732416		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5305896001732416 | validation: 0.7744202886625492]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477185699640381		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6477185699640381 | validation: 0.4055855160141956]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5094351346086087		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5094351346086087 | validation: 0.5141537711047978]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36936065889329056		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.36936065889329056 | validation: 0.35378649208047763]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3517489834092438		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3517489834092438 | validation: 0.4868218813413604]
	TIME [epoch: 1.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555318850166103		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.3555318850166103 | validation: 0.35936391463796663]
	TIME [epoch: 1.39 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3836506381298521		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.3836506381298521 | validation: 0.5154656100080164]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830336459638185		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.3830336459638185 | validation: 0.42199055853804773]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3722383942226473		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.3722383942226473 | validation: 0.47066348148874726]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39298351566989725		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.39298351566989725 | validation: 0.7484811991755276]
	TIME [epoch: 1.39 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5749195127945161		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5749195127945161 | validation: 0.4219038664081713]
	TIME [epoch: 1.39 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719718289084761		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5719718289084761 | validation: 0.7766287123574461]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5757643146276802		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5757643146276802 | validation: 0.5385039749974293]
	TIME [epoch: 1.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120190723583981		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6120190723583981 | validation: 0.4982268218244559]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37021491852442184		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.37021491852442184 | validation: 0.3119702416803929]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29272501046577426		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.29272501046577426 | validation: 0.4011933543111179]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28349017206808197		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.28349017206808197 | validation: 0.33240741299714754]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2956163848941706		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.2956163848941706 | validation: 0.4813172789237021]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411763590867159		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3411763590867159 | validation: 0.39590838804783424]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43960531166737254		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.43960531166737254 | validation: 0.7068815535167937]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5050003719154131		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5050003719154131 | validation: 0.41285160092331896]
	TIME [epoch: 1.39 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399170554574571		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6399170554574571 | validation: 0.6378241599077402]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42738933000608526		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.42738933000608526 | validation: 0.2994098775060898]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3224415024616092		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.3224415024616092 | validation: 0.5058310235086915]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3302823970119268		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3302823970119268 | validation: 0.2764288866816008]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33491828199849966		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.33491828199849966 | validation: 0.4980243366589074]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33173919852178885		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.33173919852178885 | validation: 0.2816851993764901]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3563360482359624		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3563360482359624 | validation: 0.5783453038721673]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44750116763216197		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.44750116763216197 | validation: 0.3591493942169839]
	TIME [epoch: 1.39 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49358933329901716		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.49358933329901716 | validation: 0.48956529842414787]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3537781252475394		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.3537781252475394 | validation: 0.2845891010727263]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857098798000749		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.2857098798000749 | validation: 0.44303003258017154]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2999967416589337		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.2999967416589337 | validation: 0.3621653192564298]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3368377416186651		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.3368377416186651 | validation: 0.5073388725801211]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3732973646206584		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3732973646206584 | validation: 0.4081096313863241]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36362636350985705		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.36362636350985705 | validation: 0.427204482462529]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37151846460546334		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.37151846460546334 | validation: 0.30144714456987104]
	TIME [epoch: 1.39 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30011223008063526		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.30011223008063526 | validation: 0.39682953827664147]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2923035728183878		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.2923035728183878 | validation: 0.2712163270109609]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36098682586415964		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.36098682586415964 | validation: 0.7815682105071344]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384051334129571		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5384051334129571 | validation: 0.3572832023114438]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4328355623670666		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.4328355623670666 | validation: 0.41496931378786484]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29814112731525194		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.29814112731525194 | validation: 0.32001359038891525]
	TIME [epoch: 1.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2480285294243022		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.2480285294243022 | validation: 0.270267692354015]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24209004302019865		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.24209004302019865 | validation: 0.4136211019347683]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25114478854140154		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.25114478854140154 | validation: 0.2764568142589699]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30185850057161057		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.30185850057161057 | validation: 0.6267394263442678]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4358146747380134		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.4358146747380134 | validation: 0.282791153426245]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4227189818500998		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.4227189818500998 | validation: 0.43126472000936256]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364313931383709		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.3364313931383709 | validation: 0.2523224984898617]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2826304925802057		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2826304925802057 | validation: 0.39044937883828795]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2638127561274535		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.2638127561274535 | validation: 0.2757953706846686]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25194283516481336		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.25194283516481336 | validation: 0.39700054857060385]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29842036589835647		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.29842036589835647 | validation: 0.35000111002355083]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32109624302240486		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.32109624302240486 | validation: 0.4589752307497932]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3626190739950988		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.3626190739950988 | validation: 0.28513379773430547]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26107339163687127		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.26107339163687127 | validation: 0.31361129007362526]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23429603327882792		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.23429603327882792 | validation: 0.23577545148773657]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2210351915241239		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2210351915241239 | validation: 0.29920216407639394]
	TIME [epoch: 1.39 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22216452622981472		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.22216452622981472 | validation: 0.2363858755747295]
	TIME [epoch: 1.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24626369682130858		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.24626369682130858 | validation: 0.4853841717364179]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3495203216390826		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.3495203216390826 | validation: 0.24663454356685316]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30281041881017684		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.30281041881017684 | validation: 0.6357751436564273]
	TIME [epoch: 1.39 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42295882262332557		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.42295882262332557 | validation: 0.45516964883231226]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4899178620979373		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.4899178620979373 | validation: 0.3347643829186892]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2425797928176704		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.2425797928176704 | validation: 0.2149370941033676]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18431153404552114		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.18431153404552114 | validation: 0.24554841871791272]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625578966572114		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.17625578966572114 | validation: 0.24519261487600408]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17878360614422892		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.17878360614422892 | validation: 0.2301014562831596]
	TIME [epoch: 1.39 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1856015173314644		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.1856015173314644 | validation: 0.37147644437697563]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24859938425775782		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.24859938425775782 | validation: 0.3497958891285443]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41757692867581314		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.41757692867581314 | validation: 0.6736425110753393]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5338244348442721		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.5338244348442721 | validation: 0.194212198199737]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23595874957227161		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.23595874957227161 | validation: 0.32690984042217636]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25565015247328776		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.25565015247328776 | validation: 0.42038282889437045]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32721465821344337		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.32721465821344337 | validation: 0.23390780381966159]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23235781650539283		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.23235781650539283 | validation: 0.26619563691871995]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22253331926033423		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.22253331926033423 | validation: 0.17456768961845676]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19301341634989932		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.19301341634989932 | validation: 0.2636931042085988]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19191766148063857		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.19191766148063857 | validation: 0.22270294000579943]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24247235132077785		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.24247235132077785 | validation: 0.6255176089515199]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3787298423284278		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.3787298423284278 | validation: 0.23952099966701978]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29254999592765435		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.29254999592765435 | validation: 0.30953605396053463]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23060272691672942		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.23060272691672942 | validation: 0.1817829570825773]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20934005333146766		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.20934005333146766 | validation: 0.2679983017615145]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20318211141461145		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.20318211141461145 | validation: 0.21039645352360564]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21558092447049762		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.21558092447049762 | validation: 0.33010634804351896]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24048359967948707		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.24048359967948707 | validation: 0.23541932489201553]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24882787010038318		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.24882787010038318 | validation: 0.45228608408272475]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3125460769351416		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.3125460769351416 | validation: 0.17726412073618636]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20325596778606886		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.20325596778606886 | validation: 0.21435074721290312]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16684073511898373		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.16684073511898373 | validation: 0.1880869018373314]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16193205918048928		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.16193205918048928 | validation: 0.23372531066407048]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20806473014105384		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.20806473014105384 | validation: 0.425327814179475]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31396223741797963		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.31396223741797963 | validation: 0.37824671846158386]
	TIME [epoch: 1.39 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30703883236660257		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.30703883236660257 | validation: 0.207150350799098]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16842056976021041		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.16842056976021041 | validation: 0.2001527477390761]
	TIME [epoch: 1.39 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14693116287906874		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.14693116287906874 | validation: 0.15948017504878217]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18814928729093253		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.18814928729093253 | validation: 0.44852370663650715]
	TIME [epoch: 1.39 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3198452073803237		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.3198452073803237 | validation: 0.3188168006733119]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41104288738969114		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.41104288738969114 | validation: 0.4286417955836921]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29705310661257406		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.29705310661257406 | validation: 0.20515425720314165]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600711177646574		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.1600711177646574 | validation: 0.16317268552002975]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140219287750731		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.140219287750731 | validation: 0.23144173313750352]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15050985373928533		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.15050985373928533 | validation: 0.16867003420408277]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17208937766361915		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.17208937766361915 | validation: 0.3510311564744619]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23394400220099043		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.23394400220099043 | validation: 0.1919147274734865]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27579043844676654		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.27579043844676654 | validation: 0.3399377197667153]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2853957769607083		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.2853957769607083 | validation: 0.18766187111323898]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20885355184025037		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.20885355184025037 | validation: 0.22646545482911892]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17662809920326317		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.17662809920326317 | validation: 0.1805373636595444]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15223769377775564		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.15223769377775564 | validation: 0.18685379695189924]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1449560215607753		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.1449560215607753 | validation: 0.17837263771519374]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15567429524760287		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.15567429524760287 | validation: 0.2453180437017394]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20727790249207936		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.20727790249207936 | validation: 0.24028206265707747]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21990611240894084		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.21990611240894084 | validation: 0.33588861066209197]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26150458904967155		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.26150458904967155 | validation: 0.21602825645890333]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19019363926692756		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.19019363926692756 | validation: 0.19020446247892647]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17890674236417858		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.17890674236417858 | validation: 0.3297216175172891]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21354316582453844		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.21354316582453844 | validation: 0.18560839130337103]
	TIME [epoch: 1.39 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19565251532013705		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.19565251532013705 | validation: 0.2986337905445236]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18601528424348218		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.18601528424348218 | validation: 0.15291044769312034]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21226093323437567		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.21226093323437567 | validation: 0.30949467816520854]
	TIME [epoch: 1.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666008454178026		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.2666008454178026 | validation: 0.16863890662022693]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22674772343875635		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.22674772343875635 | validation: 0.24700697363330285]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17403415482864965		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.17403415482864965 | validation: 0.1597803902157986]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559011258699137		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.14559011258699137 | validation: 0.20156509001589118]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14463110512417576		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.14463110512417576 | validation: 0.16798855318648107]
	TIME [epoch: 1.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14186857397184868		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.14186857397184868 | validation: 0.215782254221147]
	TIME [epoch: 1.39 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648578630277551		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.1648578630277551 | validation: 0.17868455272822373]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176328415839702		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.176328415839702 | validation: 0.23517359649227032]
	TIME [epoch: 1.39 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22798904262940242		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.22798904262940242 | validation: 0.1965674299911719]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18416641948741042		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.18416641948741042 | validation: 0.17662110716207052]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725986809435352		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.15725986809435352 | validation: 0.17109157874120232]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12426444734990966		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.12426444734990966 | validation: 0.1389422767145661]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12584406091553557		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.12584406091553557 | validation: 0.23284705946898365]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14708434595012462		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.14708434595012462 | validation: 0.24272904535456946]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21505018067474285		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.21505018067474285 | validation: 0.39276997945044834]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26140206554176354		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.26140206554176354 | validation: 0.19035231287563928]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29900105612931077		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.29900105612931077 | validation: 0.2827217862505377]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23844645738591425		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.23844645738591425 | validation: 0.14659781294239355]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11422021432572715		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.11422021432572715 | validation: 0.11330328553311242]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10739746893900823		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.10739746893900823 | validation: 0.15197438197806112]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12160189013728095		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.12160189013728095 | validation: 0.14450664992613427]
	TIME [epoch: 1.39 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15326881974801526		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.15326881974801526 | validation: 0.2831791214345298]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.208835037391896		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.208835037391896 | validation: 0.1457389584666231]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15221420787585813		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.15221420787585813 | validation: 0.21519945156084017]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16091408465339463		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.16091408465339463 | validation: 0.1814517439110026]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21039535988781174		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.21039535988781174 | validation: 0.2655801312493104]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20018147310290682		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.20018147310290682 | validation: 0.14703249694657525]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1547284213785264		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.1547284213785264 | validation: 0.1918102557576856]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13592374418255226		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.13592374418255226 | validation: 0.13262940797678616]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234391388583644		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1234391388583644 | validation: 0.20132868510430701]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14726196622348287		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.14726196622348287 | validation: 0.12001054777904052]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409057853934824		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.1409057853934824 | validation: 0.18865083563970814]
	TIME [epoch: 1.39 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16314152492465311		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.16314152492465311 | validation: 0.14589377090289324]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742101473923362		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.1742101473923362 | validation: 0.18202874855736498]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16799936192907716		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.16799936192907716 | validation: 0.14251549539745662]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615332431193752		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.13615332431193752 | validation: 0.2000847429735309]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289713962395205		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.1289713962395205 | validation: 0.15173295499751926]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13902986291140348		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.13902986291140348 | validation: 0.28251239241908177]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17367357809308331		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.17367357809308331 | validation: 0.14981630057716264]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16795294462131707		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.16795294462131707 | validation: 0.20505103743438058]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13341068598978653		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.13341068598978653 | validation: 0.10523360000980238]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125308858931826		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.125308858931826 | validation: 0.1558840521256292]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12958967834510554		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.12958967834510554 | validation: 0.10842734263800691]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1411792505218882		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.1411792505218882 | validation: 0.15471608774254308]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523850942796392		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1523850942796392 | validation: 0.14449987352439914]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16776783887103883		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.16776783887103883 | validation: 0.3300934383803131]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24224831853823536		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.24224831853823536 | validation: 0.11729697274287756]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990295161350254		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.09990295161350254 | validation: 0.1183786609524858]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08913684533129196		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.08913684533129196 | validation: 0.13313711501360567]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1048568711450535		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1048568711450535 | validation: 0.14723685991314034]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11493286848495439		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.11493286848495439 | validation: 0.16136843114322927]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14554478543743177		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.14554478543743177 | validation: 0.14490855083378548]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14572811096678545		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.14572811096678545 | validation: 0.1655289591996072]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15801988476579407		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.15801988476579407 | validation: 0.23424521062612788]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627820056785986		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.1627820056785986 | validation: 0.18400532037734219]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163723176316422		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.163723176316422 | validation: 0.20826238485410836]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12288295220499158		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.12288295220499158 | validation: 0.11303826069308323]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299721346960667		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.12299721346960667 | validation: 0.22929381143486272]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17160664889933677		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.17160664889933677 | validation: 0.14688575753237984]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18227102068198447		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.18227102068198447 | validation: 0.18894141933042222]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612655962820778		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1612655962820778 | validation: 0.11632790230651638]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12446933874426869		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.12446933874426869 | validation: 0.13349222416911974]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050665026391967		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.10050665026391967 | validation: 0.08594227783535362]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08496400672474301		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.08496400672474301 | validation: 0.11330291925474766]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08266717492126874		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.08266717492126874 | validation: 0.08807141169904359]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08838717576099399		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.08838717576099399 | validation: 0.16851482093312625]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1089153149648558		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1089153149648558 | validation: 0.13662473532850275]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16035864259824303		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.16035864259824303 | validation: 0.2913028313443468]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21075397796282758		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.21075397796282758 | validation: 0.10919850049888549]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14590959761345326		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.14590959761345326 | validation: 0.1144968583681635]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11495942496327059		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.11495942496327059 | validation: 0.09734445118306655]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031287349190215		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.10031287349190215 | validation: 0.09233175337035233]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08846263806781467		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.08846263806781467 | validation: 0.09812313095512797]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755377064380991		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.08755377064380991 | validation: 0.16849719807317465]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12010757571131406		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.12010757571131406 | validation: 0.15689492771429295]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12831501829285324		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.12831501829285324 | validation: 0.16699821001414283]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1475472603990425		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.1475472603990425 | validation: 0.14158479124159837]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13623889257352093		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.13623889257352093 | validation: 0.12854919443728124]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12433042758227746		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.12433042758227746 | validation: 0.16556724149215213]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10781149943670836		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.10781149943670836 | validation: 0.134191438623981]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11485094222647081		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.11485094222647081 | validation: 0.20322871855873595]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13342728546145793		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.13342728546145793 | validation: 0.11503711868286076]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15240909086256246		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.15240909086256246 | validation: 0.15917772774106764]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15298259990678628		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.15298259990678628 | validation: 0.08988467873322319]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1074058213529705		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.1074058213529705 | validation: 0.10649945153521514]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807252239363606		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.0807252239363606 | validation: 0.08187153424965697]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209482036138402		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.07209482036138402 | validation: 0.09136304709476631]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07483689289967901		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.07483689289967901 | validation: 0.1036887400970734]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08520461679803798		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08520461679803798 | validation: 0.14790179035489426]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1071671236708903		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1071671236708903 | validation: 0.11988894383040605]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11551494103603131		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.11551494103603131 | validation: 0.19154615828685595]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687116755650986		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.1687116755650986 | validation: 0.11920633650181599]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1735511071991169		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.1735511071991169 | validation: 0.16556210216182443]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206324454903355		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1206324454903355 | validation: 0.08592287151575277]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344457711098052		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.08344457711098052 | validation: 0.09144253579101272]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06924309264779864		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.06924309264779864 | validation: 0.07626809892957916]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07022987899383676		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.07022987899383676 | validation: 0.1012804586332996]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819677600143361		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.07819677600143361 | validation: 0.09700053400240387]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10991849773099693		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.10991849773099693 | validation: 0.1590198915103588]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802551242861744		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.13802551242861744 | validation: 0.13826475591908094]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13551539020172607		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.13551539020172607 | validation: 0.09351902394849604]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08736166758419293		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.08736166758419293 | validation: 0.11865517319553934]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08855530662931177		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.08855530662931177 | validation: 0.11040319842348073]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10630209146581464		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.10630209146581464 | validation: 0.22257734525843023]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13412933411140407		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.13412933411140407 | validation: 0.09739085764683131]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09895509067491923		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.09895509067491923 | validation: 0.102430850011193]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636009199914253		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.08636009199914253 | validation: 0.08170718128685117]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09694711459776635		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.09694711459776635 | validation: 0.11304718651895251]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327408226344333		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.11327408226344333 | validation: 0.09082397016099918]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10148970995369068		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.10148970995369068 | validation: 0.10658265777193897]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09400608196720449		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.09400608196720449 | validation: 0.079470976125643]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741644018418418		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.07741644018418418 | validation: 0.11018707575897234]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063819340180035		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.08063819340180035 | validation: 0.09613757469304629]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08732082314564218		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.08732082314564218 | validation: 0.160100569487941]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158885087233971		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.10158885087233971 | validation: 0.08912302134342684]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10177800645884158		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.10177800645884158 | validation: 0.12493168942529441]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09445739949143032		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.09445739949143032 | validation: 0.08505716339098315]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034710080257114		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.1034710080257114 | validation: 0.11622307243140753]
	TIME [epoch: 2.75 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12042353460508137		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.12042353460508137 | validation: 0.10089612825555201]
	TIME [epoch: 2.74 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09158088700203194		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.09158088700203194 | validation: 0.08255730563576984]
	TIME [epoch: 2.74 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07696758104344432		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.07696758104344432 | validation: 0.09447230794471628]
	TIME [epoch: 2.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685772241817282		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.06685772241817282 | validation: 0.07588979253924917]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720473691777751		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.06720473691777751 | validation: 0.08441841689932861]
	TIME [epoch: 2.74 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965742143373371		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.06965742143373371 | validation: 0.09792908198347439]
	TIME [epoch: 2.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782666824782087		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.0782666824782087 | validation: 0.07562344783687833]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08576624027506206		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.08576624027506206 | validation: 0.14495603849819583]
	TIME [epoch: 2.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1262004787596364		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.1262004787596364 | validation: 0.13866302651810494]
	TIME [epoch: 2.73 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15551446062278643		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.15551446062278643 | validation: 0.16444966209612324]
	TIME [epoch: 2.74 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11609621644427577		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.11609621644427577 | validation: 0.08964704584751727]
	TIME [epoch: 2.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715842550879018		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.0715842550879018 | validation: 0.07890728074263235]
	TIME [epoch: 2.74 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057540301083292715		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.057540301083292715 | validation: 0.06045128637389442]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050238252708262994		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.050238252708262994 | validation: 0.05995232457782404]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04890774532590895		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.04890774532590895 | validation: 0.06563256881687676]
	TIME [epoch: 2.74 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011313420610894		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.05011313420610894 | validation: 0.06443360428541485]
	TIME [epoch: 2.74 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545298427626685		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.05545298427626685 | validation: 0.10440098406350865]
	TIME [epoch: 2.73 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940946328816104		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.06940946328816104 | validation: 0.11657532505346113]
	TIME [epoch: 2.74 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10942579197310229		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.10942579197310229 | validation: 0.1671424623318353]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14801759036028897		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.14801759036028897 | validation: 0.10323338823090134]
	TIME [epoch: 2.74 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12758692665552426		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12758692665552426 | validation: 0.06967544990184668]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09067605952067419		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.09067605952067419 | validation: 0.1659247823294231]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11154892801436575		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.11154892801436575 | validation: 0.11323386297432388]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09059880757253078		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09059880757253078 | validation: 0.09645413806405526]
	TIME [epoch: 2.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07203079683358508		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.07203079683358508 | validation: 0.05576899581216685]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060832919690359075		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.060832919690359075 | validation: 0.06905041041439358]
	TIME [epoch: 2.74 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06730479831774992		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.06730479831774992 | validation: 0.05769999140693332]
	TIME [epoch: 2.74 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06891203915242737		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.06891203915242737 | validation: 0.06940333540194392]
	TIME [epoch: 2.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058958235095009		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.07058958235095009 | validation: 0.06226971522727316]
	TIME [epoch: 2.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889464510756774		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.06889464510756774 | validation: 0.08663002904385274]
	TIME [epoch: 2.74 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774492303520736		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.0774492303520736 | validation: 0.09501515583884557]
	TIME [epoch: 2.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08061344927148746		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.08061344927148746 | validation: 0.11396790988831779]
	TIME [epoch: 2.74 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09150859640756703		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.09150859640756703 | validation: 0.07772396724064858]
	TIME [epoch: 2.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686762350258333		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.0686762350258333 | validation: 0.1190931184328393]
	TIME [epoch: 2.74 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351842030558945		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.08351842030558945 | validation: 0.0948168922498543]
	TIME [epoch: 2.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.099668047938361		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.099668047938361 | validation: 0.10711554211212873]
	TIME [epoch: 2.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193602637550314		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.08193602637550314 | validation: 0.06519388428730184]
	TIME [epoch: 2.74 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918474579029073		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.06918474579029073 | validation: 0.08959022367960351]
	TIME [epoch: 2.76 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0767260297809028		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.0767260297809028 | validation: 0.08310234177769216]
	TIME [epoch: 2.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06718884279745505		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.06718884279745505 | validation: 0.06631470446760525]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06756982437885704		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.06756982437885704 | validation: 0.07410047049369908]
	TIME [epoch: 2.74 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07200784391126899		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.07200784391126899 | validation: 0.07103750870823602]
	TIME [epoch: 2.74 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515868324343879		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.07515868324343879 | validation: 0.06460691258991617]
	TIME [epoch: 2.74 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013110328665076		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.06013110328665076 | validation: 0.05499640807832569]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519217652305792		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.0519217652305792 | validation: 0.054588696385847714]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743365931522707		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.04743365931522707 | validation: 0.052507658758487744]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799235748063572		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.04799235748063572 | validation: 0.0780112181085322]
	TIME [epoch: 2.75 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583579500500708		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.0583579500500708 | validation: 0.10260084759898172]
	TIME [epoch: 2.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08348790944191656		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.08348790944191656 | validation: 0.15067064252738327]
	TIME [epoch: 2.74 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990906879021344		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0990906879021344 | validation: 0.07591651636242064]
	TIME [epoch: 2.74 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745878826864448		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.0745878826864448 | validation: 0.06937178729357074]
	TIME [epoch: 2.74 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018134562021704		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.05018134562021704 | validation: 0.05747076762917304]
	TIME [epoch: 2.74 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044933058468636294		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.044933058468636294 | validation: 0.05313425047609832]
	TIME [epoch: 2.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04928677198793723		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.04928677198793723 | validation: 0.07645777745105552]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980948652254505		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.07980948652254505 | validation: 0.16045852528461052]
	TIME [epoch: 2.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17088641910616215		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.17088641910616215 | validation: 0.08141565977862525]
	TIME [epoch: 2.74 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11567505403371219		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.11567505403371219 | validation: 0.10558194718999553]
	TIME [epoch: 2.74 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190068453944602		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.07190068453944602 | validation: 0.06734649696573188]
	TIME [epoch: 2.74 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051653299898245124		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.051653299898245124 | validation: 0.056102451943853375]
	TIME [epoch: 2.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04445057739422076		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.04445057739422076 | validation: 0.043442708095411064]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043206552760619105		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.043206552760619105 | validation: 0.0425415895146695]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03992153934741828		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.03992153934741828 | validation: 0.05393345871505002]
	TIME [epoch: 2.73 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040704755172587224		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.040704755172587224 | validation: 0.040719729023219936]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039299435373904346		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.039299435373904346 | validation: 0.05735895612916652]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696476224994688		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.04696476224994688 | validation: 0.07051092166086732]
	TIME [epoch: 2.74 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181760536223893		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.07181760536223893 | validation: 0.1830746994058503]
	TIME [epoch: 2.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15342200780066548		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.15342200780066548 | validation: 0.09985622464864033]
	TIME [epoch: 2.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11938584711482264		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.11938584711482264 | validation: 0.07630814031349406]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06648898912923513		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.06648898912923513 | validation: 0.04580918612055427]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442467939754072		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.04442467939754072 | validation: 0.04949842050328906]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042069552513684		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.04042069552513684 | validation: 0.04198941421910281]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041165706246580926		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.041165706246580926 | validation: 0.04894849783805455]
	TIME [epoch: 2.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04299640299827287		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.04299640299827287 | validation: 0.04625102361558292]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045022032995934026		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.045022032995934026 | validation: 0.05404775297794223]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922043027333868		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.04922043027333868 | validation: 0.05826281426511476]
	TIME [epoch: 2.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044100827480436		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.06044100827480436 | validation: 0.08065259140295197]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814999706171533		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.06814999706171533 | validation: 0.08505335381498305]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07825628046972159		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.07825628046972159 | validation: 0.09422283754450951]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633854978174925		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.0633854978174925 | validation: 0.05132200101768178]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044653501293748205		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.044653501293748205 | validation: 0.060173126019945004]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04339326671697462		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.04339326671697462 | validation: 0.075428067164162]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05643859893781789		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.05643859893781789 | validation: 0.13222782598170613]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373632244039854		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.10373632244039854 | validation: 0.07595934709472722]
	TIME [epoch: 2.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229220148831628		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.10229220148831628 | validation: 0.09070336299429338]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09096505785660533		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.09096505785660533 | validation: 0.04758263036960212]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282397339552687		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.05282397339552687 | validation: 0.04701096188371339]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037579044402703946		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.037579044402703946 | validation: 0.03915112374396519]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03675911111441301		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.03675911111441301 | validation: 0.04213064318581227]
	TIME [epoch: 2.74 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760053792565818		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.03760053792565818 | validation: 0.04164583068806267]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04102621566969434		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.04102621566969434 | validation: 0.05075881524234194]
	TIME [epoch: 2.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04754577655604195		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.04754577655604195 | validation: 0.05713957375256349]
	TIME [epoch: 2.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819482673414656		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.05819482673414656 | validation: 0.07206000008839464]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07312932814016815		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07312932814016815 | validation: 0.10958725519582746]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08690769525055057		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.08690769525055057 | validation: 0.06528988607660953]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06742392365291833		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.06742392365291833 | validation: 0.08514080758415453]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05507985102309204		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.05507985102309204 | validation: 0.04437118478268942]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041528735954250634		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.041528735954250634 | validation: 0.04587645870697139]
	TIME [epoch: 2.73 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035811973036912914		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.035811973036912914 | validation: 0.03226642059704387]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03295097022213222		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.03295097022213222 | validation: 0.038931045599128015]
	TIME [epoch: 2.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034125518628919445		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.034125518628919445 | validation: 0.03713621334823417]
	TIME [epoch: 2.73 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498307159707897		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.03498307159707897 | validation: 0.04731304511690401]
	TIME [epoch: 2.73 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04669769324353		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.04669769324353 | validation: 0.055271358711905055]
	TIME [epoch: 2.73 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07696497974540552		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07696497974540552 | validation: 0.09278304418609719]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1083755298130689		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.1083755298130689 | validation: 0.06770937065968896]
	TIME [epoch: 2.73 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443380108326166		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.06443380108326166 | validation: 0.0710255252317305]
	TIME [epoch: 2.73 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022953169743866		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.05022953169743866 | validation: 0.060367306431199586]
	TIME [epoch: 2.73 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048056390096083985		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.048056390096083985 | validation: 0.08215305979046375]
	TIME [epoch: 2.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047330880389351966		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.047330880389351966 | validation: 0.04514978911406636]
	TIME [epoch: 2.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441212244366209		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.04441212244366209 | validation: 0.0580119104832455]
	TIME [epoch: 2.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03833885751909259		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.03833885751909259 | validation: 0.03617970316411571]
	TIME [epoch: 2.73 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434550191542735		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.03434550191542735 | validation: 0.039622489071992756]
	TIME [epoch: 2.73 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033317591800042866		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.033317591800042866 | validation: 0.03692361784767576]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032633290437572904		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.032633290437572904 | validation: 0.053409519869577295]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04412771757885455		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.04412771757885455 | validation: 0.060104352367731595]
	TIME [epoch: 2.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603205669001613		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07603205669001613 | validation: 0.10594025216996905]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179967734191907		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1179967734191907 | validation: 0.06489169958209128]
	TIME [epoch: 2.74 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07228505933704435		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.07228505933704435 | validation: 0.05832784491822218]
	TIME [epoch: 2.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04150034726688964		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.04150034726688964 | validation: 0.042949179648600615]
	TIME [epoch: 2.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039173709432280136		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.039173709432280136 | validation: 0.05091995466384878]
	TIME [epoch: 2.73 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036500109363340076		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.036500109363340076 | validation: 0.038802168102617975]
	TIME [epoch: 2.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429452775497166		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.03429452775497166 | validation: 0.04677519038576344]
	TIME [epoch: 2.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156631300856296		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.03156631300856296 | validation: 0.03653073036159288]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031763780834591755		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.031763780834591755 | validation: 0.04867804686326404]
	TIME [epoch: 2.73 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03350057954799869		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.03350057954799869 | validation: 0.03906348800949853]
	TIME [epoch: 2.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03751771507163476		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.03751771507163476 | validation: 0.07221693240183029]
	TIME [epoch: 2.73 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041925410492000846		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.041925410492000846 | validation: 0.04077300042138954]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04187730233915764		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.04187730233915764 | validation: 0.0518408308907417]
	TIME [epoch: 2.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04098658193372045		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.04098658193372045 | validation: 0.05438851745995408]
	TIME [epoch: 2.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05650987278755476		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.05650987278755476 | validation: 0.06900896808843274]
	TIME [epoch: 2.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294052890288317		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.09294052890288317 | validation: 0.11885086807677636]
	TIME [epoch: 2.73 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10407834569185799		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.10407834569185799 | validation: 0.056414937796484536]
	TIME [epoch: 2.73 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054432376260936		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.05054432376260936 | validation: 0.049775433701137156]
	TIME [epoch: 2.73 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03348591146035745		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.03348591146035745 | validation: 0.03554352687217042]
	TIME [epoch: 2.73 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916577195993742		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.03916577195993742 | validation: 0.051270356623171046]
	TIME [epoch: 2.73 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04289682866286441		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.04289682866286441 | validation: 0.05289385290586242]
	TIME [epoch: 2.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527890230112373		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.04527890230112373 | validation: 0.04038517047656997]
	TIME [epoch: 2.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04155678346379407		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.04155678346379407 | validation: 0.04949043391181982]
	TIME [epoch: 2.73 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040711412633003895		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.040711412633003895 | validation: 0.03807241305746391]
	TIME [epoch: 2.73 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033931714805996716		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.033931714805996716 | validation: 0.044029322053240866]
	TIME [epoch: 2.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03118392633411107		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.03118392633411107 | validation: 0.030432288206621163]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035154823913980615		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.035154823913980615 | validation: 0.04613987167776386]
	TIME [epoch: 2.74 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120335174061188		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.04120335174061188 | validation: 0.04712980090841347]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218436196699922		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.06218436196699922 | validation: 0.06532085735285674]
	TIME [epoch: 2.74 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034077833997109		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.07034077833997109 | validation: 0.04056658133577863]
	TIME [epoch: 2.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05347060101479368		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.05347060101479368 | validation: 0.04511484997856727]
	TIME [epoch: 2.74 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362810111165527		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.0362810111165527 | validation: 0.03140690917862733]
	TIME [epoch: 2.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02955656514355577		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.02955656514355577 | validation: 0.04248735160422398]
	TIME [epoch: 2.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028481086155958645		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.028481086155958645 | validation: 0.03781133265197403]
	TIME [epoch: 2.74 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03252561550878673		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.03252561550878673 | validation: 0.056065146458336795]
	TIME [epoch: 2.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035902512494771006		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.035902512494771006 | validation: 0.041858527926942946]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03755221606660963		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.03755221606660963 | validation: 0.05047778796520144]
	TIME [epoch: 2.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03604392452730995		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.03604392452730995 | validation: 0.033051318157942114]
	TIME [epoch: 2.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03341622085766719		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.03341622085766719 | validation: 0.04054990796588425]
	TIME [epoch: 2.74 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033475144346045545		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.033475144346045545 | validation: 0.03827649486042384]
	TIME [epoch: 2.74 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041870861822277804		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.041870861822277804 | validation: 0.04339924999052813]
	TIME [epoch: 2.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04920781986524302		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.04920781986524302 | validation: 0.05433504070265613]
	TIME [epoch: 2.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050532183850959325		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.050532183850959325 | validation: 0.031475502161316886]
	TIME [epoch: 2.74 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038213147010415324		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.038213147010415324 | validation: 0.040759120093264406]
	TIME [epoch: 2.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031172336286479398		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.031172336286479398 | validation: 0.032795186635763696]
	TIME [epoch: 2.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03135633343411819		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.03135633343411819 | validation: 0.04678270495958575]
	TIME [epoch: 2.74 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03908009547859643		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.03908009547859643 | validation: 0.07216861720371007]
	TIME [epoch: 2.74 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911131416698597		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06911131416698597 | validation: 0.07469312583056947]
	TIME [epoch: 2.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908240194606755		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.07908240194606755 | validation: 0.07316482053575056]
	TIME [epoch: 2.73 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05550530612360017		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.05550530612360017 | validation: 0.03141173992455642]
	TIME [epoch: 2.74 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029223529983437566		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.029223529983437566 | validation: 0.041105380370328064]
	TIME [epoch: 2.74 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03001182263332001		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.03001182263332001 | validation: 0.033553828177794835]
	TIME [epoch: 2.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03268439890503106		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.03268439890503106 | validation: 0.03194178810282696]
	TIME [epoch: 2.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030522239255045214		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.030522239255045214 | validation: 0.03370164369900407]
	TIME [epoch: 2.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026974927020662242		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.026974927020662242 | validation: 0.02785284362629861]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026188830404391968		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.026188830404391968 | validation: 0.03441854471954651]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03078969624393486		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.03078969624393486 | validation: 0.03805621356186062]
	TIME [epoch: 2.73 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036034184468444634		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.036034184468444634 | validation: 0.042448554195675905]
	TIME [epoch: 2.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04209404392663565		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.04209404392663565 | validation: 0.045594256582832164]
	TIME [epoch: 2.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040351729979704556		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.040351729979704556 | validation: 0.032179306954756924]
	TIME [epoch: 2.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035507002456093145		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.035507002456093145 | validation: 0.03171639302384865]
	TIME [epoch: 2.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032388322194205274		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.032388322194205274 | validation: 0.04204191179026742]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033016361041372504		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.033016361041372504 | validation: 0.058578557994836994]
	TIME [epoch: 2.73 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455942823903818		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0455942823903818 | validation: 0.09429515139980638]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071879935724118		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.06071879935724118 | validation: 0.039292327773283835]
	TIME [epoch: 2.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041707095561619494		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.041707095561619494 | validation: 0.03216516539612949]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036865480698081016		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.036865480698081016 | validation: 0.048521583484087374]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04034431464051376		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.04034431464051376 | validation: 0.03491241562044258]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03795811887992294		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.03795811887992294 | validation: 0.032742631355734186]
	TIME [epoch: 2.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028955717078802064		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.028955717078802064 | validation: 0.027448913057981552]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024937513911585434		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.024937513911585434 | validation: 0.027513974107438312]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02432181402568234		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.02432181402568234 | validation: 0.03887400945979445]
	TIME [epoch: 2.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027544987474738193		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.027544987474738193 | validation: 0.0355546125122624]
	TIME [epoch: 2.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03505744484089169		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.03505744484089169 | validation: 0.0557199940463256]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047798714523792474		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.047798714523792474 | validation: 0.052418572048827196]
	TIME [epoch: 2.73 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505466031841489		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.0505466031841489 | validation: 0.04205273654072236]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04992631285154287		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.04992631285154287 | validation: 0.02491852926735635]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552105097434295		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.03552105097434295 | validation: 0.027026888289216556]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02657958035210745		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.02657958035210745 | validation: 0.027342220769240855]
	TIME [epoch: 2.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024693868538482905		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.024693868538482905 | validation: 0.027490612259896643]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023433771976050687		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.023433771976050687 | validation: 0.034467196513639047]
	TIME [epoch: 2.73 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02368616716443534		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.02368616716443534 | validation: 0.02149270509801331]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022017881529781128		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.022017881529781128 | validation: 0.033990643920466725]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023733783577976602		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.023733783577976602 | validation: 0.0285423051859106]
	TIME [epoch: 2.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024986591644662423		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.024986591644662423 | validation: 0.04538910714669278]
	TIME [epoch: 2.73 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031156125548552023		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.031156125548552023 | validation: 0.03392323381700587]
	TIME [epoch: 2.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032901816082399386		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.032901816082399386 | validation: 0.04000720852664333]
	TIME [epoch: 2.73 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026955321047295984		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.026955321047295984 | validation: 0.027352458624425036]
	TIME [epoch: 2.73 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02616672431766591		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.02616672431766591 | validation: 0.03540497499604552]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035554468983409845		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.035554468983409845 | validation: 0.055702568760573]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05751780989010699		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.05751780989010699 | validation: 0.041642873818074536]
	TIME [epoch: 2.73 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04749050009150526		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.04749050009150526 | validation: 0.034865363020299814]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028319415806864742		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.028319415806864742 | validation: 0.03238621254176013]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023698225574968795		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.023698225574968795 | validation: 0.023290992582054384]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024160816661377454		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.024160816661377454 | validation: 0.033569105089930544]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024536696581769435		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.024536696581769435 | validation: 0.023284932448129625]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028053625117546416		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.028053625117546416 | validation: 0.035978176149755715]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322370184102265		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.0322370184102265 | validation: 0.05086358961289139]
	TIME [epoch: 2.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05470016786141698		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.05470016786141698 | validation: 0.10516750158515703]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08647439087389427		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.08647439087389427 | validation: 0.03145455719829245]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03026879274592526		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.03026879274592526 | validation: 0.032197516083821924]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02530234024487645		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.02530234024487645 | validation: 0.03251793401601233]
	TIME [epoch: 2.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027052619884781755		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.027052619884781755 | validation: 0.02498001610987101]
	TIME [epoch: 2.73 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0232286389392319		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.0232286389392319 | validation: 0.028467225826897494]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024445996541103306		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.024445996541103306 | validation: 0.025777250966643586]
	TIME [epoch: 2.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026784969684637405		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.026784969684637405 | validation: 0.0324389943167222]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434314327784193		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.03434314327784193 | validation: 0.03714653190589673]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031313025407324675		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.031313025407324675 | validation: 0.025453883425145055]
	TIME [epoch: 2.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026583934263994947		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.026583934263994947 | validation: 0.030686379751021654]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022155881850966272		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.022155881850966272 | validation: 0.02548550850519791]
	TIME [epoch: 2.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022836793619452678		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.022836793619452678 | validation: 0.040855068147523425]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027298967970711515		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.027298967970711515 | validation: 0.04354330477486739]
	TIME [epoch: 2.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03412148708957919		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.03412148708957919 | validation: 0.045547196356254795]
	TIME [epoch: 2.73 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03231630550673352		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.03231630550673352 | validation: 0.02729288476806553]
	TIME [epoch: 2.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030740719272367746		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.030740719272367746 | validation: 0.03404734322839197]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04033450175934895		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04033450175934895 | validation: 0.03485486169327086]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04172369395257305		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.04172369395257305 | validation: 0.030700430561976945]
	TIME [epoch: 2.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02964204994560567		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.02964204994560567 | validation: 0.027647510618590422]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021935795916246042		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.021935795916246042 | validation: 0.02223566647841252]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02224687282518933		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.02224687282518933 | validation: 0.02800378702550991]
	TIME [epoch: 2.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021247923099988984		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.021247923099988984 | validation: 0.01999623899741021]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01953540481931172		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.01953540481931172 | validation: 0.027576446865363227]
	TIME [epoch: 2.73 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020414638839806817		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.020414638839806817 | validation: 0.023040677323365434]
	TIME [epoch: 2.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021778442493690396		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.021778442493690396 | validation: 0.025505876993263055]
	TIME [epoch: 2.73 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024889103333663894		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.024889103333663894 | validation: 0.04541415349312841]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501369620295685		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.03501369620295685 | validation: 0.05581090779359782]
	TIME [epoch: 2.74 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045109950478882804		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.045109950478882804 | validation: 0.03771033750308204]
	TIME [epoch: 2.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578119898125629		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.03578119898125629 | validation: 0.024435018158415922]
	TIME [epoch: 2.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02330489053819733		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.02330489053819733 | validation: 0.01884991815491456]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02013486610495271		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.02013486610495271 | validation: 0.021773971191954233]
	TIME [epoch: 2.73 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018892796082750518		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.018892796082750518 | validation: 0.027131603874755206]
	TIME [epoch: 2.73 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0219773484446773		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.0219773484446773 | validation: 0.02482665801647507]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025298947723806944		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.025298947723806944 | validation: 0.03908159783977826]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256688717064865		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.03256688717064865 | validation: 0.04573689897296676]
	TIME [epoch: 2.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05085262259987378		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.05085262259987378 | validation: 0.05350643346867186]
	TIME [epoch: 2.73 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544484992367267		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05544484992367267 | validation: 0.030497408042045916]
	TIME [epoch: 2.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029091544740465602		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.029091544740465602 | validation: 0.019779255366348038]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020184515858872466		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.020184515858872466 | validation: 0.02590068908400315]
	TIME [epoch: 2.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020793661978445514		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.020793661978445514 | validation: 0.02125496296747681]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022632957570922284		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.022632957570922284 | validation: 0.023183750093423508]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022265583866332345		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.022265583866332345 | validation: 0.020259465332524176]
	TIME [epoch: 2.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02073794985680988		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.02073794985680988 | validation: 0.02226127371440664]
	TIME [epoch: 2.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02174239915412847		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.02174239915412847 | validation: 0.023691994381794413]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02121476911595688		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.02121476911595688 | validation: 0.02883263447838651]
	TIME [epoch: 2.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02598674131986536		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.02598674131986536 | validation: 0.034645731715133146]
	TIME [epoch: 2.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029403418621246652		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.029403418621246652 | validation: 0.035348709515484855]
	TIME [epoch: 2.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03283012263194122		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.03283012263194122 | validation: 0.02561463524988047]
	TIME [epoch: 2.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031119767016501983		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.031119767016501983 | validation: 0.0276469678729934]
	TIME [epoch: 2.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029431652362757157		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.029431652362757157 | validation: 0.027709641281525768]
	TIME [epoch: 2.73 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02254176292585546		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.02254176292585546 | validation: 0.02054452831852003]
	TIME [epoch: 2.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022098778114492026		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.022098778114492026 | validation: 0.02983642931669213]
	TIME [epoch: 2.73 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021870523178008482		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.021870523178008482 | validation: 0.02142736311939869]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021905980519966475		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.021905980519966475 | validation: 0.026720935055644258]
	TIME [epoch: 2.73 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02048849010027396		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.02048849010027396 | validation: 0.018822926249699057]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020892339734845937		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.020892339734845937 | validation: 0.02633109816948992]
	TIME [epoch: 2.74 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025335185797498203		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.025335185797498203 | validation: 0.034914008828194364]
	TIME [epoch: 2.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031872964552401066		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.031872964552401066 | validation: 0.05187068506526496]
	TIME [epoch: 2.74 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050744144985080536		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.050744144985080536 | validation: 0.03242042251874359]
	TIME [epoch: 2.74 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03998461154288014		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.03998461154288014 | validation: 0.02382940986263229]
	TIME [epoch: 2.73 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02276873487699354		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.02276873487699354 | validation: 0.02232498366972703]
	TIME [epoch: 2.73 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01770545523681087		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.01770545523681087 | validation: 0.021872777323035355]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01861767085391413		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.01861767085391413 | validation: 0.02164466299546035]
	TIME [epoch: 2.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02012332250418108		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.02012332250418108 | validation: 0.021788269891708036]
	TIME [epoch: 2.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018992342912213977		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.018992342912213977 | validation: 0.02328189040895672]
	TIME [epoch: 2.73 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019142148085720682		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.019142148085720682 | validation: 0.019198326703632695]
	TIME [epoch: 2.73 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019381834276980904		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.019381834276980904 | validation: 0.02079207551987915]
	TIME [epoch: 2.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020387990042535834		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.020387990042535834 | validation: 0.021560956915857445]
	TIME [epoch: 2.73 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019616220968405495		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.019616220968405495 | validation: 0.033125095889960834]
	TIME [epoch: 2.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021997409710614195		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.021997409710614195 | validation: 0.02384775568238622]
	TIME [epoch: 2.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02721917450392706		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.02721917450392706 | validation: 0.03803418122960261]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034484391071923484		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.034484391071923484 | validation: 0.03291765782881711]
	TIME [epoch: 2.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04128152878636472		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.04128152878636472 | validation: 0.03561790615889714]
	TIME [epoch: 2.73 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888824225251164		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.03888824225251164 | validation: 0.03377928421320349]
	TIME [epoch: 2.73 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02383996707114552		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.02383996707114552 | validation: 0.017094706279881668]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017191679419453272		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.017191679419453272 | validation: 0.020244232469193546]
	TIME [epoch: 2.74 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016881885722666964		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.016881885722666964 | validation: 0.022939618484538384]
	TIME [epoch: 2.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01847198890496008		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.01847198890496008 | validation: 0.021147660890511358]
	TIME [epoch: 2.74 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020832859853668947		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.020832859853668947 | validation: 0.027585248964636433]
	TIME [epoch: 2.74 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019683464689167835		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.019683464689167835 | validation: 0.022802542221619527]
	TIME [epoch: 2.75 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02164392154177536		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.02164392154177536 | validation: 0.020984071673950656]
	TIME [epoch: 2.74 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01989142908567706		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.01989142908567706 | validation: 0.02854282069679557]
	TIME [epoch: 2.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021534028739939617		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.021534028739939617 | validation: 0.024783845782923853]
	TIME [epoch: 2.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026888840557740892		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.026888840557740892 | validation: 0.03726635156539406]
	TIME [epoch: 2.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030598981202921642		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.030598981202921642 | validation: 0.030175604666410505]
	TIME [epoch: 2.73 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030899137557065956		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.030899137557065956 | validation: 0.02769470787750592]
	TIME [epoch: 2.75 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023642449056640303		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.023642449056640303 | validation: 0.021413241627299563]
	TIME [epoch: 2.74 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02062128386102466		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.02062128386102466 | validation: 0.018023290137270376]
	TIME [epoch: 2.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018342730587385944		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.018342730587385944 | validation: 0.021411668822830354]
	TIME [epoch: 2.74 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017169815843938528		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.017169815843938528 | validation: 0.018995900513469832]
	TIME [epoch: 2.74 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017626207456340744		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.017626207456340744 | validation: 0.02101493139148999]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018995665540982313		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.018995665540982313 | validation: 0.025795341140069775]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019657948640039804		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.019657948640039804 | validation: 0.019809969285576358]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02106041227736065		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.02106041227736065 | validation: 0.02608724528098414]
	TIME [epoch: 2.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019709921449759513		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.019709921449759513 | validation: 0.018977384260730146]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02246915868772952		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.02246915868772952 | validation: 0.023627485183924782]
	TIME [epoch: 2.73 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023535979318974498		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.023535979318974498 | validation: 0.018794998775454864]
	TIME [epoch: 2.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024052687075185607		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.024052687075185607 | validation: 0.024745464417718965]
	TIME [epoch: 2.73 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02121438252910397		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.02121438252910397 | validation: 0.02579103719216709]
	TIME [epoch: 2.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018479716128266814		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.018479716128266814 | validation: 0.02146210207166027]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0234283591670829		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.0234283591670829 | validation: 0.03229164323319652]
	TIME [epoch: 2.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026377874659174214		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.026377874659174214 | validation: 0.02246438648323701]
	TIME [epoch: 2.74 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023777058535204756		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.023777058535204756 | validation: 0.02450869478954162]
	TIME [epoch: 2.73 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027672579619906684		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.027672579619906684 | validation: 0.0359652634885728]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027938609587132037		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.027938609587132037 | validation: 0.028406554519284245]
	TIME [epoch: 2.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023061557361352923		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.023061557361352923 | validation: 0.02082449190775656]
	TIME [epoch: 2.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01957734585399283		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.01957734585399283 | validation: 0.01845732409473202]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015315812745592962		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.015315812745592962 | validation: 0.01968572099092556]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016459885813527968		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.016459885813527968 | validation: 0.022720986589964954]
	TIME [epoch: 2.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018134651306171205		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.018134651306171205 | validation: 0.02411640851059369]
	TIME [epoch: 2.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019832733738885184		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.019832733738885184 | validation: 0.019571993143214186]
	TIME [epoch: 2.73 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020532503883651174		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.020532503883651174 | validation: 0.023042904217403862]
	TIME [epoch: 2.73 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026693728380375408		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.026693728380375408 | validation: 0.030389636644370034]
	TIME [epoch: 2.74 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03610341640746683		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.03610341640746683 | validation: 0.02340116438357801]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024961851904311803		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.024961851904311803 | validation: 0.017492262276937943]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017442611976815597		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.017442611976815597 | validation: 0.020078556281904626]
	TIME [epoch: 2.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016913306711230335		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.016913306711230335 | validation: 0.015163786299522431]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016697819735312362		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.016697819735312362 | validation: 0.01757465372736147]
	TIME [epoch: 2.74 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015831220420718047		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.015831220420718047 | validation: 0.017602144330177493]
	TIME [epoch: 2.74 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015277081474524341		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.015277081474524341 | validation: 0.0132809154308418]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017388404934881096		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.017388404934881096 | validation: 0.026628365868876538]
	TIME [epoch: 2.72 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020831402699485992		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.020831402699485992 | validation: 0.027760533104584553]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030431939684688176		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.030431939684688176 | validation: 0.028250579655740363]
	TIME [epoch: 2.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024901944987321203		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.024901944987321203 | validation: 0.02328308638603982]
	TIME [epoch: 2.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0200114436173928		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.0200114436173928 | validation: 0.014911861796384452]
	TIME [epoch: 2.74 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02003762901034932		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.02003762901034932 | validation: 0.02382179987228791]
	TIME [epoch: 2.74 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019936961832718064		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.019936961832718064 | validation: 0.013591731274247777]
	TIME [epoch: 2.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01745811351610163		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.01745811351610163 | validation: 0.02122028974205158]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016152467070401275		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.016152467070401275 | validation: 0.013537965349476523]
	TIME [epoch: 2.74 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015756085415922584		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.015756085415922584 | validation: 0.01935052844326245]
	TIME [epoch: 2.74 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016672239761602342		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.016672239761602342 | validation: 0.02098528229477622]
	TIME [epoch: 2.74 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016810154887327903		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.016810154887327903 | validation: 0.02075614204084042]
	TIME [epoch: 2.74 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018382788480814156		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.018382788480814156 | validation: 0.02615431175413622]
	TIME [epoch: 2.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02392341922778371		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.02392341922778371 | validation: 0.03065168760732122]
	TIME [epoch: 2.74 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03346696980116924		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03346696980116924 | validation: 0.018375036803433022]
	TIME [epoch: 2.74 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026529204647863933		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.026529204647863933 | validation: 0.016843015405958495]
	TIME [epoch: 2.74 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016344912860462822		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.016344912860462822 | validation: 0.01449755086075949]
	TIME [epoch: 2.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016121309910701927		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.016121309910701927 | validation: 0.017523637676604753]
	TIME [epoch: 2.74 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018108572077305846		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.018108572077305846 | validation: 0.016516435342354984]
	TIME [epoch: 2.74 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018000984657495698		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.018000984657495698 | validation: 0.02498051960285218]
	TIME [epoch: 2.74 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017872441382483433		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.017872441382483433 | validation: 0.015612289511166999]
	TIME [epoch: 2.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015701946058698627		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.015701946058698627 | validation: 0.019877998453673718]
	TIME [epoch: 2.74 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014560645965473601		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.014560645965473601 | validation: 0.015365675761787767]
	TIME [epoch: 2.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016686200557494542		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.016686200557494542 | validation: 0.025187196341390897]
	TIME [epoch: 2.74 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020715172690145556		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.020715172690145556 | validation: 0.03404878164428387]
	TIME [epoch: 2.74 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028917684640949855		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.028917684640949855 | validation: 0.030333815416616595]
	TIME [epoch: 2.74 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026224300029639613		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.026224300029639613 | validation: 0.01887234215256768]
	TIME [epoch: 2.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021451478331436125		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.021451478331436125 | validation: 0.016933399956342254]
	TIME [epoch: 2.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01831877489776704		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.01831877489776704 | validation: 0.020686529394012887]
	TIME [epoch: 2.74 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015285953915384926		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.015285953915384926 | validation: 0.016694236516028416]
	TIME [epoch: 2.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015065110881628947		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.015065110881628947 | validation: 0.01445696498255602]
	TIME [epoch: 2.74 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016946318880323327		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.016946318880323327 | validation: 0.016693653507384967]
	TIME [epoch: 2.74 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01697608590376999		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.01697608590376999 | validation: 0.018471274227434276]
	TIME [epoch: 2.74 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01817252645159674		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.01817252645159674 | validation: 0.01883757660361192]
	TIME [epoch: 2.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017187803285647146		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.017187803285647146 | validation: 0.020028882251197578]
	TIME [epoch: 2.75 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015441570435075179		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.015441570435075179 | validation: 0.01821820427328552]
	TIME [epoch: 2.74 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01777382718163748		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.01777382718163748 | validation: 0.02085554844215296]
	TIME [epoch: 2.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01902117966643164		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.01902117966643164 | validation: 0.022903958230276113]
	TIME [epoch: 2.72 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018223580216251662		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.018223580216251662 | validation: 0.01779669828505359]
	TIME [epoch: 2.72 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019954811129873202		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.019954811129873202 | validation: 0.02155324017621875]
	TIME [epoch: 2.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021773021444193973		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.021773021444193973 | validation: 0.01878868690514356]
	TIME [epoch: 2.72 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021974268181318354		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.021974268181318354 | validation: 0.016366438847280142]
	TIME [epoch: 2.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018393728007998912		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.018393728007998912 | validation: 0.019883142911595864]
	TIME [epoch: 2.72 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01674198941630395		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.01674198941630395 | validation: 0.012054319335272446]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017465245300983014		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.017465245300983014 | validation: 0.01788418634645852]
	TIME [epoch: 2.74 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014460037994118108		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.014460037994118108 | validation: 0.015347964140544824]
	TIME [epoch: 2.75 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014224612591908476		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.014224612591908476 | validation: 0.0133268470141386]
	TIME [epoch: 2.74 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012948530019846579		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.012948530019846579 | validation: 0.01805693335840869]
	TIME [epoch: 2.74 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01361831032374376		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.01361831032374376 | validation: 0.01496790608082893]
	TIME [epoch: 2.74 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017345687701550955		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.017345687701550955 | validation: 0.017574886478058927]
	TIME [epoch: 2.74 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019414874500045365		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.019414874500045365 | validation: 0.01762354683368064]
	TIME [epoch: 2.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024314200021998654		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.024314200021998654 | validation: 0.023834826535710042]
	TIME [epoch: 2.74 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02623023171831819		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.02623023171831819 | validation: 0.03670421941104207]
	TIME [epoch: 2.74 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032898991019473744		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.032898991019473744 | validation: 0.021576733932075112]
	TIME [epoch: 2.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020697240904871527		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.020697240904871527 | validation: 0.02124314745472973]
	TIME [epoch: 2.74 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015472170838732424		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.015472170838732424 | validation: 0.01274389258693498]
	TIME [epoch: 2.74 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01820628956646659		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.01820628956646659 | validation: 0.018661785586355905]
	TIME [epoch: 2.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014974091840497398		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.014974091840497398 | validation: 0.019082837352953907]
	TIME [epoch: 2.74 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014226825674053622		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.014226825674053622 | validation: 0.010891506247689565]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_894.pth
	Model improved!!!
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014545016443328802		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.014545016443328802 | validation: 0.014712271174498338]
	TIME [epoch: 2.74 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015194409337366334		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.015194409337366334 | validation: 0.018459129544716114]
	TIME [epoch: 2.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013627181315743449		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.013627181315743449 | validation: 0.01739397238991436]
	TIME [epoch: 2.72 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01377075620768828		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.01377075620768828 | validation: 0.015971725548542894]
	TIME [epoch: 2.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015643756676889727		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.015643756676889727 | validation: 0.013121331224012678]
	TIME [epoch: 2.72 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019153268376461342		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.019153268376461342 | validation: 0.019799198661538776]
	TIME [epoch: 2.72 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02335248515123757		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.02335248515123757 | validation: 0.020588619023626988]
	TIME [epoch: 2.73 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022102986879940164		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.022102986879940164 | validation: 0.01738803344043337]
	TIME [epoch: 2.74 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017070199120392246		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.017070199120392246 | validation: 0.024640765415016974]
	TIME [epoch: 2.73 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01855024652593357		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.01855024652593357 | validation: 0.01665963344075565]
	TIME [epoch: 2.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016243269008690886		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.016243269008690886 | validation: 0.01777595077653529]
	TIME [epoch: 2.73 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013892890079781657		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.013892890079781657 | validation: 0.015577078842135572]
	TIME [epoch: 2.73 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013921247026772016		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.013921247026772016 | validation: 0.011960536927978294]
	TIME [epoch: 2.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014572793435028231		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.014572793435028231 | validation: 0.022039885483201696]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014515747228037987		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.014515747228037987 | validation: 0.013730767620781638]
	TIME [epoch: 2.73 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015304135192919852		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.015304135192919852 | validation: 0.018611667988962178]
	TIME [epoch: 2.73 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01473701104695672		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.01473701104695672 | validation: 0.018201168840199313]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018297347800854354		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.018297347800854354 | validation: 0.01658780901542134]
	TIME [epoch: 2.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020569711500647927		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.020569711500647927 | validation: 0.023147036731169936]
	TIME [epoch: 2.74 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022432465509288912		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.022432465509288912 | validation: 0.019040752884623438]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020368272741242772		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.020368272741242772 | validation: 0.01772470816774856]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016603079178056143		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.016603079178056143 | validation: 0.011839513158215464]
	TIME [epoch: 2.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013956162443366137		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.013956162443366137 | validation: 0.013665884431699006]
	TIME [epoch: 2.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014119996380227958		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.014119996380227958 | validation: 0.0154978764350721]
	TIME [epoch: 2.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014434707435202478		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.014434707435202478 | validation: 0.013117020962704829]
	TIME [epoch: 2.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014410526555214291		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.014410526555214291 | validation: 0.01835075526518768]
	TIME [epoch: 2.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01540984352449399		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.01540984352449399 | validation: 0.019698007094244407]
	TIME [epoch: 2.73 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017665658464476222		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.017665658464476222 | validation: 0.019627267499761104]
	TIME [epoch: 2.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019180523864964813		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.019180523864964813 | validation: 0.022971983101857874]
	TIME [epoch: 2.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018813977947770413		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.018813977947770413 | validation: 0.012662771942935448]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0137856251484784		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.0137856251484784 | validation: 0.012502949846374978]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013885664794871648		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.013885664794871648 | validation: 0.020341804062194047]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014896736231682932		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.014896736231682932 | validation: 0.014344921252756649]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016037371820539187		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.016037371820539187 | validation: 0.021937234535811603]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015756082640063677		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.015756082640063677 | validation: 0.012411120512493191]
	TIME [epoch: 2.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015765891408774015		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.015765891408774015 | validation: 0.0190982497580524]
	TIME [epoch: 2.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0147579470068419		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.0147579470068419 | validation: 0.017439669906880173]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013523640433864293		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.013523640433864293 | validation: 0.016027442575797714]
	TIME [epoch: 2.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01779131138407556		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.01779131138407556 | validation: 0.0259121318743476]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028010021541766773		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.028010021541766773 | validation: 0.024317784152429568]
	TIME [epoch: 2.72 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02501630428258537		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.02501630428258537 | validation: 0.013287287754838019]
	TIME [epoch: 2.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014866241379867886		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.014866241379867886 | validation: 0.018079225038508753]
	TIME [epoch: 2.72 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015442413903364868		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.015442413903364868 | validation: 0.01552208376499642]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018620807169887933		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.018620807169887933 | validation: 0.016082941547983676]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01603052588639752		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.01603052588639752 | validation: 0.016597372877562213]
	TIME [epoch: 2.72 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013965063422960902		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.013965063422960902 | validation: 0.011440414156886014]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012120905585267072		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.012120905585267072 | validation: 0.014698975194047826]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012330186551017997		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.012330186551017997 | validation: 0.018060859119578634]
	TIME [epoch: 2.72 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015459723790288388		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.015459723790288388 | validation: 0.01796583466434638]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017796101966808184		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.017796101966808184 | validation: 0.019544127831246916]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014802268951607425		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.014802268951607425 | validation: 0.013720067997282703]
	TIME [epoch: 2.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01348384270069864		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.01348384270069864 | validation: 0.014411995752548779]
	TIME [epoch: 2.74 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013767315363133975		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.013767315363133975 | validation: 0.012684371814100526]
	TIME [epoch: 2.73 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01269360108798526		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.01269360108798526 | validation: 0.015451340300388583]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013736950104114017		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.013736950104114017 | validation: 0.014132951299428832]
	TIME [epoch: 2.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014801084739085778		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.014801084739085778 | validation: 0.023922230897866627]
	TIME [epoch: 2.73 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016429971354982274		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.016429971354982274 | validation: 0.017813602185551003]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015663961704862134		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.015663961704862134 | validation: 0.014516528086544744]
	TIME [epoch: 2.74 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015912079418885047		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.015912079418885047 | validation: 0.02094856587753372]
	TIME [epoch: 2.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01766818777619501		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.01766818777619501 | validation: 0.013716637581244341]
	TIME [epoch: 2.73 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016445470939067685		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.016445470939067685 | validation: 0.017354644375253248]
	TIME [epoch: 2.74 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01393491517516356		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.01393491517516356 | validation: 0.014984707531980813]
	TIME [epoch: 2.75 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014527281087450064		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.014527281087450064 | validation: 0.015941352571661748]
	TIME [epoch: 2.75 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014817142119369964		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.014817142119369964 | validation: 0.016546717455812432]
	TIME [epoch: 2.74 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015000983983542545		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.015000983983542545 | validation: 0.014485551304968558]
	TIME [epoch: 2.74 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01681981682310033		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.01681981682310033 | validation: 0.01471560356208217]
	TIME [epoch: 2.74 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015183159827218674		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.015183159827218674 | validation: 0.013039454247272398]
	TIME [epoch: 2.74 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01389788988885072		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.01389788988885072 | validation: 0.01630193753884083]
	TIME [epoch: 2.75 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013861941105186876		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.013861941105186876 | validation: 0.012716307942431837]
	TIME [epoch: 2.75 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01324291459465738		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.01324291459465738 | validation: 0.01463813485251826]
	TIME [epoch: 2.75 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013243437516597786		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.013243437516597786 | validation: 0.0137117437791077]
	TIME [epoch: 2.75 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013317976743686405		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.013317976743686405 | validation: 0.014546102398145378]
	TIME [epoch: 2.75 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014292463119581414		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.014292463119581414 | validation: 0.014892736013462915]
	TIME [epoch: 2.75 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01563539826366252		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.01563539826366252 | validation: 0.014192968287869546]
	TIME [epoch: 2.75 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015431496911507537		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.015431496911507537 | validation: 0.0169059938898944]
	TIME [epoch: 2.74 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01616809503006063		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.01616809503006063 | validation: 0.021183061392761215]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018204174337543996		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.018204174337543996 | validation: 0.016538312795718036]
	TIME [epoch: 2.73 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019128071229356534		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.019128071229356534 | validation: 0.015916560846578276]
	TIME [epoch: 2.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014652217324393898		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.014652217324393898 | validation: 0.013560796101815542]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012445923408943878		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.012445923408943878 | validation: 0.01383969872861145]
	TIME [epoch: 2.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013843191820046657		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.013843191820046657 | validation: 0.0130026487647117]
	TIME [epoch: 2.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011962882346887341		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.011962882346887341 | validation: 0.011985342249231491]
	TIME [epoch: 2.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013950862043970566		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.013950862043970566 | validation: 0.013423005137906186]
	TIME [epoch: 2.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014927661056662198		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.014927661056662198 | validation: 0.011030085535767988]
	TIME [epoch: 2.74 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015214955463617563		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.015214955463617563 | validation: 0.014884105111201152]
	TIME [epoch: 2.74 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015426318833500994		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.015426318833500994 | validation: 0.01470504194361707]
	TIME [epoch: 2.73 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014558944730739714		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.014558944730739714 | validation: 0.01665105210177964]
	TIME [epoch: 2.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01346007326326951		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.01346007326326951 | validation: 0.012149682091395176]
	TIME [epoch: 2.73 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014917310509520982		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.014917310509520982 | validation: 0.015006009546344946]
	TIME [epoch: 2.73 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013808203179089375		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.013808203179089375 | validation: 0.014416589949904425]
	TIME [epoch: 2.73 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014169555300767139		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.014169555300767139 | validation: 0.014337724218041048]
	TIME [epoch: 2.73 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01322607275480077		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.01322607275480077 | validation: 0.016663173879014205]
	TIME [epoch: 2.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015491207481975274		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.015491207481975274 | validation: 0.013846728670299359]
	TIME [epoch: 2.74 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015920175433589655		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.015920175433589655 | validation: 0.018701365052620034]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016448235694188806		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.016448235694188806 | validation: 0.01870467777107455]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01601865865836384		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.01601865865836384 | validation: 0.012518829977238854]
	TIME [epoch: 2.73 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015349982748677231		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.015349982748677231 | validation: 0.011609369837930173]
	TIME [epoch: 2.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014949281544963318		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.014949281544963318 | validation: 0.017296117868928262]
	TIME [epoch: 2.73 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015722188924524438		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.015722188924524438 | validation: 0.014377764259088432]
	TIME [epoch: 2.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013017780498963299		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.013017780498963299 | validation: 0.013856776333716105]
	TIME [epoch: 2.73 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014215345671334376		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.014215345671334376 | validation: 0.013026800687213425]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_0_v_mmd4_20250518_223001/states/model_phi1_4a_distortion_v1_0_v_mmd4_995.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2528.465 seconds.
