Args:
Namespace(name='model_phi1_4a_distortion_v1r_1_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.07471651, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2241522107

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.560056953353916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.560056953353916 | validation: 7.756984212414384]
	TIME [epoch: 191 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.3714015968286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3714015968286 | validation: 7.6061378333765335]
	TIME [epoch: 0.808 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.754463801946415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.754463801946415 | validation: 7.330367838061225]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.836342761209732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.836342761209732 | validation: 7.014578174859185]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.659810513163093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.659810513163093 | validation: 6.684369291590373]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.3743428634667865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3743428634667865 | validation: 6.338112620420311]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.239826034238158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.239826034238158 | validation: 5.528583607080565]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.763514685518508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.763514685518508 | validation: 4.330991066849089]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.3674083561962505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3674083561962505 | validation: 7.825179188151944]
	TIME [epoch: 0.707 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.614298139558573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.614298139558573 | validation: 7.769101799262157]
	TIME [epoch: 0.705 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.505818327733448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.505818327733448 | validation: 7.309500999689522]
	TIME [epoch: 0.707 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.586903124323548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.586903124323548 | validation: 4.614592418332605]
	TIME [epoch: 0.725 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.186584880277798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.186584880277798 | validation: 4.40297142351677]
	TIME [epoch: 0.705 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.120040805693082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.120040805693082 | validation: 3.248100818014838]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.317901035284197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.317901035284197 | validation: 3.243175436020218]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.02799691609794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.02799691609794 | validation: 6.706010558346126]
	TIME [epoch: 0.708 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7967450469271125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7967450469271125 | validation: 7.518611408209939]
	TIME [epoch: 0.708 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.018474879941179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.018474879941179 | validation: 7.462667036464899]
	TIME [epoch: 0.709 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.936123326809438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.936123326809438 | validation: 7.112669282144712]
	TIME [epoch: 0.708 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.217434621316158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.217434621316158 | validation: 5.229584795107494]
	TIME [epoch: 0.707 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.944904278883753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.944904278883753 | validation: 2.898896956727759]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.273073735677485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.273073735677485 | validation: 3.3616725463718]
	TIME [epoch: 0.709 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.791918756305799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.791918756305799 | validation: 5.68373656376257]
	TIME [epoch: 0.706 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.885939989394486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.885939989394486 | validation: 5.796070301083017]
	TIME [epoch: 0.704 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.365611805343596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.365611805343596 | validation: 4.920868386099688]
	TIME [epoch: 0.706 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.279584663626988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.279584663626988 | validation: 1.619621196903001]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4399206069512376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4399206069512376 | validation: 3.488169216728126]
	TIME [epoch: 0.711 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3539581318034837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3539581318034837 | validation: 2.2292425683572588]
	TIME [epoch: 0.746 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4644570130718653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4644570130718653 | validation: 4.711415438259202]
	TIME [epoch: 0.711 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.262291925331517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.262291925331517 | validation: 3.2605042485720546]
	TIME [epoch: 0.705 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.871053225881431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.871053225881431 | validation: 6.617840264406474]
	TIME [epoch: 0.704 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.558097251838415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.558097251838415 | validation: 4.643102566567889]
	TIME [epoch: 0.704 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.443195582393909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.443195582393909 | validation: 2.9389669662629867]
	TIME [epoch: 0.705 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6001498804239738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6001498804239738 | validation: 1.824040447501625]
	TIME [epoch: 0.704 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9853897434750767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9853897434750767 | validation: 6.449773428064737]
	TIME [epoch: 0.755 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.381331273828132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.381331273828132 | validation: 4.051439954229902]
	TIME [epoch: 0.704 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6868616055310146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6868616055310146 | validation: 2.676647289466891]
	TIME [epoch: 0.704 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.58255117189962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.58255117189962 | validation: 1.684005278025344]
	TIME [epoch: 0.708 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3823022177468727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3823022177468727 | validation: 4.69296472480526]
	TIME [epoch: 0.706 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.287280875911438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.287280875911438 | validation: 2.691069039851174]
	TIME [epoch: 0.704 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4674661662926676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4674661662926676 | validation: 2.927946722630633]
	TIME [epoch: 0.705 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.067964801706731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.067964801706731 | validation: 2.6352634623370648]
	TIME [epoch: 0.705 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1950653701130056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1950653701130056 | validation: 3.2774230034667315]
	TIME [epoch: 0.705 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8504638457067615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8504638457067615 | validation: 2.253508672588843]
	TIME [epoch: 0.705 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.491195726957183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.491195726957183 | validation: 2.0112090132416127]
	TIME [epoch: 0.705 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9412054198545667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9412054198545667 | validation: 2.345306309613059]
	TIME [epoch: 0.708 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2039529200930232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2039529200930232 | validation: 1.8635632702273677]
	TIME [epoch: 0.71 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9937776749445855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9937776749445855 | validation: 3.564579639562588]
	TIME [epoch: 0.706 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4264261913933405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4264261913933405 | validation: 1.5770276809360952]
	TIME [epoch: 0.832 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8233014062975634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8233014062975634 | validation: 1.959367754732024]
	TIME [epoch: 0.71 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.300623846104069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.300623846104069 | validation: 2.6813095024626805]
	TIME [epoch: 0.709 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4071170021685977		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.4071170021685977 | validation: 1.7166904344270038]
	TIME [epoch: 0.707 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4414592589584347		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.4414592589584347 | validation: 2.140733904381328]
	TIME [epoch: 0.708 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9824729117598496		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.9824729117598496 | validation: 1.3660098978714412]
	TIME [epoch: 0.718 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2251470651065466		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.2251470651065466 | validation: 1.8959175444951275]
	TIME [epoch: 0.711 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8137259137801571		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.8137259137801571 | validation: 1.3298616017682294]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.169815233163662		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.169815233163662 | validation: 2.138014602974025]
	TIME [epoch: 0.71 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9625640748092046		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.9625640748092046 | validation: 1.2199859494562018]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.028964743133152		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.028964743133152 | validation: 2.0984340460410738]
	TIME [epoch: 0.712 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9090916263444428		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.9090916263444428 | validation: 1.138589269996918]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.947127179716663		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.947127179716663 | validation: 1.9297346003933133]
	TIME [epoch: 0.712 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.765560025977975		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.765560025977975 | validation: 1.1991996233613311]
	TIME [epoch: 0.71 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.99111313787947		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.99111313787947 | validation: 1.9328062828330446]
	TIME [epoch: 0.783 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.745760169954319		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.745760169954319 | validation: 1.139084417770947]
	TIME [epoch: 0.711 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9109328356095177		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.9109328356095177 | validation: 1.9458211853091247]
	TIME [epoch: 0.711 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7337397918372066		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.7337397918372066 | validation: 1.0884913699840562]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8194974487026432		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.8194974487026432 | validation: 1.8744223151355914]
	TIME [epoch: 0.712 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6723463163512213		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.6723463163512213 | validation: 1.0498915768437063]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7788510097479742		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.7788510097479742 | validation: 1.884636958760028]
	TIME [epoch: 0.8 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.649408794696778		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.649408794696778 | validation: 1.110651699941118]
	TIME [epoch: 0.708 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8022934608226984		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8022934608226984 | validation: 1.840782767590621]
	TIME [epoch: 0.706 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7380738983011168		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.7380738983011168 | validation: 1.3532730163435989]
	TIME [epoch: 0.707 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9074470372989174		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.9074470372989174 | validation: 1.6163761980131683]
	TIME [epoch: 0.708 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5400269691534265		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.5400269691534265 | validation: 0.9992112470456729]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6731649737754886		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.6731649737754886 | validation: 1.882290491807628]
	TIME [epoch: 0.71 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6487209522227761		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.6487209522227761 | validation: 0.9834090638754861]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6801022050268086		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.6801022050268086 | validation: 1.673678040796558]
	TIME [epoch: 0.71 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5673387487328074		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.5673387487328074 | validation: 1.148677149696449]
	TIME [epoch: 0.708 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7220209000984676		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.7220209000984676 | validation: 1.7074566883902849]
	TIME [epoch: 0.707 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6136474768626337		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.6136474768626337 | validation: 1.0762755397361468]
	TIME [epoch: 0.709 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6623091987055998		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.6623091987055998 | validation: 1.5912937340938442]
	TIME [epoch: 0.707 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5297610576253835		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.5297610576253835 | validation: 0.9861316970284485]
	TIME [epoch: 0.707 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6529666027707524		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.6529666027707524 | validation: 1.6307536122779227]
	TIME [epoch: 0.706 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5174442934965762		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.5174442934965762 | validation: 0.9684903433835393]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5762445458658545		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.5762445458658545 | validation: 1.5536579322813155]
	TIME [epoch: 0.709 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5017596700145475		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.5017596700145475 | validation: 1.032652175849021]
	TIME [epoch: 0.709 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5996616278626925		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.5996616278626925 | validation: 1.6036222669581008]
	TIME [epoch: 0.707 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5336715872351354		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.5336715872351354 | validation: 0.992796355299128]
	TIME [epoch: 0.707 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.584977009468201		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.584977009468201 | validation: 1.5391624259414707]
	TIME [epoch: 0.708 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4818371879683259		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.4818371879683259 | validation: 0.9218812764302673]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.540575740977376		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.540575740977376 | validation: 1.6123925342910335]
	TIME [epoch: 0.709 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4900320726064602		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.4900320726064602 | validation: 0.9326220349745697]
	TIME [epoch: 0.711 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5299885143461192		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.5299885143461192 | validation: 1.5794343891345155]
	TIME [epoch: 0.707 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4724499141558414		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4724499141558414 | validation: 0.96950465237269]
	TIME [epoch: 0.707 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5695233881580561		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.5695233881580561 | validation: 1.8249367847638156]
	TIME [epoch: 0.706 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.65928251731016		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.65928251731016 | validation: 0.9180026572119488]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4455227334190643		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.4455227334190643 | validation: 1.5462000177746293]
	TIME [epoch: 0.709 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4635243270281693		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.4635243270281693 | validation: 0.9147011007018726]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4943612127522465		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.4943612127522465 | validation: 1.4541585746891972]
	TIME [epoch: 0.712 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4168728685654746		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.4168728685654746 | validation: 0.8890285857696997]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.43581167335974		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.43581167335974 | validation: 1.5404659999022774]
	TIME [epoch: 0.711 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4638631240804691		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.4638631240804691 | validation: 0.8981567008613824]
	TIME [epoch: 0.707 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4295429616505724		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.4295429616505724 | validation: 1.437359142045436]
	TIME [epoch: 0.708 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3986533700372923		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3986533700372923 | validation: 0.8830708289827757]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.433666785635047		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.433666785635047 | validation: 1.4980700700311997]
	TIME [epoch: 0.71 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.428830743861946		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.428830743861946 | validation: 0.9795237237649568]
	TIME [epoch: 0.707 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4674464793943423		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.4674464793943423 | validation: 1.591727200010495]
	TIME [epoch: 0.708 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5211833219502264		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5211833219502264 | validation: 0.8906684634829167]
	TIME [epoch: 0.708 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660662263002155		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.3660662263002155 | validation: 1.437551625136539]
	TIME [epoch: 0.708 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3957360961170864		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.3957360961170864 | validation: 0.8953817890037348]
	TIME [epoch: 0.707 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4403754797165396		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.4403754797165396 | validation: 1.3990643850604059]
	TIME [epoch: 0.709 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.413568765516203		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.413568765516203 | validation: 1.0570825117754588]
	TIME [epoch: 0.709 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4933296766478505		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.4933296766478505 | validation: 1.2775239412361632]
	TIME [epoch: 0.707 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3755319856831565		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.3755319856831565 | validation: 0.9481016212656235]
	TIME [epoch: 0.708 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.398816081827175		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.398816081827175 | validation: 1.3784992692331781]
	TIME [epoch: 0.709 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3969522412144983		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.3969522412144983 | validation: 0.9296524075865901]
	TIME [epoch: 0.782 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409314872407764		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.409314872407764 | validation: 1.3326471272549836]
	TIME [epoch: 0.708 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367287420089675		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.367287420089675 | validation: 0.9410226629290698]
	TIME [epoch: 0.706 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3658620159115014		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.3658620159115014 | validation: 1.400602016421585]
	TIME [epoch: 0.711 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3951005901197713		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.3951005901197713 | validation: 0.9384804703757162]
	TIME [epoch: 0.708 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3927275017697722		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.3927275017697722 | validation: 1.3304039209867373]
	TIME [epoch: 0.707 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3705061334732074		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.3705061334732074 | validation: 0.9010487828203616]
	TIME [epoch: 0.708 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3616780456531057		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.3616780456531057 | validation: 1.3391434123204828]
	TIME [epoch: 0.708 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3606720259920524		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3606720259920524 | validation: 0.8942386634290074]
	TIME [epoch: 0.707 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.371734306734344		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.371734306734344 | validation: 1.3045766238002436]
	TIME [epoch: 0.708 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.334011013886631		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.334011013886631 | validation: 0.9187363677208655]
	TIME [epoch: 0.707 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3271497005435624		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.3271497005435624 | validation: 1.3235271713008294]
	TIME [epoch: 0.707 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3771270646902178		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.3771270646902178 | validation: 0.9510358929320767]
	TIME [epoch: 0.709 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4145073183284411		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.4145073183284411 | validation: 1.2468775322197345]
	TIME [epoch: 0.707 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349313554738225		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.349313554738225 | validation: 0.9515432420530804]
	TIME [epoch: 0.716 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3526370211718568		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3526370211718568 | validation: 1.3069903289219593]
	TIME [epoch: 0.709 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3608738714724973		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3608738714724973 | validation: 0.9444437360339033]
	TIME [epoch: 0.708 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3686746439211444		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.3686746439211444 | validation: 1.2764504814513327]
	TIME [epoch: 0.708 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3459950457015353		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.3459950457015353 | validation: 0.9272451866143405]
	TIME [epoch: 0.709 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3225885840685978		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.3225885840685978 | validation: 1.3731515036757505]
	TIME [epoch: 0.709 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3701600156918223		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.3701600156918223 | validation: 0.9105445327633366]
	TIME [epoch: 0.708 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3886814877457458		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.3886814877457458 | validation: 1.4752735103587078]
	TIME [epoch: 0.707 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4412602749136258		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.4412602749136258 | validation: 0.9906510797201581]
	TIME [epoch: 0.707 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3698349418426152		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.3698349418426152 | validation: 1.3608434217335785]
	TIME [epoch: 0.709 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3484515497548306		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3484515497548306 | validation: 0.9009067062079918]
	TIME [epoch: 0.708 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3489209991582924		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3489209991582924 | validation: 1.2740106410095517]
	TIME [epoch: 0.707 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3357793506876923		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.3357793506876923 | validation: 0.9211691266409414]
	TIME [epoch: 0.708 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3137255794043463		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.3137255794043463 | validation: 1.3215180052946525]
	TIME [epoch: 0.709 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.358738280373675		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.358738280373675 | validation: 0.9459081555158605]
	TIME [epoch: 0.731 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367085493681538		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.367085493681538 | validation: 1.238477650446399]
	TIME [epoch: 0.708 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3406744382952482		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3406744382952482 | validation: 0.9906333780927021]
	TIME [epoch: 0.708 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3640561158719413		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.3640561158719413 | validation: 1.2543052934413506]
	TIME [epoch: 0.709 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3290363076320963		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.3290363076320963 | validation: 0.9283689383019852]
	TIME [epoch: 0.709 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3287457296719596		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.3287457296719596 | validation: 1.3443746830110135]
	TIME [epoch: 0.707 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3422844383022647		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.3422844383022647 | validation: 0.89609983268614]
	TIME [epoch: 0.709 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3603157364604297		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.3603157364604297 | validation: 1.2430595649709888]
	TIME [epoch: 0.707 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3161704706293407		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3161704706293407 | validation: 0.8998228581508926]
	TIME [epoch: 0.707 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3163697811744535		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.3163697811744535 | validation: 1.3497917087523268]
	TIME [epoch: 0.707 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3569762409286898		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.3569762409286898 | validation: 0.9657669239818572]
	TIME [epoch: 0.71 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4019117600397162		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.4019117600397162 | validation: 1.4122809491449506]
	TIME [epoch: 0.707 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3985236367916742		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3985236367916742 | validation: 0.9220692349983968]
	TIME [epoch: 0.707 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3044093199371127		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3044093199371127 | validation: 1.2610949507497335]
	TIME [epoch: 0.706 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3167359975660446		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.3167359975660446 | validation: 0.9217519868215572]
	TIME [epoch: 0.708 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3308001223668233		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.3308001223668233 | validation: 1.278533334680385]
	TIME [epoch: 0.713 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3222739181016254		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.3222739181016254 | validation: 0.9316307271528452]
	TIME [epoch: 0.706 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3215587670240059		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.3215587670240059 | validation: 1.2301085124098194]
	TIME [epoch: 0.708 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3446606149829257		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3446606149829257 | validation: 0.9956311623116583]
	TIME [epoch: 0.709 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3832285945915823		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3832285945915823 | validation: 1.2061977221582936]
	TIME [epoch: 0.707 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3081287367590353		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.3081287367590353 | validation: 0.9001336928424455]
	TIME [epoch: 0.736 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3306426639081503		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.3306426639081503 | validation: 1.2749719786084905]
	TIME [epoch: 0.708 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3199364094868389		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.3199364094868389 | validation: 0.9020276033886344]
	TIME [epoch: 0.707 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3280531508710898		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.3280531508710898 | validation: 1.253724080238742]
	TIME [epoch: 0.709 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3249374039971615		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.3249374039971615 | validation: 0.9177782843674485]
	TIME [epoch: 0.707 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.317342517029322		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.317342517029322 | validation: 1.3060465874763982]
	TIME [epoch: 0.708 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3262854604728997		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3262854604728997 | validation: 0.9379345485411694]
	TIME [epoch: 0.709 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3636857646835112		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3636857646835112 | validation: 1.3834128407483777]
	TIME [epoch: 0.707 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3785335375834467		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3785335375834467 | validation: 0.9164379849958352]
	TIME [epoch: 0.707 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3060496815134515		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3060496815134515 | validation: 1.2668631680623863]
	TIME [epoch: 0.709 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3081685523903093		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3081685523903093 | validation: 0.9149385538945822]
	TIME [epoch: 0.708 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.311686883800747		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.311686883800747 | validation: 1.233983750782898]
	TIME [epoch: 0.708 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3041705863679076		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.3041705863679076 | validation: 0.964816025820409]
	TIME [epoch: 0.707 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3480341493669636		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3480341493669636 | validation: 1.237603187763515]
	TIME [epoch: 0.708 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3272060446392515		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3272060446392515 | validation: 0.9919656383140556]
	TIME [epoch: 0.709 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3192719290604473		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3192719290604473 | validation: 1.195813532821562]
	TIME [epoch: 0.708 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3046211007551707		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3046211007551707 | validation: 0.9182112863429733]
	TIME [epoch: 0.706 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3149471334320868		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3149471334320868 | validation: 1.2634393711370837]
	TIME [epoch: 0.709 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3206413187415327		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3206413187415327 | validation: 0.9047664361475086]
	TIME [epoch: 0.709 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.308742367793434		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.308742367793434 | validation: 1.2617534017896428]
	TIME [epoch: 0.706 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3148495996552367		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3148495996552367 | validation: 0.9035365791527683]
	TIME [epoch: 0.708 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3021619157362443		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.3021619157362443 | validation: 1.3267074341752725]
	TIME [epoch: 0.708 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3408598982200937		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.3408598982200937 | validation: 0.9222532758918054]
	TIME [epoch: 0.707 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3269759581304856		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.3269759581304856 | validation: 1.30170602461894]
	TIME [epoch: 0.708 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3279304876644396		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3279304876644396 | validation: 0.8807327778504082]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2967225739683685		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.2967225739683685 | validation: 1.2823876476955607]
	TIME [epoch: 0.709 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3066743209973717		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3066743209973717 | validation: 0.904203345861248]
	TIME [epoch: 0.707 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2958394809438756		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.2958394809438756 | validation: 1.1873173265115238]
	TIME [epoch: 0.71 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.292204814589644		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.292204814589644 | validation: 0.9512534387989957]
	TIME [epoch: 0.709 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318092760781654		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.318092760781654 | validation: 1.1760151361652498]
	TIME [epoch: 0.707 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3182096610453566		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3182096610453566 | validation: 0.9739153746819578]
	TIME [epoch: 0.707 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3305648709520437		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.3305648709520437 | validation: 1.236768932656525]
	TIME [epoch: 0.708 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3088860016240593		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.3088860016240593 | validation: 0.9137393313529562]
	TIME [epoch: 0.708 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2900712443259466		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.2900712443259466 | validation: 1.1883127843173054]
	TIME [epoch: 0.708 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.291002255359633		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.291002255359633 | validation: 0.9055989665534127]
	TIME [epoch: 0.707 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2860607980287002		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2860607980287002 | validation: 1.2471724757556402]
	TIME [epoch: 0.712 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.300132061925117		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.300132061925117 | validation: 0.9125793908636706]
	TIME [epoch: 0.709 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2947345829702628		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2947345829702628 | validation: 1.2350112183267337]
	TIME [epoch: 203 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2958575058886743		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.2958575058886743 | validation: 0.9262380926124533]
	TIME [epoch: 1.42 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2906678447522217		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.2906678447522217 | validation: 1.2643676393968102]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2999097942357474		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.2999097942357474 | validation: 0.8939381279432788]
	TIME [epoch: 1.38 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3007051227686954		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.3007051227686954 | validation: 1.2150347745413335]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2853277431425403		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2853277431425403 | validation: 0.8869961415221561]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2761926755629511		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.2761926755629511 | validation: 1.2104499510179743]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275113687265978		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.275113687265978 | validation: 0.8899004629779882]
	TIME [epoch: 1.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2945528817625513		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.2945528817625513 | validation: 1.2017844317186228]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2962984585104658		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.2962984585104658 | validation: 0.9999693298997582]
	TIME [epoch: 1.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2966207310313507		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.2966207310313507 | validation: 1.1754325635612608]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2737119079460002		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2737119079460002 | validation: 0.9037131586457211]
	TIME [epoch: 1.38 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2915996486008867		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.2915996486008867 | validation: 1.272528634275902]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2968326997745327		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.2968326997745327 | validation: 0.9092620848743475]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2738393076915917		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.2738393076915917 | validation: 1.2579756388156813]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2861049010644396		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.2861049010644396 | validation: 0.8800182171516898]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2932397356117786		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2932397356117786 | validation: 1.309922073671318]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3109809855719428		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.3109809855719428 | validation: 0.9151415488234808]
	TIME [epoch: 1.38 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2715438289217749		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.2715438289217749 | validation: 1.2657083125107396]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2850920317930739		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.2850920317930739 | validation: 0.8687376239738198]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2856440010079404		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.2856440010079404 | validation: 1.2132471613683955]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262429465326634		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.262429465326634 | validation: 0.9066909511806092]
	TIME [epoch: 1.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2467216546129896		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2467216546129896 | validation: 1.1674846367510758]
	TIME [epoch: 1.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2553994261611445		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2553994261611445 | validation: 0.90630102577382]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3040443591417616		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.3040443591417616 | validation: 1.1720230740900477]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2790731183887782		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.2790731183887782 | validation: 0.9312376450250541]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2733779061796564		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.2733779061796564 | validation: 1.1685602594868922]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2576988725649967		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.2576988725649967 | validation: 0.9132724282698051]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2886560710228099		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.2886560710228099 | validation: 1.2281038272804323]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2757788092169127		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.2757788092169127 | validation: 0.8807007835813103]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244273783763242		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.244273783763242 | validation: 1.2048424790162067]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274591725159883		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.274591725159883 | validation: 0.8809852862288733]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2578357158220614		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.2578357158220614 | validation: 1.2241843000824404]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2672193616015017		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.2672193616015017 | validation: 0.8950908060587892]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2825033586257955		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.2825033586257955 | validation: 1.2176980155346668]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2864602467825887		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2864602467825887 | validation: 0.9074211899604736]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2909919216914005		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.2909919216914005 | validation: 1.2006308973975788]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677495434273685		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.2677495434273685 | validation: 0.8811207806499972]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.269113084899649		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.269113084899649 | validation: 1.2041650452255501]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2645551955260526		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.2645551955260526 | validation: 0.9238985768383433]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2395802791180552		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.2395802791180552 | validation: 1.1509435876339955]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2549383393583446		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.2549383393583446 | validation: 0.9285949032279973]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.27920755609274		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.27920755609274 | validation: 1.1555283059722867]
	TIME [epoch: 1.44 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.266467424353244		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.266467424353244 | validation: 0.8768294205782994]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.266255040790996		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.266255040790996 | validation: 1.147445906201892]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2519916167402634		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.2519916167402634 | validation: 0.9123634239603842]
	TIME [epoch: 1.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2293685890842028		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.2293685890842028 | validation: 1.1740451010604562]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2445624636096246		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.2445624636096246 | validation: 0.8773886713036902]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2554610572463847		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.2554610572463847 | validation: 1.2422699380048636]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2797359434697555		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.2797359434697555 | validation: 0.898598018797649]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275647936181065		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.275647936181065 | validation: 1.2373226956301087]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2746137112441092		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.2746137112441092 | validation: 0.8593246089767614]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2579677472239623		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.2579677472239623 | validation: 1.2058048178347844]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2523688013099021		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.2523688013099021 | validation: 0.8987756421035027]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2654644146314977		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.2654644146314977 | validation: 1.1825565541216672]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.282055409853532		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.282055409853532 | validation: 0.9308631139767645]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2626712074900313		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.2626712074900313 | validation: 1.1203772965876457]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2459241996079664		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.2459241996079664 | validation: 0.908645434580706]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2525175313099817		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.2525175313099817 | validation: 1.1910686912878345]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.250859420489861		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.250859420489861 | validation: 0.8717452712585404]
	TIME [epoch: 1.47 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26404096448325		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.26404096448325 | validation: 1.1712042181922844]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.231537399319383		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.231537399319383 | validation: 0.8877502152969541]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2379241868584168		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.2379241868584168 | validation: 1.2034064065536498]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2614082079011497		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.2614082079011497 | validation: 0.8718172052405735]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2756662622034722		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.2756662622034722 | validation: 1.2012813021019637]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2625005722181641		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.2625005722181641 | validation: 0.9074318059736639]
	TIME [epoch: 1.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2458180991601777		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.2458180991601777 | validation: 1.149486387886445]
	TIME [epoch: 1.39 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2302907419894988		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.2302907419894988 | validation: 0.871661478836232]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2477178013223786		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.2477178013223786 | validation: 1.2322245032561923]
	TIME [epoch: 1.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2633927950380313		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.2633927950380313 | validation: 0.908481432851022]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2667416409197985		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.2667416409197985 | validation: 1.0585836589539863]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365805119487465		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.2365805119487465 | validation: 0.9334646641984119]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2541104034932602		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.2541104034932602 | validation: 1.1408459144921907]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2522844966241615		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.2522844966241615 | validation: 0.8695184892397704]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2544062909796656		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.2544062909796656 | validation: 1.2155130442414315]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.263882765064168		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.263882765064168 | validation: 0.866494872312228]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2551672291357487		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.2551672291357487 | validation: 1.1306433494430626]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2276441090407149		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.2276441090407149 | validation: 0.903194074392764]
	TIME [epoch: 1.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2510739795656918		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.2510739795656918 | validation: 1.2430171992627292]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268500509975749		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.268500509975749 | validation: 0.8902897638076948]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.260974452519863		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.260974452519863 | validation: 1.1333897273245335]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2327832543946122		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.2327832543946122 | validation: 0.9130728411532865]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2332498116977062		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.2332498116977062 | validation: 1.1705749286594826]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2469962794478138		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.2469962794478138 | validation: 0.8738572363020588]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2437324428717693		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.2437324428717693 | validation: 1.1542921377927646]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.242635361394009		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.242635361394009 | validation: 0.8750372390042584]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2360222995901748		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.2360222995901748 | validation: 1.0734214191404694]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2300982688115059		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.2300982688115059 | validation: 0.9574768475394789]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2488142097360184		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.2488142097360184 | validation: 1.1341836921768451]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2846205483729065		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.2846205483729065 | validation: 0.9422719720784832]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.257248972951616		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.257248972951616 | validation: 1.1537159490562765]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2434781249437856		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.2434781249437856 | validation: 0.8728218013620282]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2593427370672057		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.2593427370672057 | validation: 1.2220720221205417]
	TIME [epoch: 1.38 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.265862977267303		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.265862977267303 | validation: 0.8925805151657589]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2481209275982865		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.2481209275982865 | validation: 1.1999788248711039]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238183021639836		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.238183021639836 | validation: 0.8902205284929162]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238524283715562		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.238524283715562 | validation: 1.131229655903711]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2265727201572694		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.2265727201572694 | validation: 0.9001485367124158]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2309353531217773		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.2309353531217773 | validation: 1.1371398646546007]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2317999677044784		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.2317999677044784 | validation: 0.9051787779132667]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2576124541945086		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.2576124541945086 | validation: 1.1092541918580854]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2432508986200954		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.2432508986200954 | validation: 0.9443452823863749]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2318411055812644		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.2318411055812644 | validation: 1.1208601257603712]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2304730996126518		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.2304730996126518 | validation: 0.898656070012115]
	TIME [epoch: 1.38 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.254225609167012		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.254225609167012 | validation: 1.1024724947578302]
	TIME [epoch: 1.38 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2346169075457207		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.2346169075457207 | validation: 0.8916028139926567]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2173211630435952		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.2173211630435952 | validation: 1.216596954254892]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2529943119071718		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.2529943119071718 | validation: 0.8661483316317722]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.254049449092263		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.254049449092263 | validation: 1.1586960826015742]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2478663166710862		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.2478663166710862 | validation: 0.898354810744435]
	TIME [epoch: 1.38 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2476388417112803		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.2476388417112803 | validation: 1.1138650713666416]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2330287520928345		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.2330287520928345 | validation: 0.8561077215357522]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.246917801179823		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.246917801179823 | validation: 1.1281336708546699]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.223147497168395		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.223147497168395 | validation: 0.8864219054887905]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2171681486776416		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.2171681486776416 | validation: 1.1556073568924068]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2382415469796209		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.2382415469796209 | validation: 0.9044841090251534]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2330048687434554		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.2330048687434554 | validation: 1.1582187848077374]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2384012732352923		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.2384012732352923 | validation: 0.895195161423094]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2611506496625389		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.2611506496625389 | validation: 1.0637163671279148]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.220349182657937		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.220349182657937 | validation: 0.9414426013476174]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.213508102541825		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.213508102541825 | validation: 1.0875365216967132]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2193968591269404		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.2193968591269404 | validation: 0.8770556161167624]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2428426271541502		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.2428426271541502 | validation: 1.1775205167729614]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2465316842551122		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.2465316842551122 | validation: 0.8968437782840009]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2240451318683592		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.2240451318683592 | validation: 1.1258768221296382]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2290923637445528		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.2290923637445528 | validation: 0.8768330199576]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2227402872093176		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.2227402872093176 | validation: 1.2041189720598633]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2541412739740145		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.2541412739740145 | validation: 0.8714548349076522]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2486885882713514		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.2486885882713514 | validation: 1.1474734979652401]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2412811630833016		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.2412811630833016 | validation: 0.8805394659522655]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2294810020356648		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.2294810020356648 | validation: 1.1202942893356838]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2278463842862692		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.2278463842862692 | validation: 0.876225962896525]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2230289418334477		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.2230289418334477 | validation: 1.0958659384606626]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215298808408839		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.215298808408839 | validation: 0.8921883616201811]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2324034439833012		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.2324034439833012 | validation: 1.0474256834139548]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2180804182301552		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.2180804182301552 | validation: 0.9193064941737149]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2218427850813123		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.2218427850813123 | validation: 1.087938897329226]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2331402370939395		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.2331402370939395 | validation: 0.8955213318167581]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2147042683690195		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.2147042683690195 | validation: 1.1025393907559522]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2271161600226206		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.2271161600226206 | validation: 0.8830314351867987]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2128066003872053		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.2128066003872053 | validation: 1.0785997184414926]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2097237812123711		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.2097237812123711 | validation: 0.8895154568423508]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2168576062620517		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.2168576062620517 | validation: 1.2080246873185478]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.260215273682529		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.260215273682529 | validation: 0.8676703091989822]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2509138249788474		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.2509138249788474 | validation: 1.0642590707966946]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.202536011707682		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.202536011707682 | validation: 0.9442324462003091]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1951568898686733		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.1951568898686733 | validation: 1.0501936210772518]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2050579149230474		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.2050579149230474 | validation: 0.873006036221298]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.226795922941185		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.226795922941185 | validation: 1.166388118814319]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2361998468771518		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.2361998468771518 | validation: 0.8625881875294376]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2068895203450614		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.2068895203450614 | validation: 1.115093379220194]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2240778784280397		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.2240778784280397 | validation: 0.881964848471734]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2208340995811793		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.2208340995811793 | validation: 1.0486859491971032]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2255485645491349		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.2255485645491349 | validation: 0.9330303556696986]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2412644755346458		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.2412644755346458 | validation: 1.0710984645811448]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2197799238062643		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.2197799238062643 | validation: 0.9050038146340103]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2218953625736637		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.2218953625736637 | validation: 1.0975166399778933]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.209626307230122		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.209626307230122 | validation: 0.8486655564651109]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2259298827316565		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.2259298827316565 | validation: 1.1281174292047813]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2306286946625717		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.2306286946625717 | validation: 0.8317577758845149]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2531728573245369		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.2531728573245369 | validation: 1.1126596408177503]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2147809016974096		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.2147809016974096 | validation: 0.8755512653591255]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2127989235442054		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.2127989235442054 | validation: 1.0522276038727172]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2021976877248697		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.2021976877248697 | validation: 0.857560120801502]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2060320343686615		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.2060320343686615 | validation: 1.146654419888749]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2384125581964922		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.2384125581964922 | validation: 0.8797324572223509]
	TIME [epoch: 1.38 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2305018805829178		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.2305018805829178 | validation: 1.003635434466437]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2034423581149958		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.2034423581149958 | validation: 0.8971179847862875]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2222298042548376		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.2222298042548376 | validation: 1.1122599788935241]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2246798574017637		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.2246798574017637 | validation: 0.8673906050280519]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2402217764158745		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.2402217764158745 | validation: 1.052256952662128]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2096712362604822		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.2096712362604822 | validation: 0.8792744800472154]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2187511677843796		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.2187511677843796 | validation: 1.1160435745881958]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2257128379057838		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.2257128379057838 | validation: 0.8998610757418167]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2095989071942863		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.2095989071942863 | validation: 1.0833454541712524]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2097788090430899		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.2097788090430899 | validation: 0.8501318193529228]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2042698298142245		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.2042698298142245 | validation: 1.09690499085914]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.224330530050661		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.224330530050661 | validation: 0.8691714482906969]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221179711019913		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.221179711019913 | validation: 1.1463311658941842]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.235813248975128		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.235813248975128 | validation: 0.8813942014213908]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2016271056535048		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.2016271056535048 | validation: 1.0504792239539584]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2107668242172116		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.2107668242172116 | validation: 0.8774554795917049]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2157094632374201		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.2157094632374201 | validation: 1.071650480909928]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2017840192275686		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.2017840192275686 | validation: 0.8847734356032545]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2085023435830986		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.2085023435830986 | validation: 1.0777235764971387]
	TIME [epoch: 1.38 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2070142225166753		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.2070142225166753 | validation: 0.8732826711447894]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.209028019678821		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.209028019678821 | validation: 1.0756116381176575]
	TIME [epoch: 1.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20444033134643		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.20444033134643 | validation: 0.9007724604316689]
	TIME [epoch: 1.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.200347446957496		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.200347446957496 | validation: 1.0576716241076916]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205004997685563		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.205004997685563 | validation: 0.8660174049052689]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.19985939160367		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.19985939160367 | validation: 1.0760171738256172]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2185031164019855		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.2185031164019855 | validation: 0.8585244858282384]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2135809908638726		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.2135809908638726 | validation: 1.0572274306652827]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2087154773545856		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.2087154773545856 | validation: 0.904831764924969]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.193855823623238		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.193855823623238 | validation: 1.0730538495507715]
	TIME [epoch: 1.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2097035178143316		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.2097035178143316 | validation: 0.8918163434143622]
	TIME [epoch: 1.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2286884110080731		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.2286884110080731 | validation: 1.0380148304943695]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2053888827167618		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.2053888827167618 | validation: 0.8880821289702601]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195456320052612		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.195456320052612 | validation: 1.1003050114361952]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2188650684270053		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.2188650684270053 | validation: 0.8308719630231224]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2083083275145448		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.2083083275145448 | validation: 1.0344769772392526]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2133907077715151		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.2133907077715151 | validation: 0.9002486123385247]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1975799063659942		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.1975799063659942 | validation: 1.0631236586838737]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195151099846362		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.195151099846362 | validation: 0.8413308053436004]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2279227904786079		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.2279227904786079 | validation: 1.0955383625643043]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2092368799552835		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.2092368799552835 | validation: 0.8740104152098683]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1924692580593736		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.1924692580593736 | validation: 0.993449300777059]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1888969418339073		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1888969418339073 | validation: 0.8898096240606943]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1784197391507802		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.1784197391507802 | validation: 1.095454027439361]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.209222222735522		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.209222222735522 | validation: 0.8141761191084621]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2256898189393284		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.2256898189393284 | validation: 1.063989718968791]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.198570417019045		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.198570417019045 | validation: 0.8785414235261073]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191523804537067		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.191523804537067 | validation: 1.0090942334372355]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1856827681302737		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.1856827681302737 | validation: 0.9091445965049779]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1863020975028713		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.1863020975028713 | validation: 1.0013774971305933]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1971922382238065		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.1971922382238065 | validation: 0.8292337368907913]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2114785504512526		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.2114785504512526 | validation: 1.1155451008359358]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2206624783185047		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.2206624783185047 | validation: 0.8507634851747636]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1914199382669157		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.1914199382669157 | validation: 0.9689291077619047]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.184098413243022		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.184098413243022 | validation: 0.9437933564152868]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1769998269467077		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.1769998269467077 | validation: 0.8239429923899064]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2260142787852737		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.2260142787852737 | validation: 1.1728768836268852]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2287578597197621		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.2287578597197621 | validation: 0.8680227260972271]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1860900108411612		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.1860900108411612 | validation: 0.9876507892105768]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1962196670858969		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.1962196670858969 | validation: 0.8851876311389945]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2206063175515345		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.2206063175515345 | validation: 1.0778029839893914]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2271215082307563		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.2271215082307563 | validation: 0.8673078281430442]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1899571532487174		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.1899571532487174 | validation: 1.0372966217613395]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191181509016049		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.191181509016049 | validation: 0.8608272233954989]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1914998280839837		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.1914998280839837 | validation: 1.054733500884094]
	TIME [epoch: 1.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1854852843457668		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.1854852843457668 | validation: 0.8369168652286998]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192156904437843		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.192156904437843 | validation: 1.0804768290034552]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2029339345017411		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.2029339345017411 | validation: 0.8365078162740649]
	TIME [epoch: 1.39 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1833428955231478		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.1833428955231478 | validation: 1.0457422457075027]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1944334242228423		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.1944334242228423 | validation: 0.8311607156284513]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1868867715497071		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.1868867715497071 | validation: 1.0556750863011324]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188830689055293		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.188830689055293 | validation: 0.9034424327283492]
	TIME [epoch: 1.39 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1856379533998624		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.1856379533998624 | validation: 1.000358125765789]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1819570104772341		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.1819570104772341 | validation: 0.8398583376467741]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2090723531727325		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.2090723531727325 | validation: 1.0999120025222218]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2144073595411893		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.2144073595411893 | validation: 0.8844012227284929]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1678003035178524		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.1678003035178524 | validation: 0.9756807472323857]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1762684425901977		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.1762684425901977 | validation: 0.8574773308896712]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176999163836457		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.176999163836457 | validation: 1.0533832296982526]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2018243858615467		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.2018243858615467 | validation: 0.8518380577057081]
	TIME [epoch: 1.45 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20420631902703		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.20420631902703 | validation: 0.9879010109212978]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183445171213975		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.183445171213975 | validation: 0.8987794844885983]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1834121747737743		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.1834121747737743 | validation: 0.9652810650344291]
	TIME [epoch: 1.38 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.185657787746379		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.185657787746379 | validation: 0.8406807891986694]
	TIME [epoch: 1.38 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1813523286777412		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.1813523286777412 | validation: 1.0681664789377447]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1987927735086599		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.1987927735086599 | validation: 0.8420428171514467]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176031945921457		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.176031945921457 | validation: 0.9938693896322681]
	TIME [epoch: 1.38 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177507713929863		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.177507713929863 | validation: 0.8543135965376072]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844412624055745		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.1844412624055745 | validation: 1.0775913416125698]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199399435178571		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.199399435178571 | validation: 0.809458188272037]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.213288916047896		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.213288916047896 | validation: 1.0227402771936227]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1907803526213385		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.1907803526213385 | validation: 0.8412349691350448]
	TIME [epoch: 1.48 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1693759836292055		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.1693759836292055 | validation: 1.0470420250583143]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1885621367814736		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.1885621367814736 | validation: 0.8253508838914663]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2095624448900926		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.2095624448900926 | validation: 0.9762173255160571]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1836483160837004		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.1836483160837004 | validation: 0.8246840773414226]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1740727445000088		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.1740727445000088 | validation: 1.033875426192816]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1965505090735293		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.1965505090735293 | validation: 0.8228198402859447]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2004787232077925		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.2004787232077925 | validation: 1.0008848272755635]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164028370533942		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.164028370533942 | validation: 0.8226110834355324]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177568715619146		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.177568715619146 | validation: 0.9947992284484827]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1798410998898994		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.1798410998898994 | validation: 0.8269875121766177]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844247607131266		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.1844247607131266 | validation: 1.0239694762066578]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.18390838327967		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.18390838327967 | validation: 0.8323667454555047]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169631815572727		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.169631815572727 | validation: 1.0116694893111657]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1818025497655154		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.1818025497655154 | validation: 0.7849468355423215]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1838051351919032		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.1838051351919032 | validation: 0.9845726110995738]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.170128207927028		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.170128207927028 | validation: 0.8842574792721731]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1621159910295966		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.1621159910295966 | validation: 1.0223694404802963]
	TIME [epoch: 1.51 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1936532738680614		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.1936532738680614 | validation: 0.8282648631700102]
	TIME [epoch: 1.38 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1736169729190666		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.1736169729190666 | validation: 0.965068498012577]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1728488646176023		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.1728488646176023 | validation: 0.8504929824461048]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1634601955042763		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.1634601955042763 | validation: 0.9577446026066783]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1591886108698795		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1591886108698795 | validation: 0.8544074719150754]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508559277908175		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.1508559277908175 | validation: 0.9592525119339483]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1691345531018582		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.1691345531018582 | validation: 0.7897062221584124]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191170848408959		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.191170848408959 | validation: 1.0340184994591055]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1774356950924472		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.1774356950924472 | validation: 0.8606716165718459]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1665445643983294		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.1665445643983294 | validation: 0.9745448635449803]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1649626691337327		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.1649626691337327 | validation: 0.81008970438806]
	TIME [epoch: 1.38 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926907690495807		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1926907690495807 | validation: 0.9754558890904875]
	TIME [epoch: 1.49 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1721881952493594		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.1721881952493594 | validation: 0.8802853978697499]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1626951766540667		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.1626951766540667 | validation: 0.9241477975174925]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165522052390578		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.165522052390578 | validation: 0.8560800320846087]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172674686532302		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.172674686532302 | validation: 0.9801518545368414]
	TIME [epoch: 1.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1740529352560578		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.1740529352560578 | validation: 0.8273889914527655]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1634904860629		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.1634904860629 | validation: 0.9893055898247947]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1754948766063158		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.1754948766063158 | validation: 0.818015696483236]
	TIME [epoch: 1.38 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1612621354988908		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.1612621354988908 | validation: 0.9825927427808621]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1593453925930821		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.1593453925930821 | validation: 0.8551888693054205]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1593197850833952		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.1593197850833952 | validation: 0.8937036081195929]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155902060150487		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.155902060150487 | validation: 0.8767931188347369]
	TIME [epoch: 1.38 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1563718483222365		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.1563718483222365 | validation: 0.8019798662085148]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1755263825415125		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.1755263825415125 | validation: 1.0572806377394184]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1988124648060206		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.1988124648060206 | validation: 0.7959622902306285]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1674712746907863		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.1674712746907863 | validation: 0.9781822098364717]
	TIME [epoch: 206 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1578590481741646		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.1578590481741646 | validation: 0.819072002925811]
	TIME [epoch: 2.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175046120332851		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.175046120332851 | validation: 0.9677007056382593]
	TIME [epoch: 2.73 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1696266921813965		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.1696266921813965 | validation: 0.7970963453337694]
	TIME [epoch: 2.73 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1586433949422756		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.1586433949422756 | validation: 0.9465046061322735]
	TIME [epoch: 2.73 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1644952877992478		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.1644952877992478 | validation: 0.8161916744574074]
	TIME [epoch: 2.73 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575850623844657		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.1575850623844657 | validation: 0.9719945480771346]
	TIME [epoch: 2.74 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1705498234600635		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.1705498234600635 | validation: 0.8441340939217821]
	TIME [epoch: 2.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1609989511000793		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.1609989511000793 | validation: 0.9138943416749168]
	TIME [epoch: 2.73 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.153424602584562		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.153424602584562 | validation: 0.8044325167029037]
	TIME [epoch: 2.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1703108063456151		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.1703108063456151 | validation: 0.9987960506954878]
	TIME [epoch: 2.73 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.174715017486623		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.174715017486623 | validation: 0.8038263340154345]
	TIME [epoch: 2.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554493880712084		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.1554493880712084 | validation: 0.9162955373123052]
	TIME [epoch: 2.73 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139811619623196		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.139811619623196 | validation: 0.8206939861563871]
	TIME [epoch: 2.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1527851971644811		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.1527851971644811 | validation: 0.9760755893089247]
	TIME [epoch: 2.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1700931458087749		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.1700931458087749 | validation: 0.7798783257003635]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1804387998411414		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.1804387998411414 | validation: 0.9846216548840115]
	TIME [epoch: 2.73 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1699038058961506		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.1699038058961506 | validation: 0.8050523511122502]
	TIME [epoch: 2.73 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610554383692102		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.1610554383692102 | validation: 0.8929573329462599]
	TIME [epoch: 2.73 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1489493099385901		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.1489493099385901 | validation: 0.7963349694137145]
	TIME [epoch: 2.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542157332838598		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.1542157332838598 | validation: 1.0572789632428232]
	TIME [epoch: 2.73 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1890750009685476		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.1890750009685476 | validation: 0.8037304879937907]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1555206837991654		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.1555206837991654 | validation: 0.838569021944179]
	TIME [epoch: 2.73 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1591285094564754		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.1591285094564754 | validation: 0.8970230270948036]
	TIME [epoch: 2.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.138280142451618		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.138280142451618 | validation: 0.7751658549275646]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1828311224337684		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.1828311224337684 | validation: 1.045359955009779]
	TIME [epoch: 2.82 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1885313772527528		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.1885313772527528 | validation: 0.8463494985438889]
	TIME [epoch: 2.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1365286885749937		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.1365286885749937 | validation: 0.8097745764243532]
	TIME [epoch: 2.73 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1551987011880567		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.1551987011880567 | validation: 0.8304203927762428]
	TIME [epoch: 2.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144103966587083		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.144103966587083 | validation: 1.0428378365021016]
	TIME [epoch: 2.73 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1953893641790452		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.1953893641790452 | validation: 0.8315243931577485]
	TIME [epoch: 2.73 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1607776141955488		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.1607776141955488 | validation: 0.8572373808854601]
	TIME [epoch: 2.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141903041321731		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.141903041321731 | validation: 0.875264099563188]
	TIME [epoch: 2.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1491046341173201		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.1491046341173201 | validation: 0.8185338345323449]
	TIME [epoch: 2.76 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1504405111015128		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.1504405111015128 | validation: 1.0672933825602806]
	TIME [epoch: 2.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2016132151787762		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.2016132151787762 | validation: 0.8100430271749058]
	TIME [epoch: 2.73 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1531284421922119		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.1531284421922119 | validation: 0.8838484892013027]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1448868008421078		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.1448868008421078 | validation: 0.823041658591232]
	TIME [epoch: 2.77 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1579483292548787		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.1579483292548787 | validation: 0.9616244853749518]
	TIME [epoch: 2.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1640593390105969		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.1640593390105969 | validation: 0.8204411706272041]
	TIME [epoch: 2.73 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1686525409578539		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.1686525409578539 | validation: 0.9721235254794159]
	TIME [epoch: 2.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.138189144060422		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.138189144060422 | validation: 0.8266656082066896]
	TIME [epoch: 2.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152167872494056		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.152167872494056 | validation: 0.9196362311869596]
	TIME [epoch: 2.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1605517688497162		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.1605517688497162 | validation: 0.8017314629650141]
	TIME [epoch: 2.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1612655510758554		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.1612655510758554 | validation: 0.9215512840001079]
	TIME [epoch: 2.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15342754855943		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.15342754855943 | validation: 0.8343972134641393]
	TIME [epoch: 2.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1552307933545956		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.1552307933545956 | validation: 0.8735665189711075]
	TIME [epoch: 2.73 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1487768476426188		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.1487768476426188 | validation: 1.0481602058807165]
	TIME [epoch: 2.74 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183789455969227		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.183789455969227 | validation: 0.8056914777865581]
	TIME [epoch: 2.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1768126639626553		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.1768126639626553 | validation: 0.9458446761750875]
	TIME [epoch: 2.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1466610585045678		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.1466610585045678 | validation: 0.8222841458433519]
	TIME [epoch: 2.73 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1629040540149647		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.1629040540149647 | validation: 0.9390847130501103]
	TIME [epoch: 2.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1445124956059247		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.1445124956059247 | validation: 0.783257468612097]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1524235797911402		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.1524235797911402 | validation: 0.9092670759284843]
	TIME [epoch: 2.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1539285971613285		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.1539285971613285 | validation: 0.8341962188564885]
	TIME [epoch: 2.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1442314361542116		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.1442314361542116 | validation: 0.9224040369088344]
	TIME [epoch: 2.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1555863925232468		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.1555863925232468 | validation: 0.7963938370056804]
	TIME [epoch: 2.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1664956120871022		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.1664956120871022 | validation: 0.927048269783653]
	TIME [epoch: 2.73 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1545979469224514		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.1545979469224514 | validation: 0.8230210738138575]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1528138049651568		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.1528138049651568 | validation: 0.8305312985170155]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1448042159833196		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.1448042159833196 | validation: 0.921307248235891]
	TIME [epoch: 2.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1668748475262438		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.1668748475262438 | validation: 0.8046398288808059]
	TIME [epoch: 2.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149914241608828		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 1.149914241608828 | validation: 0.9339804394853563]
	TIME [epoch: 2.73 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1477353376786454		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 1.1477353376786454 | validation: 0.7716615862644357]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1728012851627585		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 1.1728012851627585 | validation: 0.9196942358906744]
	TIME [epoch: 2.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1492857388711584		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.1492857388711584 | validation: 0.798397098989223]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152697973875318		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.152697973875318 | validation: 0.9991031349120697]
	TIME [epoch: 2.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.167942188051022		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.167942188051022 | validation: 0.8045578052277466]
	TIME [epoch: 2.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150834396345071		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.150834396345071 | validation: 0.8937465286799413]
	TIME [epoch: 2.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453068202277894		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 1.1453068202277894 | validation: 0.7803647243637718]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1390236563915195		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 1.1390236563915195 | validation: 0.9327939764230098]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460991104915612		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.1460991104915612 | validation: 0.8064954308202437]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1514541222581955		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.1514541222581955 | validation: 0.8992861870588972]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146119665383576		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.146119665383576 | validation: 0.8205857393330311]
	TIME [epoch: 2.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1437230016852271		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 1.1437230016852271 | validation: 0.8151126881739282]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1354019119221939		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 1.1354019119221939 | validation: 0.7846520775255784]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152326562297457		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 1.152326562297457 | validation: 1.0022851341006214]
	TIME [epoch: 2.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1781753507378958		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 1.1781753507378958 | validation: 0.7853973512852509]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1622687616646188		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 1.1622687616646188 | validation: 0.9138646119731322]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140829829952661		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 1.140829829952661 | validation: 0.7828437503201646]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425984146539834		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 1.1425984146539834 | validation: 0.9521927142863906]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1521386223570154		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 1.1521386223570154 | validation: 0.798706526466855]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1499492784277683		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 1.1499492784277683 | validation: 0.9261523679540149]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1485359559180934		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 1.1485359559180934 | validation: 0.7788013792103815]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474035518581909		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 1.1474035518581909 | validation: 0.9379642593940706]
	TIME [epoch: 2.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151553349322911		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 1.151553349322911 | validation: 0.8108460373768956]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13719804026963		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 1.13719804026963 | validation: 0.8730518078351246]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146899513393614		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 1.146899513393614 | validation: 0.7915919795501711]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.168100572593647		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 1.168100572593647 | validation: 0.9432822687609459]
	TIME [epoch: 2.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149729494053704		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 1.149729494053704 | validation: 0.7923583546474943]
	TIME [epoch: 2.73 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1465676573307513		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 1.1465676573307513 | validation: 0.8595005304090754]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1427077738185059		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.1427077738185059 | validation: 0.8322101388113364]
	TIME [epoch: 2.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149814438555356		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 1.149814438555356 | validation: 0.9532065855587151]
	TIME [epoch: 2.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165904493364738		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 1.165904493364738 | validation: 0.7671398460384358]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1744476991378865		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 1.1744476991378865 | validation: 0.890721316537942]
	TIME [epoch: 2.81 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1404514085273227		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 1.1404514085273227 | validation: 0.8730180849047168]
	TIME [epoch: 2.73 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1509038339516757		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 1.1509038339516757 | validation: 0.7615641466978645]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1811877283574233		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 1.1811877283574233 | validation: 0.924224478584735]
	TIME [epoch: 2.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154913462621934		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 1.154913462621934 | validation: 0.810590566267125]
	TIME [epoch: 2.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1374364406188984		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.1374364406188984 | validation: 0.8152135149028423]
	TIME [epoch: 2.73 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303125974083215		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 1.1303125974083215 | validation: 0.9879911901092409]
	TIME [epoch: 2.76 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176351688696198		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 1.176351688696198 | validation: 0.7741226065392017]
	TIME [epoch: 2.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1320224901693106		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 1.1320224901693106 | validation: 0.8297330433455543]
	TIME [epoch: 2.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1386597519925763		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 1.1386597519925763 | validation: 0.9757444085708434]
	TIME [epoch: 2.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1883517146807518		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 1.1883517146807518 | validation: 0.8252622484115559]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1465226653913079		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 1.1465226653913079 | validation: 0.83913283326021]
	TIME [epoch: 2.72 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136437203354607		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 1.136437203354607 | validation: 0.882148792653282]
	TIME [epoch: 2.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1440896709474724		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 1.1440896709474724 | validation: 0.8458372222629859]
	TIME [epoch: 2.73 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14100002918023		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 1.14100002918023 | validation: 0.8821267711367776]
	TIME [epoch: 2.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1424143585399074		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 1.1424143585399074 | validation: 0.831377535934795]
	TIME [epoch: 2.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152591425659505		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 1.152591425659505 | validation: 0.8469403504305089]
	TIME [epoch: 2.74 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1556203679290866		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 1.1556203679290866 | validation: 0.810099530285553]
	TIME [epoch: 2.73 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1320049744905678		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 1.1320049744905678 | validation: 0.8735420264167697]
	TIME [epoch: 2.73 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1500381636287857		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 1.1500381636287857 | validation: 0.77675060483717]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152766270785919		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 1.152766270785919 | validation: 0.8698412356915464]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1415743237682379		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 1.1415743237682379 | validation: 0.8983227007989477]
	TIME [epoch: 2.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1489738917426635		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 1.1489738917426635 | validation: 0.8982523993653455]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1361808982963983		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 1.1361808982963983 | validation: 0.7720395204769853]
	TIME [epoch: 2.73 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1663935958717213		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 1.1663935958717213 | validation: 0.9718558573085091]
	TIME [epoch: 2.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1484424575733685		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 1.1484424575733685 | validation: 0.8272974886867402]
	TIME [epoch: 2.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143350699178754		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 1.143350699178754 | validation: 0.8848416269110309]
	TIME [epoch: 2.73 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1507052320474276		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 1.1507052320474276 | validation: 0.821402683047673]
	TIME [epoch: 2.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399031452717068		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 1.1399031452717068 | validation: 0.9046713206148239]
	TIME [epoch: 2.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1479195122235486		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 1.1479195122235486 | validation: 0.7987627643253993]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542733038346444		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 1.1542733038346444 | validation: 0.9333707099701182]
	TIME [epoch: 2.73 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1485822886193942		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 1.1485822886193942 | validation: 0.7991769631736064]
	TIME [epoch: 2.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1427255365419013		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 1.1427255365419013 | validation: 0.8963594212389734]
	TIME [epoch: 2.73 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1472209732481122		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 1.1472209732481122 | validation: 0.7885491675737968]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1394139656116518		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 1.1394139656116518 | validation: 0.9060263644438543]
	TIME [epoch: 2.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139237132456389		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 1.139237132456389 | validation: 0.8037586390278145]
	TIME [epoch: 2.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1522999514983008		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 1.1522999514983008 | validation: 0.8748934263021138]
	TIME [epoch: 2.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1398424578084165		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 1.1398424578084165 | validation: 0.7595491272273382]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1611183228261224		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 1.1611183228261224 | validation: 0.9481495468384443]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1530251010666295		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 1.1530251010666295 | validation: 0.8724283043020237]
	TIME [epoch: 2.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498047241816551		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 1.1498047241816551 | validation: 0.8019392677153769]
	TIME [epoch: 2.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1529898586386351		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 1.1529898586386351 | validation: 0.8533093902206652]
	TIME [epoch: 2.73 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129451514731581		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 1.129451514731581 | validation: 0.8318054885471762]
	TIME [epoch: 2.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1334452845450032		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 1.1334452845450032 | validation: 0.8764876986693952]
	TIME [epoch: 2.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128472167353977		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 1.128472167353977 | validation: 0.7769855020004189]
	TIME [epoch: 2.73 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1424568777538437		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 1.1424568777538437 | validation: 0.920198679822303]
	TIME [epoch: 2.73 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1642534974489853		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 1.1642534974489853 | validation: 0.8048978062826067]
	TIME [epoch: 2.73 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1397143021542946		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 1.1397143021542946 | validation: 0.875922316874094]
	TIME [epoch: 2.73 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1422646777800574		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 1.1422646777800574 | validation: 0.7575314328634482]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1605964439352776		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 1.1605964439352776 | validation: 0.86556694640283]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1726884245173281		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 1.1726884245173281 | validation: 0.7827729625928526]
	TIME [epoch: 2.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171486401822958		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 1.171486401822958 | validation: 0.790124854516543]
	TIME [epoch: 2.8 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1781342028151414		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 1.1781342028151414 | validation: 0.8268296230887038]
	TIME [epoch: 2.72 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154488863745366		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 1.154488863745366 | validation: 0.7855903531701207]
	TIME [epoch: 2.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1749836353794874		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 1.1749836353794874 | validation: 0.8513069585246766]
	TIME [epoch: 2.81 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816809636053405		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 1.1816809636053405 | validation: 0.7558472157211779]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584913740445821		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 1.1584913740445821 | validation: 0.7941812045611957]
	TIME [epoch: 2.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147613849446294		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 1.147613849446294 | validation: 0.9017692574437013]
	TIME [epoch: 2.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.148598884290786		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 1.148598884290786 | validation: 0.7802487789315062]
	TIME [epoch: 2.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140344426683841		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 1.140344426683841 | validation: 0.8109659390919147]
	TIME [epoch: 2.73 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1472659078510106		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 1.1472659078510106 | validation: 0.8001472272106706]
	TIME [epoch: 2.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1641964998023142		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 1.1641964998023142 | validation: 0.9081022898516444]
	TIME [epoch: 2.73 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1999646529463326		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 1.1999646529463326 | validation: 0.7746618562739491]
	TIME [epoch: 2.73 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2179787803978281		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 1.2179787803978281 | validation: 0.8612998923276897]
	TIME [epoch: 2.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2146902863565303		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 1.2146902863565303 | validation: 0.8087403012643251]
	TIME [epoch: 2.82 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2062713210834848		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 1.2062713210834848 | validation: 0.8582695145979813]
	TIME [epoch: 2.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2281824045036263		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 1.2281824045036263 | validation: 0.8948322305961304]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.231111912215322		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 1.231111912215322 | validation: 0.7813247541147481]
	TIME [epoch: 2.73 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2174134322878534		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 1.2174134322878534 | validation: 0.7757323342358196]
	TIME [epoch: 2.73 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180577349161643		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 1.180577349161643 | validation: 0.9050412419095611]
	TIME [epoch: 2.73 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966676318070395		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 1.1966676318070395 | validation: 0.7469715972607963]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1848940256886704		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 1.1848940256886704 | validation: 0.8356990994912924]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.161984163695437		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 1.161984163695437 | validation: 0.7660339100408969]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1767473209684778		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 1.1767473209684778 | validation: 0.8138762269126277]
	TIME [epoch: 2.75 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1599000559441088		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 1.1599000559441088 | validation: 0.8400258156275607]
	TIME [epoch: 2.73 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1702415670353425		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 1.1702415670353425 | validation: 0.734514254944031]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1700868754534461		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 1.1700868754534461 | validation: 0.8551556061189105]
	TIME [epoch: 2.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1732224214270135		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 1.1732224214270135 | validation: 0.7661718944934822]
	TIME [epoch: 2.72 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162069114553195		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 1.162069114553195 | validation: 0.8194955186496998]
	TIME [epoch: 2.73 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1585702192297742		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 1.1585702192297742 | validation: 0.7866596070532014]
	TIME [epoch: 2.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1543274736061162		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 1.1543274736061162 | validation: 0.9665039044095396]
	TIME [epoch: 2.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554699171922107		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 1.1554699171922107 | validation: 0.8194665898633826]
	TIME [epoch: 2.72 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1493759398938148		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 1.1493759398938148 | validation: 0.7976462135578617]
	TIME [epoch: 2.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283674531994554		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 1.1283674531994554 | validation: 0.8707143668203617]
	TIME [epoch: 2.72 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1240634461090353		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 1.1240634461090353 | validation: 0.8157947846465972]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1337882686310112		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 1.1337882686310112 | validation: 0.9427962843470876]
	TIME [epoch: 2.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1411231017653012		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 1.1411231017653012 | validation: 0.7951740425106716]
	TIME [epoch: 2.86 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141522463283183		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 1.141522463283183 | validation: 0.7799225297808463]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1330225679496204		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 1.1330225679496204 | validation: 0.7754002092704582]
	TIME [epoch: 2.73 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1346639223266017		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 1.1346639223266017 | validation: 0.884542672254108]
	TIME [epoch: 2.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1405816301923926		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 1.1405816301923926 | validation: 0.7840230493581676]
	TIME [epoch: 2.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140303321812156		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 1.140303321812156 | validation: 0.8286877902348112]
	TIME [epoch: 2.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1262012618106376		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 1.1262012618106376 | validation: 0.823213695353326]
	TIME [epoch: 2.72 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139420141465685		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 1.139420141465685 | validation: 0.7474105801485892]
	TIME [epoch: 2.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13997516808342		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 1.13997516808342 | validation: 0.8566072596642557]
	TIME [epoch: 2.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1396792872613508		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 1.1396792872613508 | validation: 0.8374903904012009]
	TIME [epoch: 2.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1450599692888432		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 1.1450599692888432 | validation: 0.7578954847119586]
	TIME [epoch: 2.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1461630117771102		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 1.1461630117771102 | validation: 0.8790534851964626]
	TIME [epoch: 2.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1241968737282964		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 1.1241968737282964 | validation: 0.7758630365794525]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123633913148165		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 1.123633913148165 | validation: 0.8376323638646985]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1166678563826937		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 1.1166678563826937 | validation: 0.8011361791933359]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130642895675274		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 1.130642895675274 | validation: 0.8318489342734451]
	TIME [epoch: 2.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132071455486935		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 1.132071455486935 | validation: 0.7664672795517249]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1409782287608174		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 1.1409782287608174 | validation: 0.8731216061888333]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221969165501906		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 1.1221969165501906 | validation: 0.741511829998295]
	TIME [epoch: 2.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1355238806636838		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 1.1355238806636838 | validation: 0.8438088818856662]
	TIME [epoch: 2.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1436817221204911		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 1.1436817221204911 | validation: 0.781065636668883]
	TIME [epoch: 2.73 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.157812473426821		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 1.157812473426821 | validation: 0.8099629615732348]
	TIME [epoch: 2.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464015513130617		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 1.1464015513130617 | validation: 0.8032458208443295]
	TIME [epoch: 2.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1640341100994345		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 1.1640341100994345 | validation: 0.958099337852152]
	TIME [epoch: 2.73 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1892758847427605		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 1.1892758847427605 | validation: 0.7815419305064943]
	TIME [epoch: 2.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1611928858028961		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 1.1611928858028961 | validation: 0.8091550976599123]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1598421117278097		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 1.1598421117278097 | validation: 0.8446050682740354]
	TIME [epoch: 2.73 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844210527446877		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 1.1844210527446877 | validation: 0.7755869900867293]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1752437368003075		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 1.1752437368003075 | validation: 0.8685001225651465]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1928515098037762		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 1.1928515098037762 | validation: 0.7773596208814864]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.203151024025498		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 1.203151024025498 | validation: 0.7977956835435451]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1907722431328795		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 1.1907722431328795 | validation: 0.8131441625269938]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1867598531859955		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 1.1867598531859955 | validation: 0.8875437702660843]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1982901068843546		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 1.1982901068843546 | validation: 0.8203153415460145]
	TIME [epoch: 2.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1934574020251076		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 1.1934574020251076 | validation: 0.7422225863700211]
	TIME [epoch: 2.73 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207608031442635		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 1.207608031442635 | validation: 0.8680434358114705]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1890267481655514		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 1.1890267481655514 | validation: 0.7979786995698716]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1789780556918095		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 1.1789780556918095 | validation: 0.8282278677160785]
	TIME [epoch: 2.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1791985913835927		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 1.1791985913835927 | validation: 0.7834186820214383]
	TIME [epoch: 2.73 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176470393072032		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 1.176470393072032 | validation: 0.8085043940157693]
	TIME [epoch: 2.73 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1604880420382069		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 1.1604880420382069 | validation: 0.7893529165625968]
	TIME [epoch: 2.78 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16034969407325		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 1.16034969407325 | validation: 0.7746314393533944]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460792278156482		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 1.1460792278156482 | validation: 0.8692358646860596]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1277181973962338		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 1.1277181973962338 | validation: 0.7989536411662881]
	TIME [epoch: 2.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1189134732383432		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 1.1189134732383432 | validation: 0.777186926819418]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1194744248713315		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 1.1194744248713315 | validation: 0.924097342578858]
	TIME [epoch: 2.84 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1480426543022124		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 1.1480426543022124 | validation: 0.7859180241048268]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221156065682631		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 1.1221156065682631 | validation: 0.808330446031863]
	TIME [epoch: 2.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283882098090676		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 1.1283882098090676 | validation: 0.8134532216777148]
	TIME [epoch: 2.73 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0982377851638527		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 1.0982377851638527 | validation: 0.7329942377023435]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134412037208599		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 1.134412037208599 | validation: 0.8099243059739604]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1077559270552035		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 1.1077559270552035 | validation: 0.7828132167826221]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114033916220862		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 1.114033916220862 | validation: 0.7852693272168613]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0933832166029904		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 1.0933832166029904 | validation: 0.8511514666409236]
	TIME [epoch: 2.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119248819252184		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 1.119248819252184 | validation: 0.7395090853598297]
	TIME [epoch: 2.72 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122343479507104		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 1.1122343479507104 | validation: 0.8145907831953534]
	TIME [epoch: 2.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1070878987282364		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 1.1070878987282364 | validation: 0.789469474378156]
	TIME [epoch: 2.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1154735106633458		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 1.1154735106633458 | validation: 0.7391479554807967]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1069403836345089		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 1.1069403836345089 | validation: 0.8599828185969494]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1133644714586302		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 1.1133644714586302 | validation: 0.7686052075511172]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108811277028239		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 1.1108811277028239 | validation: 0.7883939298684117]
	TIME [epoch: 2.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1029873939422883		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 1.1029873939422883 | validation: 0.8162930138379565]
	TIME [epoch: 2.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103370087485342		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 1.103370087485342 | validation: 0.7455413447919464]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1116010737678057		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 1.1116010737678057 | validation: 0.7850729029861452]
	TIME [epoch: 2.72 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1097412357021312		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 1.1097412357021312 | validation: 0.8600877349936682]
	TIME [epoch: 2.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056051982280082		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 1.1056051982280082 | validation: 0.7625893791992078]
	TIME [epoch: 2.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104451526408426		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 1.104451526408426 | validation: 0.8449786485993139]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1136487090339875		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 1.1136487090339875 | validation: 0.7850960008404654]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1760138157291062		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 1.1760138157291062 | validation: 0.8371182145092662]
	TIME [epoch: 2.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1748645630056496		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 1.1748645630056496 | validation: 0.7808775694021494]
	TIME [epoch: 2.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1289247777869702		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 1.1289247777869702 | validation: 0.889185231990357]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1510276089277762		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 1.1510276089277762 | validation: 0.8032857721711173]
	TIME [epoch: 2.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1438364181953455		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 1.1438364181953455 | validation: 0.7948484483904616]
	TIME [epoch: 2.72 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1255540811753244		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 1.1255540811753244 | validation: 0.9055663414606299]
	TIME [epoch: 2.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535999759477304		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 1.1535999759477304 | validation: 0.8142473083359619]
	TIME [epoch: 2.72 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1245393746561083		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 1.1245393746561083 | validation: 0.747265233274983]
	TIME [epoch: 2.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370461448687499		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 1.1370461448687499 | validation: 0.8250071283563364]
	TIME [epoch: 2.72 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1304365626280675		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 1.1304365626280675 | validation: 0.8229050186357885]
	TIME [epoch: 2.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216814080801367		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 1.1216814080801367 | validation: 0.7746878422369717]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1205914264223344		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 1.1205914264223344 | validation: 0.8266603741441657]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181833297023627		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 1.1181833297023627 | validation: 0.818589116014196]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150645547114093		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 1.1150645547114093 | validation: 0.7818941026808073]
	TIME [epoch: 2.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1228961050528048		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 1.1228961050528048 | validation: 0.7861023962191194]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185028571780418		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 1.1185028571780418 | validation: 0.7797975327834552]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994805868752293		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 1.0994805868752293 | validation: 0.8175726871520947]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1099881057706649		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 1.1099881057706649 | validation: 0.806603618441592]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1046173074952608		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 1.1046173074952608 | validation: 0.7952614826097956]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1087971982753015		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 1.1087971982753015 | validation: 0.740463329300162]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183068351372907		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 1.1183068351372907 | validation: 0.8139162960626058]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1074065072643697		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 1.1074065072643697 | validation: 0.8106618039700151]
	TIME [epoch: 2.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102779640870283		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 1.102779640870283 | validation: 0.761292252480481]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1194592991480803		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 1.1194592991480803 | validation: 0.8324539623049345]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1119854379497616		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 1.1119854379497616 | validation: 0.8834785496511373]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1212294988224567		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 1.1212294988224567 | validation: 0.799442400933517]
	TIME [epoch: 2.76 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1099366772867871		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 1.1099366772867871 | validation: 0.7779655107723568]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1094715276823164		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 1.1094715276823164 | validation: 0.7632768664710572]
	TIME [epoch: 2.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122482504608033		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 1.1122482504608033 | validation: 0.8375559416207002]
	TIME [epoch: 2.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1062187172908136		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 1.1062187172908136 | validation: 0.7729698190976738]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163678642952384		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 1.1163678642952384 | validation: 0.8285205288674915]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1145349307964982		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 1.1145349307964982 | validation: 0.7769404203040438]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1045062094617408		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 1.1045062094617408 | validation: 0.8250580410419558]
	TIME [epoch: 2.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105901176205204		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 1.105901176205204 | validation: 0.757275735161556]
	TIME [epoch: 2.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104896115951999		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 1.104896115951999 | validation: 0.7619860433874361]
	TIME [epoch: 2.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104480509745299		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 1.104480509745299 | validation: 0.8804993559321211]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1257162467133548		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 1.1257162467133548 | validation: 0.7647995873537492]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1045238481018524		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 1.1045238481018524 | validation: 0.8033076396002136]
	TIME [epoch: 2.72 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1119832231651015		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 1.1119832231651015 | validation: 0.7559496972258292]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1104273078013653		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 1.1104273078013653 | validation: 0.7760667362917775]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107759445657418		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 1.107759445657418 | validation: 0.852683852439009]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1107528421934714		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 1.1107528421934714 | validation: 0.7587754043841907]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116513127110719		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 1.116513127110719 | validation: 0.8015018731230685]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0930841997356708		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 1.0930841997356708 | validation: 0.7422792280678248]
	TIME [epoch: 2.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120966684429737		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 1.120966684429737 | validation: 0.8571388404741335]
	TIME [epoch: 2.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122237488552562		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 1.1122237488552562 | validation: 0.7632343595676617]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0988475048092026		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 1.0988475048092026 | validation: 0.7610955068377289]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099544645200545		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 1.099544645200545 | validation: 0.7637201411632607]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184646539334944		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 1.1184646539334944 | validation: 0.814462091624677]
	TIME [epoch: 2.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1126665248005354		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 1.1126665248005354 | validation: 0.7527621972408967]
	TIME [epoch: 2.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1265490523392827		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 1.1265490523392827 | validation: 0.7795290324842619]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182367971931286		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 1.1182367971931286 | validation: 0.8892294902657089]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1298987378236787		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 1.1298987378236787 | validation: 0.7700466404334035]
	TIME [epoch: 2.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12143224257917		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 1.12143224257917 | validation: 0.7828911574928047]
	TIME [epoch: 2.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1236684380974915		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 1.1236684380974915 | validation: 0.8596043761793108]
	TIME [epoch: 2.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1222586998391986		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 1.1222586998391986 | validation: 0.8033110429703241]
	TIME [epoch: 2.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1204643574009125		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 1.1204643574009125 | validation: 0.7846310231994283]
	TIME [epoch: 2.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1206039392685783		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 1.1206039392685783 | validation: 0.7934273836044219]
	TIME [epoch: 2.84 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111229574838751		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 1.111229574838751 | validation: 0.8608328787032733]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1213860512488285		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 1.1213860512488285 | validation: 0.7674699564353344]
	TIME [epoch: 2.8 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1021952354344005		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 1.1021952354344005 | validation: 0.7980952046466183]
	TIME [epoch: 2.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098833485358316		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 1.098833485358316 | validation: 0.8089263428959867]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1128529825146645		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 1.1128529825146645 | validation: 0.8308662228250191]
	TIME [epoch: 2.74 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167065258564675		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 1.1167065258564675 | validation: 0.8044395547481857]
	TIME [epoch: 2.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111392707000146		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 1.111392707000146 | validation: 0.7643458624469236]
	TIME [epoch: 2.83 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1082587014068495		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 1.1082587014068495 | validation: 0.7989124983035018]
	TIME [epoch: 2.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102972301214269		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 1.102972301214269 | validation: 0.8491932960913721]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1105014811876266		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 1.1105014811876266 | validation: 0.8039472715581205]
	TIME [epoch: 2.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1032632117028855		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 1.1032632117028855 | validation: 0.8279132949833793]
	TIME [epoch: 2.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078467517574349		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 1.1078467517574349 | validation: 0.7939965432929333]
	TIME [epoch: 2.73 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0862146333207394		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 1.0862146333207394 | validation: 0.7474707453408606]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0978330193564072		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 1.0978330193564072 | validation: 0.8543203461873715]
	TIME [epoch: 2.74 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103665921561593		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 1.103665921561593 | validation: 0.7628440714843848]
	TIME [epoch: 2.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1217018055329002		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 1.1217018055329002 | validation: 0.8028672164162443]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1436498792431544		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 1.1436498792431544 | validation: 0.7906468865482736]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1355274226544272		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 1.1355274226544272 | validation: 0.8150668903824727]
	TIME [epoch: 2.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281878665807026		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 1.1281878665807026 | validation: 0.8101460446859072]
	TIME [epoch: 2.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281710477047475		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 1.1281710477047475 | validation: 0.826505840933171]
	TIME [epoch: 2.85 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129937905776308		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 1.129937905776308 | validation: 0.8281002776638844]
	TIME [epoch: 2.74 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137622699949328		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 1.1137622699949328 | validation: 0.7846514341428734]
	TIME [epoch: 2.73 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1134925176500323		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 1.1134925176500323 | validation: 0.7783687964054172]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0993646275046591		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 1.0993646275046591 | validation: 0.8795164465934534]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1255703867830154		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 1.1255703867830154 | validation: 0.8208277840531992]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_1_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_1_v_mmd4_831.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2231.049 seconds.
