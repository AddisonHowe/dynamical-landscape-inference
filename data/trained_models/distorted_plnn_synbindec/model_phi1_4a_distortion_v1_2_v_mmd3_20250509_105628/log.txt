Args:
Namespace(name='model_phi1_4a_distortion_v1_2_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.04682813, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3597781466

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.392470843573791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.392470843573791 | validation: 3.2562993955986217]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2048032138327733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2048032138327733 | validation: 3.9735299153826418]
	TIME [epoch: 0.589 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.734513844547485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.734513844547485 | validation: 3.730730669748431]
	TIME [epoch: 0.831 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7782047290326695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7782047290326695 | validation: 2.7731136439985695]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.064372349926983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.064372349926983 | validation: 3.014512893550258]
	TIME [epoch: 0.576 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9988847143111186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9988847143111186 | validation: 2.7839469989086614]
	TIME [epoch: 0.573 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8951270336295374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8951270336295374 | validation: 2.667402710666286]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.833117047370905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.833117047370905 | validation: 2.679633662992879]
	TIME [epoch: 0.573 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.780580434088619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.780580434088619 | validation: 2.3283051494673046]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7141805372051704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7141805372051704 | validation: 2.2249552476025083]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6339107273710214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6339107273710214 | validation: 2.1440669391811515]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5486811009393118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5486811009393118 | validation: 2.025067094272323]
	TIME [epoch: 0.58 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4740235343497994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4740235343497994 | validation: 1.9978518256869187]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.446923500065345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.446923500065345 | validation: 2.975157992572357]
	TIME [epoch: 0.579 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.744929311625024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.744929311625024 | validation: 2.566417359663365]
	TIME [epoch: 0.574 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.564809372822732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.564809372822732 | validation: 2.1523184202267074]
	TIME [epoch: 0.574 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4485594431461135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4485594431461135 | validation: 1.858966192249049]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3213455002351044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3213455002351044 | validation: 1.749797749568559]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2788605684414303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2788605684414303 | validation: 1.6335375574115674]
	TIME [epoch: 0.577 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1987011104244347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1987011104244347 | validation: 1.6057340682100436]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.166249578513673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.166249578513673 | validation: 1.5831822447961672]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1235616481847335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1235616481847335 | validation: 1.5501514716349458]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1212837910415767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1212837910415767 | validation: 1.9096125981520615]
	TIME [epoch: 0.575 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1745401922390215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1745401922390215 | validation: 1.716584687008421]
	TIME [epoch: 0.574 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0852886102704042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0852886102704042 | validation: 1.4733105308931305]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.005986548796489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.005986548796489 | validation: 1.4497360743566081]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.944103222502291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.944103222502291 | validation: 1.4397589672501168]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9118928270879303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9118928270879303 | validation: 1.4065115611465242]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.891349226711124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.891349226711124 | validation: 1.428781285447001]
	TIME [epoch: 0.574 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8689653547393053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8689653547393053 | validation: 1.418176247885129]
	TIME [epoch: 0.574 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8574831281860007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8574831281860007 | validation: 1.416925200591598]
	TIME [epoch: 0.573 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8576284107612804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8576284107612804 | validation: 1.4135510729703644]
	TIME [epoch: 0.572 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8532588444754736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8532588444754736 | validation: 1.4114078483773715]
	TIME [epoch: 0.572 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8524277311665325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8524277311665325 | validation: 1.4216469423796405]
	TIME [epoch: 0.572 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8485124067128678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8485124067128678 | validation: 1.4272847664681274]
	TIME [epoch: 0.571 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8539283545296308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8539283545296308 | validation: 1.4260909486238138]
	TIME [epoch: 0.57 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8501385727811943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8501385727811943 | validation: 1.4224661714782028]
	TIME [epoch: 0.569 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8506390911121804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8506390911121804 | validation: 1.429707389309157]
	TIME [epoch: 0.57 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.856139430056228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.856139430056228 | validation: 1.4307226183170894]
	TIME [epoch: 0.57 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8502539772858309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8502539772858309 | validation: 1.4374441494955983]
	TIME [epoch: 0.57 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8473003475315841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8473003475315841 | validation: 1.4296949252030322]
	TIME [epoch: 0.57 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8486448318989086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8486448318989086 | validation: 1.432760588922445]
	TIME [epoch: 0.57 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.85074825733112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.85074825733112 | validation: 1.4477966894375998]
	TIME [epoch: 0.57 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8487001703071126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8487001703071126 | validation: 1.4322980845891635]
	TIME [epoch: 0.57 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8440508984699393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8440508984699393 | validation: 1.4378951589996136]
	TIME [epoch: 0.572 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8471509631863194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8471509631863194 | validation: 1.4467898407867779]
	TIME [epoch: 0.57 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.844966131119414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.844966131119414 | validation: 1.4544865197389514]
	TIME [epoch: 0.569 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8502262976742674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8502262976742674 | validation: 1.452864281851608]
	TIME [epoch: 0.569 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8543563419156233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8543563419156233 | validation: 1.5387742423262791]
	TIME [epoch: 0.57 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.879903578014455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.879903578014455 | validation: 1.4674327241645715]
	TIME [epoch: 0.569 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8471741044058079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8471741044058079 | validation: 1.4613903980035712]
	TIME [epoch: 0.57 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.847030370494372		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.847030370494372 | validation: 1.4712911514387805]
	TIME [epoch: 0.576 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8431838731395318		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.8431838731395318 | validation: 1.4698574179878168]
	TIME [epoch: 0.569 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.839183833198148		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.839183833198148 | validation: 1.4585309240918163]
	TIME [epoch: 0.571 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8420481995431774		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.8420481995431774 | validation: 1.4708519191381548]
	TIME [epoch: 0.569 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8425729243551552		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.8425729243551552 | validation: 1.4611176959375265]
	TIME [epoch: 0.57 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8368614690315275		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.8368614690315275 | validation: 1.4982721056051724]
	TIME [epoch: 0.572 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.842840134461032		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.842840134461032 | validation: 1.471867912567099]
	TIME [epoch: 0.569 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8420040665193793		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.8420040665193793 | validation: 1.4909293268787622]
	TIME [epoch: 0.568 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8353299061100325		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.8353299061100325 | validation: 1.486668353369559]
	TIME [epoch: 0.569 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8454887822748935		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.8454887822748935 | validation: 1.526600561686042]
	TIME [epoch: 0.571 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8468555650992073		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.8468555650992073 | validation: 1.4972172905496757]
	TIME [epoch: 0.57 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.836061831107621		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.836061831107621 | validation: 1.4839178319137503]
	TIME [epoch: 0.569 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8375080459206763		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.8375080459206763 | validation: 1.527020647418378]
	TIME [epoch: 0.569 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8423305663170937		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.8423305663170937 | validation: 1.4963764191706592]
	TIME [epoch: 0.57 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.829079536638406		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.829079536638406 | validation: 1.5013938048367939]
	TIME [epoch: 0.571 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8329200141254756		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.8329200141254756 | validation: 1.5083309278042263]
	TIME [epoch: 0.57 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8288551182875503		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.8288551182875503 | validation: 1.5043641024335308]
	TIME [epoch: 0.57 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.830126777294206		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.830126777294206 | validation: 1.516681953053103]
	TIME [epoch: 0.572 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8304577160804645		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.8304577160804645 | validation: 1.5106448800265058]
	TIME [epoch: 0.575 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8264515006257216		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8264515006257216 | validation: 1.528906091658781]
	TIME [epoch: 0.572 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828040683006868		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.828040683006868 | validation: 1.5218248694793632]
	TIME [epoch: 0.574 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828432998441068		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.828432998441068 | validation: 1.5229553522347974]
	TIME [epoch: 0.569 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8255259658521192		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.8255259658521192 | validation: 1.5387113396174783]
	TIME [epoch: 0.569 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8288880333809554		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.8288880333809554 | validation: 1.5497762691166668]
	TIME [epoch: 0.568 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8338139278534513		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.8338139278534513 | validation: 1.5660866819513573]
	TIME [epoch: 0.569 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8376606385979901		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.8376606385979901 | validation: 1.5285254621508646]
	TIME [epoch: 0.567 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8239427216257058		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.8239427216257058 | validation: 1.5318155646629439]
	TIME [epoch: 0.568 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.824647374817989		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.824647374817989 | validation: 1.5516257535679707]
	TIME [epoch: 0.57 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828042985625068		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.828042985625068 | validation: 1.5461136179056951]
	TIME [epoch: 0.571 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8220011188132212		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.8220011188132212 | validation: 1.5505361778234308]
	TIME [epoch: 0.57 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8231438194635508		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.8231438194635508 | validation: 1.5446377443701502]
	TIME [epoch: 0.569 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8222011849234172		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.8222011849234172 | validation: 1.562418555930764]
	TIME [epoch: 0.57 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8170959708540517		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.8170959708540517 | validation: 1.571647936838617]
	TIME [epoch: 0.571 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8173557075994589		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.8173557075994589 | validation: 1.5558692623972252]
	TIME [epoch: 0.569 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8221440538417932		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.8221440538417932 | validation: 1.5693426358557452]
	TIME [epoch: 0.57 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8159687772920325		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.8159687772920325 | validation: 1.5855018514762633]
	TIME [epoch: 0.571 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.822036720757239		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.822036720757239 | validation: 1.5897281971367703]
	TIME [epoch: 0.571 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.822573819915211		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.822573819915211 | validation: 1.5931607020686707]
	TIME [epoch: 0.57 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8246279093173832		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.8246279093173832 | validation: 1.5805954579314825]
	TIME [epoch: 0.569 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8118750056471147		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.8118750056471147 | validation: 1.593921992719126]
	TIME [epoch: 0.57 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.813093588651526		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.813093588651526 | validation: 1.6025284492391056]
	TIME [epoch: 0.57 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8118815005469409		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.8118815005469409 | validation: 1.5890027033343248]
	TIME [epoch: 0.569 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.809352452758457		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.809352452758457 | validation: 1.5957587040680696]
	TIME [epoch: 0.57 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8043567649325691		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.8043567649325691 | validation: 1.607298300620991]
	TIME [epoch: 0.571 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8054823704429277		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.8054823704429277 | validation: 1.6033906454595042]
	TIME [epoch: 0.571 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8064060188627273		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.8064060188627273 | validation: 1.6152799138868812]
	TIME [epoch: 0.571 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8046082787348265		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.8046082787348265 | validation: 1.6155747536373946]
	TIME [epoch: 0.572 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8151593175627903		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.8151593175627903 | validation: 1.6518618798394369]
	TIME [epoch: 0.571 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8223134504678131		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.8223134504678131 | validation: 1.6087556900247681]
	TIME [epoch: 0.569 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8067936123555932		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.8067936123555932 | validation: 1.6305017330122444]
	TIME [epoch: 0.572 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8084526381957367		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.8084526381957367 | validation: 1.6408483076044948]
	TIME [epoch: 0.573 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8080125053194855		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8080125053194855 | validation: 1.6242410599503456]
	TIME [epoch: 0.577 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.798762990054097		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.798762990054097 | validation: 1.6325994940156094]
	TIME [epoch: 0.574 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.798590811546993		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.798590811546993 | validation: 1.6379668552731628]
	TIME [epoch: 0.571 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7948923891325042		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.7948923891325042 | validation: 1.6493369067119183]
	TIME [epoch: 0.571 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7961916663085964		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.7961916663085964 | validation: 1.6389787108433307]
	TIME [epoch: 0.57 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7981985601291757		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.7981985601291757 | validation: 1.652613936563852]
	TIME [epoch: 0.57 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.786637404507587		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.786637404507587 | validation: 1.6493707819447636]
	TIME [epoch: 0.57 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7865886460302014		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.7865886460302014 | validation: 1.6405820745793165]
	TIME [epoch: 0.569 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.772445023830575		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.772445023830575 | validation: 1.6583471643903078]
	TIME [epoch: 0.569 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7771002885564742		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.7771002885564742 | validation: 1.660446198863905]
	TIME [epoch: 0.569 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8227776013764743		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.8227776013764743 | validation: 1.6841594484876943]
	TIME [epoch: 0.57 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7793941250760759		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.7793941250760759 | validation: 1.7068747745979473]
	TIME [epoch: 0.569 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7784224803490956		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.7784224803490956 | validation: 1.6503366950726965]
	TIME [epoch: 0.57 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7761418667547992		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.7761418667547992 | validation: 1.6448998915600477]
	TIME [epoch: 0.57 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7607146845015502		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.7607146845015502 | validation: 1.6853406398040889]
	TIME [epoch: 0.571 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7653086300032843		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.7653086300032843 | validation: 1.6558447227009638]
	TIME [epoch: 0.587 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7537151104342714		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.7537151104342714 | validation: 1.650885175334966]
	TIME [epoch: 0.57 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7394755106945763		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.7394755106945763 | validation: 1.684565643769939]
	TIME [epoch: 0.57 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7457096715588551		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.7457096715588551 | validation: 1.6497571019529638]
	TIME [epoch: 0.573 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.743753798493481		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.743753798493481 | validation: 1.6511475807357994]
	TIME [epoch: 0.57 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7237287591857904		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.7237287591857904 | validation: 1.6464319997307524]
	TIME [epoch: 0.588 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7138623012258916		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.7138623012258916 | validation: 1.6312821596867721]
	TIME [epoch: 0.572 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7053774827471306		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.7053774827471306 | validation: 1.6764195409295153]
	TIME [epoch: 0.571 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7264781010194883		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.7264781010194883 | validation: 1.668248293325615]
	TIME [epoch: 0.57 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8074258804572179		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.8074258804572179 | validation: 1.6541624143139466]
	TIME [epoch: 0.572 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7208144132752903		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.7208144132752903 | validation: 1.7379236149628157]
	TIME [epoch: 0.572 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7524762990481861		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.7524762990481861 | validation: 1.6588220231196198]
	TIME [epoch: 0.57 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.688100144604089		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.688100144604089 | validation: 1.64605692164752]
	TIME [epoch: 0.57 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6831972034575433		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.6831972034575433 | validation: 1.6742016635713342]
	TIME [epoch: 0.57 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.682770718513682		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.682770718513682 | validation: 1.6362275476740238]
	TIME [epoch: 0.571 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6560975050501898		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.6560975050501898 | validation: 1.640427665656866]
	TIME [epoch: 0.569 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.631672182155656		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.631672182155656 | validation: 1.6185521763542625]
	TIME [epoch: 0.57 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6118538585915014		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.6118538585915014 | validation: 1.6066232644162797]
	TIME [epoch: 0.57 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6315094978123592		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.6315094978123592 | validation: 1.7040206669540567]
	TIME [epoch: 0.57 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6619069023806958		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.6619069023806958 | validation: 1.5835641727567276]
	TIME [epoch: 0.569 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5208534702250194		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.5208534702250194 | validation: 1.5184296452912416]
	TIME [epoch: 0.57 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5025949094005853		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5025949094005853 | validation: 1.5588619201282128]
	TIME [epoch: 0.571 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4768325544948		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.4768325544948 | validation: 1.4706216566690669]
	TIME [epoch: 0.569 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3911598035224555		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3911598035224555 | validation: 1.4989688226528795]
	TIME [epoch: 0.57 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4027748064998804		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.4027748064998804 | validation: 1.5218905274130812]
	TIME [epoch: 0.572 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.435050738179836		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.435050738179836 | validation: 1.487893528653826]
	TIME [epoch: 0.57 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4651856661128664		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.4651856661128664 | validation: 1.400651835731042]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.276286952536814		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.276286952536814 | validation: 1.399184095900387]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.377107021753935		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.377107021753935 | validation: 1.4381210469077055]
	TIME [epoch: 0.575 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2895189449914717		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.2895189449914717 | validation: 1.419259695536434]
	TIME [epoch: 0.572 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2620503814183255		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.2620503814183255 | validation: 1.3447218171928357]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2790454271603477		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2790454271603477 | validation: 1.3397255410561575]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228842274842066		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.228842274842066 | validation: 1.3580520154990081]
	TIME [epoch: 0.575 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2360168530155178		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.2360168530155178 | validation: 1.400125002935473]
	TIME [epoch: 0.58 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3056034489584978		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3056034489584978 | validation: 1.465756893063622]
	TIME [epoch: 0.571 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3496908526383373		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.3496908526383373 | validation: 1.2646169950415573]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2298761386013626		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2298761386013626 | validation: 1.4058500381861962]
	TIME [epoch: 0.571 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2356381671735985		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.2356381671735985 | validation: 1.290203951560949]
	TIME [epoch: 0.57 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1888476656457685		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1888476656457685 | validation: 1.2794650991149084]
	TIME [epoch: 0.571 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1744487007642066		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1744487007642066 | validation: 1.2986666271780347]
	TIME [epoch: 0.571 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1752118489331103		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1752118489331103 | validation: 1.2851979606788293]
	TIME [epoch: 0.571 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190606603510398		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.190606603510398 | validation: 1.358571596250148]
	TIME [epoch: 0.571 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2640205949528949		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2640205949528949 | validation: 1.328690672424339]
	TIME [epoch: 0.572 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.239024509091625		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.239024509091625 | validation: 1.3143471137663392]
	TIME [epoch: 0.569 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1931645830091868		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1931645830091868 | validation: 1.2732864875953214]
	TIME [epoch: 0.569 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1538676907683085		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1538676907683085 | validation: 1.2532858608541426]
	TIME [epoch: 0.569 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175964225747024		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.175964225747024 | validation: 1.2739989712543573]
	TIME [epoch: 0.572 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254389425019644		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1254389425019644 | validation: 1.2091507528977716]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1099919702169059		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1099919702169059 | validation: 1.4094911768583216]
	TIME [epoch: 0.572 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241919457636614		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.241919457636614 | validation: 1.4281681464375198]
	TIME [epoch: 0.57 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3214541249473337		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.3214541249473337 | validation: 1.1859852141073373]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2039155603561107		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2039155603561107 | validation: 1.326523002501812]
	TIME [epoch: 0.575 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154281113782951		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.154281113782951 | validation: 1.2076185885673076]
	TIME [epoch: 0.572 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0934833968815572		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0934833968815572 | validation: 1.1660827769497295]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0859537730683413		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0859537730683413 | validation: 1.6433610241052343]
	TIME [epoch: 0.574 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3947669163771077		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3947669163771077 | validation: 1.3048723186716353]
	TIME [epoch: 0.571 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1332742907280702		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1332742907280702 | validation: 1.2068085260305832]
	TIME [epoch: 0.572 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143626081747047		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.143626081747047 | validation: 1.198478088308686]
	TIME [epoch: 0.571 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0824847631427221		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.0824847631427221 | validation: 1.1593999000603517]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0591044186331628		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0591044186331628 | validation: 1.1434786684768878]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064177831878712		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.064177831878712 | validation: 1.2289880168085814]
	TIME [epoch: 0.572 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313059349207784		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1313059349207784 | validation: 1.4216822939869633]
	TIME [epoch: 0.571 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3067072393778947		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3067072393778947 | validation: 1.1738803688468973]
	TIME [epoch: 0.571 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1267495514750812		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1267495514750812 | validation: 1.1651634434470277]
	TIME [epoch: 0.573 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0460878257328914		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0460878257328914 | validation: 1.1277545230532906]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0405740601724494		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0405740601724494 | validation: 1.1337919795002285]
	TIME [epoch: 0.574 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339772666343423		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.0339772666343423 | validation: 1.1286922553615726]
	TIME [epoch: 0.572 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0151143606507378		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0151143606507378 | validation: 1.0946343207405453]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0507814855118895		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0507814855118895 | validation: 1.3081483620653005]
	TIME [epoch: 0.573 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1307359913340898		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1307359913340898 | validation: 1.1734921835646355]
	TIME [epoch: 0.573 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127465358920892		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1127465358920892 | validation: 1.1310003467440317]
	TIME [epoch: 0.571 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0718387970482137		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.0718387970482137 | validation: 1.308031698264366]
	TIME [epoch: 0.573 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009885406203906		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.2009885406203906 | validation: 1.2912195181367387]
	TIME [epoch: 0.57 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1667288031347947		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1667288031347947 | validation: 1.3395922368921909]
	TIME [epoch: 0.569 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2718125243016862		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2718125243016862 | validation: 1.2315751609740633]
	TIME [epoch: 0.569 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221680666757188		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.221680666757188 | validation: 1.0929413993153736]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345458032414388		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1345458032414388 | validation: 1.1433677133133415]
	TIME [epoch: 0.573 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1495696259289931		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.1495696259289931 | validation: 1.081629682882442]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286001715579976		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1286001715579976 | validation: 1.0750102012220744]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1343564286931878		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1343564286931878 | validation: 1.1012475500622674]
	TIME [epoch: 0.575 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196716514266363		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.1196716514266363 | validation: 1.0656116360197483]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1530912498912214		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.1530912498912214 | validation: 1.185115806131299]
	TIME [epoch: 0.574 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2609047011001742		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2609047011001742 | validation: 1.1671223649393454]
	TIME [epoch: 0.572 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2198248078400373		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2198248078400373 | validation: 1.1277028114259573]
	TIME [epoch: 171 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1341681427586088		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.1341681427586088 | validation: 1.0801892433215639]
	TIME [epoch: 1.13 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1189880497653144		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1189880497653144 | validation: 1.0849549522316508]
	TIME [epoch: 1.13 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1014550317736462		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1014550317736462 | validation: 1.0487870820356648]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0553368396037976		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.0553368396037976 | validation: 1.1852431972120705]
	TIME [epoch: 1.12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130073279444374		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0130073279444374 | validation: 1.086834845186413]
	TIME [epoch: 1.12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978051612747456		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.978051612747456 | validation: 1.1227170078777715]
	TIME [epoch: 1.12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0264285632922074		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.0264285632922074 | validation: 1.2295026863913439]
	TIME [epoch: 1.12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150628713995387		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.150628713995387 | validation: 1.2852628625734352]
	TIME [epoch: 1.12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2476587771943564		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.2476587771943564 | validation: 1.0791842239887366]
	TIME [epoch: 1.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0324051856131151		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.0324051856131151 | validation: 1.084698522953704]
	TIME [epoch: 1.12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597789224922523		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9597789224922523 | validation: 1.053595704155704]
	TIME [epoch: 1.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.963088768779059		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.963088768779059 | validation: 1.0492362948166452]
	TIME [epoch: 1.12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9831717655152216		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.9831717655152216 | validation: 1.679932161314891]
	TIME [epoch: 1.12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3929930037526181		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.3929930037526181 | validation: 1.1470785659370186]
	TIME [epoch: 1.12 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0345739652305652		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0345739652305652 | validation: 1.1399615168951305]
	TIME [epoch: 1.12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0517707661631375		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.0517707661631375 | validation: 1.096546612416554]
	TIME [epoch: 1.12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705247506250008		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9705247506250008 | validation: 1.0923075323321705]
	TIME [epoch: 1.12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9673836006140554		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9673836006140554 | validation: 1.0120891957786198]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9734200123441609		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9734200123441609 | validation: 1.130212491344835]
	TIME [epoch: 1.12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9895455007883642		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9895455007883642 | validation: 1.013299076833367]
	TIME [epoch: 1.12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9634566522080595		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9634566522080595 | validation: 1.0431588502502287]
	TIME [epoch: 1.12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267761356169694		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9267761356169694 | validation: 0.9874526658148302]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926331993661656		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.926331993661656 | validation: 1.018460731002682]
	TIME [epoch: 1.12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9510936756453617		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9510936756453617 | validation: 1.3017297870072655]
	TIME [epoch: 1.12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.238676392280686		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.238676392280686 | validation: 1.2473211011285041]
	TIME [epoch: 1.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.158294952409073		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.158294952409073 | validation: 1.0051310859838611]
	TIME [epoch: 1.12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9403954964067127		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9403954964067127 | validation: 1.0766764484417028]
	TIME [epoch: 1.13 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9512496619946889		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9512496619946889 | validation: 0.9966916538343216]
	TIME [epoch: 1.12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9252481952396531		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.9252481952396531 | validation: 1.002054830950273]
	TIME [epoch: 1.12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9176602908770783		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.9176602908770783 | validation: 1.010115784220952]
	TIME [epoch: 1.12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905614984056336		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.905614984056336 | validation: 0.9748349088500348]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225469348732334		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.9225469348732334 | validation: 1.0700218341007302]
	TIME [epoch: 1.12 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640540961987175		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.9640540961987175 | validation: 1.0871729053561734]
	TIME [epoch: 1.12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.076383969585845		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.076383969585845 | validation: 1.1868931932238758]
	TIME [epoch: 1.12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1389952326887556		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.1389952326887556 | validation: 0.9594019444795845]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9174637478027587		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9174637478027587 | validation: 1.0505994700978927]
	TIME [epoch: 1.12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248851040495915		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.9248851040495915 | validation: 0.9722273843044879]
	TIME [epoch: 1.12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9734824184172075		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9734824184172075 | validation: 1.1137924649696769]
	TIME [epoch: 1.11 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0229817104392054		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.0229817104392054 | validation: 1.012358003141652]
	TIME [epoch: 1.11 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0048359027915093		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.0048359027915093 | validation: 1.0604598174195077]
	TIME [epoch: 1.11 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9961399503261729		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.9961399503261729 | validation: 1.0116471994085108]
	TIME [epoch: 1.12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196300332339266		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.9196300332339266 | validation: 0.931105078028919]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039898789257282		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9039898789257282 | validation: 1.000045459107835]
	TIME [epoch: 1.12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267604507096735		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.9267604507096735 | validation: 0.9932236917764743]
	TIME [epoch: 1.12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9385387678059581		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9385387678059581 | validation: 0.9638399846205948]
	TIME [epoch: 1.12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332671093389103		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9332671093389103 | validation: 1.0750555477916366]
	TIME [epoch: 1.12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9949446174869979		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9949446174869979 | validation: 1.0881927531307956]
	TIME [epoch: 1.12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0769484115593404		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.0769484115593404 | validation: 0.8961121499263709]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054628936820137		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9054628936820137 | validation: 1.2939206281271591]
	TIME [epoch: 1.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.071159734171479		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.071159734171479 | validation: 0.9345615694818185]
	TIME [epoch: 1.12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092253064112541		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.9092253064112541 | validation: 0.9249460929723523]
	TIME [epoch: 1.12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687665573588911		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.8687665573588911 | validation: 1.6239326871479471]
	TIME [epoch: 1.12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4464104312856636		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.4464104312856636 | validation: 1.7076557759756732]
	TIME [epoch: 1.12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5008753701185862		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.5008753701185862 | validation: 1.6596390853287872]
	TIME [epoch: 1.11 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4480249171886328		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.4480249171886328 | validation: 1.5255365410439659]
	TIME [epoch: 1.11 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383443713964159		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.383443713964159 | validation: 1.1806273704387562]
	TIME [epoch: 1.11 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2219497365061007		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.2219497365061007 | validation: 0.8886885695231153]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0331692685571132		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.0331692685571132 | validation: 1.0033261629012866]
	TIME [epoch: 1.12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0780569864015856		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0780569864015856 | validation: 0.9796032260754434]
	TIME [epoch: 1.12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055329223390133		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.055329223390133 | validation: 0.8824427888980051]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0455927627608197		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.0455927627608197 | validation: 0.9564874045175188]
	TIME [epoch: 1.12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084908721571575		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.084908721571575 | validation: 0.9137660471728323]
	TIME [epoch: 1.12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0895359476910282		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.0895359476910282 | validation: 1.0402637243667094]
	TIME [epoch: 1.12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1087127831217916		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1087127831217916 | validation: 0.8985427861473365]
	TIME [epoch: 1.11 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0760096698757198		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.0760096698757198 | validation: 0.922344411477862]
	TIME [epoch: 1.12 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0442398568539502		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.0442398568539502 | validation: 0.8790673868006176]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0306244581865263		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0306244581865263 | validation: 0.876995589042354]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0210405723700708		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0210405723700708 | validation: 0.8808272251189945]
	TIME [epoch: 1.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021791824264349		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.021791824264349 | validation: 0.8782271237909977]
	TIME [epoch: 1.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02025062633237		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.02025062633237 | validation: 0.8591374258208392]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017407099408974		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.017407099408974 | validation: 0.8669203830526414]
	TIME [epoch: 1.12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0178098852628923		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.0178098852628923 | validation: 0.9532878325496965]
	TIME [epoch: 1.12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.056361613718727		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.056361613718727 | validation: 1.0246184098930573]
	TIME [epoch: 1.12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2197030311253128		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.2197030311253128 | validation: 1.2490843255711148]
	TIME [epoch: 1.11 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.277300291169018		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.277300291169018 | validation: 0.8711846950492762]
	TIME [epoch: 1.11 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0449725229351716		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.0449725229351716 | validation: 0.8745161931856131]
	TIME [epoch: 1.12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0198842451256251		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.0198842451256251 | validation: 0.8996251563397071]
	TIME [epoch: 1.12 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0233998980132977		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.0233998980132977 | validation: 0.8804200355935123]
	TIME [epoch: 1.11 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0514132873564987		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.0514132873564987 | validation: 0.974325975460361]
	TIME [epoch: 1.11 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0945112506725878		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0945112506725878 | validation: 0.9296400343951998]
	TIME [epoch: 1.11 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1094259983860713		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1094259983860713 | validation: 1.0648351352750376]
	TIME [epoch: 1.12 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1214551801084554		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1214551801084554 | validation: 0.882041201978002]
	TIME [epoch: 1.12 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0647449255592656		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0647449255592656 | validation: 0.9123894979116666]
	TIME [epoch: 1.12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0341639592331624		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.0341639592331624 | validation: 0.8635183481849513]
	TIME [epoch: 1.11 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.026593164610227		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.026593164610227 | validation: 0.8788135423647209]
	TIME [epoch: 1.11 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014452510472199		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.014452510472199 | validation: 0.8672525214904958]
	TIME [epoch: 1.13 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0218530682243763		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0218530682243763 | validation: 0.8846275379440975]
	TIME [epoch: 1.11 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0281350968339156		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.0281350968339156 | validation: 0.8741356086701993]
	TIME [epoch: 1.11 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0640215599045566		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.0640215599045566 | validation: 1.0467218131834972]
	TIME [epoch: 1.12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159179712042698		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.159179712042698 | validation: 1.0348539238018721]
	TIME [epoch: 1.12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1790826515426185		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.1790826515426185 | validation: 1.0009820532090734]
	TIME [epoch: 1.12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0723178028132752		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.0723178028132752 | validation: 0.8536046250595805]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0260991196780584		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0260991196780584 | validation: 0.8814575986202396]
	TIME [epoch: 1.12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121779882567907		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.0121779882567907 | validation: 0.8554006173193509]
	TIME [epoch: 1.12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0217759652350435		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0217759652350435 | validation: 0.9081917454821892]
	TIME [epoch: 1.13 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0334706121956114		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.0334706121956114 | validation: 0.8944970512906842]
	TIME [epoch: 1.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0784723687201427		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0784723687201427 | validation: 1.0378730281420758]
	TIME [epoch: 1.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1188051618182207		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.1188051618182207 | validation: 0.8987229321748832]
	TIME [epoch: 1.12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086554397314122		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.086554397314122 | validation: 0.9278892715713124]
	TIME [epoch: 1.12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0467021929518978		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0467021929518978 | validation: 0.8626714167958629]
	TIME [epoch: 1.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0219192578876548		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.0219192578876548 | validation: 0.898163693710481]
	TIME [epoch: 1.13 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0278567253733837		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.0278567253733837 | validation: 0.8565432723782612]
	TIME [epoch: 1.12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.028668074317818		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.028668074317818 | validation: 0.9295414451713534]
	TIME [epoch: 1.11 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0447594233259039		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.0447594233259039 | validation: 0.8846915974993612]
	TIME [epoch: 1.12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0778261363476447		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.0778261363476447 | validation: 1.0204189560750503]
	TIME [epoch: 1.12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0951104684514656		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.0951104684514656 | validation: 0.899992761918513]
	TIME [epoch: 1.12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077686667236756		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.077686667236756 | validation: 0.9371591556542832]
	TIME [epoch: 1.12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040578571728892		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.040578571728892 | validation: 0.8356415303123991]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0150397334127437		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.0150397334127437 | validation: 0.8655582332056565]
	TIME [epoch: 1.12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.99943109321012		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.99943109321012 | validation: 0.8425921251608188]
	TIME [epoch: 1.12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000007579403614		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.000007579403614 | validation: 0.8646883145124763]
	TIME [epoch: 1.12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9958339420639576		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.9958339420639576 | validation: 0.823060816721874]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9997991047125561		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.9997991047125561 | validation: 0.8651985810619476]
	TIME [epoch: 1.12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0118382227378824		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.0118382227378824 | validation: 0.8303606034090415]
	TIME [epoch: 1.12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0649435795095676		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.0649435795095676 | validation: 1.0204457026663738]
	TIME [epoch: 1.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149599305324273		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.149599305324273 | validation: 1.0908090588830226]
	TIME [epoch: 1.12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2135121692288984		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.2135121692288984 | validation: 0.913879340509152]
	TIME [epoch: 1.11 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0308649611233545		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.0308649611233545 | validation: 0.8368151693760204]
	TIME [epoch: 1.12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0071660864880931		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.0071660864880931 | validation: 0.8915575053446949]
	TIME [epoch: 1.12 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0134197439311279		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.0134197439311279 | validation: 0.8275518099162]
	TIME [epoch: 1.12 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9979587051166103		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.9979587051166103 | validation: 0.85786897070849]
	TIME [epoch: 1.12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9913129054858331		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.9913129054858331 | validation: 0.8313044968617049]
	TIME [epoch: 1.12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001732158442587		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.001732158442587 | validation: 0.9165585716218783]
	TIME [epoch: 1.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0337491082315295		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.0337491082315295 | validation: 0.9226014340464499]
	TIME [epoch: 1.12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1214176806650114		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1214176806650114 | validation: 1.049917714284672]
	TIME [epoch: 1.12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1301841956530854		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.1301841956530854 | validation: 0.8430399912751647]
	TIME [epoch: 1.12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0351496362287957		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.0351496362287957 | validation: 0.8480896032606454]
	TIME [epoch: 1.13 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0040874836550244		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.0040874836550244 | validation: 0.8467352825261765]
	TIME [epoch: 1.12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993787799385555		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.9993787799385555 | validation: 0.8380733042559596]
	TIME [epoch: 1.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9979710324717164		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.9979710324717164 | validation: 0.8641673177012088]
	TIME [epoch: 1.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9966954527212267		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.9966954527212267 | validation: 0.8309750589964043]
	TIME [epoch: 1.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9897364999943642		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.9897364999943642 | validation: 0.8231228292885048]
	TIME [epoch: 1.12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980205428625837		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.9980205428625837 | validation: 0.907357520165842]
	TIME [epoch: 1.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034717061835061		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.034717061835061 | validation: 0.9303627635168636]
	TIME [epoch: 1.12 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1211636496080268		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.1211636496080268 | validation: 1.03222862052089]
	TIME [epoch: 1.12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0947210797465499		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.0947210797465499 | validation: 0.834975585520795]
	TIME [epoch: 1.12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230726480320178		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.0230726480320178 | validation: 0.8533541238605213]
	TIME [epoch: 1.12 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.98527766019638		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.98527766019638 | validation: 0.8278035140103586]
	TIME [epoch: 1.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9758339893366738		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9758339893366738 | validation: 0.8372931083667456]
	TIME [epoch: 1.12 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9767917302964194		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.9767917302964194 | validation: 0.8452633065516818]
	TIME [epoch: 1.12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9926492509813406		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.9926492509813406 | validation: 0.8338707796906912]
	TIME [epoch: 1.12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0245611909042305		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.0245611909042305 | validation: 0.9493627186047537]
	TIME [epoch: 1.12 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0734928307022564		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0734928307022564 | validation: 0.8844332268291123]
	TIME [epoch: 1.12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.068749130684196		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.068749130684196 | validation: 0.909149686297872]
	TIME [epoch: 1.12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014418293390445		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.014418293390445 | validation: 0.8338914060346392]
	TIME [epoch: 1.12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9895898868573213		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.9895898868573213 | validation: 0.838992949327015]
	TIME [epoch: 1.12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9898811568818254		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9898811568818254 | validation: 0.8094935172968154]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9875491807681834		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9875491807681834 | validation: 0.8774634758230037]
	TIME [epoch: 1.12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976735069780452		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9976735069780452 | validation: 0.838363137634471]
	TIME [epoch: 1.12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9912965047254622		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.9912965047254622 | validation: 0.8596551321013526]
	TIME [epoch: 1.12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9831480363940784		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.9831480363940784 | validation: 0.814450767895012]
	TIME [epoch: 1.12 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9737906110708798		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.9737906110708798 | validation: 0.8207516005170806]
	TIME [epoch: 1.12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728486660909939		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.9728486660909939 | validation: 0.8002101929457368]
	TIME [epoch: 1.13 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987723150785687		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.9987723150785687 | validation: 0.9533782098129935]
	TIME [epoch: 1.12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1139568046060258		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.1139568046060258 | validation: 0.9617291997700346]
	TIME [epoch: 1.12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142450405563304		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.142450405563304 | validation: 0.9213405162015649]
	TIME [epoch: 1.12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0309107917548501		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0309107917548501 | validation: 0.8125508270647095]
	TIME [epoch: 1.12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9890707283700249		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.9890707283700249 | validation: 0.7989231645219917]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9631426716516011		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.9631426716516011 | validation: 0.8091835134116199]
	TIME [epoch: 1.12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9681920177856111		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.9681920177856111 | validation: 0.7924353832070855]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9636719808061337		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9636719808061337 | validation: 0.8021802542531671]
	TIME [epoch: 1.12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971717304063352		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.971717304063352 | validation: 0.7977627110639285]
	TIME [epoch: 1.12 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001832828386994		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.001832828386994 | validation: 0.8996925140740398]
	TIME [epoch: 1.12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0421888063318219		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.0421888063318219 | validation: 0.9147497457062965]
	TIME [epoch: 1.11 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0926730353102807		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0926730353102807 | validation: 0.9435023660444923]
	TIME [epoch: 1.11 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0408811636990425		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0408811636990425 | validation: 0.7986503370456084]
	TIME [epoch: 1.11 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9887615979907579		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9887615979907579 | validation: 0.8161101766021237]
	TIME [epoch: 1.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9712734400550543		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.9712734400550543 | validation: 0.7902389380106929]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9638239628237029		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9638239628237029 | validation: 0.8270272466145692]
	TIME [epoch: 1.12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604149549581252		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.9604149549581252 | validation: 0.7981079060192902]
	TIME [epoch: 1.12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646556104924489		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.9646556104924489 | validation: 0.8407477153377715]
	TIME [epoch: 1.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9673048979622693		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.9673048979622693 | validation: 0.782668653213614]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9648606616824217		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.9648606616824217 | validation: 0.7863687887541296]
	TIME [epoch: 1.12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9598803900842333		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.9598803900842333 | validation: 0.7839257188178078]
	TIME [epoch: 1.12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668481205291872		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.9668481205291872 | validation: 0.7910776332260943]
	TIME [epoch: 1.12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0007700454106658		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.0007700454106658 | validation: 0.9437443078963454]
	TIME [epoch: 1.12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0691816059187573		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0691816059187573 | validation: 0.9555390647659099]
	TIME [epoch: 1.12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1536997674983733		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.1536997674983733 | validation: 0.9648538252953485]
	TIME [epoch: 1.12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0544781668921996		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0544781668921996 | validation: 0.7793329187720064]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9747324167013133		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9747324167013133 | validation: 0.7889995084285771]
	TIME [epoch: 1.12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950012641436507		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.950012641436507 | validation: 0.8076951429502661]
	TIME [epoch: 1.12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960649921311911		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.960649921311911 | validation: 0.7797031308355485]
	TIME [epoch: 1.12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9607403588538114		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.9607403588538114 | validation: 0.832443561530491]
	TIME [epoch: 1.12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9718872668788315		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9718872668788315 | validation: 0.7876020425861056]
	TIME [epoch: 1.12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9977490085463394		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9977490085463394 | validation: 0.8748201439570965]
	TIME [epoch: 1.12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0258620132797045		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.0258620132797045 | validation: 0.8190358796292373]
	TIME [epoch: 1.12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0058279463836146		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.0058279463836146 | validation: 0.8183375246832512]
	TIME [epoch: 1.11 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9821746459925685		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9821746459925685 | validation: 0.7902600875424912]
	TIME [epoch: 1.12 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785930941460143		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9785930941460143 | validation: 0.8208373192863649]
	TIME [epoch: 1.11 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9757898584888921		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.9757898584888921 | validation: 0.7830758259563222]
	TIME [epoch: 1.11 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606764268428991		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.9606764268428991 | validation: 0.785941783052832]
	TIME [epoch: 1.11 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9511778984224424		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9511778984224424 | validation: 0.7605215202776556]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500753004289599		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.9500753004289599 | validation: 0.760755485947191]
	TIME [epoch: 1.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516871498905448		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.9516871498905448 | validation: 0.793357435394424]
	TIME [epoch: 1.12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9609942886914421		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.9609942886914421 | validation: 0.8305423326493205]
	TIME [epoch: 1.12 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0297934786718816		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.0297934786718816 | validation: 1.0026863846394425]
	TIME [epoch: 1.12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1130103637496895		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.1130103637496895 | validation: 0.8025944111324327]
	TIME [epoch: 1.12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00836758036883		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.00836758036883 | validation: 0.8112171120226468]
	TIME [epoch: 1.12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529386495184022		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9529386495184022 | validation: 0.7686114211432328]
	TIME [epoch: 1.12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485402287736406		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9485402287736406 | validation: 0.7676343266638719]
	TIME [epoch: 1.12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9490275873882419		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9490275873882419 | validation: 0.778897656941597]
	TIME [epoch: 1.12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.953801927324706		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.953801927324706 | validation: 0.7671189347654048]
	TIME [epoch: 1.12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723155649991228		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.9723155649991228 | validation: 0.80386245449145]
	TIME [epoch: 1.13 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802994270603256		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.9802994270603256 | validation: 0.7565784473136556]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9844655919364107		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.9844655919364107 | validation: 0.839679356060437]
	TIME [epoch: 1.12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9638695082233845		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.9638695082233845 | validation: 0.7858529807855272]
	TIME [epoch: 1.12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9653312769563893		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.9653312769563893 | validation: 0.8603320343700157]
	TIME [epoch: 1.12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779191750361659		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.9779191750361659 | validation: 0.8000131805466858]
	TIME [epoch: 1.12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0125037197129836		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.0125037197129836 | validation: 0.819750978934448]
	TIME [epoch: 1.12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.002964647942025		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.002964647942025 | validation: 0.7883708509254715]
	TIME [epoch: 1.12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9704596329086704		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9704596329086704 | validation: 0.8043089979711392]
	TIME [epoch: 1.13 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9499108629258624		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.9499108629258624 | validation: 0.7529675730534655]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9423165178093413		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.9423165178093413 | validation: 0.7825012175116214]
	TIME [epoch: 1.12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419841971462938		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.9419841971462938 | validation: 0.7543905047536802]
	TIME [epoch: 1.12 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.941072399264412		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.941072399264412 | validation: 0.774477000815855]
	TIME [epoch: 1.12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494940571076822		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.9494940571076822 | validation: 0.7680400660239318]
	TIME [epoch: 1.11 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9729676165316843		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9729676165316843 | validation: 0.8707576158112706]
	TIME [epoch: 1.12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122532414205636		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.0122532414205636 | validation: 0.8104436838942605]
	TIME [epoch: 1.12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9973487762028553		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.9973487762028553 | validation: 0.8677403865876809]
	TIME [epoch: 1.12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9776327566262255		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9776327566262255 | validation: 0.786972804997234]
	TIME [epoch: 1.12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9840207699796736		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9840207699796736 | validation: 0.806687340153097]
	TIME [epoch: 1.12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9629047493495787		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.9629047493495787 | validation: 0.7629459006914256]
	TIME [epoch: 1.12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487047642443728		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9487047642443728 | validation: 0.8131259734715628]
	TIME [epoch: 1.12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9440020528615791		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.9440020528615791 | validation: 0.7597569008072625]
	TIME [epoch: 1.13 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397875515927153		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.9397875515927153 | validation: 0.7744461425504406]
	TIME [epoch: 1.12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9437571593898804		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9437571593898804 | validation: 0.7593572671475517]
	TIME [epoch: 1.12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529141099003383		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9529141099003383 | validation: 0.7907800287059312]
	TIME [epoch: 1.12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9627613636033033		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.9627613636033033 | validation: 0.7841483262171339]
	TIME [epoch: 1.12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864425587325526		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9864425587325526 | validation: 0.8410984635902102]
	TIME [epoch: 1.13 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9893559858046029		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.9893559858046029 | validation: 0.8105135283386371]
	TIME [epoch: 1.12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9845944967280863		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9845944967280863 | validation: 0.8284452253303414]
	TIME [epoch: 1.12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519742494218426		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.9519742494218426 | validation: 0.7452378577206689]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929730023007704		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.929730023007704 | validation: 0.7401907428535397]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.922495097047605		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.922495097047605 | validation: 0.7757980837935277]
	TIME [epoch: 1.12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245409850294323		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9245409850294323 | validation: 0.7460749688713877]
	TIME [epoch: 1.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9372502714619755		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9372502714619755 | validation: 0.8181577117419568]
	TIME [epoch: 1.12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658679664346622		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9658679664346622 | validation: 0.7819251666162061]
	TIME [epoch: 1.12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0099519905448389		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.0099519905448389 | validation: 0.8271737994751422]
	TIME [epoch: 1.11 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778625633744826		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.9778625633744826 | validation: 0.7526154647097165]
	TIME [epoch: 1.11 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9549537973098273		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.9549537973098273 | validation: 0.7502554175589451]
	TIME [epoch: 1.11 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9484806825748721		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9484806825748721 | validation: 0.7666558993042765]
	TIME [epoch: 1.12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643746557874875		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9643746557874875 | validation: 0.7513129907246374]
	TIME [epoch: 1.11 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298024045260604		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.9298024045260604 | validation: 0.7478530720235517]
	TIME [epoch: 1.11 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287253986239642		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.9287253986239642 | validation: 0.7751956216749251]
	TIME [epoch: 1.11 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9329051422134518		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.9329051422134518 | validation: 0.7377104615261081]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9310293771173701		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9310293771173701 | validation: 0.7771542615373037]
	TIME [epoch: 1.12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269212211901088		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.9269212211901088 | validation: 0.7590578412526967]
	TIME [epoch: 1.12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527997321414599		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9527997321414599 | validation: 0.8750232805292824]
	TIME [epoch: 1.12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0121976267742396		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.0121976267742396 | validation: 0.8394973388858683]
	TIME [epoch: 1.12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0228908681832454		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.0228908681832454 | validation: 0.8183263126715794]
	TIME [epoch: 1.12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524243265265916		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.9524243265265916 | validation: 0.7440024027512478]
	TIME [epoch: 1.11 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9233433194055144		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9233433194055144 | validation: 0.7510767453294565]
	TIME [epoch: 1.12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278729383966939		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.9278729383966939 | validation: 0.7403092424506462]
	TIME [epoch: 1.11 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278742319760355		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.9278742319760355 | validation: 0.7347252135423266]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287853945458966		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.9287853945458966 | validation: 0.7370500490453745]
	TIME [epoch: 1.12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9201520965683383		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.9201520965683383 | validation: 0.7444373881999673]
	TIME [epoch: 1.12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220008505584495		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.9220008505584495 | validation: 0.7160602742681004]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9213556545648214		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9213556545648214 | validation: 0.7678596638465024]
	TIME [epoch: 1.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472600798359758		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.9472600798359758 | validation: 0.7645776736322575]
	TIME [epoch: 1.12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9956164390257957		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.9956164390257957 | validation: 0.8664289321149177]
	TIME [epoch: 1.12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.986091675348346		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.986091675348346 | validation: 0.7807509003788885]
	TIME [epoch: 1.12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465973302074008		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.9465973302074008 | validation: 0.7664917428164997]
	TIME [epoch: 1.12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9493886105875683		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.9493886105875683 | validation: 0.743304236943412]
	TIME [epoch: 1.11 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520369448127383		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.9520369448127383 | validation: 0.7277876368637466]
	TIME [epoch: 1.12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908424459000544		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.908424459000544 | validation: 0.7232022176793045]
	TIME [epoch: 1.12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087193495470239		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.9087193495470239 | validation: 0.7363733126960362]
	TIME [epoch: 1.11 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107760968544176		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.9107760968544176 | validation: 0.7211664912713752]
	TIME [epoch: 1.11 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9116675868380076		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.9116675868380076 | validation: 0.7407248074273832]
	TIME [epoch: 1.12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171624314211632		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.9171624314211632 | validation: 0.7255501309568088]
	TIME [epoch: 1.11 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9195335381921358		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.9195335381921358 | validation: 0.7840072149410912]
	TIME [epoch: 1.12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9274606312908235		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.9274606312908235 | validation: 0.7375960392344348]
	TIME [epoch: 1.11 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268671907965381		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.9268671907965381 | validation: 0.8010280269958749]
	TIME [epoch: 1.11 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9623833255407863		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.9623833255407863 | validation: 0.7581902078547627]
	TIME [epoch: 1.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0074496807471032		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.0074496807471032 | validation: 0.7772872138405884]
	TIME [epoch: 1.12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.949503557695632		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.949503557695632 | validation: 0.7413168976298423]
	TIME [epoch: 1.12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520180401503876		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.9520180401503876 | validation: 0.8069814716698803]
	TIME [epoch: 1.11 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9767920257487483		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.9767920257487483 | validation: 0.7368564280825486]
	TIME [epoch: 1.11 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367000762314327		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.9367000762314327 | validation: 0.7534670478934908]
	TIME [epoch: 1.12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156797799055193		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.9156797799055193 | validation: 0.7038474660891936]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146040292437141		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.9146040292437141 | validation: 0.727237254094477]
	TIME [epoch: 1.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105415786580726		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.9105415786580726 | validation: 0.7074280622806763]
	TIME [epoch: 1.12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067835785011994		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.9067835785011994 | validation: 0.7512148651980298]
	TIME [epoch: 1.12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9121038837413576		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.9121038837413576 | validation: 0.7098228286409075]
	TIME [epoch: 1.13 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280269696713045		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.9280269696713045 | validation: 0.7973883185302235]
	TIME [epoch: 1.11 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9501354180895046		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.9501354180895046 | validation: 0.7376817359873542]
	TIME [epoch: 1.11 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9570194813469639		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.9570194813469639 | validation: 0.7547027237751832]
	TIME [epoch: 1.11 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302434113617537		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.9302434113617537 | validation: 0.7189927332141844]
	TIME [epoch: 1.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9129758367759953		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.9129758367759953 | validation: 0.7376981954730972]
	TIME [epoch: 1.11 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037824581686418		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.9037824581686418 | validation: 0.7229387800301605]
	TIME [epoch: 1.11 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067092837097485		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.9067092837097485 | validation: 0.723285206234737]
	TIME [epoch: 1.11 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056317844642482		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.9056317844642482 | validation: 0.7084215895951846]
	TIME [epoch: 1.13 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189524879399908		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.9189524879399908 | validation: 0.8153763031828736]
	TIME [epoch: 1.11 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9400455029199943		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.9400455029199943 | validation: 0.747683237311684]
	TIME [epoch: 1.11 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.950960736653547		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.950960736653547 | validation: 0.7642463511622899]
	TIME [epoch: 1.11 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9393049190119723		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.9393049190119723 | validation: 0.7293727419858087]
	TIME [epoch: 1.11 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448040881672867		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.9448040881672867 | validation: 0.7641320184086768]
	TIME [epoch: 1.12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311396670351582		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.9311396670351582 | validation: 0.7007143007542146]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074972378043047		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.9074972378043047 | validation: 0.7032881904744941]
	TIME [epoch: 1.12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957406963006495		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.8957406963006495 | validation: 0.7047763624544763]
	TIME [epoch: 1.12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8954695044958387		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.8954695044958387 | validation: 0.7116064450845903]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046112438824736		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.9046112438824736 | validation: 0.7269927606742335]
	TIME [epoch: 2.24 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127643538280116		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.9127643538280116 | validation: 0.720436792076971]
	TIME [epoch: 2.21 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256591946793773		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.9256591946793773 | validation: 0.7942753720724988]
	TIME [epoch: 2.22 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447993028750551		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.9447993028750551 | validation: 0.7492982769847409]
	TIME [epoch: 2.23 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.93887517737916		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.93887517737916 | validation: 0.7638191632948821]
	TIME [epoch: 2.22 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9184350301375298		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.9184350301375298 | validation: 0.6989742334657219]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90502147140821		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.90502147140821 | validation: 0.7273421099183942]
	TIME [epoch: 2.22 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086851961326474		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.9086851961326474 | validation: 0.7073081519269742]
	TIME [epoch: 2.24 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014562609094123		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.9014562609094123 | validation: 0.7178247959569707]
	TIME [epoch: 2.22 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132141981352071		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.9132141981352071 | validation: 0.6916746699080917]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913438451740662		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.913438451740662 | validation: 0.707064412502877]
	TIME [epoch: 2.22 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906521921515859		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.906521921515859 | validation: 0.6997876699273342]
	TIME [epoch: 2.23 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127876350090586		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.9127876350090586 | validation: 0.7067812507612841]
	TIME [epoch: 2.22 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9110496847308335		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.9110496847308335 | validation: 0.7022696443197978]
	TIME [epoch: 2.23 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901854085893399		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8901854085893399 | validation: 0.6755658810903326]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859089624570132		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8859089624570132 | validation: 0.7402028370606729]
	TIME [epoch: 2.22 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060782294938371		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.9060782294938371 | validation: 0.7951818766577429]
	TIME [epoch: 2.22 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724089388861562		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.9724089388861562 | validation: 0.8705376054542828]
	TIME [epoch: 2.23 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970716887795071		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.9970716887795071 | validation: 0.708815409655946]
	TIME [epoch: 2.21 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189905043850454		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.9189905043850454 | validation: 0.6769403338211724]
	TIME [epoch: 2.22 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891895683822322		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.891895683822322 | validation: 0.7115417136576084]
	TIME [epoch: 2.21 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89217724755654		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.89217724755654 | validation: 0.6957592279191096]
	TIME [epoch: 2.22 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8927510062099561		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.8927510062099561 | validation: 0.6941567852721319]
	TIME [epoch: 2.21 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938124386643675		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.8938124386643675 | validation: 0.6820222102103263]
	TIME [epoch: 2.21 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8845128146014034		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8845128146014034 | validation: 0.6840389709144641]
	TIME [epoch: 2.21 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8867494609993005		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.8867494609993005 | validation: 0.6934924178226642]
	TIME [epoch: 2.21 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972556999373958		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.8972556999373958 | validation: 0.6974390261192026]
	TIME [epoch: 2.21 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056997067234231		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.9056997067234231 | validation: 0.7212437135970309]
	TIME [epoch: 2.21 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102442557052407		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.9102442557052407 | validation: 0.7285847286323883]
	TIME [epoch: 2.23 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9381863648387324		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.9381863648387324 | validation: 0.8468185894459443]
	TIME [epoch: 2.21 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646006537232927		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.9646006537232927 | validation: 0.6951830650899524]
	TIME [epoch: 2.23 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284024882292762		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.9284024882292762 | validation: 0.6953831688028197]
	TIME [epoch: 2.21 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865486999770639		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.8865486999770639 | validation: 0.6857529161346314]
	TIME [epoch: 2.23 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776063119203685		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.8776063119203685 | validation: 0.6816620057036008]
	TIME [epoch: 2.22 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675216503784705		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.8675216503784705 | validation: 0.6807415594439955]
	TIME [epoch: 2.21 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777143897024869		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.8777143897024869 | validation: 0.6593358883683771]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872237489034601		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.872237489034601 | validation: 0.6872129201060426]
	TIME [epoch: 2.22 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779249350919366		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.8779249350919366 | validation: 0.6682901740749968]
	TIME [epoch: 2.22 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831051306024561		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.8831051306024561 | validation: 0.7340004735397387]
	TIME [epoch: 2.22 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052276637015882		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.9052276637015882 | validation: 0.7017531380749318]
	TIME [epoch: 2.22 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463940478435363		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.9463940478435363 | validation: 0.7507777963437907]
	TIME [epoch: 2.22 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290912570076238		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.9290912570076238 | validation: 0.6759119850704152]
	TIME [epoch: 2.22 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863829099626448		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8863829099626448 | validation: 0.686288555828984]
	TIME [epoch: 2.21 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803838794689318		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.8803838794689318 | validation: 0.6720406108327777]
	TIME [epoch: 2.23 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846406508214563		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.8846406508214563 | validation: 0.7046495552304406]
	TIME [epoch: 2.22 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9175186275628411		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.9175186275628411 | validation: 0.7374268858123163]
	TIME [epoch: 2.23 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964914989407388		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.964914989407388 | validation: 0.775482427256227]
	TIME [epoch: 2.22 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253224008263125		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.9253224008263125 | validation: 0.6749208345935214]
	TIME [epoch: 2.23 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717283205127521		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.8717283205127521 | validation: 0.6696491089646923]
	TIME [epoch: 2.22 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8720992374858694		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.8720992374858694 | validation: 0.6826042939540813]
	TIME [epoch: 2.23 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671158711503839		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.8671158711503839 | validation: 0.6654521485376643]
	TIME [epoch: 2.21 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651134292776594		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.8651134292776594 | validation: 0.6756552461676169]
	TIME [epoch: 2.23 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731652450565219		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.8731652450565219 | validation: 0.6651348034971026]
	TIME [epoch: 2.22 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856003613610609		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.8856003613610609 | validation: 0.7148328668877998]
	TIME [epoch: 2.21 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9051377332111648		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.9051377332111648 | validation: 0.6907192050557304]
	TIME [epoch: 2.22 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9113659600072844		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.9113659600072844 | validation: 0.7298525090140227]
	TIME [epoch: 2.21 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006982069210263		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.9006982069210263 | validation: 0.6966925985770894]
	TIME [epoch: 2.21 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925153074805657		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.925153074805657 | validation: 0.7134986542266044]
	TIME [epoch: 2.21 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210241361686952		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.9210241361686952 | validation: 0.6790826350891077]
	TIME [epoch: 2.23 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830530179862084		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.8830530179862084 | validation: 0.6652372527970636]
	TIME [epoch: 2.21 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8690524922656718		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.8690524922656718 | validation: 0.6742674617661578]
	TIME [epoch: 2.22 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8628009389560211		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.8628009389560211 | validation: 0.6577899791144928]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653628018688021		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.8653628018688021 | validation: 0.6689165466374963]
	TIME [epoch: 2.22 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663821320521862		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.8663821320521862 | validation: 0.6705256145068207]
	TIME [epoch: 2.21 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744221630061927		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.8744221630061927 | validation: 0.7103112258710083]
	TIME [epoch: 2.22 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.888410471324633		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.888410471324633 | validation: 0.7164452242007647]
	TIME [epoch: 2.21 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9483635571083078		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.9483635571083078 | validation: 0.7452997618064068]
	TIME [epoch: 2.21 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9365474923096316		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.9365474923096316 | validation: 0.6653955551891951]
	TIME [epoch: 2.22 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846709221814149		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8846709221814149 | validation: 0.6674152027503777]
	TIME [epoch: 2.21 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633668473010702		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.8633668473010702 | validation: 0.660414177503114]
	TIME [epoch: 2.21 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674844886178397		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.8674844886178397 | validation: 0.6582275741500165]
	TIME [epoch: 2.23 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873441441195973		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.873441441195973 | validation: 0.6725866398424007]
	TIME [epoch: 2.22 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87887548692135		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.87887548692135 | validation: 0.643609228845621]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770012272628601		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.8770012272628601 | validation: 0.6976342774159193]
	TIME [epoch: 2.23 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8700936381012807		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.8700936381012807 | validation: 0.6516743295783144]
	TIME [epoch: 2.21 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767161837255542		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.8767161837255542 | validation: 0.6718330201249727]
	TIME [epoch: 2.21 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758354473390249		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.8758354473390249 | validation: 0.6672199653281604]
	TIME [epoch: 2.21 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810084077570615		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.8810084077570615 | validation: 0.6925500906330708]
	TIME [epoch: 2.23 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8809508068178531		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.8809508068178531 | validation: 0.6608040488906406]
	TIME [epoch: 2.21 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717677765047259		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.8717677765047259 | validation: 0.651625638823862]
	TIME [epoch: 2.23 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866683853290456		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.866683853290456 | validation: 0.6423066577910966]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639946597568087		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.8639946597568087 | validation: 0.6651002243161426]
	TIME [epoch: 2.21 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722540707225409		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.8722540707225409 | validation: 0.6385700912011142]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798760162001349		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.8798760162001349 | validation: 0.6624469803793032]
	TIME [epoch: 2.21 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877242438361843		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.877242438361843 | validation: 0.6400338285103777]
	TIME [epoch: 2.21 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705537571884818		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.8705537571884818 | validation: 0.6545863885278991]
	TIME [epoch: 2.21 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923131050948996		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.8923131050948996 | validation: 0.8178032171948862]
	TIME [epoch: 2.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9522911623221768		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.9522911623221768 | validation: 0.685739019111686]
	TIME [epoch: 2.22 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972361824036673		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8972361824036673 | validation: 0.665663415244862]
	TIME [epoch: 2.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682495572950256		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.8682495572950256 | validation: 0.6681904259093906]
	TIME [epoch: 2.22 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631909701031382		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.8631909701031382 | validation: 0.6387247833940135]
	TIME [epoch: 2.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604987039225366		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.8604987039225366 | validation: 0.6431199806989396]
	TIME [epoch: 2.23 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587923710083518		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.8587923710083518 | validation: 0.6085510405577296]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554909522119275		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.8554909522119275 | validation: 0.6430139804839187]
	TIME [epoch: 2.22 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540618339883451		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.8540618339883451 | validation: 0.6300106590488306]
	TIME [epoch: 2.21 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593308744495363		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.8593308744495363 | validation: 0.6407481127129472]
	TIME [epoch: 2.21 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590632442982227		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.8590632442982227 | validation: 0.6282736696908799]
	TIME [epoch: 2.21 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649489252680398		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.8649489252680398 | validation: 0.6505064791353842]
	TIME [epoch: 2.21 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862929336132166		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.8862929336132166 | validation: 0.6231951473275859]
	TIME [epoch: 2.21 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754131603943546		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.8754131603943546 | validation: 0.6632205849713291]
	TIME [epoch: 2.21 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723710481507826		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.8723710481507826 | validation: 0.6380344975195844]
	TIME [epoch: 2.23 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646123702958289		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.8646123702958289 | validation: 0.6651792611247653]
	TIME [epoch: 2.21 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654640381543436		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.8654640381543436 | validation: 0.664207883524165]
	TIME [epoch: 2.23 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814085265819551		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.8814085265819551 | validation: 0.7434789333837233]
	TIME [epoch: 2.21 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284709546236245		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.9284709546236245 | validation: 0.660705535435552]
	TIME [epoch: 2.24 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9164020229202373		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.9164020229202373 | validation: 0.6453105545387562]
	TIME [epoch: 2.21 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865216062644285		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.865216062644285 | validation: 0.6330573146308152]
	TIME [epoch: 2.23 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541811996121645		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.8541811996121645 | validation: 0.6218843510497772]
	TIME [epoch: 2.21 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479041798983086		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.8479041798983086 | validation: 0.6194513870454599]
	TIME [epoch: 2.23 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515239724154406		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.8515239724154406 | validation: 0.6211386459081617]
	TIME [epoch: 2.21 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420944033447728		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.8420944033447728 | validation: 0.6125319693292323]
	TIME [epoch: 2.21 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438344944381322		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.8438344944381322 | validation: 0.6212525180159217]
	TIME [epoch: 2.21 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569276739299477		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.8569276739299477 | validation: 0.6196474948539167]
	TIME [epoch: 2.21 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869519873121166		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.869519873121166 | validation: 0.6747934051136116]
	TIME [epoch: 2.21 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840459131667799		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.8840459131667799 | validation: 0.6928108077457938]
	TIME [epoch: 2.21 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908413308926697		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.908413308926697 | validation: 0.7256673261578719]
	TIME [epoch: 2.23 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906502747693215		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.906502747693215 | validation: 0.6201882907378654]
	TIME [epoch: 2.21 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572010738306556		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.8572010738306556 | validation: 0.6073814520917815]
	TIME [epoch: 2.24 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420158811703012		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.8420158811703012 | validation: 0.6185862406035268]
	TIME [epoch: 2.21 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376133155293336		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.8376133155293336 | validation: 0.6165453649160497]
	TIME [epoch: 2.22 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426912982883313		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.8426912982883313 | validation: 0.6165508038911565]
	TIME [epoch: 2.21 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453720142848593		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.8453720142848593 | validation: 0.6052137124569346]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447548741503039		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.8447548741503039 | validation: 0.6348196975012785]
	TIME [epoch: 2.21 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572606851373685		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.8572606851373685 | validation: 0.6402459101390008]
	TIME [epoch: 2.23 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893620774485765		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.8893620774485765 | validation: 0.6643184195949892]
	TIME [epoch: 2.21 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863813277839913		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.8863813277839913 | validation: 0.6191052584472774]
	TIME [epoch: 2.21 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8566101221679805		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.8566101221679805 | validation: 0.6113406881225517]
	TIME [epoch: 2.21 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8386727304052937		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.8386727304052937 | validation: 0.6123256264108344]
	TIME [epoch: 2.23 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440425254171582		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.8440425254171582 | validation: 0.6209315745589038]
	TIME [epoch: 2.21 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550659315357118		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.8550659315357118 | validation: 0.624827072649157]
	TIME [epoch: 2.24 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.88204107514855		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.88204107514855 | validation: 0.6531130937011022]
	TIME [epoch: 2.21 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815287308986368		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.8815287308986368 | validation: 0.6439643698816234]
	TIME [epoch: 2.21 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719466191200433		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.8719466191200433 | validation: 0.6503738132308072]
	TIME [epoch: 2.21 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528173931550015		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.8528173931550015 | validation: 0.6007920685893922]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516551211921903		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.8516551211921903 | validation: 0.6081655919188602]
	TIME [epoch: 2.21 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427500058422228		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.8427500058422228 | validation: 0.6144351291177305]
	TIME [epoch: 2.22 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468615974459474		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.8468615974459474 | validation: 0.6140470981236201]
	TIME [epoch: 2.21 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848520923107782		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.848520923107782 | validation: 0.6273999696551801]
	TIME [epoch: 2.21 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574697219538778		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.8574697219538778 | validation: 0.6212163309289986]
	TIME [epoch: 2.21 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558539296920934		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.8558539296920934 | validation: 0.6056258787898385]
	TIME [epoch: 2.22 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550094874223402		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.8550094874223402 | validation: 0.6240779051940146]
	TIME [epoch: 2.23 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850967776181609		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.850967776181609 | validation: 0.5957570995000122]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549745331321954		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.8549745331321954 | validation: 0.6282342114298132]
	TIME [epoch: 2.22 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857382886419584		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.857382886419584 | validation: 0.6196523771343605]
	TIME [epoch: 2.21 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654675925559724		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.8654675925559724 | validation: 0.6385156317649143]
	TIME [epoch: 2.23 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589700355214916		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.8589700355214916 | validation: 0.5987327245309748]
	TIME [epoch: 2.21 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464324154927886		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.8464324154927886 | validation: 0.5978291257563536]
	TIME [epoch: 2.23 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378955275883473		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.8378955275883473 | validation: 0.6002091712839022]
	TIME [epoch: 2.21 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841069377078037		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.841069377078037 | validation: 0.5838728978578173]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457936870454714		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.8457936870454714 | validation: 0.5926148391793932]
	TIME [epoch: 2.21 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854896944470553		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.854896944470553 | validation: 0.5878984961049575]
	TIME [epoch: 2.23 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403661662783364		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.8403661662783364 | validation: 0.5706695002175306]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381643089064437		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.8381643089064437 | validation: 0.5850413528096589]
	TIME [epoch: 2.21 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336925943096237		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.8336925943096237 | validation: 0.5760365970064656]
	TIME [epoch: 2.21 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8348576603756911		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.8348576603756911 | validation: 0.5786518197230553]
	TIME [epoch: 2.24 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347143032336218		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.8347143032336218 | validation: 0.6026647161988358]
	TIME [epoch: 2.21 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470746211311189		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.8470746211311189 | validation: 0.6007907496709706]
	TIME [epoch: 2.22 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8668522795871806		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.8668522795871806 | validation: 0.7156618766266281]
	TIME [epoch: 2.21 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001138402374075		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.9001138402374075 | validation: 0.5927073141702925]
	TIME [epoch: 2.21 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438326810686434		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.8438326810686434 | validation: 0.5687240509997994]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296415477939352		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.8296415477939352 | validation: 0.5904418764139499]
	TIME [epoch: 2.22 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417334761705061		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.8417334761705061 | validation: 0.5755602880987614]
	TIME [epoch: 2.21 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512362828208794		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.8512362828208794 | validation: 0.6048187918073177]
	TIME [epoch: 2.22 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593796469995539		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.8593796469995539 | validation: 0.5703996612720436]
	TIME [epoch: 2.21 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368504176507042		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.8368504176507042 | validation: 0.5840516106263594]
	TIME [epoch: 2.21 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424141023377382		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.8424141023377382 | validation: 0.6252232786533014]
	TIME [epoch: 2.23 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533393435696462		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.8533393435696462 | validation: 0.6241286375968713]
	TIME [epoch: 2.21 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831843341468049		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.8831843341468049 | validation: 0.6096616718740986]
	TIME [epoch: 2.24 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636836852489793		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.8636836852489793 | validation: 0.5796038059858122]
	TIME [epoch: 2.21 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8402594896200162		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.8402594896200162 | validation: 0.56608341667242]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286153497623846		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.8286153497623846 | validation: 0.5751029279472287]
	TIME [epoch: 2.21 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265279763512817		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.8265279763512817 | validation: 0.5657912828320992]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306764722894913		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.8306764722894913 | validation: 0.5781973367474389]
	TIME [epoch: 2.23 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292396386452847		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.8292396386452847 | validation: 0.5680100688298455]
	TIME [epoch: 2.23 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326533809870733		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.8326533809870733 | validation: 0.5901687281039129]
	TIME [epoch: 2.21 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379963067127713		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.8379963067127713 | validation: 0.5903727057329585]
	TIME [epoch: 2.23 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510814479942349		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.8510814479942349 | validation: 0.6033639775891082]
	TIME [epoch: 2.21 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8436837165450284		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.8436837165450284 | validation: 0.5806112038323586]
	TIME [epoch: 2.23 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477247832287665		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.8477247832287665 | validation: 0.5750165011151175]
	TIME [epoch: 2.21 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835688169852622		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.835688169852622 | validation: 0.5587536967309387]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372034283388251		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.8372034283388251 | validation: 0.5658673106631588]
	TIME [epoch: 2.21 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446721495142052		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.8446721495142052 | validation: 0.6095202678299324]
	TIME [epoch: 2.23 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617106788011174		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.8617106788011174 | validation: 0.6014920723560048]
	TIME [epoch: 2.21 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743057731634906		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.8743057731634906 | validation: 0.5805105447203645]
	TIME [epoch: 2.23 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8396170584682815		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.8396170584682815 | validation: 0.5547022189119816]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261021784337724		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.8261021784337724 | validation: 0.5541898317510191]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8225861180317741		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.8225861180317741 | validation: 0.5604811215919003]
	TIME [epoch: 2.21 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283319856456995		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.8283319856456995 | validation: 0.5400252606654371]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281717807097304		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.8281717807097304 | validation: 0.5505786268634079]
	TIME [epoch: 2.21 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312547320678471		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.8312547320678471 | validation: 0.56419124859858]
	TIME [epoch: 2.21 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418189411568692		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.8418189411568692 | validation: 0.5834068957749031]
	TIME [epoch: 2.22 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610727375395163		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.8610727375395163 | validation: 0.629938250299372]
	TIME [epoch: 2.21 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8768753598004548		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.8768753598004548 | validation: 0.5745233518560907]
	TIME [epoch: 2.23 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8450013673389006		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.8450013673389006 | validation: 0.5480087039503456]
	TIME [epoch: 2.22 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822840451287399		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.822840451287399 | validation: 0.5507388892636497]
	TIME [epoch: 2.21 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139975525802009		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.8139975525802009 | validation: 0.5481447984999032]
	TIME [epoch: 2.23 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145143637446023		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.8145143637446023 | validation: 0.5528210474474522]
	TIME [epoch: 2.21 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8201656012725103		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.8201656012725103 | validation: 0.5362189256726313]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207292597543295		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.8207292597543295 | validation: 0.5534982870146292]
	TIME [epoch: 2.21 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8313905928423758		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.8313905928423758 | validation: 0.5446868949423401]
	TIME [epoch: 2.21 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472713450878565		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.8472713450878565 | validation: 0.552096326764938]
	TIME [epoch: 2.21 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361044981906318		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.8361044981906318 | validation: 0.5408217552979393]
	TIME [epoch: 2.21 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250491997718546		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.8250491997718546 | validation: 0.5447270316110157]
	TIME [epoch: 2.22 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186120460741444		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.8186120460741444 | validation: 0.5568114429140153]
	TIME [epoch: 2.23 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301869084817232		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.8301869084817232 | validation: 0.5930168666036858]
	TIME [epoch: 2.21 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509408431563777		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.8509408431563777 | validation: 0.6136595588973934]
	TIME [epoch: 2.23 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621155121187724		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.8621155121187724 | validation: 0.5721372428930405]
	TIME [epoch: 2.21 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372378847729113		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.8372378847729113 | validation: 0.5474304657512822]
	TIME [epoch: 2.23 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267023358110208		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.8267023358110208 | validation: 0.5427629525479061]
	TIME [epoch: 2.21 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374362275027233		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.8374362275027233 | validation: 0.5442922939624347]
	TIME [epoch: 2.22 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8441313134676418		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.8441313134676418 | validation: 0.541108070538144]
	TIME [epoch: 2.21 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173549512736493		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.8173549512736493 | validation: 0.5411134408363273]
	TIME [epoch: 2.21 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165103247676113		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.8165103247676113 | validation: 0.5512098870130192]
	TIME [epoch: 2.21 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8238833229304288		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.8238833229304288 | validation: 0.5567811348228314]
	TIME [epoch: 2.21 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830583470973366		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.830583470973366 | validation: 0.5572220170475296]
	TIME [epoch: 2.21 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843598929239499		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.843598929239499 | validation: 0.562781778296381]
	TIME [epoch: 2.22 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8561195051526777		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.8561195051526777 | validation: 0.5567598558893588]
	TIME [epoch: 2.23 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401746358623706		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.8401746358623706 | validation: 0.5495806291973259]
	TIME [epoch: 2.21 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315456718746341		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.8315456718746341 | validation: 0.5239680471142368]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824277796032975		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.824277796032975 | validation: 0.536592666116897]
	TIME [epoch: 2.22 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136243905694807		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.8136243905694807 | validation: 0.5201665704649792]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187327080224737		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.8187327080224737 | validation: 0.5308798796684581]
	TIME [epoch: 2.21 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259007030067024		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.8259007030067024 | validation: 0.5450523975197631]
	TIME [epoch: 2.22 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382839591214767		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.8382839591214767 | validation: 0.5633811512519403]
	TIME [epoch: 2.21 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8450311024677578		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.8450311024677578 | validation: 0.5594931660746786]
	TIME [epoch: 2.22 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8398648012316974		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.8398648012316974 | validation: 0.545858724596585]
	TIME [epoch: 2.21 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287727153447526		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.8287727153447526 | validation: 0.539923514492083]
	TIME [epoch: 2.22 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163336433076341		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.8163336433076341 | validation: 0.5300906946435957]
	TIME [epoch: 2.22 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813479469619546		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.813479469619546 | validation: 0.5392326261849736]
	TIME [epoch: 2.22 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168497044802159		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.8168497044802159 | validation: 0.5401694301252833]
	TIME [epoch: 2.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155113137695904		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.8155113137695904 | validation: 0.5336520314287224]
	TIME [epoch: 2.22 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204935503101771		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.8204935503101771 | validation: 0.5507292059375816]
	TIME [epoch: 2.21 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289794544802658		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.8289794544802658 | validation: 0.5383819189239761]
	TIME [epoch: 2.22 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8435551258754626		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.8435551258754626 | validation: 0.545769433855166]
	TIME [epoch: 2.21 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320211843150839		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.8320211843150839 | validation: 0.5314454486689709]
	TIME [epoch: 2.21 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245812832366244		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.8245812832366244 | validation: 0.5329187018580734]
	TIME [epoch: 2.21 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815791553844615		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.815791553844615 | validation: 0.5321483047313489]
	TIME [epoch: 2.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173125101954446		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.8173125101954446 | validation: 0.5316891867593635]
	TIME [epoch: 2.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270486731441438		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.8270486731441438 | validation: 0.5518563528277733]
	TIME [epoch: 2.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831579361323783		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.831579361323783 | validation: 0.5298744479761336]
	TIME [epoch: 2.22 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195076857634509		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.8195076857634509 | validation: 0.5249725819983682]
	TIME [epoch: 2.22 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152347237141081		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.8152347237141081 | validation: 0.5242626535075431]
	TIME [epoch: 2.22 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8255725342699356		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.8255725342699356 | validation: 0.5175076473427942]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832750426568654		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.832750426568654 | validation: 0.525037805462046]
	TIME [epoch: 2.22 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280769334914715		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.8280769334914715 | validation: 0.5157731787378305]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162260167959086		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.8162260167959086 | validation: 0.5108631240912095]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8138627129541994		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.8138627129541994 | validation: 0.5198916875487336]
	TIME [epoch: 2.21 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137334623417494		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.8137334623417494 | validation: 0.5416706252461145]
	TIME [epoch: 2.21 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298086236258321		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.8298086236258321 | validation: 0.5950668009008887]
	TIME [epoch: 2.22 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850254560132949		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.850254560132949 | validation: 0.5211122686059215]
	TIME [epoch: 2.21 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8366440091675154		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.8366440091675154 | validation: 0.5200613366048404]
	TIME [epoch: 2.21 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8191212214509767		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.8191212214509767 | validation: 0.5122714775673215]
	TIME [epoch: 2.21 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819169688441296		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.819169688441296 | validation: 0.5068645650237684]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157459216405115		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.8157459216405115 | validation: 0.49905642107612036]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150854685177418		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.8150854685177418 | validation: 0.5129539321319815]
	TIME [epoch: 2.21 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178350401368119		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.8178350401368119 | validation: 0.5200116990093004]
	TIME [epoch: 2.21 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200594339039924		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.8200594339039924 | validation: 0.5184660535173528]
	TIME [epoch: 2.22 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231766587217046		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.8231766587217046 | validation: 0.5327334854840531]
	TIME [epoch: 2.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237717878508707		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.8237717878508707 | validation: 0.5231653813414247]
	TIME [epoch: 2.21 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202734008294289		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.8202734008294289 | validation: 0.5165509369537952]
	TIME [epoch: 2.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164620323334343		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.8164620323334343 | validation: 0.4991982001506994]
	TIME [epoch: 2.21 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098550667353271		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.8098550667353271 | validation: 0.5108294129547705]
	TIME [epoch: 2.21 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070176632821909		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.8070176632821909 | validation: 0.5002309245718853]
	TIME [epoch: 2.21 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031082712186333		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.8031082712186333 | validation: 0.5058141360479574]
	TIME [epoch: 2.22 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024585110463062		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.8024585110463062 | validation: 0.5030603186087194]
	TIME [epoch: 2.21 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805022280853452		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.805022280853452 | validation: 0.49885797836203577]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260623918155747		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.8260623918155747 | validation: 0.5212788925332513]
	TIME [epoch: 2.21 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509913661730684		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.8509913661730684 | validation: 0.5401030461679266]
	TIME [epoch: 2.21 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832817457564379		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.832817457564379 | validation: 0.5725718556977425]
	TIME [epoch: 2.21 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562810973531189		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.8562810973531189 | validation: 0.5297279461439921]
	TIME [epoch: 2.22 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236444018615984		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.8236444018615984 | validation: 0.49960614431228567]
	TIME [epoch: 2.21 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044568657457449		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.8044568657457449 | validation: 0.5018992356305961]
	TIME [epoch: 2.21 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801277828325372		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.801277828325372 | validation: 0.5032464572121814]
	TIME [epoch: 2.21 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069986348500424		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.8069986348500424 | validation: 0.5015700003720842]
	TIME [epoch: 2.21 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070192682591111		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.8070192682591111 | validation: 0.5036599520169049]
	TIME [epoch: 2.21 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112664208803452		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.8112664208803452 | validation: 0.4964652982674214]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222031698153885		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.8222031698153885 | validation: 0.5073676276938177]
	TIME [epoch: 2.21 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308764921997702		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.8308764921997702 | validation: 0.5165451825724744]
	TIME [epoch: 2.22 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189415997960723		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.8189415997960723 | validation: 0.5295722137008007]
	TIME [epoch: 2.22 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227837340383692		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.8227837340383692 | validation: 0.5445255541561261]
	TIME [epoch: 2.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341403647242842		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.8341403647242842 | validation: 0.5156338809647932]
	TIME [epoch: 2.21 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167961575227554		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.8167961575227554 | validation: 0.49141271418055954]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131959294381348		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.8131959294381348 | validation: 0.49293832308666974]
	TIME [epoch: 2.21 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117929734322806		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.8117929734322806 | validation: 0.5041383902023906]
	TIME [epoch: 2.21 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174003910922767		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.8174003910922767 | validation: 0.5069507139921202]
	TIME [epoch: 2.21 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105402492966265		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.8105402492966265 | validation: 0.5015432077822405]
	TIME [epoch: 2.21 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119556988968544		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.8119556988968544 | validation: 0.49852518512261557]
	TIME [epoch: 2.21 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8027757102170514		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.8027757102170514 | validation: 0.4823317824909527]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047873900605491		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.8047873900605491 | validation: 0.4955657005800461]
	TIME [epoch: 2.21 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088708408857803		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.8088708408857803 | validation: 0.5084697189003631]
	TIME [epoch: 2.22 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104544601891607		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.8104544601891607 | validation: 0.4982792769522341]
	TIME [epoch: 2.21 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8144376848986192		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.8144376848986192 | validation: 0.48751786684852344]
	TIME [epoch: 2.22 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149466163905796		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.8149466163905796 | validation: 0.5002962391944993]
	TIME [epoch: 2.21 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120230165626194		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.8120230165626194 | validation: 0.4992245872719182]
	TIME [epoch: 2.21 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125449870764712		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.8125449870764712 | validation: 0.48694896245811714]
	TIME [epoch: 2.21 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022526660693214		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.8022526660693214 | validation: 0.493566797321868]
	TIME [epoch: 2.22 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012206539990588		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.8012206539990588 | validation: 0.49646761886652957]
	TIME [epoch: 2.21 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070651599027567		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.8070651599027567 | validation: 0.5044686040041064]
	TIME [epoch: 2.21 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198033383942865		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.8198033383942865 | validation: 0.564716346827934]
	TIME [epoch: 2.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579337615697626		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.8579337615697626 | validation: 0.5295148795285325]
	TIME [epoch: 2.21 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377782957475786		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.8377782957475786 | validation: 0.48840545578254546]
	TIME [epoch: 2.21 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084307758958025		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.8084307758958025 | validation: 0.4825635975058995]
	TIME [epoch: 2.21 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80336194744565		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.80336194744565 | validation: 0.49615145537366134]
	TIME [epoch: 2.21 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809111070921615		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.809111070921615 | validation: 0.48815465039413675]
	TIME [epoch: 2.22 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059661514576186		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.8059661514576186 | validation: 0.4856974824077132]
	TIME [epoch: 2.22 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013754540163339		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.8013754540163339 | validation: 0.4928522924517143]
	TIME [epoch: 2.21 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003589604139887		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.8003589604139887 | validation: 0.4837340553506527]
	TIME [epoch: 2.22 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049382012634075		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.8049382012634075 | validation: 0.49722699749264304]
	TIME [epoch: 2.21 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158860244012414		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.8158860244012414 | validation: 0.5043254307225863]
	TIME [epoch: 2.22 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829728430593406		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.829728430593406 | validation: 0.49898270995309074]
	TIME [epoch: 2.21 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8219827221286672		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.8219827221286672 | validation: 0.5006985399806361]
	TIME [epoch: 2.22 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136146901654427		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.8136146901654427 | validation: 0.49925688160077314]
	TIME [epoch: 2.21 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065021159985013		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.8065021159985013 | validation: 0.47967594141719466]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020078492565912		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.8020078492565912 | validation: 0.4788590993215408]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011865342213286		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.8011865342213286 | validation: 0.4900037235862869]
	TIME [epoch: 2.22 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041979064341104		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.8041979064341104 | validation: 0.4775967584376047]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108907965853581		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.8108907965853581 | validation: 0.49799376393393263]
	TIME [epoch: 2.21 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149518088034138		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.8149518088034138 | validation: 0.5000175476429979]
	TIME [epoch: 2.22 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271137869413662		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.8271137869413662 | validation: 0.5012986946329482]
	TIME [epoch: 2.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142505186218777		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.8142505186218777 | validation: 0.4861975790353512]
	TIME [epoch: 2.22 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806325106957369		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.806325106957369 | validation: 0.484309539008388]
	TIME [epoch: 2.21 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955838239475594		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.7955838239475594 | validation: 0.4797054270408656]
	TIME [epoch: 2.21 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009156137816351		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.8009156137816351 | validation: 0.47600749063916725]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798282776453244		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.798282776453244 | validation: 0.4865552152883753]
	TIME [epoch: 2.21 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992069490074915		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.7992069490074915 | validation: 0.4810586884311988]
	TIME [epoch: 2.21 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009460415685355		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.8009460415685355 | validation: 0.4977027552153603]
	TIME [epoch: 2.21 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098268015794267		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.8098268015794267 | validation: 0.4999439476454464]
	TIME [epoch: 2.21 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813287754407082		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.813287754407082 | validation: 0.4957094067059215]
	TIME [epoch: 2.22 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120474211181786		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.8120474211181786 | validation: 0.4801234396948182]
	TIME [epoch: 2.22 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801289953391929		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.801289953391929 | validation: 0.48114541230610436]
	TIME [epoch: 2.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056767834038461		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.8056767834038461 | validation: 0.504281200128946]
	TIME [epoch: 2.22 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355739239878469		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.8355739239878469 | validation: 0.47693224311315624]
	TIME [epoch: 2.21 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803475487040263		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.803475487040263 | validation: 0.48006276740536075]
	TIME [epoch: 2.21 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001310400134156		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.8001310400134156 | validation: 0.47362438859569655]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019084591617164		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.8019084591617164 | validation: 0.4732298679878792]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974449550034549		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.7974449550034549 | validation: 0.479853632855761]
	TIME [epoch: 2.22 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963271154614269		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.7963271154614269 | validation: 0.4664560830671737]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996950849937414		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.7996950849937414 | validation: 0.47290342941685587]
	TIME [epoch: 2.21 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998912710789498		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.7998912710789498 | validation: 0.47741404264192433]
	TIME [epoch: 2.21 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984292703752895		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.7984292703752895 | validation: 0.5057427476959074]
	TIME [epoch: 2.23 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155127793095565		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.8155127793095565 | validation: 0.5251441250888679]
	TIME [epoch: 2.21 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526402385256404		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.8526402385256404 | validation: 0.5011955008102121]
	TIME [epoch: 2.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367555931509867		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.8367555931509867 | validation: 0.47340755933643186]
	TIME [epoch: 2.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990806002308051		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.7990806002308051 | validation: 0.47238494885865107]
	TIME [epoch: 2.21 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023516555794714		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.8023516555794714 | validation: 0.4818793226242146]
	TIME [epoch: 2.22 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026990074496759		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.8026990074496759 | validation: 0.4787287599520189]
	TIME [epoch: 2.21 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066691590211846		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.8066691590211846 | validation: 0.48238341410024665]
	TIME [epoch: 2.22 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001233124242171		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.8001233124242171 | validation: 0.47825917560252695]
	TIME [epoch: 2.21 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034764626539453		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.8034764626539453 | validation: 0.47384405445791683]
	TIME [epoch: 2.22 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080242358586416		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.8080242358586416 | validation: 0.4611271810113248]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803153616344581		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.803153616344581 | validation: 0.4713584357655687]
	TIME [epoch: 2.22 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797639575382209		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.797639575382209 | validation: 0.46436569652557297]
	TIME [epoch: 2.22 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010900224242608		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.8010900224242608 | validation: 0.4692889149373784]
	TIME [epoch: 2.23 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009135172285808		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.8009135172285808 | validation: 0.4807669610814156]
	TIME [epoch: 2.21 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000671799499162		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.8000671799499162 | validation: 0.47552179641962466]
	TIME [epoch: 2.23 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936590883764189		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.7936590883764189 | validation: 0.4727963491859609]
	TIME [epoch: 2.21 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960840692989303		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.7960840692989303 | validation: 0.4651709854042527]
	TIME [epoch: 2.23 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977145591220429		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.7977145591220429 | validation: 0.47177610859474434]
	TIME [epoch: 2.21 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045221448290364		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.8045221448290364 | validation: 0.4874425767604679]
	TIME [epoch: 2.21 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8141570229369895		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.8141570229369895 | validation: 0.5132617163891109]
	TIME [epoch: 2.21 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297938154640927		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.8297938154640927 | validation: 0.50734797199584]
	TIME [epoch: 2.21 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127575385807893		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.8127575385807893 | validation: 0.4741951475091238]
	TIME [epoch: 2.21 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801350252231583		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.801350252231583 | validation: 0.4666959075236621]
	TIME [epoch: 2.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977055892991552		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.7977055892991552 | validation: 0.4697726512049357]
	TIME [epoch: 2.23 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975428598412181		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.7975428598412181 | validation: 0.46522845157285775]
	TIME [epoch: 2.22 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954287293730715		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.7954287293730715 | validation: 0.4718587080778576]
	TIME [epoch: 2.21 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935493658167926		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.7935493658167926 | validation: 0.4533036849929286]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957657130045533		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.7957657130045533 | validation: 0.47334854232776524]
	TIME [epoch: 2.22 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030440662295124		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.8030440662295124 | validation: 0.4691458025196303]
	TIME [epoch: 2.21 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034922413252805		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.8034922413252805 | validation: 0.4629367273598998]
	TIME [epoch: 2.23 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156798444555043		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.8156798444555043 | validation: 0.46763964741371666]
	TIME [epoch: 2.23 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132660248702601		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.8132660248702601 | validation: 0.45428194192898275]
	TIME [epoch: 2.23 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973861068311808		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.7973861068311808 | validation: 0.45951252051008257]
	TIME [epoch: 2.21 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791105134156358		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.791105134156358 | validation: 0.45806736584048446]
	TIME [epoch: 2.23 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971901254899887		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.7971901254899887 | validation: 0.4677554027990226]
	TIME [epoch: 2.21 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023009223851001		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.8023009223851001 | validation: 0.4936219179695098]
	TIME [epoch: 2.23 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197024189913265		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.8197024189913265 | validation: 0.4926407452226717]
	TIME [epoch: 2.22 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266309606440811		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.8266309606440811 | validation: 0.4749783882570145]
	TIME [epoch: 2.21 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091107370948806		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.8091107370948806 | validation: 0.4633151420595094]
	TIME [epoch: 2.21 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065889861888309		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.8065889861888309 | validation: 0.4634366863139304]
	TIME [epoch: 2.21 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013218751099649		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.8013218751099649 | validation: 0.4534274063674169]
	TIME [epoch: 2.21 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955041876367639		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.7955041876367639 | validation: 0.4651422866886053]
	TIME [epoch: 2.21 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949568662246503		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.7949568662246503 | validation: 0.4623221666352967]
	TIME [epoch: 2.22 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921228634160659		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.7921228634160659 | validation: 0.4646206639531341]
	TIME [epoch: 2.21 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966893973330501		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.7966893973330501 | validation: 0.4644534107930833]
	TIME [epoch: 2.23 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971258752752007		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.7971258752752007 | validation: 0.4636994665557497]
	TIME [epoch: 2.21 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985568119533965		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.7985568119533965 | validation: 0.4672422674835744]
	TIME [epoch: 2.22 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999344531225399		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.7999344531225399 | validation: 0.4649091183178442]
	TIME [epoch: 2.21 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055787110061826		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.8055787110061826 | validation: 0.4655468538827002]
	TIME [epoch: 2.23 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963906717557593		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.7963906717557593 | validation: 0.45654546233966775]
	TIME [epoch: 2.22 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991321016380363		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.7991321016380363 | validation: 0.47810062463688674]
	TIME [epoch: 2.22 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189285091978948		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.8189285091978948 | validation: 0.47521150184638317]
	TIME [epoch: 2.21 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130169947909656		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.8130169947909656 | validation: 0.46580390150199946]
	TIME [epoch: 2.21 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986516241346194		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.7986516241346194 | validation: 0.46699519126153916]
	TIME [epoch: 2.21 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993605067391215		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.7993605067391215 | validation: 0.4648853581480185]
	TIME [epoch: 2.21 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802894979644962		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.802894979644962 | validation: 0.47580795975298695]
	TIME [epoch: 2.21 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034450270001442		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.8034450270001442 | validation: 0.4666633773213331]
	TIME [epoch: 2.21 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7948031918734794		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.7948031918734794 | validation: 0.47070942070503946]
	TIME [epoch: 2.22 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932210657950082		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.7932210657950082 | validation: 0.4466223825056645]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_900.pth
	Model improved!!!
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795584850781717		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.795584850781717 | validation: 0.45959692742316477]
	TIME [epoch: 2.21 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800725939099888		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.800725939099888 | validation: 0.459860621046401]
	TIME [epoch: 2.21 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099548978990408		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.8099548978990408 | validation: 0.4607944488241459]
	TIME [epoch: 2.23 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055874116034852		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.8055874116034852 | validation: 0.46014777608110863]
	TIME [epoch: 2.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999369887443817		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.7999369887443817 | validation: 0.46259125534036327]
	TIME [epoch: 2.22 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803286683163874		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.803286683163874 | validation: 0.4773106571207086]
	TIME [epoch: 2.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992310282771539		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.7992310282771539 | validation: 0.46943272839664074]
	TIME [epoch: 2.22 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000170902215388		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.8000170902215388 | validation: 0.456970968238894]
	TIME [epoch: 2.21 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020549673335254		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.8020549673335254 | validation: 0.46528856281286046]
	TIME [epoch: 2.22 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975422808848655		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.7975422808848655 | validation: 0.4528757422675409]
	TIME [epoch: 2.21 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057274892817656		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.8057274892817656 | validation: 0.4591522342332276]
	TIME [epoch: 2.21 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802455984574639		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.802455984574639 | validation: 0.4507326403098866]
	TIME [epoch: 2.21 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799849442303306		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.799849442303306 | validation: 0.4607289407087445]
	TIME [epoch: 2.21 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939012503716848		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.7939012503716848 | validation: 0.45482266182455117]
	TIME [epoch: 2.21 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901946043444492		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.7901946043444492 | validation: 0.45912143183368026]
	TIME [epoch: 2.21 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926062485067368		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.7926062485067368 | validation: 0.45384146797494096]
	TIME [epoch: 2.22 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897932417727688		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.7897932417727688 | validation: 0.46598524805695346]
	TIME [epoch: 2.21 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794313641307555		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.794313641307555 | validation: 0.44482702721812706]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932725020593935		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.7932725020593935 | validation: 0.45344620475942216]
	TIME [epoch: 2.21 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910150239099503		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.7910150239099503 | validation: 0.4561473092150237]
	TIME [epoch: 2.22 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018759432983772		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.8018759432983772 | validation: 0.45870770655338355]
	TIME [epoch: 2.21 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068930952200836		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.8068930952200836 | validation: 0.44838646217853495]
	TIME [epoch: 2.22 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981003141978994		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.7981003141978994 | validation: 0.45362562827406816]
	TIME [epoch: 2.21 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927068873447591		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.7927068873447591 | validation: 0.4487587418478527]
	TIME [epoch: 2.21 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944052881023627		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.7944052881023627 | validation: 0.453446528021561]
	TIME [epoch: 2.21 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800782047664595		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.800782047664595 | validation: 0.48692743242498937]
	TIME [epoch: 2.21 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815484230865963		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.815484230865963 | validation: 0.46691544486229675]
	TIME [epoch: 2.21 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094851152535075		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.8094851152535075 | validation: 0.45205646000446986]
	TIME [epoch: 2.21 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798828883377393		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.798828883377393 | validation: 0.44651800688712656]
	TIME [epoch: 2.24 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956850449921653		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.7956850449921653 | validation: 0.4537876263266749]
	TIME [epoch: 2.21 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956064293225066		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.7956064293225066 | validation: 0.4591442474913822]
	TIME [epoch: 2.21 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934785692106547		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.7934785692106547 | validation: 0.43570027748643225]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914497760676901		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.7914497760676901 | validation: 0.443046059197892]
	TIME [epoch: 2.22 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950433051639284		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.7950433051639284 | validation: 0.44518519616801044]
	TIME [epoch: 2.22 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910294166586898		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.7910294166586898 | validation: 0.4430676667436947]
	TIME [epoch: 3.02 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795707771068486		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.795707771068486 | validation: 0.45322644709025167]
	TIME [epoch: 2.22 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7931892010577855		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.7931892010577855 | validation: 0.4546531564307735]
	TIME [epoch: 2.21 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917594336863115		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.7917594336863115 | validation: 0.4462901362706562]
	TIME [epoch: 2.21 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798597540102882		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.798597540102882 | validation: 0.4710943339244989]
	TIME [epoch: 2.21 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194303020773052		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.8194303020773052 | validation: 0.47031195065409465]
	TIME [epoch: 2.21 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240145908019176		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.8240145908019176 | validation: 0.47053187377421424]
	TIME [epoch: 2.24 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987953275030271		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.7987953275030271 | validation: 0.44901567431948275]
	TIME [epoch: 2.21 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890621865501558		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.7890621865501558 | validation: 0.4464421594950994]
	TIME [epoch: 2.23 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877911929691814		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.7877911929691814 | validation: 0.449153686660887]
	TIME [epoch: 2.21 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796982272723328		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.796982272723328 | validation: 0.4535788715003345]
	TIME [epoch: 2.22 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929628297696669		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.7929628297696669 | validation: 0.4525425463122387]
	TIME [epoch: 2.21 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905382811387865		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.7905382811387865 | validation: 0.4536077056725965]
	TIME [epoch: 2.23 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.796424408190305		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.796424408190305 | validation: 0.45673491869381033]
	TIME [epoch: 2.21 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950349138857306		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.7950349138857306 | validation: 0.44683571453463644]
	TIME [epoch: 2.21 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991880146227374		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.7991880146227374 | validation: 0.4601858354308459]
	TIME [epoch: 2.21 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050724717417742		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.8050724717417742 | validation: 0.46699996946374384]
	TIME [epoch: 2.21 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797077514261478		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.797077514261478 | validation: 0.4516066476705543]
	TIME [epoch: 2.21 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927929325078412		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.7927929325078412 | validation: 0.4512161840933015]
	TIME [epoch: 2.21 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915588951896446		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.7915588951896446 | validation: 0.46192304200760675]
	TIME [epoch: 2.24 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930269082353121		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.7930269082353121 | validation: 0.44417042072487284]
	TIME [epoch: 2.21 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792960471787175		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.792960471787175 | validation: 0.4475546108233938]
	TIME [epoch: 2.22 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896671485431853		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.7896671485431853 | validation: 0.4438647240467031]
	TIME [epoch: 2.21 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920838421104432		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.7920838421104432 | validation: 0.44934726908063816]
	TIME [epoch: 2.23 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075498858202186		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.8075498858202186 | validation: 0.45947196760300674]
	TIME [epoch: 2.21 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091978555154953		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.8091978555154953 | validation: 0.45402811294780554]
	TIME [epoch: 2.21 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980105049780157		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.7980105049780157 | validation: 0.44820331094713384]
	TIME [epoch: 2.21 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906446933197708		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.7906446933197708 | validation: 0.45276005920491325]
	TIME [epoch: 2.21 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884209691732719		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.7884209691732719 | validation: 0.43818121946494126]
	TIME [epoch: 2.21 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918814037654852		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.7918814037654852 | validation: 0.4421895271010331]
	TIME [epoch: 2.21 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919519916642795		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.7919519916642795 | validation: 0.4498248580753276]
	TIME [epoch: 2.23 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894265031737138		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.7894265031737138 | validation: 0.447925369928959]
	TIME [epoch: 2.21 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997194327433127		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.7997194327433127 | validation: 0.4484657648421886]
	TIME [epoch: 2.23 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801616531683071		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.801616531683071 | validation: 0.4447259645439545]
	TIME [epoch: 2.21 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981312210861481		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.7981312210861481 | validation: 0.44515268348447323]
	TIME [epoch: 2.21 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960082097305005		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.7960082097305005 | validation: 0.44300649870413517]
	TIME [epoch: 2.21 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929778676824961		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.7929778676824961 | validation: 0.4476883401651776]
	TIME [epoch: 2.21 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914585857315356		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.7914585857315356 | validation: 0.4491070136114839]
	TIME [epoch: 2.21 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900735612191462		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.7900735612191462 | validation: 0.4505392704783322]
	TIME [epoch: 2.21 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922787178977362		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.7922787178977362 | validation: 0.4333357977530159]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897803486791163		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.7897803486791163 | validation: 0.43943749110724983]
	TIME [epoch: 2.21 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924421232462358		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.7924421232462358 | validation: 0.4526994613929529]
	TIME [epoch: 2.23 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953463181329496		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.7953463181329496 | validation: 0.45223064730240775]
	TIME [epoch: 2.24 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969386177253321		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.7969386177253321 | validation: 0.46486685148652357]
	TIME [epoch: 2.21 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013369359773438		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.8013369359773438 | validation: 0.4635411749684263]
	TIME [epoch: 2.21 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800730954838929		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.800730954838929 | validation: 0.4443070356727068]
	TIME [epoch: 2.22 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996478785339127		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.7996478785339127 | validation: 0.4512226783795204]
	TIME [epoch: 2.21 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951474865794512		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.7951474865794512 | validation: 0.4496476374767888]
	TIME [epoch: 2.23 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013339400536856		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.8013339400536856 | validation: 0.46252235953127774]
	TIME [epoch: 2.21 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070612203585941		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.8070612203585941 | validation: 0.4510247132783561]
	TIME [epoch: 2.23 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963523995697689		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.7963523995697689 | validation: 0.4422797642041954]
	TIME [epoch: 2.21 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889946792066409		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.7889946792066409 | validation: 0.4453453357060614]
	TIME [epoch: 2.21 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888950878791134		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.7888950878791134 | validation: 0.44403693132884015]
	TIME [epoch: 2.21 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903831287044178		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.7903831287044178 | validation: 0.44039867740024585]
	TIME [epoch: 2.21 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880298310831566		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.7880298310831566 | validation: 0.43703190292606386]
	TIME [epoch: 2.21 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920410351001544		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.7920410351001544 | validation: 0.4434688756874044]
	TIME [epoch: 2.21 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888830430411422		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.7888830430411422 | validation: 0.43895179989201605]
	TIME [epoch: 2.23 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949028148042165		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.7949028148042165 | validation: 0.44384800270734037]
	TIME [epoch: 2.21 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025152074070222		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.8025152074070222 | validation: 0.46381410959794267]
	TIME [epoch: 2.24 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988407868885162		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.7988407868885162 | validation: 0.4472660144367948]
	TIME [epoch: 2.21 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917279096887717		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.7917279096887717 | validation: 0.4461302417702926]
	TIME [epoch: 2.23 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792381080274414		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.792381080274414 | validation: 0.45122056854439524]
	TIME [epoch: 2.21 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924008116982433		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.7924008116982433 | validation: 0.4406275615802316]
	TIME [epoch: 2.23 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906666969017753		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.7906666969017753 | validation: 0.44898560386980635]
	TIME [epoch: 2.21 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887113966504288		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.7887113966504288 | validation: 0.44385876366595733]
	TIME [epoch: 2.21 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913137069028875		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.7913137069028875 | validation: 0.430958564623368]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899585149540049		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.7899585149540049 | validation: 0.45138570759248076]
	TIME [epoch: 175 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955554428952649		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.7955554428952649 | validation: 0.4437799580369934]
	TIME [epoch: 4.78 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968867150341129		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.7968867150341129 | validation: 0.44110689408440146]
	TIME [epoch: 4.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959614862866208		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.7959614862866208 | validation: 0.4434401610413978]
	TIME [epoch: 4.75 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930085518555626		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.7930085518555626 | validation: 0.4434330284173491]
	TIME [epoch: 4.76 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790406038574722		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.790406038574722 | validation: 0.4479243825354038]
	TIME [epoch: 4.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7919966986260158		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.7919966986260158 | validation: 0.44181739877072124]
	TIME [epoch: 4.75 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905898621841899		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.7905898621841899 | validation: 0.45274215987439237]
	TIME [epoch: 4.75 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905642082785016		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.7905642082785016 | validation: 0.44015333812827584]
	TIME [epoch: 4.75 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923736409890347		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.7923736409890347 | validation: 0.44977299202630666]
	TIME [epoch: 4.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7925425457448769		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.7925425457448769 | validation: 0.4427778761053331]
	TIME [epoch: 4.78 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909147717364648		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.7909147717364648 | validation: 0.44129680048307796]
	TIME [epoch: 4.77 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942951707192498		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.7942951707192498 | validation: 0.4449415052303529]
	TIME [epoch: 4.75 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949434209974848		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.7949434209974848 | validation: 0.4409079435506243]
	TIME [epoch: 4.75 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930832794317237		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.7930832794317237 | validation: 0.43519322434599456]
	TIME [epoch: 4.75 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900516462978803		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.7900516462978803 | validation: 0.4379655772626767]
	TIME [epoch: 4.76 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920636851614787		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.7920636851614787 | validation: 0.43060711666010615]
	TIME [epoch: 4.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1017.pth
	Model improved!!!
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877160777935691		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.7877160777935691 | validation: 0.4424051209137414]
	TIME [epoch: 4.75 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916442572437447		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.7916442572437447 | validation: 0.44961876202338263]
	TIME [epoch: 4.77 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892043581232171		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.7892043581232171 | validation: 0.4513844421019048]
	TIME [epoch: 4.75 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887143936543884		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.7887143936543884 | validation: 0.44152044300097437]
	TIME [epoch: 4.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910146066872352		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.7910146066872352 | validation: 0.4453814020235594]
	TIME [epoch: 4.75 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978003596937745		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.7978003596937745 | validation: 0.4455063095491412]
	TIME [epoch: 4.76 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976614638109574		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.7976614638109574 | validation: 0.44891948927125935]
	TIME [epoch: 4.77 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010031554932482		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.8010031554932482 | validation: 0.43896937743293574]
	TIME [epoch: 4.75 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961792673654169		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.7961792673654169 | validation: 0.4364859022818044]
	TIME [epoch: 4.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887955610450772		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.7887955610450772 | validation: 0.4440135114579521]
	TIME [epoch: 4.77 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900841328598079		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.7900841328598079 | validation: 0.44812035867468225]
	TIME [epoch: 4.76 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917958770156895		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.7917958770156895 | validation: 0.4494017467316989]
	TIME [epoch: 4.77 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7938503409046037		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.7938503409046037 | validation: 0.43999023601625176]
	TIME [epoch: 4.76 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915400524386573		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.7915400524386573 | validation: 0.43997608009112255]
	TIME [epoch: 4.76 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898699568531004		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.7898699568531004 | validation: 0.44109141178898703]
	TIME [epoch: 4.76 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881659357566599		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.7881659357566599 | validation: 0.439606123281318]
	TIME [epoch: 4.75 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950071562346517		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.7950071562346517 | validation: 0.4405089725334291]
	TIME [epoch: 4.75 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926411781750068		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.7926411781750068 | validation: 0.4353024050521363]
	TIME [epoch: 4.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957489764437545		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.7957489764437545 | validation: 0.4425542287269886]
	TIME [epoch: 4.75 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933479801210188		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.7933479801210188 | validation: 0.4463338848339099]
	TIME [epoch: 4.76 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895614931250715		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.7895614931250715 | validation: 0.43921614123180447]
	TIME [epoch: 4.76 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902238153552107		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.7902238153552107 | validation: 0.43637405247212196]
	TIME [epoch: 4.76 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918906184177291		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.7918906184177291 | validation: 0.443135081753237]
	TIME [epoch: 4.75 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897056243188709		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.7897056243188709 | validation: 0.45081407540847557]
	TIME [epoch: 4.75 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956950619665546		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.7956950619665546 | validation: 0.4540982108817184]
	TIME [epoch: 4.76 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995357453523029		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.7995357453523029 | validation: 0.4404667868750508]
	TIME [epoch: 4.76 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905884182792856		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.7905884182792856 | validation: 0.43875973245453964]
	TIME [epoch: 4.75 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916757037682072		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.7916757037682072 | validation: 0.44127950838715935]
	TIME [epoch: 4.76 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896781014981482		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.7896781014981482 | validation: 0.4328221213091659]
	TIME [epoch: 4.75 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897063339108285		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.7897063339108285 | validation: 0.4305719268193956]
	TIME [epoch: 4.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1047.pth
	Model improved!!!
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904091405174115		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.7904091405174115 | validation: 0.43725229685217415]
	TIME [epoch: 4.77 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888863783310234		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.7888863783310234 | validation: 0.44774045068613183]
	TIME [epoch: 4.77 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790784015664371		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.790784015664371 | validation: 0.4424570570479385]
	TIME [epoch: 4.77 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933982437501595		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.7933982437501595 | validation: 0.44519326856440655]
	TIME [epoch: 4.77 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8027471351613673		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.8027471351613673 | validation: 0.43326910616205005]
	TIME [epoch: 4.77 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917235259025622		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.7917235259025622 | validation: 0.44262439687789207]
	TIME [epoch: 4.75 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886931251260922		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.7886931251260922 | validation: 0.4457713704417874]
	TIME [epoch: 4.76 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888094274640578		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.7888094274640578 | validation: 0.44198478849499684]
	TIME [epoch: 4.76 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847987014469089		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.7847987014469089 | validation: 0.44451430348700216]
	TIME [epoch: 4.76 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169285996333747		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.8169285996333747 | validation: 0.4288416739431433]
	TIME [epoch: 4.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861542801374899		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.7861542801374899 | validation: 0.44547570751391563]
	TIME [epoch: 4.75 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986698839292705		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.7986698839292705 | validation: 0.43686914195778137]
	TIME [epoch: 4.78 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917519517192662		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.7917519517192662 | validation: 0.4328965769792446]
	TIME [epoch: 4.78 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902464801275119		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.7902464801275119 | validation: 0.42959064735705915]
	TIME [epoch: 4.76 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883928739405631		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.7883928739405631 | validation: 0.43103458874930517]
	TIME [epoch: 4.77 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837707683212071		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.7837707683212071 | validation: 0.44057197154912875]
	TIME [epoch: 4.77 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899572864295255		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.7899572864295255 | validation: 0.43225982592327594]
	TIME [epoch: 4.77 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876021669930369		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.7876021669930369 | validation: 0.4423869189709077]
	TIME [epoch: 4.78 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788231461058534		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.788231461058534 | validation: 0.4434448275754858]
	TIME [epoch: 4.76 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892264457620365		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.7892264457620365 | validation: 0.43453472499260326]
	TIME [epoch: 4.77 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885688361323687		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.7885688361323687 | validation: 0.432671543652748]
	TIME [epoch: 4.78 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868727813224031		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.7868727813224031 | validation: 0.4363365295333357]
	TIME [epoch: 4.75 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851206652985405		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.7851206652985405 | validation: 0.4349783986463644]
	TIME [epoch: 4.77 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865616575989164		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.7865616575989164 | validation: 0.43705723082539816]
	TIME [epoch: 4.78 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877517812660875		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.7877517812660875 | validation: 0.4363380932655625]
	TIME [epoch: 4.77 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880834445078069		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.7880834445078069 | validation: 0.43116812637737545]
	TIME [epoch: 4.77 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871942460295346		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.7871942460295346 | validation: 0.4455802295644139]
	TIME [epoch: 4.77 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827199053770724		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.7827199053770724 | validation: 0.42581871735155]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1075.pth
	Model improved!!!
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911473972919927		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.7911473972919927 | validation: 0.43910549367459056]
	TIME [epoch: 4.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791265510757938		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.791265510757938 | validation: 0.44084807945517906]
	TIME [epoch: 4.78 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898871423090822		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.7898871423090822 | validation: 0.4385904256355584]
	TIME [epoch: 4.77 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867352242963004		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.7867352242963004 | validation: 0.43175178593388464]
	TIME [epoch: 4.77 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867821086742229		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.7867821086742229 | validation: 0.43096200231176784]
	TIME [epoch: 4.77 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784249740549262		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.784249740549262 | validation: 0.4306611630286419]
	TIME [epoch: 4.77 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854620875599059		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.7854620875599059 | validation: 0.4316778722746765]
	TIME [epoch: 4.76 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787964411567224		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.787964411567224 | validation: 0.4431363020117596]
	TIME [epoch: 4.78 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851630711311671		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.7851630711311671 | validation: 0.43919740838948046]
	TIME [epoch: 4.77 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883220758918457		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.7883220758918457 | validation: 0.4365368330220136]
	TIME [epoch: 4.77 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789657301592789		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.789657301592789 | validation: 0.43432698207798615]
	TIME [epoch: 4.77 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900281984638556		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.7900281984638556 | validation: 0.43499864718836734]
	TIME [epoch: 4.77 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864115912787044		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.7864115912787044 | validation: 0.43617013590139014]
	TIME [epoch: 4.77 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856846832335088		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.7856846832335088 | validation: 0.43601100515961533]
	TIME [epoch: 4.77 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888871295875537		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.7888871295875537 | validation: 0.44913607591066873]
	TIME [epoch: 4.77 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891267743169109		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.7891267743169109 | validation: 0.444133263395992]
	TIME [epoch: 4.77 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893480731512037		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.7893480731512037 | validation: 0.43847520404465784]
	TIME [epoch: 4.75 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878073977681986		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.7878073977681986 | validation: 0.4240966364157053]
	TIME [epoch: 4.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1093.pth
	Model improved!!!
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857321162658286		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.7857321162658286 | validation: 0.4291861043073478]
	TIME [epoch: 4.76 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877202329183096		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.7877202329183096 | validation: 0.436897983078035]
	TIME [epoch: 4.78 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923808271677585		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.7923808271677585 | validation: 0.43705124126070793]
	TIME [epoch: 4.76 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928725168516897		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.7928725168516897 | validation: 0.44771962053051856]
	TIME [epoch: 4.76 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937381887877536		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.7937381887877536 | validation: 0.4332958997918483]
	TIME [epoch: 4.75 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906272819151542		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.7906272819151542 | validation: 0.4285294711580452]
	TIME [epoch: 4.76 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789111201907644		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.789111201907644 | validation: 0.4299351992537281]
	TIME [epoch: 4.76 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869299812341762		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.7869299812341762 | validation: 0.4370190860580696]
	TIME [epoch: 4.76 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790656944843989		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.790656944843989 | validation: 0.4358459813363986]
	TIME [epoch: 4.77 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853892755145244		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.7853892755145244 | validation: 0.43455879560898314]
	TIME [epoch: 4.76 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909694830850592		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.7909694830850592 | validation: 0.42976410765964446]
	TIME [epoch: 4.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848940795572134		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.7848940795572134 | validation: 0.4278212519323346]
	TIME [epoch: 4.76 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7879666351825063		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.7879666351825063 | validation: 0.4325602634340858]
	TIME [epoch: 4.76 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861763503883605		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.7861763503883605 | validation: 0.424501280367903]
	TIME [epoch: 4.76 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895811010141331		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.7895811010141331 | validation: 0.43235993669373657]
	TIME [epoch: 4.77 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881202487356685		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.7881202487356685 | validation: 0.4378796475351392]
	TIME [epoch: 4.75 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907964102428018		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.7907964102428018 | validation: 0.43335345215360843]
	TIME [epoch: 4.76 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856281435587527		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.7856281435587527 | validation: 0.4315709739946996]
	TIME [epoch: 4.77 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890073032882664		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.7890073032882664 | validation: 0.4262121456604945]
	TIME [epoch: 4.76 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912870743033328		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.7912870743033328 | validation: 0.43720041281748157]
	TIME [epoch: 4.77 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891636878588975		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.7891636878588975 | validation: 0.43818619346982435]
	TIME [epoch: 4.77 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842148053191613		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.7842148053191613 | validation: 0.43240917099392373]
	TIME [epoch: 4.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896346326905634		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.7896346326905634 | validation: 0.43673162360693607]
	TIME [epoch: 4.76 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789602012847468		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.789602012847468 | validation: 0.4507539757494338]
	TIME [epoch: 4.76 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890074959295309		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.7890074959295309 | validation: 0.4329374511338828]
	TIME [epoch: 4.75 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855915605579634		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.7855915605579634 | validation: 0.4356065399904431]
	TIME [epoch: 4.78 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856801811791041		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.7856801811791041 | validation: 0.4305718228044727]
	TIME [epoch: 4.76 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784630672066404		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.784630672066404 | validation: 0.4324688055824874]
	TIME [epoch: 4.77 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794070093113397		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.794070093113397 | validation: 0.42294593991125445]
	TIME [epoch: 4.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1122.pth
	Model improved!!!
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937471887589801		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.7937471887589801 | validation: 0.43929164586075525]
	TIME [epoch: 4.76 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785037740343798		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.785037740343798 | validation: 0.43463133155174555]
	TIME [epoch: 4.77 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852472703059084		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.7852472703059084 | validation: 0.4357748567997873]
	TIME [epoch: 4.76 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785089890168224		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.785089890168224 | validation: 0.43169887170898114]
	TIME [epoch: 4.76 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858885864497559		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.7858885864497559 | validation: 0.43226814836360306]
	TIME [epoch: 4.76 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858906549816962		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.7858906549816962 | validation: 0.44115987385557365]
	TIME [epoch: 4.76 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845199244728391		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.7845199244728391 | validation: 0.4379382707640034]
	TIME [epoch: 4.76 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846077993984375		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.7846077993984375 | validation: 0.4320094299339048]
	TIME [epoch: 4.77 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841693512281275		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.7841693512281275 | validation: 0.43053225859620864]
	TIME [epoch: 4.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894449451229844		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.7894449451229844 | validation: 0.44361479373200896]
	TIME [epoch: 4.75 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901961686214216		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.7901961686214216 | validation: 0.4435889467550693]
	TIME [epoch: 4.75 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881471959461093		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.7881471959461093 | validation: 0.43461408791202694]
	TIME [epoch: 4.75 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867131282715221		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.7867131282715221 | validation: 0.4281968954455604]
	TIME [epoch: 4.75 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832587300688175		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.7832587300688175 | validation: 0.4260127715158234]
	TIME [epoch: 4.75 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838562596141222		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.7838562596141222 | validation: 0.42620595194796174]
	TIME [epoch: 4.77 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861952436792816		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.7861952436792816 | validation: 0.43262067920880876]
	TIME [epoch: 4.76 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868614972703283		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.7868614972703283 | validation: 0.4326181704908553]
	TIME [epoch: 4.77 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875090496275639		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.7875090496275639 | validation: 0.43433449446983474]
	TIME [epoch: 4.75 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909553463113733		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.7909553463113733 | validation: 0.44359573011827663]
	TIME [epoch: 4.76 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902162782694842		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.7902162782694842 | validation: 0.4271870253095125]
	TIME [epoch: 4.75 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903531966539774		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.7903531966539774 | validation: 0.4413644071494911]
	TIME [epoch: 4.77 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876433893976577		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.7876433893976577 | validation: 0.43226512157562813]
	TIME [epoch: 4.76 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877760880452123		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.7877760880452123 | validation: 0.4349857062354085]
	TIME [epoch: 4.76 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843776885964351		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.7843776885964351 | validation: 0.42787900241626176]
	TIME [epoch: 4.75 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851822539933414		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.7851822539933414 | validation: 0.43126176252660076]
	TIME [epoch: 4.75 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868428976657412		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.7868428976657412 | validation: 0.4312691994636687]
	TIME [epoch: 4.76 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880390384032495		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.7880390384032495 | validation: 0.43665451335826316]
	TIME [epoch: 4.78 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856383331161589		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.7856383331161589 | validation: 0.4358362152729458]
	TIME [epoch: 4.77 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871336027662682		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.7871336027662682 | validation: 0.4273613626069157]
	TIME [epoch: 4.77 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856083906666501		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.7856083906666501 | validation: 0.42663100522760455]
	TIME [epoch: 4.77 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850288976648687		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.7850288976648687 | validation: 0.42945454912625514]
	TIME [epoch: 4.77 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785545572181736		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.785545572181736 | validation: 0.4315555581091746]
	TIME [epoch: 4.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822860938935285		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.7822860938935285 | validation: 0.42921814165193556]
	TIME [epoch: 4.77 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785713693702369		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.785713693702369 | validation: 0.43312608287419274]
	TIME [epoch: 4.77 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858845390163804		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.7858845390163804 | validation: 0.4273712174766199]
	TIME [epoch: 4.76 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890650974915259		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.7890650974915259 | validation: 0.42651346840896354]
	TIME [epoch: 4.77 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860947789822456		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.7860947789822456 | validation: 0.43814326420484095]
	TIME [epoch: 4.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787303749493679		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.787303749493679 | validation: 0.4406777994530936]
	TIME [epoch: 4.77 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820135116882534		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.7820135116882534 | validation: 0.44136580812659326]
	TIME [epoch: 4.78 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902053902909841		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.7902053902909841 | validation: 0.43281395170358467]
	TIME [epoch: 4.77 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935636396480996		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.7935636396480996 | validation: 0.44055035511141627]
	TIME [epoch: 4.76 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909668490303164		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.7909668490303164 | validation: 0.4297102668786361]
	TIME [epoch: 4.77 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880646591595949		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.7880646591595949 | validation: 0.42329134556571885]
	TIME [epoch: 4.77 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880990596104899		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.7880990596104899 | validation: 0.43460749031259066]
	TIME [epoch: 4.76 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898624839525283		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.7898624839525283 | validation: 0.43169318266146206]
	TIME [epoch: 4.78 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784793014754435		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.784793014754435 | validation: 0.43109249267353]
	TIME [epoch: 4.76 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788359658014235		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.788359658014235 | validation: 0.4331862722529216]
	TIME [epoch: 4.77 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861543350733686		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.7861543350733686 | validation: 0.4276151448469195]
	TIME [epoch: 4.77 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868467871842892		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.7868467871842892 | validation: 0.4303258283898821]
	TIME [epoch: 4.76 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847950511153569		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.7847950511153569 | validation: 0.44171345192780226]
	TIME [epoch: 4.75 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877755681571914		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.7877755681571914 | validation: 0.43253316072019116]
	TIME [epoch: 4.76 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871763445706818		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.7871763445706818 | validation: 0.42360856544180403]
	TIME [epoch: 4.77 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847058993883067		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.7847058993883067 | validation: 0.4321836385042092]
	TIME [epoch: 4.77 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863052340171953		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.7863052340171953 | validation: 0.4315340945649453]
	TIME [epoch: 4.77 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858170209362051		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.7858170209362051 | validation: 0.43721382041122975]
	TIME [epoch: 4.77 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786468864110729		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.786468864110729 | validation: 0.43464031968553024]
	TIME [epoch: 4.77 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786864584521081		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.786864584521081 | validation: 0.4301425320331189]
	TIME [epoch: 4.78 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865456244919071		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.7865456244919071 | validation: 0.4216693608403077]
	TIME [epoch: 4.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834661261736389		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.7834661261736389 | validation: 0.43341799840632655]
	TIME [epoch: 4.76 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784073228181125		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.784073228181125 | validation: 0.4332677658993029]
	TIME [epoch: 4.77 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862472560815581		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.7862472560815581 | validation: 0.4351667106514039]
	TIME [epoch: 4.76 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785220410515625		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.785220410515625 | validation: 0.43620278663962025]
	TIME [epoch: 4.77 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827400528908064		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.7827400528908064 | validation: 0.43660469273635694]
	TIME [epoch: 4.77 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838341569752799		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.7838341569752799 | validation: 0.4294964722948633]
	TIME [epoch: 4.77 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856878179808707		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.7856878179808707 | validation: 0.42941753931636467]
	TIME [epoch: 4.78 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848694593120753		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.7848694593120753 | validation: 0.43579151490511425]
	TIME [epoch: 4.77 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843484127968526		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.7843484127968526 | validation: 0.4264454808242903]
	TIME [epoch: 4.75 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876587352802297		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.7876587352802297 | validation: 0.4389292644829187]
	TIME [epoch: 4.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789229636861592		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.789229636861592 | validation: 0.42290009700813]
	TIME [epoch: 4.75 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878848921249927		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.7878848921249927 | validation: 0.4327397652114612]
	TIME [epoch: 4.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915911850458089		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.7915911850458089 | validation: 0.43147437565465485]
	TIME [epoch: 4.76 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862427403381003		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.7862427403381003 | validation: 0.4337732460324957]
	TIME [epoch: 4.76 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853390676470484		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.7853390676470484 | validation: 0.4359574886845315]
	TIME [epoch: 4.77 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843609867786502		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.7843609867786502 | validation: 0.4265002986746728]
	TIME [epoch: 4.76 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782827540302633		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.782827540302633 | validation: 0.4349534876611408]
	TIME [epoch: 4.77 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874225911723735		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.7874225911723735 | validation: 0.42565520777811205]
	TIME [epoch: 4.76 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892328520059203		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.7892328520059203 | validation: 0.4289276263798284]
	TIME [epoch: 4.77 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850864875113379		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.7850864875113379 | validation: 0.4230759899691302]
	TIME [epoch: 4.76 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848182336557639		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.7848182336557639 | validation: 0.42776694832409085]
	TIME [epoch: 4.75 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872868216967532		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.7872868216967532 | validation: 0.41700212558943034]
	TIME [epoch: 4.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1202.pth
	Model improved!!!
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857640212535904		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.7857640212535904 | validation: 0.4313867588842937]
	TIME [epoch: 4.75 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891437770009018		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.7891437770009018 | validation: 0.4283896335605286]
	TIME [epoch: 4.75 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791356859329271		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.791356859329271 | validation: 0.42761829112744654]
	TIME [epoch: 4.76 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903593838168317		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.7903593838168317 | validation: 0.41624771400329347]
	TIME [epoch: 4.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1206.pth
	Model improved!!!
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837728652642246		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.7837728652642246 | validation: 0.41831751819628504]
	TIME [epoch: 4.75 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786331841396985		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.786331841396985 | validation: 0.43106178545379975]
	TIME [epoch: 4.75 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844890841245183		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.7844890841245183 | validation: 0.4329901019030348]
	TIME [epoch: 4.76 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832906102629136		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.7832906102629136 | validation: 0.4335486837204882]
	TIME [epoch: 4.75 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858012852383144		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.7858012852383144 | validation: 0.4354074672331069]
	TIME [epoch: 4.76 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881178506584899		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.7881178506584899 | validation: 0.4267135019189285]
	TIME [epoch: 4.75 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854055116371831		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.7854055116371831 | validation: 0.440021711014559]
	TIME [epoch: 4.76 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846854837777031		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.7846854837777031 | validation: 0.42813831630298105]
	TIME [epoch: 4.77 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836689779066588		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.7836689779066588 | validation: 0.4233628648349823]
	TIME [epoch: 4.78 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78598655140468		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.78598655140468 | validation: 0.433290896986493]
	TIME [epoch: 4.77 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890751919169192		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.7890751919169192 | validation: 0.42968665098075665]
	TIME [epoch: 4.77 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843564728980382		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.7843564728980382 | validation: 0.426312962841908]
	TIME [epoch: 4.77 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835281773698634		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.7835281773698634 | validation: 0.4414795060097075]
	TIME [epoch: 4.77 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868986856719414		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.7868986856719414 | validation: 0.4264348831242109]
	TIME [epoch: 4.78 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834119831173607		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.7834119831173607 | validation: 0.43175588590087877]
	TIME [epoch: 4.77 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845096656175629		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.7845096656175629 | validation: 0.42893166640550223]
	TIME [epoch: 4.75 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898474624867836		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.7898474624867836 | validation: 0.4322997033752678]
	TIME [epoch: 4.75 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846861928751946		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.7846861928751946 | validation: 0.43504676959580685]
	TIME [epoch: 4.75 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836908672108246		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.7836908672108246 | validation: 0.42837643467383163]
	TIME [epoch: 4.77 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850633279080296		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.7850633279080296 | validation: 0.42125969030773014]
	TIME [epoch: 4.77 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881015088677383		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.7881015088677383 | validation: 0.42371654227560357]
	TIME [epoch: 4.76 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844395894474996		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.7844395894474996 | validation: 0.43324412385932887]
	TIME [epoch: 4.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829083946167108		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.7829083946167108 | validation: 0.42826085294785055]
	TIME [epoch: 4.77 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847609085668069		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.7847609085668069 | validation: 0.4355882443190929]
	TIME [epoch: 4.77 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890696580083564		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.7890696580083564 | validation: 0.4278780053067367]
	TIME [epoch: 4.77 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850635036832264		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.7850635036832264 | validation: 0.4246781189927135]
	TIME [epoch: 4.78 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7879439780865625		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.7879439780865625 | validation: 0.43365984196038226]
	TIME [epoch: 4.76 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869502068644563		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.7869502068644563 | validation: 0.4261922572603286]
	TIME [epoch: 4.76 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864322458438312		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.7864322458438312 | validation: 0.4304375528211396]
	TIME [epoch: 4.76 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868474340161703		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.7868474340161703 | validation: 0.4251961246151675]
	TIME [epoch: 4.77 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802203881779189		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.7802203881779189 | validation: 0.42395576604387897]
	TIME [epoch: 4.77 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844836296382485		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.7844836296382485 | validation: 0.42884082111912636]
	TIME [epoch: 4.76 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781629029348967		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.781629029348967 | validation: 0.43735934962299017]
	TIME [epoch: 4.75 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856105405193817		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.7856105405193817 | validation: 0.43385516424800613]
	TIME [epoch: 4.75 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848648338213781		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.7848648338213781 | validation: 0.42313563713843494]
	TIME [epoch: 4.77 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829406666048238		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.7829406666048238 | validation: 0.4248646585122255]
	TIME [epoch: 4.77 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859852199450629		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.7859852199450629 | validation: 0.4305914865314082]
	TIME [epoch: 4.77 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824018230811841		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.7824018230811841 | validation: 0.43301912420182653]
	TIME [epoch: 4.77 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876738436367644		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.7876738436367644 | validation: 0.4278247975029409]
	TIME [epoch: 4.76 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861083915382356		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.7861083915382356 | validation: 0.42718805490494205]
	TIME [epoch: 4.75 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784518833105241		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.784518833105241 | validation: 0.42826701972856357]
	TIME [epoch: 4.77 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782676717709309		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.7782676717709309 | validation: 0.423380746372478]
	TIME [epoch: 4.77 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862340948306694		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.7862340948306694 | validation: 0.4286395753138858]
	TIME [epoch: 4.75 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834018118775834		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.7834018118775834 | validation: 0.42256819078023206]
	TIME [epoch: 4.76 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786856983640976		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.7786856983640976 | validation: 0.4343143398641612]
	TIME [epoch: 4.77 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853388593915019		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.7853388593915019 | validation: 0.43007667583311004]
	TIME [epoch: 4.75 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893022218529191		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.7893022218529191 | validation: 0.42862143668166486]
	TIME [epoch: 4.77 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845293634882942		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.7845293634882942 | validation: 0.4321175302149553]
	TIME [epoch: 4.77 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858353851189488		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.7858353851189488 | validation: 0.4252077037929022]
	TIME [epoch: 4.76 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884776525592563		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.7884776525592563 | validation: 0.4238946242768467]
	TIME [epoch: 4.77 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850012243322436		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.7850012243322436 | validation: 0.4319917059901725]
	TIME [epoch: 4.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896071938589339		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.7896071938589339 | validation: 0.4243626862528809]
	TIME [epoch: 4.76 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848554347841838		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.7848554347841838 | validation: 0.4285529231415852]
	TIME [epoch: 4.77 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840577487650185		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.7840577487650185 | validation: 0.4254480342302131]
	TIME [epoch: 4.76 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843258903520004		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.7843258903520004 | validation: 0.42515080733698307]
	TIME [epoch: 4.77 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871308394320821		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.7871308394320821 | validation: 0.4382393487010197]
	TIME [epoch: 4.78 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841268576409388		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.7841268576409388 | validation: 0.42916486560325584]
	TIME [epoch: 4.76 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820428436422835		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.7820428436422835 | validation: 0.42910901605112955]
	TIME [epoch: 4.76 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824222728262227		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.7824222728262227 | validation: 0.428139565166429]
	TIME [epoch: 4.77 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903692079752332		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.7903692079752332 | validation: 0.43412227786890983]
	TIME [epoch: 4.77 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845757181725287		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.7845757181725287 | validation: 0.4254216722363091]
	TIME [epoch: 4.77 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819776464635635		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.7819776464635635 | validation: 0.42469611956655395]
	TIME [epoch: 4.78 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787045035676816		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.787045035676816 | validation: 0.430932028663775]
	TIME [epoch: 4.78 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854587905660444		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.7854587905660444 | validation: 0.420971466623302]
	TIME [epoch: 4.77 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871894023277467		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.7871894023277467 | validation: 0.43084479604647685]
	TIME [epoch: 4.76 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884132846671946		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.7884132846671946 | validation: 0.4379015399284564]
	TIME [epoch: 4.77 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834980598620638		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.7834980598620638 | validation: 0.4319969643024171]
	TIME [epoch: 4.76 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783757310621768		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.783757310621768 | validation: 0.4340350914937474]
	TIME [epoch: 4.76 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832593148439849		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.7832593148439849 | validation: 0.4162697181246042]
	TIME [epoch: 4.77 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870501982615494		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.7870501982615494 | validation: 0.42930304857112145]
	TIME [epoch: 4.77 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868172830600705		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.7868172830600705 | validation: 0.42560271223218504]
	TIME [epoch: 4.77 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883671870576392		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.7883671870576392 | validation: 0.4317302016379433]
	TIME [epoch: 4.77 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787574329035991		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.787574329035991 | validation: 0.43064756923765835]
	TIME [epoch: 4.77 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833563957531388		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.7833563957531388 | validation: 0.42655154306514653]
	TIME [epoch: 4.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784204793146343		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.784204793146343 | validation: 0.425349990660733]
	TIME [epoch: 4.75 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845383272209243		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.7845383272209243 | validation: 0.42470282209756616]
	TIME [epoch: 4.77 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865960216974978		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.7865960216974978 | validation: 0.4238799260004966]
	TIME [epoch: 4.76 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850669665196298		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.7850669665196298 | validation: 0.4235722354349843]
	TIME [epoch: 4.77 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830000181055846		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.7830000181055846 | validation: 0.4281858143184499]
	TIME [epoch: 4.77 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787546499779123		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.787546499779123 | validation: 0.43128264826986806]
	TIME [epoch: 4.78 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784490782784562		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.784490782784562 | validation: 0.4325356972279211]
	TIME [epoch: 4.76 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862236009038415		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.7862236009038415 | validation: 0.4319578281535112]
	TIME [epoch: 4.77 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857764778484745		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.7857764778484745 | validation: 0.4301843167320703]
	TIME [epoch: 4.77 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78748516215191		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.78748516215191 | validation: 0.4324509442093037]
	TIME [epoch: 4.77 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882644700510925		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.7882644700510925 | validation: 0.42219510336317434]
	TIME [epoch: 4.77 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900901108111222		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.7900901108111222 | validation: 0.4290733542850962]
	TIME [epoch: 4.78 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829036748580682		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.7829036748580682 | validation: 0.4274967823805499]
	TIME [epoch: 4.76 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838648886582832		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.7838648886582832 | validation: 0.42583043757717676]
	TIME [epoch: 4.75 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852853169380972		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.7852853169380972 | validation: 0.4235957858328623]
	TIME [epoch: 4.75 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838468860009029		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.7838468860009029 | validation: 0.4330055672113428]
	TIME [epoch: 4.76 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847637808458271		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.7847637808458271 | validation: 0.417443457357365]
	TIME [epoch: 4.76 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851116162235813		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.7851116162235813 | validation: 0.42622630500472636]
	TIME [epoch: 4.77 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861751036888334		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.7861751036888334 | validation: 0.42577812141763727]
	TIME [epoch: 4.77 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846484289720712		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.7846484289720712 | validation: 0.42216745310723225]
	TIME [epoch: 4.77 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831370459499888		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.7831370459499888 | validation: 0.4326939149506414]
	TIME [epoch: 4.75 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808374607537024		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.7808374607537024 | validation: 0.4260471958703776]
	TIME [epoch: 4.77 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841446256422222		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.7841446256422222 | validation: 0.43207382537597727]
	TIME [epoch: 4.75 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811141468184076		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.7811141468184076 | validation: 0.4301783353000797]
	TIME [epoch: 4.76 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828910726533121		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.7828910726533121 | validation: 0.43055591191178255]
	TIME [epoch: 4.77 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889892086747932		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.7889892086747932 | validation: 0.4157781303145184]
	TIME [epoch: 4.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1306.pth
	Model improved!!!
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824099529067005		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.7824099529067005 | validation: 0.426801143996809]
	TIME [epoch: 4.75 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845908104986767		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.7845908104986767 | validation: 0.42532003771587257]
	TIME [epoch: 4.77 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839139075745203		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.7839139075745203 | validation: 0.4301346390446019]
	TIME [epoch: 4.76 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849805053633938		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.7849805053633938 | validation: 0.44014850066087324]
	TIME [epoch: 4.78 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860484930513818		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.7860484930513818 | validation: 0.4251663341012646]
	TIME [epoch: 4.76 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834557875687334		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.7834557875687334 | validation: 0.42753581358599746]
	TIME [epoch: 4.76 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860294413619795		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.7860294413619795 | validation: 0.4229989464151933]
	TIME [epoch: 4.76 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838264139976252		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.7838264139976252 | validation: 0.42393454210041154]
	TIME [epoch: 4.75 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837584356562118		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.7837584356562118 | validation: 0.43105107156925027]
	TIME [epoch: 4.77 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827547352325058		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.7827547352325058 | validation: 0.4262053839416289]
	TIME [epoch: 4.78 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840756844381778		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.7840756844381778 | validation: 0.42509687255651135]
	TIME [epoch: 4.77 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862277738573571		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.7862277738573571 | validation: 0.4175430072991567]
	TIME [epoch: 4.77 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834977053591817		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.7834977053591817 | validation: 0.4219332574940012]
	TIME [epoch: 4.77 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848057878596203		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.7848057878596203 | validation: 0.4345247426400895]
	TIME [epoch: 4.77 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841704477561148		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.7841704477561148 | validation: 0.4238548184562858]
	TIME [epoch: 4.77 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813991109498806		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.7813991109498806 | validation: 0.4190917991005618]
	TIME [epoch: 4.78 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842723653706479		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.7842723653706479 | validation: 0.42979618963436067]
	TIME [epoch: 4.77 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783189615148116		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.783189615148116 | validation: 0.43125603008480307]
	TIME [epoch: 4.77 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861795205452794		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.7861795205452794 | validation: 0.43480624691638203]
	TIME [epoch: 4.77 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843332229633344		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.7843332229633344 | validation: 0.42740741152680645]
	TIME [epoch: 4.77 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842319400053746		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.7842319400053746 | validation: 0.4318576117219939]
	TIME [epoch: 4.77 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782376639895582		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.782376639895582 | validation: 0.4302177430231815]
	TIME [epoch: 4.77 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859729077824722		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.7859729077824722 | validation: 0.4345287353401512]
	TIME [epoch: 4.76 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846852280220117		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.7846852280220117 | validation: 0.4186640789620585]
	TIME [epoch: 4.77 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783959019823954		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.783959019823954 | validation: 0.4248066649629402]
	TIME [epoch: 4.77 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852698349671831		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.7852698349671831 | validation: 0.4306176931894232]
	TIME [epoch: 4.77 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847235825721517		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.7847235825721517 | validation: 0.4319625543583932]
	TIME [epoch: 4.77 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863042284637937		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.7863042284637937 | validation: 0.4190031359824851]
	TIME [epoch: 4.77 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865014645979571		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.7865014645979571 | validation: 0.4284595047918576]
	TIME [epoch: 4.75 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856084466398064		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.7856084466398064 | validation: 0.42329215135467835]
	TIME [epoch: 4.75 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847867669378087		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.7847867669378087 | validation: 0.4247815149959191]
	TIME [epoch: 4.75 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854364263100547		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.7854364263100547 | validation: 0.4291355190257912]
	TIME [epoch: 4.77 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822192597338017		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.7822192597338017 | validation: 0.42434184965441357]
	TIME [epoch: 4.75 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848436274092083		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.7848436274092083 | validation: 0.42346714241884903]
	TIME [epoch: 4.78 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848371621914346		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.7848371621914346 | validation: 0.42132026947583406]
	TIME [epoch: 4.76 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850118486794945		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.7850118486794945 | validation: 0.42714105845485345]
	TIME [epoch: 4.76 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876935988846354		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.7876935988846354 | validation: 0.4240312320705386]
	TIME [epoch: 4.77 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837090447763949		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.7837090447763949 | validation: 0.41986552283365275]
	TIME [epoch: 4.77 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841429757541638		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.7841429757541638 | validation: 0.42192098008020484]
	TIME [epoch: 4.77 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78591668638691		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.78591668638691 | validation: 0.42780842211738757]
	TIME [epoch: 4.78 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835602096021671		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.7835602096021671 | validation: 0.42151043552941375]
	TIME [epoch: 4.77 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833512048693055		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.7833512048693055 | validation: 0.42857826009951305]
	TIME [epoch: 4.77 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849753962741705		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.7849753962741705 | validation: 0.4355938126828893]
	TIME [epoch: 4.77 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802087034437026		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.7802087034437026 | validation: 0.43366432753430395]
	TIME [epoch: 4.77 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840703506693473		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.7840703506693473 | validation: 0.42434485613919554]
	TIME [epoch: 4.76 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848873466282922		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.7848873466282922 | validation: 0.4288833623221737]
	TIME [epoch: 4.78 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799761800331082		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.7799761800331082 | validation: 0.4287875053279104]
	TIME [epoch: 4.77 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844692938769164		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.7844692938769164 | validation: 0.42157502794377955]
	TIME [epoch: 4.77 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822221576835626		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.7822221576835626 | validation: 0.4299495106690304]
	TIME [epoch: 4.76 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838470208256985		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.7838470208256985 | validation: 0.4298257461389762]
	TIME [epoch: 4.75 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785086604001853		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.785086604001853 | validation: 0.4220687579649459]
	TIME [epoch: 4.76 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838480995997362		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.7838480995997362 | validation: 0.4265537730607283]
	TIME [epoch: 4.76 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863284935338459		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.7863284935338459 | validation: 0.4173435157571197]
	TIME [epoch: 4.75 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864982068694752		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.7864982068694752 | validation: 0.41864414067421907]
	TIME [epoch: 4.75 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815663599113735		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.7815663599113735 | validation: 0.43158303742714754]
	TIME [epoch: 4.75 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829604317341808		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.7829604317341808 | validation: 0.42901512720243623]
	TIME [epoch: 4.75 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827206785604615		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.7827206785604615 | validation: 0.42898689859993216]
	TIME [epoch: 4.76 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845311133252025		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.7845311133252025 | validation: 0.42956458905928585]
	TIME [epoch: 4.76 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819520561776097		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.7819520561776097 | validation: 0.4224965934355627]
	TIME [epoch: 4.76 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848896754049283		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.7848896754049283 | validation: 0.41756694969032826]
	TIME [epoch: 4.76 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841076667930963		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.7841076667930963 | validation: 0.43633764075631687]
	TIME [epoch: 4.76 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889226666290614		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.7889226666290614 | validation: 0.42845888773397567]
	TIME [epoch: 4.75 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877551920937225		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.7877551920937225 | validation: 0.42647505989895446]
	TIME [epoch: 4.76 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861430251955163		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.7861430251955163 | validation: 0.42421155242212]
	TIME [epoch: 4.75 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795667705138428		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.7795667705138428 | validation: 0.4320976511444604]
	TIME [epoch: 4.76 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824379832313163		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.7824379832313163 | validation: 0.4256501017052342]
	TIME [epoch: 4.76 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866306900002595		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.7866306900002595 | validation: 0.42178767398681744]
	TIME [epoch: 4.76 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839622277350339		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.7839622277350339 | validation: 0.4256617298507866]
	TIME [epoch: 4.76 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834400608385121		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.7834400608385121 | validation: 0.4341453303420526]
	TIME [epoch: 4.75 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831218026577469		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.7831218026577469 | validation: 0.428708555841863]
	TIME [epoch: 4.76 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848218042496571		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.7848218042496571 | validation: 0.4171652532244351]
	TIME [epoch: 4.77 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783716556443893		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.783716556443893 | validation: 0.4249538685131226]
	TIME [epoch: 4.76 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868959463266486		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.7868959463266486 | validation: 0.4260538507588377]
	TIME [epoch: 4.75 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831062610420014		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.7831062610420014 | validation: 0.4268402891525181]
	TIME [epoch: 4.77 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872037242639754		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.7872037242639754 | validation: 0.42885068144403554]
	TIME [epoch: 4.76 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78285184774517		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.78285184774517 | validation: 0.4282694219886225]
	TIME [epoch: 4.76 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867640891299444		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.7867640891299444 | validation: 0.42697585828691303]
	TIME [epoch: 4.76 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855920446999021		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.7855920446999021 | validation: 0.43351590070973584]
	TIME [epoch: 4.77 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820263244308032		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.7820263244308032 | validation: 0.43067735325603956]
	TIME [epoch: 4.77 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856592157775961		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.7856592157775961 | validation: 0.42039682580142324]
	TIME [epoch: 4.76 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877203787375371		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.7877203787375371 | validation: 0.4205154817320593]
	TIME [epoch: 4.77 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884697893375475		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.7884697893375475 | validation: 0.4229344142450357]
	TIME [epoch: 4.77 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860253148161173		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.7860253148161173 | validation: 0.4257504491169572]
	TIME [epoch: 4.76 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857222032600857		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.7857222032600857 | validation: 0.4324797142507135]
	TIME [epoch: 4.76 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871370883546678		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.7871370883546678 | validation: 0.42490421998920525]
	TIME [epoch: 4.76 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861341076596574		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.7861341076596574 | validation: 0.42042548725596307]
	TIME [epoch: 4.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837089201995644		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.7837089201995644 | validation: 0.42501769932504846]
	TIME [epoch: 4.76 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829381139259631		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.7829381139259631 | validation: 0.4334527156675302]
	TIME [epoch: 4.76 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861392186520813		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.7861392186520813 | validation: 0.42650653111038084]
	TIME [epoch: 4.76 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864478685416976		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.7864478685416976 | validation: 0.42181266751577584]
	TIME [epoch: 4.77 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831922568913455		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.7831922568913455 | validation: 0.4294457004068873]
	TIME [epoch: 4.77 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812980319801435		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.7812980319801435 | validation: 0.41663352986244073]
	TIME [epoch: 4.77 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862480252149435		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.7862480252149435 | validation: 0.4385365804787673]
	TIME [epoch: 4.76 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839026064561272		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.7839026064561272 | validation: 0.42721583331525476]
	TIME [epoch: 4.75 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863707683795564		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.7863707683795564 | validation: 0.42722013022874994]
	TIME [epoch: 4.76 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841562417168065		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.7841562417168065 | validation: 0.4286088668710399]
	TIME [epoch: 4.77 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859668011662087		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.7859668011662087 | validation: 0.4179090796339482]
	TIME [epoch: 4.76 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874066714671346		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.7874066714671346 | validation: 0.4251462759487643]
	TIME [epoch: 4.77 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787260011591527		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.787260011591527 | validation: 0.423883895331593]
	TIME [epoch: 4.77 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7851390158937772		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.7851390158937772 | validation: 0.4347666586629892]
	TIME [epoch: 4.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865986308805234		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.7865986308805234 | validation: 0.4298948946368601]
	TIME [epoch: 4.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_2_v_mmd3_1407.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4406.534 seconds.
