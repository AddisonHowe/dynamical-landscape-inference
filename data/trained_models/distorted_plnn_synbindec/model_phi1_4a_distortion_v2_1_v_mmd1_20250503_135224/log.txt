Args:
Namespace(name='model_phi1_4a_distortion_v2_1_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3889468895

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.727033210670511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.727033210670511 | validation: 6.2513609423554986]
	TIME [epoch: 167 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.401615917622256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.401615917622256 | validation: 6.52656456126152]
	TIME [epoch: 0.794 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.904624340418998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.904624340418998 | validation: 6.092921678915349]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.223764728515249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.223764728515249 | validation: 5.883586244163636]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.007583179420619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.007583179420619 | validation: 5.759635303009851]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.834836939766976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.834836939766976 | validation: 5.719491895302085]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5697644055574225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5697644055574225 | validation: 5.801612469142821]
	TIME [epoch: 0.706 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.738173547155376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.738173547155376 | validation: 4.522494303403059]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.869030348293564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.869030348293564 | validation: 5.1731905621990295]
	TIME [epoch: 0.703 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.55903614315094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.55903614315094 | validation: 5.173502427624024]
	TIME [epoch: 0.703 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.320185245536586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.320185245536586 | validation: 5.348631048884329]
	TIME [epoch: 0.701 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.783059815248302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.783059815248302 | validation: 5.377165084000246]
	TIME [epoch: 0.701 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.962697020817759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.962697020817759 | validation: 5.166732146538483]
	TIME [epoch: 0.701 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.035314010824554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.035314010824554 | validation: 5.098646414848475]
	TIME [epoch: 0.704 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8219657385230597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8219657385230597 | validation: 4.890326111181311]
	TIME [epoch: 0.702 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8063229990342444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8063229990342444 | validation: 5.056746466932506]
	TIME [epoch: 0.7 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5138506505691782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5138506505691782 | validation: 4.7028719359087034]
	TIME [epoch: 0.699 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.526294515725896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.526294515725896 | validation: 4.836392931403719]
	TIME [epoch: 0.701 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.442032094731973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.442032094731973 | validation: 4.135139063684988]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.525637183199634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.525637183199634 | validation: 4.264254915446376]
	TIME [epoch: 0.708 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2283347929589215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2283347929589215 | validation: 4.989625185035724]
	TIME [epoch: 0.709 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7194976242882944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7194976242882944 | validation: 3.781785760835318]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2443082386712945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2443082386712945 | validation: 3.8589865177225344]
	TIME [epoch: 0.706 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0922726947689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0922726947689 | validation: 4.573713753634748]
	TIME [epoch: 0.704 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.204024461423702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.204024461423702 | validation: 3.497815162327684]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.12608007009624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.12608007009624 | validation: 3.466694252573676]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9714766822168133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9714766822168133 | validation: 4.216936384643232]
	TIME [epoch: 0.708 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9321111669754725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9321111669754725 | validation: 3.2775064229783446]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.891453021461074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.891453021461074 | validation: 3.361093145035153]
	TIME [epoch: 0.705 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6952951142667447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6952951142667447 | validation: 3.9900460147881462]
	TIME [epoch: 0.704 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.871758660245936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.871758660245936 | validation: 3.0917399174395186]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8691155547820917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8691155547820917 | validation: 3.1986256353949702]
	TIME [epoch: 0.705 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.677599132166224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.677599132166224 | validation: 3.6876893052366926]
	TIME [epoch: 0.703 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.679924721128897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.679924721128897 | validation: 2.933188994785041]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6475659250894825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6475659250894825 | validation: 3.0302481992692094]
	TIME [epoch: 0.704 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.427068285888114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.427068285888114 | validation: 3.4206825174713718]
	TIME [epoch: 0.702 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5229581637884193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5229581637884193 | validation: 2.7376584854694226]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5231801706168198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5231801706168198 | validation: 2.8545861261439]
	TIME [epoch: 0.705 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3205795697904965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3205795697904965 | validation: 3.205027113751304]
	TIME [epoch: 0.708 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4222104803712496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4222104803712496 | validation: 2.789229239703607]
	TIME [epoch: 0.705 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5038985440270403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5038985440270403 | validation: 3.2786405639772784]
	TIME [epoch: 0.702 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3226585668157878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3226585668157878 | validation: 2.697051836601896]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1204688029218635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1204688029218635 | validation: 2.6181037036527264]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0812961014891083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0812961014891083 | validation: 2.6956659578528854]
	TIME [epoch: 0.704 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0033798908846556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0033798908846556 | validation: 2.5107586344639854]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9540320480831082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9540320480831082 | validation: 2.6757588273125794]
	TIME [epoch: 0.705 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9177088775404993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9177088775404993 | validation: 2.55496078034234]
	TIME [epoch: 0.702 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1602152561382177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1602152561382177 | validation: 3.145347185485501]
	TIME [epoch: 0.705 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3570875741536144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3570875741536144 | validation: 2.2945776230597854]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8455913652802043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8455913652802043 | validation: 2.343853332257584]
	TIME [epoch: 0.705 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.778950823854261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.778950823854261 | validation: 2.5067937363999193]
	TIME [epoch: 0.704 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7808715719714445		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.7808715719714445 | validation: 2.1076821284450724]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7169947052518648		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.7169947052518648 | validation: 2.184943519991387]
	TIME [epoch: 0.701 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5707235122294778		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.5707235122294778 | validation: 2.0982243534851968]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5352307361757704		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.5352307361757704 | validation: 2.071494144918359]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4825747864936998		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4825747864936998 | validation: 1.9403443142258752]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4506221116448783		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.4506221116448783 | validation: 2.354616736612674]
	TIME [epoch: 0.705 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5592782481683063		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.5592782481683063 | validation: 2.222959257454448]
	TIME [epoch: 0.704 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2176264938567143		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.2176264938567143 | validation: 2.2949065418644947]
	TIME [epoch: 0.704 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5608572478202967		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.5608572478202967 | validation: 1.8526337779376334]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3982677779024715		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.3982677779024715 | validation: 1.849780613500318]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3747536322327691		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.3747536322327691 | validation: 1.941789322258674]
	TIME [epoch: 0.706 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364005541649659		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.364005541649659 | validation: 1.7273785929573244]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3527428512109265		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.3527428512109265 | validation: 1.8794313001157552]
	TIME [epoch: 0.705 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3305072789762087		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.3305072789762087 | validation: 1.6797884484146903]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3514406244287687		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.3514406244287687 | validation: 1.8741826632236689]
	TIME [epoch: 0.706 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.375154299399736		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.375154299399736 | validation: 1.574024209919746]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.43907971228427		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.43907971228427 | validation: 2.058189061130571]
	TIME [epoch: 0.704 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4243670915825224		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.4243670915825224 | validation: 1.5639139184779531]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3866216621267318		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.3866216621267318 | validation: 1.7589214323440325]
	TIME [epoch: 0.704 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2794643797753957		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.2794643797753957 | validation: 1.5630956588492344]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.248447438188671		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.248447438188671 | validation: 1.5701421512830747]
	TIME [epoch: 0.702 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2335434754309482		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2335434754309482 | validation: 1.6246810017227133]
	TIME [epoch: 0.699 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2374944580540181		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2374944580540181 | validation: 1.507666327767799]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2343968332746624		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2343968332746624 | validation: 1.7130898381493038]
	TIME [epoch: 0.706 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2560807624341543		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2560807624341543 | validation: 1.4752247660057483]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3589447520336606		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.3589447520336606 | validation: 1.9015074634091171]
	TIME [epoch: 0.708 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3848966007014076		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.3848966007014076 | validation: 1.4346995950665276]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3435749928997194		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.3435749928997194 | validation: 1.5826365718487754]
	TIME [epoch: 0.705 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2812446683381677		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2812446683381677 | validation: 1.6446026100052273]
	TIME [epoch: 0.701 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283818474508488		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.283818474508488 | validation: 1.5145489489879331]
	TIME [epoch: 0.702 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2821664435899698		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2821664435899698 | validation: 1.4805119265871207]
	TIME [epoch: 0.702 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2119926217683363		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.2119926217683363 | validation: 1.5356712873904792]
	TIME [epoch: 0.706 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.218881889977699		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.218881889977699 | validation: 1.4455147830442938]
	TIME [epoch: 0.705 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205186292570378		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.205186292570378 | validation: 1.4908559678144633]
	TIME [epoch: 0.704 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2094537854675327		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2094537854675327 | validation: 1.4459145792624275]
	TIME [epoch: 0.703 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.200606078956088		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.200606078956088 | validation: 1.4850102093083373]
	TIME [epoch: 0.704 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969607159481965		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1969607159481965 | validation: 1.497022392200138]
	TIME [epoch: 0.703 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.196868726977683		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.196868726977683 | validation: 1.4272664078518746]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911341546656373		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.1911341546656373 | validation: 1.5865318143008438]
	TIME [epoch: 0.707 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208861178058491		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.208861178058491 | validation: 1.4673025205885393]
	TIME [epoch: 0.705 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3140442197879554		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.3140442197879554 | validation: 2.083327134712803]
	TIME [epoch: 0.707 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4704609264265083		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.4704609264265083 | validation: 1.4442149832463116]
	TIME [epoch: 0.705 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3947116884156867		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3947116884156867 | validation: 1.532058557628834]
	TIME [epoch: 0.706 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3126232608616681		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.3126232608616681 | validation: 1.5267504138574186]
	TIME [epoch: 0.705 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2126853299071476		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2126853299071476 | validation: 1.4909199936884119]
	TIME [epoch: 0.704 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.221428494344895		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.221428494344895 | validation: 1.4722604198872156]
	TIME [epoch: 0.703 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2262538452166036		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2262538452166036 | validation: 1.4527530711942578]
	TIME [epoch: 0.705 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210854724563976		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.210854724563976 | validation: 1.5447443149259137]
	TIME [epoch: 0.703 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1977155607461638		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.1977155607461638 | validation: 1.4575369476184972]
	TIME [epoch: 0.704 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190292180072621		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.190292180072621 | validation: 1.4910532155490916]
	TIME [epoch: 0.707 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.198656476050158		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.198656476050158 | validation: 1.5061305017617916]
	TIME [epoch: 0.704 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.203630202990552		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.203630202990552 | validation: 1.4360099348959279]
	TIME [epoch: 0.704 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207693855507337		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.207693855507337 | validation: 1.5141661125097137]
	TIME [epoch: 0.705 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217201261599566		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.217201261599566 | validation: 1.4760068166083347]
	TIME [epoch: 0.705 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2188902190481843		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2188902190481843 | validation: 1.6225504213083601]
	TIME [epoch: 0.704 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2709650286204315		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.2709650286204315 | validation: 1.4384730637370577]
	TIME [epoch: 0.706 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2939204136050502		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2939204136050502 | validation: 1.6673215861458446]
	TIME [epoch: 0.708 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2668321969864076		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.2668321969864076 | validation: 1.42148232445404]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2287569934140306		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.2287569934140306 | validation: 1.5215153611594174]
	TIME [epoch: 0.706 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2045950223620614		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.2045950223620614 | validation: 1.4750231755657655]
	TIME [epoch: 0.704 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1979066969623016		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1979066969623016 | validation: 1.4627137202483618]
	TIME [epoch: 0.704 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.197010328678402		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.197010328678402 | validation: 1.495755546347909]
	TIME [epoch: 0.703 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.185330438487903		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.185330438487903 | validation: 1.4476489032512116]
	TIME [epoch: 0.703 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1776818989984077		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1776818989984077 | validation: 1.5551346584227155]
	TIME [epoch: 0.706 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194114556735708		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.194114556735708 | validation: 1.4107481145893805]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2305865570166672		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.2305865570166672 | validation: 1.7085537120650487]
	TIME [epoch: 0.705 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2688417728854904		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2688417728854904 | validation: 1.3789992582500645]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3099203025092494		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.3099203025092494 | validation: 1.5056828159386673]
	TIME [epoch: 0.704 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969185291488158		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1969185291488158 | validation: 1.5288453788751788]
	TIME [epoch: 0.702 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1938526530574334		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1938526530574334 | validation: 1.417328076262052]
	TIME [epoch: 0.702 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.198514388462036		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.198514388462036 | validation: 1.4692286800378407]
	TIME [epoch: 0.704 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1957106403356885		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.1957106403356885 | validation: 1.5126898526422752]
	TIME [epoch: 0.704 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205611530693418		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.205611530693418 | validation: 1.4485617453135728]
	TIME [epoch: 0.702 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.243141393041957		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.243141393041957 | validation: 1.5467243558260277]
	TIME [epoch: 0.701 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365558150612		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2365558150612 | validation: 1.5163069117751051]
	TIME [epoch: 0.7 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2022946769910825		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.2022946769910825 | validation: 1.4715126928057025]
	TIME [epoch: 0.706 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1828219583991746		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1828219583991746 | validation: 1.5022768251062968]
	TIME [epoch: 0.7 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1876628759096386		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1876628759096386 | validation: 1.4146078550587955]
	TIME [epoch: 0.702 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1959496308262076		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1959496308262076 | validation: 1.6482380071384508]
	TIME [epoch: 0.702 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2195062526329257		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.2195062526329257 | validation: 1.421356098476269]
	TIME [epoch: 0.702 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3034963652491929		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3034963652491929 | validation: 1.5521865847459813]
	TIME [epoch: 0.702 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2001816489239827		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.2001816489239827 | validation: 1.46409328303842]
	TIME [epoch: 0.702 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1875461339635844		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1875461339635844 | validation: 1.4124704432799202]
	TIME [epoch: 0.701 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2020375748156449		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.2020375748156449 | validation: 1.5151991723337774]
	TIME [epoch: 0.7 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192057034313228		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.192057034313228 | validation: 1.451253526721466]
	TIME [epoch: 0.7 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1925864758159055		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.1925864758159055 | validation: 1.4885722515552606]
	TIME [epoch: 0.702 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1811575662546274		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1811575662546274 | validation: 1.492485106241738]
	TIME [epoch: 0.701 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194511868443095		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.194511868443095 | validation: 1.4623286167412648]
	TIME [epoch: 0.7 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1934326302260925		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1934326302260925 | validation: 1.556728298499045]
	TIME [epoch: 0.701 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208046999535242		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.208046999535242 | validation: 1.4079212253849014]
	TIME [epoch: 0.703 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2556448040774622		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2556448040774622 | validation: 1.7348637833070553]
	TIME [epoch: 0.7 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2675912387511428		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.2675912387511428 | validation: 1.3775078671004888]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2509025637258162		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.2509025637258162 | validation: 1.4517083155628177]
	TIME [epoch: 0.705 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164934211762648		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.164934211762648 | validation: 1.6275526115026873]
	TIME [epoch: 0.703 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2115290371676815		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2115290371676815 | validation: 1.3720953205076434]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215860906637118		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.215860906637118 | validation: 1.4553293653248485]
	TIME [epoch: 0.707 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1698629856852154		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1698629856852154 | validation: 1.5243971533110063]
	TIME [epoch: 0.706 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1855345520107352		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1855345520107352 | validation: 1.3965514088125797]
	TIME [epoch: 0.706 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1731467889661442		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1731467889661442 | validation: 1.4525780900354048]
	TIME [epoch: 0.705 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1653143375481145		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.1653143375481145 | validation: 1.4734318559242863]
	TIME [epoch: 0.705 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1715709154710967		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1715709154710967 | validation: 1.4181229626801237]
	TIME [epoch: 0.704 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1766351605419068		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.1766351605419068 | validation: 1.530742186250122]
	TIME [epoch: 0.704 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1851232367785374		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1851232367785374 | validation: 1.475272927487678]
	TIME [epoch: 0.707 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2190631918012362		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.2190631918012362 | validation: 1.50426079349026]
	TIME [epoch: 0.708 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1872668019312018		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1872668019312018 | validation: 1.4852104356374929]
	TIME [epoch: 0.705 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1707681277425832		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.1707681277425832 | validation: 1.4045176952404108]
	TIME [epoch: 0.704 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1699835471423918		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1699835471423918 | validation: 1.554060973471501]
	TIME [epoch: 0.705 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1735080260126005		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1735080260126005 | validation: 1.3804195882263945]
	TIME [epoch: 0.704 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2783084730756213		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2783084730756213 | validation: 1.6915858688315055]
	TIME [epoch: 0.704 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2531001043153391		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.2531001043153391 | validation: 1.3614586359949372]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2207612435459132		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.2207612435459132 | validation: 1.4181809758919497]
	TIME [epoch: 0.704 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155065617074185		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.155065617074185 | validation: 1.544151974776662]
	TIME [epoch: 0.7 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1725107020966794		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1725107020966794 | validation: 1.3959284583721985]
	TIME [epoch: 0.698 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1674493491203226		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1674493491203226 | validation: 1.4368305335355203]
	TIME [epoch: 0.699 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1650781986823533		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1650781986823533 | validation: 1.5375438755423594]
	TIME [epoch: 0.698 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1577300373301227		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.1577300373301227 | validation: 1.4414577421488837]
	TIME [epoch: 0.704 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1596411836083294		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1596411836083294 | validation: 1.458491400840279]
	TIME [epoch: 0.697 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1619870695685195		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.1619870695685195 | validation: 1.4266548890015827]
	TIME [epoch: 0.699 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1597139108240868		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1597139108240868 | validation: 1.5493913427107977]
	TIME [epoch: 0.698 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1817048938267036		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1817048938267036 | validation: 1.407669675357722]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2220844285616659		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.2220844285616659 | validation: 1.6171214432350167]
	TIME [epoch: 0.697 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2015452155172515		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.2015452155172515 | validation: 1.3412343811943932]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2028030085342032		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.2028030085342032 | validation: 1.5747061335336188]
	TIME [epoch: 0.703 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2027996459217816		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2027996459217816 | validation: 1.3904904793499917]
	TIME [epoch: 0.703 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1900824236001215		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.1900824236001215 | validation: 1.4641621644886422]
	TIME [epoch: 0.703 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1558587927809179		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.1558587927809179 | validation: 1.4368487053760683]
	TIME [epoch: 0.701 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1421753576155873		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1421753576155873 | validation: 1.425895526655016]
	TIME [epoch: 0.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399103669359332		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.1399103669359332 | validation: 1.4727073867390947]
	TIME [epoch: 0.701 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1407124170104288		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1407124170104288 | validation: 1.3812509097103463]
	TIME [epoch: 0.702 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1521382326049119		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1521382326049119 | validation: 1.6979131510147647]
	TIME [epoch: 0.701 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2150327336905113		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.2150327336905113 | validation: 1.3544158437681912]
	TIME [epoch: 0.701 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3000064860762217		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3000064860762217 | validation: 1.3947335016434634]
	TIME [epoch: 0.702 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140995481447778		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.140995481447778 | validation: 1.6371590235606903]
	TIME [epoch: 0.702 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.186865378543752		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.186865378543752 | validation: 1.3464119242818327]
	TIME [epoch: 0.705 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770756782914782		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1770756782914782 | validation: 1.3605922037285951]
	TIME [epoch: 0.705 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132328443075057		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.132328443075057 | validation: 1.5547245905247504]
	TIME [epoch: 0.706 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1828804729008595		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1828804729008595 | validation: 1.3711872504467995]
	TIME [epoch: 0.704 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1571312396979254		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1571312396979254 | validation: 1.3793032956415507]
	TIME [epoch: 0.705 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1418114358434006		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1418114358434006 | validation: 1.4819563917131497]
	TIME [epoch: 0.703 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1518261293531107		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.1518261293531107 | validation: 1.3816410341559473]
	TIME [epoch: 0.704 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1353606360286106		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1353606360286106 | validation: 1.4266765180013723]
	TIME [epoch: 0.705 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132255128818228		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.132255128818228 | validation: 1.396864592618011]
	TIME [epoch: 0.704 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1418756700210693		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1418756700210693 | validation: 1.4029748383386094]
	TIME [epoch: 0.703 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1473312255149133		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.1473312255149133 | validation: 1.4623645665771052]
	TIME [epoch: 0.704 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1354371195454465		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1354371195454465 | validation: 1.3185844433676264]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1701665985728442		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1701665985728442 | validation: 1.6883379228076436]
	TIME [epoch: 0.706 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2176417864368327		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2176417864368327 | validation: 1.2966059686335534]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2966712552211686		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.2966712552211686 | validation: 1.3704928815394093]
	TIME [epoch: 0.708 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161951387484401		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1161951387484401 | validation: 1.5866038310802033]
	TIME [epoch: 0.705 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177862279442454		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.177862279442454 | validation: 1.3288443095190436]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.166582503537931		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.166582503537931 | validation: 1.3703439457923698]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1193213868219933		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1193213868219933 | validation: 1.5086153728246576]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15407432444764		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.15407432444764 | validation: 1.3173200043439302]
	TIME [epoch: 1.38 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131564674773777		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.131564674773777 | validation: 1.4016394709997302]
	TIME [epoch: 1.37 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1049509798891228		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.1049509798891228 | validation: 1.4373940927499955]
	TIME [epoch: 1.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1132434050418547		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1132434050418547 | validation: 1.3525240518488222]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111999252135974		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.111999252135974 | validation: 1.4779074159644623]
	TIME [epoch: 1.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1261441386426412		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.1261441386426412 | validation: 1.3318475173284459]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1228348878914245		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1228348878914245 | validation: 1.4821116930642835]
	TIME [epoch: 1.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11709264830068		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.11709264830068 | validation: 1.3099807494480378]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1669004661818687		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.1669004661818687 | validation: 1.6502553022364368]
	TIME [epoch: 1.38 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1963341256420637		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1963341256420637 | validation: 1.264903697424675]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.227581317271989		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.227581317271989 | validation: 1.3225345865841682]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0998672157082967		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.0998672157082967 | validation: 1.5014774374503488]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1259476056228315		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.1259476056228315 | validation: 1.2725679613701786]
	TIME [epoch: 1.38 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1400279465283414		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1400279465283414 | validation: 1.3411833740320371]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102778013708002		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.102778013708002 | validation: 1.4308641556909]
	TIME [epoch: 1.37 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0921057680190516		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0921057680190516 | validation: 1.3008070075325509]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1016253888005414		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1016253888005414 | validation: 1.4698880971723738]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1012528338779783		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1012528338779783 | validation: 1.2656346454768757]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161239824772144		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.1161239824772144 | validation: 1.527339162076963]
	TIME [epoch: 1.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1188456989102726		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1188456989102726 | validation: 1.2431554228562551]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1197606445822073		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.1197606445822073 | validation: 1.4782315516974223]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0978031560504975		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.0978031560504975 | validation: 1.2286057389002942]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1353245787865054		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1353245787865054 | validation: 1.4767784020875425]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1109100516297439		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1109100516297439 | validation: 1.2270107764020552]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.094307328124925		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.094307328124925 | validation: 1.3751884485303674]
	TIME [epoch: 1.37 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.074477785536735		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.074477785536735 | validation: 1.2403229593169192]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0778086163636982		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0778086163636982 | validation: 1.467071265946041]
	TIME [epoch: 1.37 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0987470209965084		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.0987470209965084 | validation: 1.2466912944554434]
	TIME [epoch: 1.37 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1953751296944612		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1953751296944612 | validation: 1.4448614743658477]
	TIME [epoch: 1.37 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0946512632740493		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.0946512632740493 | validation: 1.1783449751630781]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0747642355292713		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.0747642355292713 | validation: 1.3137232259433236]
	TIME [epoch: 1.37 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0591102508390484		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.0591102508390484 | validation: 1.2136887191466217]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0529527242535204		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.0529527242535204 | validation: 1.3429530646824035]
	TIME [epoch: 1.37 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0604113962523247		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.0604113962523247 | validation: 1.1841768367420697]
	TIME [epoch: 1.37 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1008337335297627		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1008337335297627 | validation: 1.4872320670575607]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123327290425775		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.123327290425775 | validation: 1.1611389995765777]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1840030025197033		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1840030025197033 | validation: 1.2440757583545214]
	TIME [epoch: 1.37 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034600171704624		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.034600171704624 | validation: 1.294928106827688]
	TIME [epoch: 1.37 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0404586349916654		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.0404586349916654 | validation: 1.1434464061258856]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0695462734845682		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.0695462734845682 | validation: 1.304597628207821]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0409114943593794		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.0409114943593794 | validation: 1.113177647691298]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0345049168045675		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.0345049168045675 | validation: 1.2758745286598419]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0244112375113186		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.0244112375113186 | validation: 1.115123758153376]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0420951384471266		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.0420951384471266 | validation: 1.3854368269322697]
	TIME [epoch: 1.37 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0813120688740454		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.0813120688740454 | validation: 1.1407148961524645]
	TIME [epoch: 1.37 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1954251913258611		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.1954251913258611 | validation: 1.2738620020731923]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0085181216833043		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.0085181216833043 | validation: 1.2477629840069013]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0044329767334406		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.0044329767334406 | validation: 1.0756446047840638]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0308616543671858		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.0308616543671858 | validation: 1.2649783967099453]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0295493241517248		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.0295493241517248 | validation: 1.0875729647367283]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0579109223516456		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.0579109223516456 | validation: 1.2266863999313935]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0209289165793012		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.0209289165793012 | validation: 1.0514975119559555]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0302770532965206		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.0302770532965206 | validation: 1.2665211618385463]
	TIME [epoch: 1.37 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0169403011806817		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.0169403011806817 | validation: 1.061965015798974]
	TIME [epoch: 1.37 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053801873355942		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.053801873355942 | validation: 1.2743677209616662]
	TIME [epoch: 1.37 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0315574503891223		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.0315574503891223 | validation: 1.0304181726430774]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021717203955329		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.021717203955329 | validation: 1.2221199744825506]
	TIME [epoch: 1.37 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9860918511566813		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.9860918511566813 | validation: 1.0106848262397765]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939333355612173		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.9939333355612173 | validation: 1.1975355341477731]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939376538673423		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.9939376538673423 | validation: 1.018174482024605]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0324020188247476		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.0324020188247476 | validation: 1.1837854530070617]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835155490766981		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.9835155490766981 | validation: 0.9932689261642049]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9757588625431818		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.9757588625431818 | validation: 1.1943449737379366]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9867142872269631		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9867142872269631 | validation: 1.0092258200043218]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170399790848639		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0170399790848639 | validation: 1.2325585627073885]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0104165605265139		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0104165605265139 | validation: 0.9674740342348476]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0129967998161098		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.0129967998161098 | validation: 1.1087303619552895]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9601395198807049		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.9601395198807049 | validation: 0.978239391508806]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9509737034763185		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9509737034763185 | validation: 1.0767500737912516]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360020458362535		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.9360020458362535 | validation: 0.9521452878090693]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.955351788766057		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.955351788766057 | validation: 1.1230376512366225]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9694392085920731		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.9694392085920731 | validation: 0.9437784288389098]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0427814528596866		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0427814528596866 | validation: 1.2543387815750133]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0147413999363593		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.0147413999363593 | validation: 0.9102894678969492]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9811914232917246		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9811914232917246 | validation: 1.0178792634707285]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006726574941161		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.9006726574941161 | validation: 0.9496244476838372]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915928228735662		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8915928228735662 | validation: 0.9445841348375368]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912447547168506		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8912447547168506 | validation: 0.9256137442645687]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8950583677879632		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8950583677879632 | validation: 0.9094824610995916]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8950244304426677		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8950244304426677 | validation: 0.9160354584624703]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904448405197646		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8904448405197646 | validation: 0.881945377391447]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994941610489348		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.8994941610489348 | validation: 1.0584917701407595]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248235760488589		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.9248235760488589 | validation: 1.0330754121552481]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283900982131398		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.283900982131398 | validation: 1.203721274414647]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0100526196651125		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0100526196651125 | validation: 0.8548384645918653]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924382935861124		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.924382935861124 | validation: 0.9175331441115198]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623921426795217		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8623921426795217 | validation: 0.9359866383711989]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674133810830984		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8674133810830984 | validation: 0.8632489070872386]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920597904207804		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8920597904207804 | validation: 1.0002563063981165]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909433191990451		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.909433191990451 | validation: 0.8482541915274778]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856962075819325		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.9856962075819325 | validation: 1.0804026110373524]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9404733127064193		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.9404733127064193 | validation: 0.8352586060109126]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9543438843009706		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.9543438843009706 | validation: 0.9346855565647616]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777667748813772		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8777667748813772 | validation: 0.8408267246249386]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610351418333727		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.8610351418333727 | validation: 0.8868796748510346]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859705290679708		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.859705290679708 | validation: 0.8071646180751435]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579857918061786		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8579857918061786 | validation: 0.9175984611772248]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8796716880580651		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8796716880580651 | validation: 0.8186147171706057]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617360393387501		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.9617360393387501 | validation: 1.1017640037169014]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9804215863470845		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.9804215863470845 | validation: 0.8339514292086054]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9905803055882593		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.9905803055882593 | validation: 0.861153066529925]
	TIME [epoch: 1.38 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8435994527073971		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8435994527073971 | validation: 0.8993857987596456]
	TIME [epoch: 1.38 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417185488818376		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8417185488818376 | validation: 0.7786077172422337]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569751253919059		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8569751253919059 | validation: 0.8887463814422735]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492225279855634		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8492225279855634 | validation: 0.785663741085138]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8503791913807821		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8503791913807821 | validation: 0.872793397003574]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852425297078114		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.852425297078114 | validation: 0.7746220551696177]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744694780788345		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8744694780788345 | validation: 0.9438729131122918]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836174178086803		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8836174178086803 | validation: 0.8168249109160981]
	TIME [epoch: 1.38 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9429684462628589		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.9429684462628589 | validation: 0.9591939364769344]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8809268068481888		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8809268068481888 | validation: 0.7797634709615636]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372461229861715		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8372461229861715 | validation: 0.816149766115046]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299157620804625		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8299157620804625 | validation: 0.7686456173493321]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299607893955093		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8299607893955093 | validation: 0.8082850093759015]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8171983366348635		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.8171983366348635 | validation: 0.7294352096231449]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8396160745221792		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8396160745221792 | validation: 0.8730189700167004]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359739626305895		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.8359739626305895 | validation: 0.7204585128137437]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707965779237029		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.8707965779237029 | validation: 0.9790637480982003]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905883684418552		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.8905883684418552 | validation: 0.7878912464476704]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929675921043354		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.929675921043354 | validation: 0.8360350996994509]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8141051176260714		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8141051176260714 | validation: 0.7432732239023916]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857974465165276		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.7857974465165276 | validation: 0.7496251155140098]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866101168769615		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.7866101168769615 | validation: 0.7781342951489489]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019541611837226		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.8019541611837226 | validation: 0.7372463003953604]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211252509925536		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.8211252509925536 | validation: 0.8493222175009438]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216001382394095		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.8216001382394095 | validation: 0.7463668563375927]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465189142475816		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.8465189142475816 | validation: 0.8887902188608665]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501497854906799		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8501497854906799 | validation: 0.7240464144811334]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596233500075391		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8596233500075391 | validation: 0.7992058982044249]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984233750150566		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.7984233750150566 | validation: 0.7548242184876097]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003054657026573		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8003054657026573 | validation: 0.7954688799542995]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775453412329988		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.775453412329988 | validation: 0.7433534909921373]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740416158882977		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.7740416158882977 | validation: 0.8174391965522498]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910121518471585		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.7910121518471585 | validation: 0.7336092534590136]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176021235439445		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.8176021235439445 | validation: 0.8455212455276574]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206914927543525		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.8206914927543525 | validation: 0.6870257719184802]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408985775556602		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.8408985775556602 | validation: 0.8136304820442741]
	TIME [epoch: 1.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128103367784565		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8128103367784565 | validation: 0.7056259099596984]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994841650808872		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.7994841650808872 | validation: 0.7461497408152309]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617778520297052		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.7617778520297052 | validation: 0.7146417903474636]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531073862813784		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.7531073862813784 | validation: 0.7278095406536065]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571862331112336		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.7571862331112336 | validation: 0.7409151681376576]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680083464783657		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7680083464783657 | validation: 0.666630157979218]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923779902277339		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.7923779902277339 | validation: 0.8554558300456817]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121125520438927		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.8121125520438927 | validation: 0.7005914297823567]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331300521044827		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.8331300521044827 | validation: 0.7959636229053857]
	TIME [epoch: 1.37 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728136222866897		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.7728136222866897 | validation: 0.7027273695971172]
	TIME [epoch: 1.37 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439777373696302		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7439777373696302 | validation: 0.7082834012850587]
	TIME [epoch: 1.37 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323470996381454		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7323470996381454 | validation: 0.6864981780824023]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238791963585809		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7238791963585809 | validation: 0.6387860828128803]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282227242928703		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7282227242928703 | validation: 0.667074916350688]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313947089674961		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.7313947089674961 | validation: 0.6835499735635563]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536888724151323		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7536888724151323 | validation: 0.6684578811732381]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431443839277274		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.7431443839277274 | validation: 0.6698650873667168]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368827045592502		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.7368827045592502 | validation: 0.6922815501700748]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214962767739246		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.7214962767739246 | validation: 0.6435686464151054]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558791392436155		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7558791392436155 | validation: 0.9671828397129119]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623584286956668		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.8623584286956668 | validation: 0.7985784219694421]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9478899529417061		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9478899529417061 | validation: 0.6653438659206632]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124181301741651		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7124181301741651 | validation: 0.7367005026717716]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509642131202332		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7509642131202332 | validation: 0.6356137501691793]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683174166535645		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.7683174166535645 | validation: 0.6981197179974681]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170329038431317		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7170329038431317 | validation: 0.6402066296253346]
	TIME [epoch: 1.38 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079840418480986		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7079840418480986 | validation: 0.6566386429240683]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028915579828058		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7028915579828058 | validation: 0.7001677936648307]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099405347324543		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.7099405347324543 | validation: 0.643205802151771]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012930336630657		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.7012930336630657 | validation: 0.6845750968666571]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6970782317078361		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.6970782317078361 | validation: 0.6149496814839728]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182999261069318		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7182999261069318 | validation: 0.7150945935182009]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292235741409573		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.7292235741409573 | validation: 0.6310856728480594]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585901507228541		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.7585901507228541 | validation: 0.7658336233167033]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377794893466592		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.7377794893466592 | validation: 0.629543431915584]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194276665899441		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.7194276665899441 | validation: 0.6793125112286198]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974871358930657		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.6974871358930657 | validation: 0.625561794896119]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931697224042076		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.6931697224042076 | validation: 0.606155332418962]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6866580026305253		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.6866580026305253 | validation: 0.6060689645030695]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6740223541467077		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.6740223541467077 | validation: 0.5916691437060899]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816753171378082		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.6816753171378082 | validation: 0.6435693860628717]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871664111146955		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.6871664111146955 | validation: 0.5902432912485528]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7036772417731737		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7036772417731737 | validation: 0.8138135890401794]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556813705235028		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7556813705235028 | validation: 0.6498236765389308]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717675773325929		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.7717675773325929 | validation: 0.627299154025102]
	TIME [epoch: 1.38 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660559980152473		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.660559980152473 | validation: 0.614617813732782]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671524385535316		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.6671524385535316 | validation: 0.5521137570318331]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6707915511589783		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6707915511589783 | validation: 0.6352077094108788]
	TIME [epoch: 1.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680726602295042		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.6680726602295042 | validation: 0.5551180387841698]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672363069215588		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.672363069215588 | validation: 0.6576213552031134]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746936478766512		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.6746936478766512 | validation: 0.5773096180346244]
	TIME [epoch: 1.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757297660312854		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.6757297660312854 | validation: 0.6351181927199163]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692383084060458		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.6692383084060458 | validation: 0.6151473254958493]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653183720873409		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.6653183720873409 | validation: 0.5940175638444206]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560267797716539		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.6560267797716539 | validation: 0.5594419570916989]
	TIME [epoch: 1.38 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638872864992534		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.638872864992534 | validation: 0.5774352877742207]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361776023013593		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.6361776023013593 | validation: 0.5729241332159084]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296007224891583		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.6296007224891583 | validation: 0.5824628207666221]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315651510085523		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.6315651510085523 | validation: 0.5499447401988505]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64277865335858		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.64277865335858 | validation: 0.7309930411268049]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686123142800909		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.686123142800909 | validation: 0.6032975679968335]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592139435540896		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.7592139435540896 | validation: 0.7014169947016088]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644465875170447		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6644465875170447 | validation: 0.5636878637127816]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212878102424833		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.6212878102424833 | validation: 0.5045065355468193]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305281464633046		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.6305281464633046 | validation: 0.6120261604192557]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272835662657866		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.6272835662657866 | validation: 0.5241372458982683]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160984333555568		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.6160984333555568 | validation: 0.5566449695144238]
	TIME [epoch: 1.37 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6070330574139815		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.6070330574139815 | validation: 0.5716960099351073]
	TIME [epoch: 1.37 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077994633732334		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.6077994633732334 | validation: 0.5421784253019227]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083622459382795		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.6083622459382795 | validation: 0.5573655656797913]
	TIME [epoch: 1.37 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6080953018369886		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.6080953018369886 | validation: 0.5580845225105334]
	TIME [epoch: 1.37 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033540415672527		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.6033540415672527 | validation: 0.5298744540592095]
	TIME [epoch: 1.37 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6015946385322313		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.6015946385322313 | validation: 0.5959480428837276]
	TIME [epoch: 1.37 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602864037564099		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.602864037564099 | validation: 0.5362393107265216]
	TIME [epoch: 1.37 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322142498411771		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.6322142498411771 | validation: 0.7642804904427122]
	TIME [epoch: 1.37 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886951764883377		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.6886951764883377 | validation: 0.5055400027749187]
	TIME [epoch: 1.37 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403955939137461		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.6403955939137461 | validation: 0.5286078190818028]
	TIME [epoch: 1.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5720694478828958		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.5720694478828958 | validation: 0.5501546032702835]
	TIME [epoch: 1.37 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575901448299361		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.575901448299361 | validation: 0.518195871079869]
	TIME [epoch: 1.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5742914694426378		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.5742914694426378 | validation: 0.5654523139501342]
	TIME [epoch: 1.37 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5772256685237698		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.5772256685237698 | validation: 0.4998780043658106]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710735274504068		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5710735274504068 | validation: 0.5479936710550244]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.571757868601792		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.571757868601792 | validation: 0.5015190789066306]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5757018131103402		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.5757018131103402 | validation: 0.559438210633927]
	TIME [epoch: 1.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860132389010435		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5860132389010435 | validation: 0.5046975833394661]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867207375472836		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.5867207375472836 | validation: 0.6189097598043838]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925558227453415		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.5925558227453415 | validation: 0.45672242003343566]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981553932751915		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.5981553932751915 | validation: 0.6230235853080754]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867211933515212		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.5867211933515212 | validation: 0.4449308682447221]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5724428161297354		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.5724428161297354 | validation: 0.5119478556050047]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5451560142501597		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.5451560142501597 | validation: 0.4965254252804194]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394943292236739		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.5394943292236739 | validation: 0.4865731625640434]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5331164963033942		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5331164963033942 | validation: 0.4924253288740845]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5392688756562211		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.5392688756562211 | validation: 0.4737787352225475]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5312791935402223		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.5312791935402223 | validation: 0.5026361325052535]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5321160885232391		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5321160885232391 | validation: 0.41707618717010997]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.54080389289154		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.54080389289154 | validation: 0.5716661415327213]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559928062721904		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.559928062721904 | validation: 0.4499557101877075]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236771292475004		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.6236771292475004 | validation: 0.704403410211917]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645470436421457		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.645470436421457 | validation: 0.489365909709181]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670382346111781		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.5670382346111781 | validation: 0.4587764514581083]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191013194019045		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.5191013194019045 | validation: 0.5061082704084774]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5243989989923774		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.5243989989923774 | validation: 0.4548354550497381]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.515499995180721		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.515499995180721 | validation: 0.49095533349550513]
	TIME [epoch: 1.37 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154417005423599		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.5154417005423599 | validation: 0.4619243718611749]
	TIME [epoch: 1.37 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5090529715699093		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.5090529715699093 | validation: 0.4580760081856638]
	TIME [epoch: 1.37 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5071618429807807		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.5071618429807807 | validation: 0.4223057168966876]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022368758016795		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.5022368758016795 | validation: 0.4477762867152917]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034029167604952		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.5034029167604952 | validation: 0.3909612195716903]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5058868485409065		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.5058868485409065 | validation: 0.5191368153669453]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185619126455047		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5185619126455047 | validation: 0.41400745105496456]
	TIME [epoch: 1.38 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6024226110299075		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.6024226110299075 | validation: 0.704819964366935]
	TIME [epoch: 1.38 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260950421655096		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.6260950421655096 | validation: 0.445158572658332]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193512801712405		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5193512801712405 | validation: 0.4091826066941702]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4925765336139004		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.4925765336139004 | validation: 0.5227727905723059]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112342663871904		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.5112342663871904 | validation: 0.3904049294995892]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5012776177868289		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.5012776177868289 | validation: 0.47051686796402276]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4861715489054224		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.4861715489054224 | validation: 0.41124238509754785]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853688932546841		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4853688932546841 | validation: 0.41797465446927035]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4859204540415523		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.4859204540415523 | validation: 0.4307933359863957]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47821353729772836		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.47821353729772836 | validation: 0.4043246501901111]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4790227650068531		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.4790227650068531 | validation: 0.456951893950437]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4794747931428548		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.4794747931428548 | validation: 0.3611203834568968]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5037154084759485		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.5037154084759485 | validation: 0.5634782522566039]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5490569481275124		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.5490569481275124 | validation: 0.37000237813528014]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5578210995160112		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.5578210995160112 | validation: 0.5107416440694286]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5152082250637052		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.5152082250637052 | validation: 0.4783690533065286]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5073769496580505		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.5073769496580505 | validation: 0.3914361072877977]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4795054165521853		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.4795054165521853 | validation: 0.41013495478496803]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4643201311220719		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.4643201311220719 | validation: 0.40016735032876327]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46675013834345175		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.46675013834345175 | validation: 0.4064374038891887]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4743304719512959		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.4743304719512959 | validation: 0.38395786508053487]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4678714909568549		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.4678714909568549 | validation: 0.40912648944553975]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46940478025252197		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.46940478025252197 | validation: 0.36626626878285423]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48483031803892174		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.48483031803892174 | validation: 0.5366375066280679]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5103618926751823		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.5103618926751823 | validation: 0.3775312578322127]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5411457629045389		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.5411457629045389 | validation: 0.5204044490076797]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49514382707622795		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.49514382707622795 | validation: 0.3698567586363941]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560220291892941		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.4560220291892941 | validation: 0.3903092825044216]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.452385696189027		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.452385696189027 | validation: 0.4262031468661765]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.458343227792695		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.458343227792695 | validation: 0.3619280351075772]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4689032811917568		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.4689032811917568 | validation: 0.44214529987579304]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47603605671104016		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.47603605671104016 | validation: 0.3439962088536124]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4660166162814726		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.4660166162814726 | validation: 0.4746640592746536]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47401574146670955		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.47401574146670955 | validation: 0.3393998033594824]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4772020017626958		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4772020017626958 | validation: 0.4727508972293712]
	TIME [epoch: 1.38 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756515883087553		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.4756515883087553 | validation: 0.35714351140281086]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4621970955448576		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.4621970955448576 | validation: 0.4205503448412635]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45408218746173445		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.45408218746173445 | validation: 0.3808018021291707]
	TIME [epoch: 1.38 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45311552369082586		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.45311552369082586 | validation: 0.39205454451121513]
	TIME [epoch: 1.38 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47358880788150887		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.47358880788150887 | validation: 0.4367751693382264]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47297846632211454		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.47297846632211454 | validation: 0.3611584143065978]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544925854717593		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.4544925854717593 | validation: 0.3592298486991926]
	TIME [epoch: 1.38 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44192505922491165		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.44192505922491165 | validation: 0.3900996700864108]
	TIME [epoch: 1.38 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43790944156834516		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.43790944156834516 | validation: 0.35652841825164105]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4424248847001617		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.4424248847001617 | validation: 0.4108012183586248]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44304044357022526		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.44304044357022526 | validation: 0.30811942108008017]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4672758989088159		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.4672758989088159 | validation: 0.5839682153801012]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5283228418279152		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.5283228418279152 | validation: 0.33881424951786077]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5437552460494277		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.5437552460494277 | validation: 0.4102041108314568]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4582866141887836		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4582866141887836 | validation: 0.4375780135811848]
	TIME [epoch: 175 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4427875196002692		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.4427875196002692 | validation: 0.3178433734785278]
	TIME [epoch: 2.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45872297461742506		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.45872297461742506 | validation: 0.4095887738636062]
	TIME [epoch: 2.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44113748185086166		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.44113748185086166 | validation: 0.34558310981167994]
	TIME [epoch: 2.73 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43879253202400775		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.43879253202400775 | validation: 0.35316521893392183]
	TIME [epoch: 2.72 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362025639589502		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.4362025639589502 | validation: 0.37958067004009194]
	TIME [epoch: 2.72 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4344930395559047		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.4344930395559047 | validation: 0.32161828857132435]
	TIME [epoch: 2.73 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43484417578363593		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.43484417578363593 | validation: 0.3855455536348164]
	TIME [epoch: 2.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42889250746618923		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.42889250746618923 | validation: 0.3342193222738689]
	TIME [epoch: 2.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4279129602395731		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.4279129602395731 | validation: 0.38560454646323034]
	TIME [epoch: 2.72 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43486381745552377		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.43486381745552377 | validation: 0.30919644004747493]
	TIME [epoch: 2.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4500377404184134		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.4500377404184134 | validation: 0.4717963151576787]
	TIME [epoch: 2.72 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607393163699517		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.4607393163699517 | validation: 0.34453706624342145]
	TIME [epoch: 2.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889641566971517		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.4889641566971517 | validation: 0.45878696771486105]
	TIME [epoch: 2.72 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47213988443899124		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.47213988443899124 | validation: 0.3547010045792582]
	TIME [epoch: 2.72 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4379854809912582		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.4379854809912582 | validation: 0.3306105917178232]
	TIME [epoch: 2.73 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255753739364068		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.4255753739364068 | validation: 0.38885932404043055]
	TIME [epoch: 2.73 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43138381331260034		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.43138381331260034 | validation: 0.3234090578504637]
	TIME [epoch: 2.73 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332024324219891		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.4332024324219891 | validation: 0.39231860578601624]
	TIME [epoch: 2.72 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4236320346698036		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4236320346698036 | validation: 0.3164859662875114]
	TIME [epoch: 2.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42682957923700654		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.42682957923700654 | validation: 0.38015266668006026]
	TIME [epoch: 2.72 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43007271845562256		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.43007271845562256 | validation: 0.2907286174816776]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43373224619071127		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.43373224619071127 | validation: 0.42436098447205484]
	TIME [epoch: 2.72 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44457435810440304		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.44457435810440304 | validation: 0.29170784196817223]
	TIME [epoch: 2.72 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44676033916905034		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.44676033916905034 | validation: 0.43100787065149393]
	TIME [epoch: 2.73 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44456616076043454		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.44456616076043454 | validation: 0.3074950510264581]
	TIME [epoch: 2.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.426726381474337		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.426726381474337 | validation: 0.37248953347198466]
	TIME [epoch: 2.72 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4250701678707587		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4250701678707587 | validation: 0.3779120521953595]
	TIME [epoch: 2.73 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43588791398809035		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.43588791398809035 | validation: 0.3513214270107786]
	TIME [epoch: 2.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4303762358235889		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.4303762358235889 | validation: 0.3544720450000022]
	TIME [epoch: 2.72 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42427441138336475		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.42427441138336475 | validation: 0.3429555281267749]
	TIME [epoch: 2.73 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41279235781401263		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.41279235781401263 | validation: 0.3041079300234826]
	TIME [epoch: 2.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41849552810638957		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.41849552810638957 | validation: 0.38886523992056227]
	TIME [epoch: 2.72 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258497339994361		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.4258497339994361 | validation: 0.28205635307692]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43410597639886195		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.43410597639886195 | validation: 0.462226438758133]
	TIME [epoch: 2.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46309244596465854		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.46309244596465854 | validation: 0.3104467374590236]
	TIME [epoch: 2.73 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45713443659510655		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.45713443659510655 | validation: 0.39224018188886123]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42124880213532095		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.42124880213532095 | validation: 0.35102648506894596]
	TIME [epoch: 2.73 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093387327085156		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.4093387327085156 | validation: 0.31475444704151007]
	TIME [epoch: 2.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151300003642159		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.4151300003642159 | validation: 0.36826448347845653]
	TIME [epoch: 2.73 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4082677430150211		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.4082677430150211 | validation: 0.30148023479547553]
	TIME [epoch: 2.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41335082155520836		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.41335082155520836 | validation: 0.3608550605427783]
	TIME [epoch: 2.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4125712456099614		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.4125712456099614 | validation: 0.31350005213453624]
	TIME [epoch: 2.72 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4131862165052084		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.4131862165052084 | validation: 0.38306609560473937]
	TIME [epoch: 2.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41106393355309956		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.41106393355309956 | validation: 0.3077209997529895]
	TIME [epoch: 2.72 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42107571237313834		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.42107571237313834 | validation: 0.3844182691766939]
	TIME [epoch: 2.72 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42964352886492846		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.42964352886492846 | validation: 0.3438590097733971]
	TIME [epoch: 2.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43590387362654937		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.43590387362654937 | validation: 0.3433458559386051]
	TIME [epoch: 2.72 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4128972798199566		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.4128972798199566 | validation: 0.3345356124828199]
	TIME [epoch: 2.72 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40234018298030544		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.40234018298030544 | validation: 0.30657440778719747]
	TIME [epoch: 2.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39868240804052835		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.39868240804052835 | validation: 0.3513275569293633]
	TIME [epoch: 2.72 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4058133838255749		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.4058133838255749 | validation: 0.2901685993424743]
	TIME [epoch: 2.72 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41125336931623113		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.41125336931623113 | validation: 0.4245498306260584]
	TIME [epoch: 2.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43025219594903796		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.43025219594903796 | validation: 0.2706273913275838]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453426051092854		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.453426051092854 | validation: 0.39450718024643083]
	TIME [epoch: 2.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41375148716295285		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.41375148716295285 | validation: 0.306706518467214]
	TIME [epoch: 2.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39971005071507903		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.39971005071507903 | validation: 0.3047612396571404]
	TIME [epoch: 2.72 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4057194149059258		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.4057194149059258 | validation: 0.35308674701057957]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40572133710726377		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.40572133710726377 | validation: 0.31288877357892153]
	TIME [epoch: 2.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40119606611382497		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.40119606611382497 | validation: 0.3210500359031836]
	TIME [epoch: 2.72 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3935763723567682		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.3935763723567682 | validation: 0.3107156248001545]
	TIME [epoch: 2.72 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987947304922402		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.3987947304922402 | validation: 0.3252356183703851]
	TIME [epoch: 2.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39700222326684215		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.39700222326684215 | validation: 0.32844460621885585]
	TIME [epoch: 2.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010041099142677		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.4010041099142677 | validation: 0.3294610751128779]
	TIME [epoch: 2.72 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39864049409671753		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.39864049409671753 | validation: 0.30074407758949206]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4118235627365456		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.4118235627365456 | validation: 0.32003457010838965]
	TIME [epoch: 2.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41653776976846757		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.41653776976846757 | validation: 0.3639250957283486]
	TIME [epoch: 2.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4075312639240298		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.4075312639240298 | validation: 0.2895413469197896]
	TIME [epoch: 2.72 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4099486102619862		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.4099486102619862 | validation: 0.4413346742518625]
	TIME [epoch: 2.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43781199219451095		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.43781199219451095 | validation: 0.26966941004388917]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4304115345152826		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.4304115345152826 | validation: 0.3396777203923376]
	TIME [epoch: 2.72 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39602617342461516		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.39602617342461516 | validation: 0.33091826842049776]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3916087968232378		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.3916087968232378 | validation: 0.2826348817731263]
	TIME [epoch: 2.73 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39615517033803294		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.39615517033803294 | validation: 0.3448852992770582]
	TIME [epoch: 2.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907555683271219		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.3907555683271219 | validation: 0.30465887148975496]
	TIME [epoch: 2.72 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38605973986222114		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.38605973986222114 | validation: 0.31293597834291886]
	TIME [epoch: 2.72 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3882980591637919		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.3882980591637919 | validation: 0.3103057215534657]
	TIME [epoch: 2.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3845127038653372		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.3845127038653372 | validation: 0.29742786408664906]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38407726023772654		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.38407726023772654 | validation: 0.3198156022649365]
	TIME [epoch: 2.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886796766264253		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.3886796766264253 | validation: 0.306447894746046]
	TIME [epoch: 2.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40054864283372155		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.40054864283372155 | validation: 0.36953247224124547]
	TIME [epoch: 2.72 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4152506304786416		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.4152506304786416 | validation: 0.30302114539227265]
	TIME [epoch: 2.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40953629061474467		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.40953629061474467 | validation: 0.3655631726772331]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39192549053415193		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.39192549053415193 | validation: 0.2516452210993368]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3949048536973135		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.3949048536973135 | validation: 0.36135893257161184]
	TIME [epoch: 2.72 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3977684695285801		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.3977684695285801 | validation: 0.2593021634694534]
	TIME [epoch: 2.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.387135218954855		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.387135218954855 | validation: 0.28706668715397726]
	TIME [epoch: 2.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38029108924908356		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.38029108924908356 | validation: 0.31380363573829256]
	TIME [epoch: 2.72 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.379128692144527		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.379128692144527 | validation: 0.27201059347318157]
	TIME [epoch: 2.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3826934071240608		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.3826934071240608 | validation: 0.3359817901576679]
	TIME [epoch: 2.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3809672953304873		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.3809672953304873 | validation: 0.2612769434638545]
	TIME [epoch: 2.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38867806837510877		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.38867806837510877 | validation: 0.33865127813564616]
	TIME [epoch: 2.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37996164154376305		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.37996164154376305 | validation: 0.2517879754866556]
	TIME [epoch: 2.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38536288905132876		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.38536288905132876 | validation: 0.3218455106454452]
	TIME [epoch: 2.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38162838006129096		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.38162838006129096 | validation: 0.26856034037412735]
	TIME [epoch: 2.72 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721166192770298		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.3721166192770298 | validation: 0.3359439512472668]
	TIME [epoch: 2.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774838088945925		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.3774838088945925 | validation: 0.2663948004545803]
	TIME [epoch: 2.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792511021727568		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.3792511021727568 | validation: 0.36900845172796737]
	TIME [epoch: 2.72 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39063211691358585		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.39063211691358585 | validation: 0.27576675666897416]
	TIME [epoch: 2.72 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39769096757971034		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.39769096757971034 | validation: 0.3403143579137402]
	TIME [epoch: 2.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792740281152533		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.3792740281152533 | validation: 0.30532936955886136]
	TIME [epoch: 2.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3704350566218284		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.3704350566218284 | validation: 0.2554544942951325]
	TIME [epoch: 2.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3730362904790105		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.3730362904790105 | validation: 0.3458445119927245]
	TIME [epoch: 2.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3720500245840522		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.3720500245840522 | validation: 0.2701673939425393]
	TIME [epoch: 2.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37748121028637954		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.37748121028637954 | validation: 0.3212722161933996]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3723729690401184		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.3723729690401184 | validation: 0.27363021169621565]
	TIME [epoch: 2.72 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37096881295744183		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.37096881295744183 | validation: 0.29415560201778534]
	TIME [epoch: 2.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3732252810138762		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.3732252810138762 | validation: 0.3000814893477024]
	TIME [epoch: 2.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3620451821156542		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.3620451821156542 | validation: 0.2758203228192227]
	TIME [epoch: 2.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3689141866964576		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.3689141866964576 | validation: 0.30556580044385406]
	TIME [epoch: 2.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604949352598489		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.3604949352598489 | validation: 0.2806675227768443]
	TIME [epoch: 2.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3553568287134531		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.3553568287134531 | validation: 0.2887600672620617]
	TIME [epoch: 2.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3570899903973692		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.3570899903973692 | validation: 0.30231949742845765]
	TIME [epoch: 2.72 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36315532944022194		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.36315532944022194 | validation: 0.269993592330565]
	TIME [epoch: 2.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36245087059217523		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.36245087059217523 | validation: 0.3328073767661048]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3693520937074029		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.3693520937074029 | validation: 0.2777714011526395]
	TIME [epoch: 2.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380715725728018		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.380715725728018 | validation: 0.3132197884618303]
	TIME [epoch: 2.72 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652239669651381		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.3652239669651381 | validation: 0.3075509963128928]
	TIME [epoch: 2.72 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3596387826665077		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.3596387826665077 | validation: 0.238904864672391]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3724551211771863		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3724551211771863 | validation: 0.3873567902151837]
	TIME [epoch: 2.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.382894439270705		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.382894439270705 | validation: 0.2374767464600458]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672709660111263		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.3672709660111263 | validation: 0.2801560902104019]
	TIME [epoch: 2.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35231517215863667		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.35231517215863667 | validation: 0.2873820416832393]
	TIME [epoch: 2.72 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3522056027588741		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.3522056027588741 | validation: 0.2562388296394828]
	TIME [epoch: 2.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34956521702048776		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.34956521702048776 | validation: 0.2874888076497039]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34650325745340366		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.34650325745340366 | validation: 0.2767295233638056]
	TIME [epoch: 2.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437057661537304		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.3437057661537304 | validation: 0.2588763708099405]
	TIME [epoch: 2.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34826800742279845		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.34826800742279845 | validation: 0.2882067560350768]
	TIME [epoch: 2.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35183141548229563		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.35183141548229563 | validation: 0.23693156645177843]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35144531256831274		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.35144531256831274 | validation: 0.3415318881407718]
	TIME [epoch: 2.72 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3580873923954866		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.3580873923954866 | validation: 0.25177209355525554]
	TIME [epoch: 2.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35783773911494676		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.35783773911494676 | validation: 0.283620741835002]
	TIME [epoch: 2.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.344594930941578		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.344594930941578 | validation: 0.257022738301984]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3444705165180672		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.3444705165180672 | validation: 0.25638491200793395]
	TIME [epoch: 2.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3398737960955841		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.3398737960955841 | validation: 0.2876629135605537]
	TIME [epoch: 2.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33722561631998405		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.33722561631998405 | validation: 0.24267154394157392]
	TIME [epoch: 2.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34112114228353085		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.34112114228353085 | validation: 0.3367817704009291]
	TIME [epoch: 2.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35525467635049657		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.35525467635049657 | validation: 0.23180649647057772]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530275137040528		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.3530275137040528 | validation: 0.2983190212774287]
	TIME [epoch: 2.72 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3395667178157478		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.3395667178157478 | validation: 0.2584206979973798]
	TIME [epoch: 2.72 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3320978176457738		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.3320978176457738 | validation: 0.2606756909604146]
	TIME [epoch: 2.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3338887639565573		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.3338887639565573 | validation: 0.29014757818846576]
	TIME [epoch: 2.73 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32642940026123163		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.32642940026123163 | validation: 0.2499920657923397]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3270136155485431		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.3270136155485431 | validation: 0.2572625169920906]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33062192035873117		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.33062192035873117 | validation: 0.27326960294610975]
	TIME [epoch: 2.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33098938054270705		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.33098938054270705 | validation: 0.2717200429291973]
	TIME [epoch: 2.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33668214187182904		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.33668214187182904 | validation: 0.2862251084411534]
	TIME [epoch: 2.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3744083017194053		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.3744083017194053 | validation: 0.3155690531830873]
	TIME [epoch: 2.73 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34546023448913665		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.34546023448913665 | validation: 0.21870796758165426]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3357870982544233		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.3357870982544233 | validation: 0.3013441036406291]
	TIME [epoch: 2.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3333465712472043		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.3333465712472043 | validation: 0.25414561518300616]
	TIME [epoch: 2.72 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.324418372052837		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.324418372052837 | validation: 0.2407590498882823]
	TIME [epoch: 2.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32327156517266453		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.32327156517266453 | validation: 0.2869686713559801]
	TIME [epoch: 2.72 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278910298348016		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.3278910298348016 | validation: 0.23003151386467602]
	TIME [epoch: 2.72 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32106846803646977		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.32106846803646977 | validation: 0.2744442817747629]
	TIME [epoch: 2.72 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195655339393303		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.3195655339393303 | validation: 0.23296196344516967]
	TIME [epoch: 2.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32161713604522035		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.32161713604522035 | validation: 0.28305429236300933]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3276423129155306		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.3276423129155306 | validation: 0.24661428775617475]
	TIME [epoch: 2.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379698446897861		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.3379698446897861 | validation: 0.29365565106474806]
	TIME [epoch: 2.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3282654942115687		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.3282654942115687 | validation: 0.22383648597375175]
	TIME [epoch: 2.72 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3260117662448252		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.3260117662448252 | validation: 0.2836225177497064]
	TIME [epoch: 2.72 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182174201753565		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.3182174201753565 | validation: 0.2320589378083331]
	TIME [epoch: 2.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129327558864109		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.3129327558864109 | validation: 0.2624359802549721]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30750112767412735		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.30750112767412735 | validation: 0.25636234469942365]
	TIME [epoch: 2.72 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.309053630976741		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.309053630976741 | validation: 0.2292378217093282]
	TIME [epoch: 2.72 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31204010372199653		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.31204010372199653 | validation: 0.2795456638956955]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31714889198967905		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.31714889198967905 | validation: 0.22602178794368202]
	TIME [epoch: 2.72 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32623604455492355		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.32623604455492355 | validation: 0.2938009448450097]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239653608632787		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.3239653608632787 | validation: 0.20764113158716127]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31787819996028643		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.31787819996028643 | validation: 0.2911717069112955]
	TIME [epoch: 2.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3094080250219442		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.3094080250219442 | validation: 0.22113741155336905]
	TIME [epoch: 2.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30590518688560475		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.30590518688560475 | validation: 0.24643659751329738]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3010292399344478		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.3010292399344478 | validation: 0.2507961069154338]
	TIME [epoch: 2.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29460297137248126		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.29460297137248126 | validation: 0.22164972656740412]
	TIME [epoch: 2.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298219805548458		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.298219805548458 | validation: 0.29842264532070817]
	TIME [epoch: 2.72 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30169925527811237		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.30169925527811237 | validation: 0.20280030971280283]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30083949514390373		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.30083949514390373 | validation: 0.3047052383678719]
	TIME [epoch: 2.72 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106634275886692		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3106634275886692 | validation: 0.24399156313215667]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33774513449357463		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.33774513449357463 | validation: 0.2748410896435489]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.315004536007188		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.315004536007188 | validation: 0.23028659731153694]
	TIME [epoch: 2.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30332174213548135		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.30332174213548135 | validation: 0.22536631913747948]
	TIME [epoch: 2.72 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2936300016565924		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.2936300016565924 | validation: 0.25039629059687374]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904728855879574		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.2904728855879574 | validation: 0.2386315262926485]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28761837078081676		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.28761837078081676 | validation: 0.23957967953871942]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.290311622640567		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.290311622640567 | validation: 0.2504852264176091]
	TIME [epoch: 2.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2917955093606042		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.2917955093606042 | validation: 0.24780219710757723]
	TIME [epoch: 2.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29218152091253885		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.29218152091253885 | validation: 0.2146390877358534]
	TIME [epoch: 2.72 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3003018406351124		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.3003018406351124 | validation: 0.326993216746823]
	TIME [epoch: 2.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30847149303194843		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.30847149303194843 | validation: 0.20721375684016496]
	TIME [epoch: 2.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213412250467244		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.3213412250467244 | validation: 0.2441667013964255]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2870836194057709		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.2870836194057709 | validation: 0.2631915353730858]
	TIME [epoch: 2.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29008127704755177		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.29008127704755177 | validation: 0.19790668512415457]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2891038346133951		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.2891038346133951 | validation: 0.2610474962771096]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848195345434666		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.2848195345434666 | validation: 0.2204837651202301]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28022406167384434		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.28022406167384434 | validation: 0.2463409245654515]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28429759540099575		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.28429759540099575 | validation: 0.21529108197178207]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829464205483793		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.2829464205483793 | validation: 0.26820939586049736]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28818267363690125		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.28818267363690125 | validation: 0.21735239165978026]
	TIME [epoch: 2.73 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2999802259136715		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.2999802259136715 | validation: 0.2775621363892466]
	TIME [epoch: 2.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864022867610466		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.2864022867610466 | validation: 0.18896506795019616]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280164821963095		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.280164821963095 | validation: 0.24061884733305236]
	TIME [epoch: 2.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27483283337619446		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.27483283337619446 | validation: 0.21669708573325253]
	TIME [epoch: 2.72 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27239858194425787		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.27239858194425787 | validation: 0.2222464182356454]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26824699219282033		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.26824699219282033 | validation: 0.23020630600682523]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26615100921838647		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.26615100921838647 | validation: 0.2178237806217098]
	TIME [epoch: 2.72 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709126460008294		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2709126460008294 | validation: 0.2343703924043795]
	TIME [epoch: 2.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27132001312466997		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.27132001312466997 | validation: 0.23426670022210475]
	TIME [epoch: 2.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784142863881909		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.2784142863881909 | validation: 0.2251393324056747]
	TIME [epoch: 2.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3001982341958992		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.3001982341958992 | validation: 0.32248917630277324]
	TIME [epoch: 2.72 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3018323419314286		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.3018323419314286 | validation: 0.19150472380399763]
	TIME [epoch: 2.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296642057480682		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.296642057480682 | validation: 0.21351156367248236]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2642230643651905		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.2642230643651905 | validation: 0.2486938107094631]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671750287548286		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.2671750287548286 | validation: 0.20292766466954648]
	TIME [epoch: 2.72 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26942752130246034		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.26942752130246034 | validation: 0.2496990519475332]
	TIME [epoch: 2.72 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2608404848010254		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.2608404848010254 | validation: 0.2178767241557837]
	TIME [epoch: 2.72 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25914854845740837		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.25914854845740837 | validation: 0.23161454444220086]
	TIME [epoch: 2.72 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26180661707992564		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.26180661707992564 | validation: 0.22509489561055]
	TIME [epoch: 2.72 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2587721673464237		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.2587721673464237 | validation: 0.21560470790727765]
	TIME [epoch: 2.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2601793382172358		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.2601793382172358 | validation: 0.2056122304234223]
	TIME [epoch: 2.72 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25885073748242865		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.25885073748242865 | validation: 0.24920754264767414]
	TIME [epoch: 2.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25976231195492483		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.25976231195492483 | validation: 0.19733371326412047]
	TIME [epoch: 2.72 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803516531479782		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.2803516531479782 | validation: 0.3345709305743826]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29119442060194084		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.29119442060194084 | validation: 0.2099299257103788]
	TIME [epoch: 2.72 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864538959403089		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.2864538959403089 | validation: 0.19389846325842575]
	TIME [epoch: 2.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252007758570163		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.252007758570163 | validation: 0.2551126822523159]
	TIME [epoch: 2.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26164310658018197		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.26164310658018197 | validation: 0.2004199758250422]
	TIME [epoch: 2.72 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27304095409844964		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.27304095409844964 | validation: 0.23280811370799054]
	TIME [epoch: 2.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25196025856548454		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.25196025856548454 | validation: 0.2049808100320358]
	TIME [epoch: 2.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24443771208482898		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.24443771208482898 | validation: 0.2067208761298739]
	TIME [epoch: 2.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24597072054925728		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.24597072054925728 | validation: 0.23034552817172702]
	TIME [epoch: 2.72 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25090853833806764		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.25090853833806764 | validation: 0.2012644222351308]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2573916720935131		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.2573916720935131 | validation: 0.25215835409360304]
	TIME [epoch: 2.72 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2552764305780415		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.2552764305780415 | validation: 0.19212776574141466]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702074750400172		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.2702074750400172 | validation: 0.2606566504634544]
	TIME [epoch: 2.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25854745491596653		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.25854745491596653 | validation: 0.2054434316179343]
	TIME [epoch: 2.72 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26310468523838154		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.26310468523838154 | validation: 0.2084099435950516]
	TIME [epoch: 2.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2476119194580744		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.2476119194580744 | validation: 0.20477122657070623]
	TIME [epoch: 2.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23323555895460565		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.23323555895460565 | validation: 0.20613464886847557]
	TIME [epoch: 2.73 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24118605866851148		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.24118605866851148 | validation: 0.23842884538796671]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24355265160436138		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.24355265160436138 | validation: 0.18290228954837168]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25031869295267045		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.25031869295267045 | validation: 0.2615059806588717]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559225069147296		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.2559225069147296 | validation: 0.20841785308884223]
	TIME [epoch: 2.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27909562919831593		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.27909562919831593 | validation: 0.22561953527157522]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24162947635233126		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.24162947635233126 | validation: 0.2013343780946126]
	TIME [epoch: 2.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23122758111199737		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.23122758111199737 | validation: 0.19599817828708205]
	TIME [epoch: 2.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23365945719915407		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.23365945719915407 | validation: 0.22611249445475395]
	TIME [epoch: 2.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22939838820300076		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.22939838820300076 | validation: 0.18902997099209343]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2358596174315256		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.2358596174315256 | validation: 0.2506731639783505]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24411910641258192		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.24411910641258192 | validation: 0.2204284395218865]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863165752969107		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.2863165752969107 | validation: 0.23940085157041163]
	TIME [epoch: 2.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24030747166299163		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.24030747166299163 | validation: 0.18385984866306093]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23037090346768913		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.23037090346768913 | validation: 0.2027865909211248]
	TIME [epoch: 2.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2281540854743463		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.2281540854743463 | validation: 0.21108118505989926]
	TIME [epoch: 2.72 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282511561136243		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.2282511561136243 | validation: 0.18486358450428178]
	TIME [epoch: 2.72 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22876887563713413		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.22876887563713413 | validation: 0.2612717595113251]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23922053653365935		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.23922053653365935 | validation: 0.19284981448890193]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711408425247064		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.2711408425247064 | validation: 0.23567666019633773]
	TIME [epoch: 2.72 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23352539449021867		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.23352539449021867 | validation: 0.1931905834386854]
	TIME [epoch: 2.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22614569182579924		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.22614569182579924 | validation: 0.2031012139246034]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22191763803300826		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.22191763803300826 | validation: 0.19083172716295294]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21709489114031144		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.21709489114031144 | validation: 0.20309377376675242]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2203285517698871		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.2203285517698871 | validation: 0.21088508815995421]
	TIME [epoch: 2.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22631035673091485		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.22631035673091485 | validation: 0.1924644569531162]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206370266996507		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.2206370266996507 | validation: 0.18645309084218767]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23068221731501518		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.23068221731501518 | validation: 0.2775933421687226]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24907693502779205		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.24907693502779205 | validation: 0.2208631379742382]
	TIME [epoch: 2.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036147879054116		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.3036147879054116 | validation: 0.17770033936349178]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21490185515301582		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.21490185515301582 | validation: 0.27523388138257193]
	TIME [epoch: 2.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2459055619510542		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.2459055619510542 | validation: 0.19005343090638213]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25458474071099674		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.25458474071099674 | validation: 0.176087609003449]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21294795512362727		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.21294795512362727 | validation: 0.24051012375280778]
	TIME [epoch: 2.73 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23007102512331373		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.23007102512331373 | validation: 0.18751436310995873]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372159453490562		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.2372159453490562 | validation: 0.2067718172853549]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21698171718107365		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.21698171718107365 | validation: 0.18544905051161387]
	TIME [epoch: 2.72 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21167853439036158		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.21167853439036158 | validation: 0.1893062991999963]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20843331253819372		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.20843331253819372 | validation: 0.1935526901802693]
	TIME [epoch: 2.73 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20953963470712877		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.20953963470712877 | validation: 0.17161896477273145]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20908874629386504		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.20908874629386504 | validation: 0.22509931189687668]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21913361692571676		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.21913361692571676 | validation: 0.1836169987615838]
	TIME [epoch: 2.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252690858172627		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.252690858172627 | validation: 0.23672569220916478]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22091509607565735		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.22091509607565735 | validation: 0.1757973508880228]
	TIME [epoch: 2.73 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21143256808458785		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.21143256808458785 | validation: 0.16708166185989068]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20923755387082202		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.20923755387082202 | validation: 0.21432227032725773]
	TIME [epoch: 2.73 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20852273791706652		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.20852273791706652 | validation: 0.16922575229578612]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21371677548577503		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.21371677548577503 | validation: 0.22808410414338304]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22230287019249895		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.22230287019249895 | validation: 0.19595155812754209]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25955684135614054		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.25955684135614054 | validation: 0.20462867509406177]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089802539060425		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.2089802539060425 | validation: 0.16519809589315704]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2042656538230518		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.2042656538230518 | validation: 0.1952910761063783]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19772691823781116		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.19772691823781116 | validation: 0.1959890199316877]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20195260890675001		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.20195260890675001 | validation: 0.17498991125547284]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21085347773181998		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.21085347773181998 | validation: 0.26757660754741]
	TIME [epoch: 2.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2352274385168395		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.2352274385168395 | validation: 0.19266279819268162]
	TIME [epoch: 2.72 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661021217263005		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.2661021217263005 | validation: 0.17496235493595333]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20337224394084719		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.20337224394084719 | validation: 0.2511963387966127]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22759879478740075		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.22759879478740075 | validation: 0.17774284383802313]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23983036204534933		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.23983036204534933 | validation: 0.18804113081204127]
	TIME [epoch: 2.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.197689145222218		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.197689145222218 | validation: 0.22565570881372704]
	TIME [epoch: 2.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21076126464894665		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.21076126464894665 | validation: 0.16822279273120364]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22208977609357852		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.22208977609357852 | validation: 0.1935210167787068]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19913078617217778		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.19913078617217778 | validation: 0.19259858334970267]
	TIME [epoch: 2.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19248499908764036		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.19248499908764036 | validation: 0.1806029452838297]
	TIME [epoch: 2.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19373950709373502		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.19373950709373502 | validation: 0.19969850210983273]
	TIME [epoch: 2.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1985637242406015		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.1985637242406015 | validation: 0.16853735445213372]
	TIME [epoch: 2.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19587498942486747		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.19587498942486747 | validation: 0.2038774644776297]
	TIME [epoch: 2.73 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20047048367336345		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.20047048367336345 | validation: 0.17664797067821889]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21930921874200698		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.21930921874200698 | validation: 0.25107248103859453]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22143749935795243		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.22143749935795243 | validation: 0.15315968438936675]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2264377144513755		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.2264377144513755 | validation: 0.19587898848752883]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19098685495869408		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.19098685495869408 | validation: 0.19413347873591966]
	TIME [epoch: 2.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896993501450337		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1896993501450337 | validation: 0.15229812410756904]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1996599141861246		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.1996599141861246 | validation: 0.22863374330593678]
	TIME [epoch: 2.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034018297912388		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.2034018297912388 | validation: 0.16442899155241963]
	TIME [epoch: 2.71 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2149205264581158		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.2149205264581158 | validation: 0.20478552885867407]
	TIME [epoch: 2.71 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19044472564140846		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.19044472564140846 | validation: 0.166887966152754]
	TIME [epoch: 2.71 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18458840274432872		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.18458840274432872 | validation: 0.1751336681647856]
	TIME [epoch: 2.71 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1843265758497099		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.1843265758497099 | validation: 0.18605039255297873]
	TIME [epoch: 2.71 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18698261481176892		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.18698261481176892 | validation: 0.17171724347840103]
	TIME [epoch: 2.71 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18283679459699614		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.18283679459699614 | validation: 0.1828071888807716]
	TIME [epoch: 2.71 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18342668015882624		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.18342668015882624 | validation: 0.17524291688611973]
	TIME [epoch: 2.71 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552095100613322		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.18552095100613322 | validation: 0.18047664915422434]
	TIME [epoch: 2.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18409674971174098		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.18409674971174098 | validation: 0.1742974354958265]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18213338825246042		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.18213338825246042 | validation: 0.18034178689633534]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18433212130073984		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.18433212130073984 | validation: 0.1759045253538782]
	TIME [epoch: 2.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17751015943478535		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.17751015943478535 | validation: 0.17895986591978594]
	TIME [epoch: 2.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18036297119979142		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.18036297119979142 | validation: 0.16911308603763464]
	TIME [epoch: 2.73 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761874700194059		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.1761874700194059 | validation: 0.22417893783934725]
	TIME [epoch: 2.73 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20991812725696485		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.20991812725696485 | validation: 0.2626443615374167]
	TIME [epoch: 2.73 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34036562546702487		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.34036562546702487 | validation: 0.1712497947246056]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19972997673248052		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.19972997673248052 | validation: 0.3594077678313923]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31120326448717917		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.31120326448717917 | validation: 0.14630125060618246]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21733827311956094		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.21733827311956094 | validation: 0.14447291106857474]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_832.pth
	Model improved!!!
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20303936961544103		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.20303936961544103 | validation: 0.23473142446459838]
	TIME [epoch: 2.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.199693720389322		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.199693720389322 | validation: 0.16523617888691322]
	TIME [epoch: 2.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1786768370225823		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.1786768370225823 | validation: 0.15847153772972075]
	TIME [epoch: 2.73 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763476203169179		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.1763476203169179 | validation: 0.1781664185334999]
	TIME [epoch: 2.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18000123771836138		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.18000123771836138 | validation: 0.15922833216891333]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17692574945690312		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.17692574945690312 | validation: 0.17967820692099742]
	TIME [epoch: 2.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17379782827106752		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.17379782827106752 | validation: 0.1612100664698689]
	TIME [epoch: 2.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1777492120294369		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.1777492120294369 | validation: 0.1941034065130327]
	TIME [epoch: 2.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17980068087393974		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.17980068087393974 | validation: 0.15629669348545944]
	TIME [epoch: 2.72 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18748629497247887		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.18748629497247887 | validation: 0.19933288131378288]
	TIME [epoch: 2.72 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1871578666594613		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.1871578666594613 | validation: 0.1527628094151996]
	TIME [epoch: 2.72 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19156116533905745		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.19156116533905745 | validation: 0.1934437262571037]
	TIME [epoch: 2.72 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1795049921593886		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.1795049921593886 | validation: 0.1490449274296731]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1805787305660666		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.1805787305660666 | validation: 0.19251507324536]
	TIME [epoch: 2.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17958754752999553		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.17958754752999553 | validation: 0.1642322976628292]
	TIME [epoch: 2.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18790861401644293		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.18790861401644293 | validation: 0.21576798167369549]
	TIME [epoch: 2.72 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.195889338818886		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.195889338818886 | validation: 0.15140768066546062]
	TIME [epoch: 2.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20746225040563468		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.20746225040563468 | validation: 0.19878731694371685]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770071956753173		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.1770071956753173 | validation: 0.1689253143236835]
	TIME [epoch: 2.73 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17258300175883398		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.17258300175883398 | validation: 0.1602320562145651]
	TIME [epoch: 2.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170945706389766		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.170945706389766 | validation: 0.18710099165266808]
	TIME [epoch: 2.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17431501156897133		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.17431501156897133 | validation: 0.15517528111939868]
	TIME [epoch: 2.72 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849417471105785		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.1849417471105785 | validation: 0.22130243792084425]
	TIME [epoch: 2.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1956926341504157		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.1956926341504157 | validation: 0.14889940267254068]
	TIME [epoch: 2.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20646508110284273		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.20646508110284273 | validation: 0.1750206802813648]
	TIME [epoch: 2.73 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17269135144206768		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.17269135144206768 | validation: 0.1754896883955447]
	TIME [epoch: 2.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16893096767483287		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.16893096767483287 | validation: 0.16031903749423676]
	TIME [epoch: 2.72 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756433911446035		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.1756433911446035 | validation: 0.20449219575286914]
	TIME [epoch: 2.72 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18571846423163585		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.18571846423163585 | validation: 0.15865317024865402]
	TIME [epoch: 2.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1970028578133123		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.1970028578133123 | validation: 0.1882674680816768]
	TIME [epoch: 2.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17233612675185414		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.17233612675185414 | validation: 0.15928122665135191]
	TIME [epoch: 2.72 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16614227654021357		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.16614227654021357 | validation: 0.16493357857726923]
	TIME [epoch: 2.72 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687174653826818		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.1687174653826818 | validation: 0.1686175050723549]
	TIME [epoch: 2.72 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162780118567862		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.162780118567862 | validation: 0.15240334466740246]
	TIME [epoch: 2.72 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16450077871744678		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.16450077871744678 | validation: 0.1954833492419085]
	TIME [epoch: 2.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17226159262296262		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.17226159262296262 | validation: 0.15205322587189696]
	TIME [epoch: 2.72 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1937557081172772		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.1937557081172772 | validation: 0.2292143654136572]
	TIME [epoch: 2.72 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19385597135256302		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.19385597135256302 | validation: 0.14300145196787403]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19210207903601492		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.19210207903601492 | validation: 0.16842095597039408]
	TIME [epoch: 2.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639474501662161		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.1639474501662161 | validation: 0.15174911353903192]
	TIME [epoch: 2.73 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15967625825681248		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.15967625825681248 | validation: 0.1683438111605367]
	TIME [epoch: 2.72 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16211398055135404		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.16211398055135404 | validation: 0.16478429000615052]
	TIME [epoch: 2.72 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16052160308517413		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.16052160308517413 | validation: 0.1531316522225298]
	TIME [epoch: 2.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16666769117566582		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.16666769117566582 | validation: 0.2070386595076686]
	TIME [epoch: 2.72 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1793349059274343		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.1793349059274343 | validation: 0.1474613371487169]
	TIME [epoch: 2.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2040714371143381		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.2040714371143381 | validation: 0.19897415297851084]
	TIME [epoch: 2.73 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17370028511186195		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.17370028511186195 | validation: 0.14848561891086476]
	TIME [epoch: 2.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627423990768145		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.1627423990768145 | validation: 0.16587990834749644]
	TIME [epoch: 2.72 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16069848699824452		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.16069848699824452 | validation: 0.15264153564635613]
	TIME [epoch: 2.72 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15963368572330602		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.15963368572330602 | validation: 0.17856239106009572]
	TIME [epoch: 2.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16145640909751635		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.16145640909751635 | validation: 0.14995452242178559]
	TIME [epoch: 2.72 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16125760193267977		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.16125760193267977 | validation: 0.19017657809430932]
	TIME [epoch: 2.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1710129875475785		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.1710129875475785 | validation: 0.15051991179447022]
	TIME [epoch: 2.72 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20142461216614174		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.20142461216614174 | validation: 0.1867833109408228]
	TIME [epoch: 2.72 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1701589258168371		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1701589258168371 | validation: 0.1401961189093108]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16312212454148353		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.16312212454148353 | validation: 0.162123295231017]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15655641454363328		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.15655641454363328 | validation: 0.16097683958924902]
	TIME [epoch: 2.72 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15782158522713136		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.15782158522713136 | validation: 0.16487553147973288]
	TIME [epoch: 2.72 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15661629923532203		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.15661629923532203 | validation: 0.15012004436216883]
	TIME [epoch: 2.72 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544488807186776		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.1544488807186776 | validation: 0.15915637881054662]
	TIME [epoch: 2.72 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15480834438337135		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.15480834438337135 | validation: 0.14732988032406924]
	TIME [epoch: 2.72 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15747231686761373		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.15747231686761373 | validation: 0.2073179626983256]
	TIME [epoch: 2.72 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18070511795037938		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.18070511795037938 | validation: 0.1623066403993539]
	TIME [epoch: 2.72 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23225821630578772		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.23225821630578772 | validation: 0.17598283129741185]
	TIME [epoch: 2.72 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568791107492966		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.1568791107492966 | validation: 0.18812523509280635]
	TIME [epoch: 2.72 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16728583261660881		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.16728583261660881 | validation: 0.12948348841608232]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1874423921428374		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.1874423921428374 | validation: 0.17101680736051605]
	TIME [epoch: 2.72 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627340448217118		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.1627340448217118 | validation: 0.14012304920033444]
	TIME [epoch: 2.72 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15655974737546824		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.15655974737546824 | validation: 0.17220781494107074]
	TIME [epoch: 2.72 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15527109849367707		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.15527109849367707 | validation: 0.15433130987244892]
	TIME [epoch: 2.72 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1512281796259476		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.1512281796259476 | validation: 0.14846066023406498]
	TIME [epoch: 2.72 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1548542674469496		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.1548542674469496 | validation: 0.17738141222223616]
	TIME [epoch: 2.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576609603871406		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.1576609603871406 | validation: 0.1475497610451054]
	TIME [epoch: 2.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18097238644790786		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.18097238644790786 | validation: 0.20488048574131473]
	TIME [epoch: 2.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18863384944719166		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.18863384944719166 | validation: 0.12928186122718968]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18224612811629365		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.18224612811629365 | validation: 0.16455997031466654]
	TIME [epoch: 2.71 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15506220138060317		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.15506220138060317 | validation: 0.1796204057928549]
	TIME [epoch: 2.71 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15824622017223566		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.15824622017223566 | validation: 0.12788920507981358]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1796566359353415		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.1796566359353415 | validation: 0.17046338502553293]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1525088871971063		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.1525088871971063 | validation: 0.14607381047590823]
	TIME [epoch: 2.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15038365231702763		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.15038365231702763 | validation: 0.16003016241590368]
	TIME [epoch: 2.73 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15129726647489233		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.15129726647489233 | validation: 0.15120367476641913]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464642353076459		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.1464642353076459 | validation: 0.15476117564960054]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14851215155502925		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.14851215155502925 | validation: 0.14346424720112427]
	TIME [epoch: 2.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15233742861506833		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.15233742861506833 | validation: 0.20100572905867115]
	TIME [epoch: 2.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16510445207451235		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.16510445207451235 | validation: 0.144927210578879]
	TIME [epoch: 2.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955969174322874		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.1955969174322874 | validation: 0.17726879358195025]
	TIME [epoch: 2.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556723871264134		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.1556723871264134 | validation: 0.14294131519031428]
	TIME [epoch: 2.72 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15052887875881688		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.15052887875881688 | validation: 0.15299048175045643]
	TIME [epoch: 2.73 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14728204033646544		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.14728204033646544 | validation: 0.1513643894055914]
	TIME [epoch: 2.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14625991220979384		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.14625991220979384 | validation: 0.1454806475065378]
	TIME [epoch: 2.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14781663984721274		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.14781663984721274 | validation: 0.16993788250885777]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15125390687266407		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.15125390687266407 | validation: 0.13710268203068673]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16722500778980576		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.16722500778980576 | validation: 0.2115778270434542]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16785464168449793		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.16785464168449793 | validation: 0.13239150032243038]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17299399440577504		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.17299399440577504 | validation: 0.1604996481327835]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14942206360277666		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.14942206360277666 | validation: 0.15396329761992364]
	TIME [epoch: 2.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1433028691994158		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.1433028691994158 | validation: 0.15433761590305048]
	TIME [epoch: 2.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14444839858940237		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.14444839858940237 | validation: 0.1559597067718631]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419078624236174		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.1419078624236174 | validation: 0.15294321407802658]
	TIME [epoch: 2.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459856890639648		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.1459856890639648 | validation: 0.1470898389532488]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14674160613042717		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.14674160613042717 | validation: 0.18449454532209283]
	TIME [epoch: 2.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15941330662816025		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.15941330662816025 | validation: 0.13873133255016115]
	TIME [epoch: 2.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18767741078544142		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.18767741078544142 | validation: 0.16849669222384323]
	TIME [epoch: 2.73 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15024305788003472		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.15024305788003472 | validation: 0.1481879465153847]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407247839238543		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.1407247839238543 | validation: 0.14363280919353213]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416597021817933		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.1416597021817933 | validation: 0.1557112912623617]
	TIME [epoch: 2.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14210165902894165		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.14210165902894165 | validation: 0.14146654579011372]
	TIME [epoch: 2.72 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14872337671531558		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.14872337671531558 | validation: 0.20351921176973234]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773959934122963		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1773959934122963 | validation: 0.13116051728988307]
	TIME [epoch: 2.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19180115794909017		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.19180115794909017 | validation: 0.14979396988300692]
	TIME [epoch: 2.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405514351638095		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.1405514351638095 | validation: 0.18224561839851897]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15164983574720403		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.15164983574720403 | validation: 0.1335235205207229]
	TIME [epoch: 2.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16347797334802663		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.16347797334802663 | validation: 0.16094617980328085]
	TIME [epoch: 2.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14834907603317024		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.14834907603317024 | validation: 0.1340613187410549]
	TIME [epoch: 2.73 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13765295783972037		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.13765295783972037 | validation: 0.14925222746102693]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14279278604375928		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.14279278604375928 | validation: 0.1566920967630847]
	TIME [epoch: 2.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14139287429141192		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.14139287429141192 | validation: 0.14462160052885992]
	TIME [epoch: 2.73 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14113028422487336		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.14113028422487336 | validation: 0.1453792178015604]
	TIME [epoch: 2.73 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723394961748164		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.13723394961748164 | validation: 0.15205158708367972]
	TIME [epoch: 2.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414633815690272		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.1414633815690272 | validation: 0.13939462593674765]
	TIME [epoch: 2.73 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14012039511637767		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.14012039511637767 | validation: 0.1682771200307253]
	TIME [epoch: 2.73 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14394939793905054		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.14394939793905054 | validation: 0.12865059455306285]
	TIME [epoch: 2.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1587827204524617		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.1587827204524617 | validation: 0.1967757912184856]
	TIME [epoch: 2.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1736582011015207		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.1736582011015207 | validation: 0.13038575791897755]
	TIME [epoch: 2.73 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1755644304181941		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.1755644304181941 | validation: 0.15223770306949805]
	TIME [epoch: 2.73 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14303500447402367		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.14303500447402367 | validation: 0.15948450510506934]
	TIME [epoch: 2.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14070574347410386		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.14070574347410386 | validation: 0.12519214281149435]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14774998935718844		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.14774998935718844 | validation: 0.17961684957654592]
	TIME [epoch: 2.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15334163742027582		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.15334163742027582 | validation: 0.1316143441530638]
	TIME [epoch: 2.71 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1457232233240141		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.1457232233240141 | validation: 0.1581373282218082]
	TIME [epoch: 3.58 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388830401210356		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.1388830401210356 | validation: 0.14237822994097799]
	TIME [epoch: 2.73 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139953554527376		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.139953554527376 | validation: 0.14173284584066614]
	TIME [epoch: 2.73 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13684089732608906		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.13684089732608906 | validation: 0.1438731429446013]
	TIME [epoch: 2.72 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1357418514981729		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.1357418514981729 | validation: 0.13529006213259057]
	TIME [epoch: 2.73 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787031139825623		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.13787031139825623 | validation: 0.14766424471291148]
	TIME [epoch: 2.73 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13485944240082737		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.13485944240082737 | validation: 0.1441652254247674]
	TIME [epoch: 2.73 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1323139869758829		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.1323139869758829 | validation: 0.14066012116888338]
	TIME [epoch: 2.73 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1350325080502971		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.1350325080502971 | validation: 0.1649355633045064]
	TIME [epoch: 2.73 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13667676228241693		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.13667676228241693 | validation: 0.1293065539069782]
	TIME [epoch: 2.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725830228930146		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.15725830228930146 | validation: 0.22102484704610417]
	TIME [epoch: 2.73 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.183064284455142		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.183064284455142 | validation: 0.13991137348961386]
	TIME [epoch: 2.73 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17545560429704163		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.17545560429704163 | validation: 0.13178190727260702]
	TIME [epoch: 2.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13495534612067772		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.13495534612067772 | validation: 0.1783547047243157]
	TIME [epoch: 2.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15276919464817978		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.15276919464817978 | validation: 0.12268422756770336]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15610164920604847		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.15610164920604847 | validation: 0.13457404323026753]
	TIME [epoch: 2.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13525765619617758		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.13525765619617758 | validation: 0.15554336825894988]
	TIME [epoch: 2.73 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13834330495766334		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.13834330495766334 | validation: 0.12574915362006217]
	TIME [epoch: 2.73 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14143049655132728		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.14143049655132728 | validation: 0.16081262284765474]
	TIME [epoch: 2.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13728212764755962		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.13728212764755962 | validation: 0.13074459251875956]
	TIME [epoch: 2.73 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875685711045146		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.13875685711045146 | validation: 0.14781953494804748]
	TIME [epoch: 2.73 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13246779241931753		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.13246779241931753 | validation: 0.13584442005315653]
	TIME [epoch: 2.73 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13300542303573315		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.13300542303573315 | validation: 0.15017382306611152]
	TIME [epoch: 2.73 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13559681711378507		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.13559681711378507 | validation: 0.12951360909546594]
	TIME [epoch: 2.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13202320875406762		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.13202320875406762 | validation: 0.16552872462692184]
	TIME [epoch: 2.73 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13814852564585028		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.13814852564585028 | validation: 0.13091676032535723]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15187308700615668		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.15187308700615668 | validation: 0.1774013159720221]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.144283038526252		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.144283038526252 | validation: 0.11087462986072083]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14690862135117277		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.14690862135117277 | validation: 0.14756350831020915]
	TIME [epoch: 2.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13246780163218405		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.13246780163218405 | validation: 0.13200820580306685]
	TIME [epoch: 2.72 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13187467155537463		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.13187467155537463 | validation: 0.14121138056692698]
	TIME [epoch: 2.72 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12997345887193945		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.12997345887193945 | validation: 0.13594463505845372]
	TIME [epoch: 2.73 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327528558533142		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.1327528558533142 | validation: 0.15215770708975404]
	TIME [epoch: 2.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13440892142061778		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.13440892142061778 | validation: 0.12211538026738054]
	TIME [epoch: 2.73 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14577012229782424		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.14577012229782424 | validation: 0.18216985331169472]
	TIME [epoch: 2.73 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14348999433181647		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.14348999433181647 | validation: 0.12359235802169062]
	TIME [epoch: 2.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14927991665180457		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.14927991665180457 | validation: 0.15860126828215224]
	TIME [epoch: 2.73 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1347967265361521		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.1347967265361521 | validation: 0.12481749414267723]
	TIME [epoch: 2.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13130038059138313		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.13130038059138313 | validation: 0.14747125433669087]
	TIME [epoch: 177 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13279837322546567		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.13279837322546567 | validation: 0.1310138970556221]
	TIME [epoch: 5.85 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13019076957895825		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.13019076957895825 | validation: 0.1562016201639412]
	TIME [epoch: 5.83 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13337227906999655		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.13337227906999655 | validation: 0.12281001468506336]
	TIME [epoch: 5.84 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377358536450456		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.1377358536450456 | validation: 0.17421837174338486]
	TIME [epoch: 5.83 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14002493551409664		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.14002493551409664 | validation: 0.12480327615085302]
	TIME [epoch: 5.84 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14476332778654705		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.14476332778654705 | validation: 0.15997984225064063]
	TIME [epoch: 5.84 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1355001260074687		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.1355001260074687 | validation: 0.12449014498504783]
	TIME [epoch: 5.84 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13028548557857678		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.13028548557857678 | validation: 0.1373958178989013]
	TIME [epoch: 5.83 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12739554134488365		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.12739554134488365 | validation: 0.14843770666986678]
	TIME [epoch: 5.84 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12770609290302043		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.12770609290302043 | validation: 0.12320797026065478]
	TIME [epoch: 5.83 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12988283369377762		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.12988283369377762 | validation: 0.15818982744932153]
	TIME [epoch: 5.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13367526850474928		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.13367526850474928 | validation: 0.11943061861172731]
	TIME [epoch: 5.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14361851428525907		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.14361851428525907 | validation: 0.16959095305985514]
	TIME [epoch: 5.83 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14184731319522276		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.14184731319522276 | validation: 0.12213157446589835]
	TIME [epoch: 5.83 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13553244033935444		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.13553244033935444 | validation: 0.15550264553216245]
	TIME [epoch: 5.84 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13120982956696856		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.13120982956696856 | validation: 0.1324388773337592]
	TIME [epoch: 5.83 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12710575850653844		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.12710575850653844 | validation: 0.14271812454639482]
	TIME [epoch: 5.83 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12506845483342233		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.12506845483342233 | validation: 0.12309840426444728]
	TIME [epoch: 5.84 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327736713958922		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.1327736713958922 | validation: 0.1695890392034739]
	TIME [epoch: 5.84 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1368358286076707		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.1368358286076707 | validation: 0.12417372915991459]
	TIME [epoch: 5.84 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14757461313336154		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.14757461313336154 | validation: 0.16087047539673047]
	TIME [epoch: 5.84 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13809081132113765		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.13809081132113765 | validation: 0.12225406513612369]
	TIME [epoch: 5.85 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12715944272237628		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.12715944272237628 | validation: 0.14384823852517256]
	TIME [epoch: 5.84 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12590976694584857		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.12590976694584857 | validation: 0.1363398721212537]
	TIME [epoch: 5.83 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12366839805909756		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.12366839805909756 | validation: 0.13251047200862187]
	TIME [epoch: 5.84 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12301752972897319		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.12301752972897319 | validation: 0.14910130053756007]
	TIME [epoch: 5.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12703211678832993		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.12703211678832993 | validation: 0.11977921141729242]
	TIME [epoch: 5.84 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13057882857777564		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.13057882857777564 | validation: 0.16463660220757564]
	TIME [epoch: 5.84 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13441553639996187		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.13441553639996187 | validation: 0.12185340692282598]
	TIME [epoch: 5.83 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1357014764389824		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.1357014764389824 | validation: 0.1507767070774882]
	TIME [epoch: 5.83 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345405239817468		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.13345405239817468 | validation: 0.1192409114748724]
	TIME [epoch: 5.84 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12796074685864223		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.12796074685864223 | validation: 0.1476584580147631]
	TIME [epoch: 5.84 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1280264359978699		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.1280264359978699 | validation: 0.12630781038320524]
	TIME [epoch: 5.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12700412751713638		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.12700412751713638 | validation: 0.153984582951184]
	TIME [epoch: 5.83 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293493260746695		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.1293493260746695 | validation: 0.12020803320446567]
	TIME [epoch: 5.84 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13567689092905988		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.13567689092905988 | validation: 0.1577264760994099]
	TIME [epoch: 5.84 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297819930195934		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.1297819930195934 | validation: 0.11722452162983482]
	TIME [epoch: 5.85 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1273900117308202		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.1273900117308202 | validation: 0.15179838594810194]
	TIME [epoch: 5.84 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032628979150615		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.13032628979150615 | validation: 0.12151516534593808]
	TIME [epoch: 5.84 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12829159275483107		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.12829159275483107 | validation: 0.151419828674897]
	TIME [epoch: 5.83 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251360776717661		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.1251360776717661 | validation: 0.12953947221931778]
	TIME [epoch: 5.84 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12476265866891804		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.12476265866891804 | validation: 0.14077911266527576]
	TIME [epoch: 5.83 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12307836093929882		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.12307836093929882 | validation: 0.1252515988181342]
	TIME [epoch: 5.83 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11966461250558236		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.11966461250558236 | validation: 0.13781666139676232]
	TIME [epoch: 5.83 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12180912333538846		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.12180912333538846 | validation: 0.12783192626853906]
	TIME [epoch: 5.84 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274420739494155		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.1274420739494155 | validation: 0.17035663171528506]
	TIME [epoch: 5.83 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13451120602751174		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.13451120602751174 | validation: 0.11804745950825649]
	TIME [epoch: 5.84 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15456133131551716		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.15456133131551716 | validation: 0.1463191121161854]
	TIME [epoch: 5.83 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12519037309020054		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.12519037309020054 | validation: 0.13070648582439412]
	TIME [epoch: 5.84 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11947960498452442		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.11947960498452442 | validation: 0.12734180387169136]
	TIME [epoch: 5.83 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12110407304893177		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.12110407304893177 | validation: 0.14647790386007167]
	TIME [epoch: 5.85 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408533519088157		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.12408533519088157 | validation: 0.11625191371309518]
	TIME [epoch: 5.85 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12695161545425881		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.12695161545425881 | validation: 0.14042257869432137]
	TIME [epoch: 5.85 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12367479885838158		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.12367479885838158 | validation: 0.11961169463333357]
	TIME [epoch: 5.85 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12136609655308345		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.12136609655308345 | validation: 0.14121654782740878]
	TIME [epoch: 5.85 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12019752368349153		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.12019752368349153 | validation: 0.1253205013678575]
	TIME [epoch: 5.83 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255807797394105		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.1255807797394105 | validation: 0.15130323559925454]
	TIME [epoch: 5.84 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12441749313885928		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.12441749313885928 | validation: 0.11463913700061493]
	TIME [epoch: 5.83 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1343166124551225		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.1343166124551225 | validation: 0.15414343051760093]
	TIME [epoch: 5.83 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12650682379710898		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.12650682379710898 | validation: 0.11179741146926783]
	TIME [epoch: 5.83 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254154901227614		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.1254154901227614 | validation: 0.1397086367830511]
	TIME [epoch: 5.84 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12007493492112178		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.12007493492112178 | validation: 0.13280993230145693]
	TIME [epoch: 5.84 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11787374762200074		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.11787374762200074 | validation: 0.12687200159920345]
	TIME [epoch: 5.84 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517951381305826		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.11517951381305826 | validation: 0.13445749986708763]
	TIME [epoch: 5.85 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12084961509846735		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.12084961509846735 | validation: 0.11967691990582541]
	TIME [epoch: 5.86 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12232805135674053		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.12232805135674053 | validation: 0.14708033167121867]
	TIME [epoch: 5.85 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12444185011578941		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.12444185011578941 | validation: 0.11396276180325399]
	TIME [epoch: 5.86 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13904829410516223		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.13904829410516223 | validation: 0.15652350911144128]
	TIME [epoch: 5.84 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126821745870386		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.126821745870386 | validation: 0.11155245919738054]
	TIME [epoch: 5.84 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12314573494171681		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.12314573494171681 | validation: 0.14094328582178792]
	TIME [epoch: 5.85 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11979511978703297		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.11979511978703297 | validation: 0.12492305900134114]
	TIME [epoch: 5.85 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11476826030485587		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.11476826030485587 | validation: 0.1305973267829358]
	TIME [epoch: 5.86 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11901545504334164		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.11901545504334164 | validation: 0.12705163851584242]
	TIME [epoch: 5.86 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601979439362804		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.11601979439362804 | validation: 0.14220338357045378]
	TIME [epoch: 5.83 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1154789907747615		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.1154789907747615 | validation: 0.11494463407545008]
	TIME [epoch: 5.82 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097657697379591		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.12097657697379591 | validation: 0.163864489096463]
	TIME [epoch: 5.83 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331084410769602		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.1331084410769602 | validation: 0.10849328927695137]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1078.pth
	Model improved!!!
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336151154022306		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.1336151154022306 | validation: 0.13566072953926503]
	TIME [epoch: 5.85 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11600649085590785		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.11600649085590785 | validation: 0.142793914818942]
	TIME [epoch: 5.84 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11690821219218346		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.11690821219218346 | validation: 0.11004226682865839]
	TIME [epoch: 5.83 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1257588140661466		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.1257588140661466 | validation: 0.15857632019854262]
	TIME [epoch: 5.83 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306936851664133		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.1306936851664133 | validation: 0.11339977497019126]
	TIME [epoch: 5.83 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12436393733577816		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.12436393733577816 | validation: 0.13308866430494112]
	TIME [epoch: 5.83 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11511618081694959		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.11511618081694959 | validation: 0.14155112868572461]
	TIME [epoch: 5.83 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1158450145414772		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.1158450145414772 | validation: 0.11991458393632662]
	TIME [epoch: 5.84 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11751760422489575		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.11751760422489575 | validation: 0.1341605035294084]
	TIME [epoch: 5.83 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532345660258303		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.11532345660258303 | validation: 0.12585935785683458]
	TIME [epoch: 5.83 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11662612552412588		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.11662612552412588 | validation: 0.13819213952969742]
	TIME [epoch: 5.83 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11456503833112859		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.11456503833112859 | validation: 0.11984598993024298]
	TIME [epoch: 5.84 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11700739856908397		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.11700739856908397 | validation: 0.14928702451207365]
	TIME [epoch: 5.84 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12669719899314394		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.12669719899314394 | validation: 0.10648868719591165]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1092.pth
	Model improved!!!
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12637769927677553		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.12637769927677553 | validation: 0.14359099216292534]
	TIME [epoch: 5.82 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12035956304059138		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.12035956304059138 | validation: 0.12447109845731395]
	TIME [epoch: 5.84 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11330697430988032		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.11330697430988032 | validation: 0.12580558783101545]
	TIME [epoch: 5.83 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463329725222784		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.11463329725222784 | validation: 0.13619948311132843]
	TIME [epoch: 5.83 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141063242643222		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.1141063242643222 | validation: 0.1157999051101824]
	TIME [epoch: 5.83 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11645304804468765		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.11645304804468765 | validation: 0.14193974946422194]
	TIME [epoch: 5.83 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11818851510327044		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.11818851510327044 | validation: 0.10988541556129111]
	TIME [epoch: 5.83 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12288075943943018		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.12288075943943018 | validation: 0.14072102855515564]
	TIME [epoch: 5.83 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11756684820740995		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.11756684820740995 | validation: 0.11433682425136693]
	TIME [epoch: 5.83 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11809896728215581		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.11809896728215581 | validation: 0.13837578644484153]
	TIME [epoch: 5.83 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1182023671172905		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.1182023671172905 | validation: 0.11876443485108537]
	TIME [epoch: 5.83 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436251206890155		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.11436251206890155 | validation: 0.1373586186204282]
	TIME [epoch: 5.84 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11477951649033742		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.11477951649033742 | validation: 0.11330802320602618]
	TIME [epoch: 5.83 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11407107290671079		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.11407107290671079 | validation: 0.13269465917434045]
	TIME [epoch: 5.83 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11307962729000676		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.11307962729000676 | validation: 0.1180382521556036]
	TIME [epoch: 5.83 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11241843440138284		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.11241843440138284 | validation: 0.13964715694261973]
	TIME [epoch: 5.83 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11287416837968584		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.11287416837968584 | validation: 0.11265003042156119]
	TIME [epoch: 5.84 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.120275899309421		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.120275899309421 | validation: 0.15716274360311255]
	TIME [epoch: 5.84 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12498842283428176		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.12498842283428176 | validation: 0.10433678417548574]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1111.pth
	Model improved!!!
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12413359656520456		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.12413359656520456 | validation: 0.1370193985725394]
	TIME [epoch: 5.84 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11232426857092268		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.11232426857092268 | validation: 0.13840686639346475]
	TIME [epoch: 5.84 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11093002547889685		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.11093002547889685 | validation: 0.11741292587742183]
	TIME [epoch: 5.84 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11253373006875667		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.11253373006875667 | validation: 0.1345782178405328]
	TIME [epoch: 5.84 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11351373967654041		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.11351373967654041 | validation: 0.11826453011879111]
	TIME [epoch: 5.84 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340994946388486		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.11340994946388486 | validation: 0.1368246630891373]
	TIME [epoch: 5.83 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11328777416417353		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.11328777416417353 | validation: 0.11620425878181047]
	TIME [epoch: 5.83 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11301102685327909		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.11301102685327909 | validation: 0.1416212769968921]
	TIME [epoch: 5.84 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11832201229990438		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.11832201229990438 | validation: 0.10492358591190731]
	TIME [epoch: 5.83 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1201312755669084		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.1201312755669084 | validation: 0.1399368491516061]
	TIME [epoch: 5.84 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251921575245613		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.11251921575245613 | validation: 0.12791920148490668]
	TIME [epoch: 5.83 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10968875899120921		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.10968875899120921 | validation: 0.120452001677547]
	TIME [epoch: 5.83 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817376873198484		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.10817376873198484 | validation: 0.13441791541355058]
	TIME [epoch: 5.84 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11461989697363066		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.11461989697363066 | validation: 0.11742679950630826]
	TIME [epoch: 5.83 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492761998908595		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.11492761998908595 | validation: 0.14380479883347683]
	TIME [epoch: 5.83 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11631856399558979		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.11631856399558979 | validation: 0.11090699168669782]
	TIME [epoch: 5.83 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11947005622304041		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.11947005622304041 | validation: 0.1319484472172992]
	TIME [epoch: 5.83 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11129503781910455		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.11129503781910455 | validation: 0.1203039404399044]
	TIME [epoch: 5.84 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11195812067724269		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.11195812067724269 | validation: 0.13371632918434484]
	TIME [epoch: 5.83 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10867728095076244		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.10867728095076244 | validation: 0.1316891580429362]
	TIME [epoch: 5.83 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10843260256123927		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.10843260256123927 | validation: 0.11614053897718005]
	TIME [epoch: 5.83 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11038435927772115		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.11038435927772115 | validation: 0.13240012116127906]
	TIME [epoch: 5.83 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10816344208000757		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.10816344208000757 | validation: 0.1254347579108941]
	TIME [epoch: 5.83 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112132454045354		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.1112132454045354 | validation: 0.1378337707361353]
	TIME [epoch: 5.84 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10878316699722688		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.10878316699722688 | validation: 0.10557421688882754]
	TIME [epoch: 5.83 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12158035732536622		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.12158035732536622 | validation: 0.151851084411607]
	TIME [epoch: 5.83 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12156163558279469		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.12156163558279469 | validation: 0.11651275936894404]
	TIME [epoch: 5.83 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11486475243465506		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.11486475243465506 | validation: 0.12832511539844288]
	TIME [epoch: 5.83 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10775558338231317		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.10775558338231317 | validation: 0.12473363071519422]
	TIME [epoch: 5.84 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080735981650061		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.1080735981650061 | validation: 0.11514485919194312]
	TIME [epoch: 5.82 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10945964973957033		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.10945964973957033 | validation: 0.12637809102396944]
	TIME [epoch: 5.83 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10951028008425659		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.10951028008425659 | validation: 0.11405662021727268]
	TIME [epoch: 5.83 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501688687141638		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.11501688687141638 | validation: 0.13658175558866018]
	TIME [epoch: 5.83 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264335352366227		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.11264335352366227 | validation: 0.10866675704096765]
	TIME [epoch: 5.84 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1109970690547844		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.1109970690547844 | validation: 0.12910541650218982]
	TIME [epoch: 5.83 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1057558657971024		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.1057558657971024 | validation: 0.12258445333713293]
	TIME [epoch: 5.84 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10778842897782767		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.10778842897782767 | validation: 0.12998316347564895]
	TIME [epoch: 5.83 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937952098051108		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.10937952098051108 | validation: 0.11473269127511561]
	TIME [epoch: 5.83 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416803993746244		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.10416803993746244 | validation: 0.13484326038752031]
	TIME [epoch: 5.83 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10709576656100996		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.10709576656100996 | validation: 0.11547440136729475]
	TIME [epoch: 5.83 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11029294846020879		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.11029294846020879 | validation: 0.14374618848923168]
	TIME [epoch: 5.83 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152580970265847		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.1152580970265847 | validation: 0.10171336572647763]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1153.pth
	Model improved!!!
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11949110122566083		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.11949110122566083 | validation: 0.1302796780580248]
	TIME [epoch: 5.83 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10956683828543687		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.10956683828543687 | validation: 0.11624964280460222]
	TIME [epoch: 5.84 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10718139850997688		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.10718139850997688 | validation: 0.13288091047436504]
	TIME [epoch: 5.83 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1073507783674138		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.1073507783674138 | validation: 0.10571618748015146]
	TIME [epoch: 5.83 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1091090124713112		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.1091090124713112 | validation: 0.12963710374936982]
	TIME [epoch: 5.83 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10779152040422561		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.10779152040422561 | validation: 0.12124492060670876]
	TIME [epoch: 5.83 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10719777763074297		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.10719777763074297 | validation: 0.13201071716963408]
	TIME [epoch: 5.83 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594683907932353		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.10594683907932353 | validation: 0.11102670231919914]
	TIME [epoch: 5.83 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10831597999764797		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.10831597999764797 | validation: 0.13181015487777423]
	TIME [epoch: 5.83 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10927104092592081		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.10927104092592081 | validation: 0.11072671492738118]
	TIME [epoch: 5.83 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11291715503945775		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.11291715503945775 | validation: 0.13243941986393287]
	TIME [epoch: 5.82 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10905389422284507		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.10905389422284507 | validation: 0.11344389027033325]
	TIME [epoch: 5.84 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10399375773342386		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.10399375773342386 | validation: 0.11529355197411689]
	TIME [epoch: 5.83 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633843630213256		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.10633843630213256 | validation: 0.12445458663360523]
	TIME [epoch: 5.82 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566884006217904		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.10566884006217904 | validation: 0.1159508707252593]
	TIME [epoch: 5.82 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10377112011875764		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.10377112011875764 | validation: 0.12153038658859679]
	TIME [epoch: 5.83 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10103914311884112		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.10103914311884112 | validation: 0.11012621006154424]
	TIME [epoch: 5.83 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10653560951413399		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.10653560951413399 | validation: 0.1339901135139537]
	TIME [epoch: 5.83 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10977685357753504		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.10977685357753504 | validation: 0.10579306456542761]
	TIME [epoch: 5.83 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.114786688993305		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.114786688993305 | validation: 0.13566909491584808]
	TIME [epoch: 5.83 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11130781152068597		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.11130781152068597 | validation: 0.10465997936610938]
	TIME [epoch: 5.84 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515907362281603		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.10515907362281603 | validation: 0.11820904295458666]
	TIME [epoch: 5.84 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10559863567371007		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.10559863567371007 | validation: 0.1206060988606354]
	TIME [epoch: 5.83 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400414985593713		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.10400414985593713 | validation: 0.11345008504168326]
	TIME [epoch: 5.83 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10294786307201147		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.10294786307201147 | validation: 0.1295242451540938]
	TIME [epoch: 5.83 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10668272817448755		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.10668272817448755 | validation: 0.10877165906431432]
	TIME [epoch: 5.83 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606432608452394		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.10606432608452394 | validation: 0.1338335314016195]
	TIME [epoch: 5.83 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11119550663937541		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.11119550663937541 | validation: 0.10915184561218841]
	TIME [epoch: 5.83 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11084319154262762		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.11084319154262762 | validation: 0.14487974638464518]
	TIME [epoch: 5.83 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875812908825254		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.10875812908825254 | validation: 0.11071851863211567]
	TIME [epoch: 5.83 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10554923358748845		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.10554923358748845 | validation: 0.12821706586211895]
	TIME [epoch: 5.83 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10030984102589491		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.10030984102589491 | validation: 0.13256570801003478]
	TIME [epoch: 5.83 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361371337478438		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.10361371337478438 | validation: 0.10590804025593908]
	TIME [epoch: 5.84 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10823642816325173		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.10823642816325173 | validation: 0.13422846211080394]
	TIME [epoch: 5.83 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929920475934164		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.10929920475934164 | validation: 0.1039784672008119]
	TIME [epoch: 5.83 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10384376407067375		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.10384376407067375 | validation: 0.11623457759883671]
	TIME [epoch: 5.84 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10216755099887107		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.10216755099887107 | validation: 0.12180473083956729]
	TIME [epoch: 5.83 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275271562752149		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.10275271562752149 | validation: 0.11578636342348864]
	TIME [epoch: 5.84 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005935011453391		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.1005935011453391 | validation: 0.1187314358622948]
	TIME [epoch: 5.83 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145197883286779		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.10145197883286779 | validation: 0.10742809040763227]
	TIME [epoch: 5.83 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093522501269471		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.10093522501269471 | validation: 0.12508436548757412]
	TIME [epoch: 5.83 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10345781542879724		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.10345781542879724 | validation: 0.10628862837014513]
	TIME [epoch: 5.82 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10293991063739796		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.10293991063739796 | validation: 0.12399785242109332]
	TIME [epoch: 5.84 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10153624705408831		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.10153624705408831 | validation: 0.11169299602445298]
	TIME [epoch: 5.86 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10272785142416471		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.10272785142416471 | validation: 0.13387217510177682]
	TIME [epoch: 5.85 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937163867721722		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.10937163867721722 | validation: 0.09810626359288657]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11311722070404492		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.11311722070404492 | validation: 0.12218253316237981]
	TIME [epoch: 5.83 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10359519519084572		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.10359519519084572 | validation: 0.10903255517733906]
	TIME [epoch: 5.84 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09926987844857023		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.09926987844857023 | validation: 0.10993949440671771]
	TIME [epoch: 5.84 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09995881204345511		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.09995881204345511 | validation: 0.11943617482173839]
	TIME [epoch: 5.83 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10054482361657478		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.10054482361657478 | validation: 0.10917490868430371]
	TIME [epoch: 5.83 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09965279185084161		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.09965279185084161 | validation: 0.11572439333888924]
	TIME [epoch: 5.82 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09839695430294967		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.09839695430294967 | validation: 0.10987483530348136]
	TIME [epoch: 5.84 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022868273504666		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.1022868273504666 | validation: 0.13385684630710667]
	TIME [epoch: 5.84 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1023876205568818		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.1023876205568818 | validation: 0.10377824113085594]
	TIME [epoch: 5.83 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687876271905061		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.10687876271905061 | validation: 0.12807806432570798]
	TIME [epoch: 5.84 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10486778476562653		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.10486778476562653 | validation: 0.11160504777852731]
	TIME [epoch: 5.83 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012427791088226		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.1012427791088226 | validation: 0.12162930246619519]
	TIME [epoch: 5.84 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752790163024429		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.09752790163024429 | validation: 0.10971782974826523]
	TIME [epoch: 5.83 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152082784460537		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.10152082784460537 | validation: 0.11868848519383356]
	TIME [epoch: 5.83 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0992665738429889		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.0992665738429889 | validation: 0.11578299652062035]
	TIME [epoch: 5.84 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097737426139082		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.097737426139082 | validation: 0.11136673811707719]
	TIME [epoch: 5.83 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081329774745623		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.10081329774745623 | validation: 0.12448044297327994]
	TIME [epoch: 5.84 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018068357474252		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.1018068357474252 | validation: 0.10586844722984119]
	TIME [epoch: 5.83 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532910094569321		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.10532910094569321 | validation: 0.14658104106861375]
	TIME [epoch: 5.83 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11141847563332956		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.11141847563332956 | validation: 0.1010176840058271]
	TIME [epoch: 5.83 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10705979685617666		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.10705979685617666 | validation: 0.11118843378425153]
	TIME [epoch: 5.83 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09923904004354889		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.09923904004354889 | validation: 0.11947046967967685]
	TIME [epoch: 5.84 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10230430021896152		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.10230430021896152 | validation: 0.104744078877804]
	TIME [epoch: 5.83 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10150574456264665		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.10150574456264665 | validation: 0.12000531308751122]
	TIME [epoch: 5.83 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09837747078857666		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.09837747078857666 | validation: 0.11244131050945844]
	TIME [epoch: 5.84 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10104104316514484		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.10104104316514484 | validation: 0.12388697369874153]
	TIME [epoch: 5.84 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09991208613392819		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.09991208613392819 | validation: 0.10274813069699658]
	TIME [epoch: 5.84 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097011050994761		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.10097011050994761 | validation: 0.11712617905874739]
	TIME [epoch: 5.84 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0981306866098155		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0981306866098155 | validation: 0.1088532835710603]
	TIME [epoch: 5.83 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09752331119134544		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.09752331119134544 | validation: 0.11822626878690633]
	TIME [epoch: 5.84 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09563891718787002		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.09563891718787002 | validation: 0.10997421359467757]
	TIME [epoch: 5.83 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0975857842918318		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.0975857842918318 | validation: 0.10889581827541948]
	TIME [epoch: 5.84 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717361332583484		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.09717361332583484 | validation: 0.11564643607439218]
	TIME [epoch: 5.83 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623633737322078		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.09623633737322078 | validation: 0.11179314591724487]
	TIME [epoch: 5.83 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0948426079290004		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.0948426079290004 | validation: 0.11822568763933852]
	TIME [epoch: 5.83 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576378245314918		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.09576378245314918 | validation: 0.11192560249464761]
	TIME [epoch: 5.83 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09793618139674422		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.09793618139674422 | validation: 0.13300615791697964]
	TIME [epoch: 5.84 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042724268543516		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.1042724268543516 | validation: 0.09579987153605736]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1237.pth
	Model improved!!!
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463744320891099		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.11463744320891099 | validation: 0.113886456660777]
	TIME [epoch: 5.84 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681136263864641		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.09681136263864641 | validation: 0.11739318071210794]
	TIME [epoch: 5.84 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09823911622841709		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.09823911622841709 | validation: 0.1002772873280706]
	TIME [epoch: 5.83 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09862831462846393		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.09862831462846393 | validation: 0.12217128520743059]
	TIME [epoch: 5.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09727579578509048		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.09727579578509048 | validation: 0.11279182675390098]
	TIME [epoch: 5.84 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966628446658595		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.0966628446658595 | validation: 0.10634310997757182]
	TIME [epoch: 5.84 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09409104015130314		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.09409104015130314 | validation: 0.11340747798154945]
	TIME [epoch: 5.84 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09644194698422638		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.09644194698422638 | validation: 0.11309616079919126]
	TIME [epoch: 5.84 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09680100005123404		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.09680100005123404 | validation: 0.10272952352239328]
	TIME [epoch: 5.84 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09612817773113312		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.09612817773113312 | validation: 0.1203883560597968]
	TIME [epoch: 5.84 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997816028811599		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.0997816028811599 | validation: 0.09759381041035614]
	TIME [epoch: 5.84 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09791671559086079		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.09791671559086079 | validation: 0.1281635407008823]
	TIME [epoch: 5.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10156082709845991		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.10156082709845991 | validation: 0.10788120392601286]
	TIME [epoch: 5.84 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09808657006059629		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.09808657006059629 | validation: 0.10942250050085961]
	TIME [epoch: 5.84 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09676442559714335		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.09676442559714335 | validation: 0.11661812321518346]
	TIME [epoch: 5.84 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09333903658304481		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.09333903658304481 | validation: 0.10657001920463012]
	TIME [epoch: 5.84 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09570552286811371		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.09570552286811371 | validation: 0.12448441553615491]
	TIME [epoch: 5.83 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09558840554640949		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.09558840554640949 | validation: 0.09990565789779751]
	TIME [epoch: 5.84 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09800222513699028		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.09800222513699028 | validation: 0.122464032025356]
	TIME [epoch: 5.84 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09778343317738612		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.09778343317738612 | validation: 0.10444730963747682]
	TIME [epoch: 5.84 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09602615823661421		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.09602615823661421 | validation: 0.11777641571073368]
	TIME [epoch: 5.83 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321729546971254		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.09321729546971254 | validation: 0.11104216476622485]
	TIME [epoch: 5.84 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09314372708669306		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.09314372708669306 | validation: 0.10635914559007008]
	TIME [epoch: 5.83 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09293817173847764		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.09293817173847764 | validation: 0.10481319289038456]
	TIME [epoch: 5.84 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09479914351014589		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.09479914351014589 | validation: 0.11701560347584268]
	TIME [epoch: 5.84 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09483238470114631		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.09483238470114631 | validation: 0.10813610374647792]
	TIME [epoch: 5.84 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977875736987052		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.0977875736987052 | validation: 0.12785465886587344]
	TIME [epoch: 5.84 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994649014154136		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.09994649014154136 | validation: 0.09645332767278382]
	TIME [epoch: 5.84 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10120401897047604		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.10120401897047604 | validation: 0.11119990805358751]
	TIME [epoch: 5.83 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391679354263513		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.09391679354263513 | validation: 0.10707306125136169]
	TIME [epoch: 5.84 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09163331339507683		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.09163331339507683 | validation: 0.10758435347397409]
	TIME [epoch: 5.84 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548885701704862		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.09548885701704862 | validation: 0.10945567533741768]
	TIME [epoch: 5.83 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446847734078957		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.09446847734078957 | validation: 0.10937521247984959]
	TIME [epoch: 5.83 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09344069806705121		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.09344069806705121 | validation: 0.11098404307939586]
	TIME [epoch: 5.84 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09331171811005923		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.09331171811005923 | validation: 0.1035522402066253]
	TIME [epoch: 5.83 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09171958485880158		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.09171958485880158 | validation: 0.10465282526821854]
	TIME [epoch: 5.83 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431079294292011		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.09431079294292011 | validation: 0.10668446317951628]
	TIME [epoch: 5.82 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09349501861319326		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.09349501861319326 | validation: 0.11302514989215409]
	TIME [epoch: 5.83 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383086497641432		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.09383086497641432 | validation: 0.11883349040772938]
	TIME [epoch: 5.83 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09187145631623996		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.09187145631623996 | validation: 0.09634809767302228]
	TIME [epoch: 5.84 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09720983590527793		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.09720983590527793 | validation: 0.1313305517281777]
	TIME [epoch: 5.83 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10251209510035518		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.10251209510035518 | validation: 0.09833214642173137]
	TIME [epoch: 5.83 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641841405153988		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.09641841405153988 | validation: 0.10887189780996279]
	TIME [epoch: 5.83 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09419796760689454		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.09419796760689454 | validation: 0.12460246958326422]
	TIME [epoch: 5.83 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509430092321566		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.09509430092321566 | validation: 0.10331251342885138]
	TIME [epoch: 5.83 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09462081025320701		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.09462081025320701 | validation: 0.11046269949774633]
	TIME [epoch: 5.84 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917112590971295		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0917112590971295 | validation: 0.11197557581126515]
	TIME [epoch: 5.82 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140628187750634		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.09140628187750634 | validation: 0.10932820755956561]
	TIME [epoch: 5.83 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09292245655789999		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.09292245655789999 | validation: 0.11663572193852148]
	TIME [epoch: 5.82 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942864078772546		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.0942864078772546 | validation: 0.09906770642209484]
	TIME [epoch: 5.84 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0944550709849259		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.0944550709849259 | validation: 0.12997580207278278]
	TIME [epoch: 5.83 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155629197529412		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.10155629197529412 | validation: 0.10061125109580288]
	TIME [epoch: 5.84 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09435001943661639		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.09435001943661639 | validation: 0.11187287239628485]
	TIME [epoch: 5.83 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09187485266613608		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.09187485266613608 | validation: 0.11707337588864584]
	TIME [epoch: 5.84 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09435458106887872		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.09435458106887872 | validation: 0.09860072875839604]
	TIME [epoch: 5.83 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304262111459584		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.09304262111459584 | validation: 0.11220017690660461]
	TIME [epoch: 5.84 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09278521527661117		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.09278521527661117 | validation: 0.11256380788304579]
	TIME [epoch: 5.83 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542454034964219		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.09542454034964219 | validation: 0.11145107393569975]
	TIME [epoch: 5.83 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09410072151576357		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.09410072151576357 | validation: 0.102528971123694]
	TIME [epoch: 5.83 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317116856873486		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.09317116856873486 | validation: 0.12151409203395147]
	TIME [epoch: 5.85 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09412232121372494		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.09412232121372494 | validation: 0.09810123409271407]
	TIME [epoch: 5.83 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09361180182921076		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.09361180182921076 | validation: 0.11324404712237651]
	TIME [epoch: 5.83 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09130197202872108		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.09130197202872108 | validation: 0.11393870896465788]
	TIME [epoch: 5.83 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087074793120728		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.09087074793120728 | validation: 0.10920065186002406]
	TIME [epoch: 5.84 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09406559376227346		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.09406559376227346 | validation: 0.12321911533729323]
	TIME [epoch: 5.83 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09374558346444936		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.09374558346444936 | validation: 0.10097803538163713]
	TIME [epoch: 5.83 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448948511553437		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.09448948511553437 | validation: 0.12029967286509931]
	TIME [epoch: 5.83 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09434399589203013		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.09434399589203013 | validation: 0.09662637463385652]
	TIME [epoch: 5.83 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09300325595080843		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.09300325595080843 | validation: 0.10814995094718403]
	TIME [epoch: 5.84 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09060126905915553		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.09060126905915553 | validation: 0.10633883362645187]
	TIME [epoch: 5.83 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909185653831085		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0909185653831085 | validation: 0.09934102997010873]
	TIME [epoch: 5.83 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955942961766723		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.08955942961766723 | validation: 0.10898713731983481]
	TIME [epoch: 5.83 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172478765277775		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.09172478765277775 | validation: 0.1003268367814138]
	TIME [epoch: 5.83 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08838843790175854		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.08838843790175854 | validation: 0.11335985066649064]
	TIME [epoch: 5.83 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08838184598662714		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.08838184598662714 | validation: 0.1008844625467593]
	TIME [epoch: 5.83 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08950437101310954		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.08950437101310954 | validation: 0.11031479321246847]
	TIME [epoch: 5.84 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08950259195279925		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.08950259195279925 | validation: 0.10581326488841399]
	TIME [epoch: 5.83 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08851443471595541		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.08851443471595541 | validation: 0.11782966973103504]
	TIME [epoch: 5.83 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09169905157567035		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.09169905157567035 | validation: 0.1022846957174985]
	TIME [epoch: 5.83 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09214866677026563		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.09214866677026563 | validation: 0.11345438206931086]
	TIME [epoch: 5.82 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09076854909429852		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.09076854909429852 | validation: 0.09888062588023586]
	TIME [epoch: 5.84 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199210734718208		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.09199210734718208 | validation: 0.10841314945151653]
	TIME [epoch: 5.83 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137269807438887		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.09137269807438887 | validation: 0.10357774045182029]
	TIME [epoch: 5.83 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08992525796422687		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.08992525796422687 | validation: 0.11030370813294454]
	TIME [epoch: 5.83 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09157218710751823		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.09157218710751823 | validation: 0.09473261659475402]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1322.pth
	Model improved!!!
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09387761700385003		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.09387761700385003 | validation: 0.11749605627465685]
	TIME [epoch: 5.83 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09362114438933147		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.09362114438933147 | validation: 0.09951108959501187]
	TIME [epoch: 5.83 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09278285738045294		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.09278285738045294 | validation: 0.11487659386154628]
	TIME [epoch: 5.83 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688970493478136		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.08688970493478136 | validation: 0.09985687404581468]
	TIME [epoch: 5.83 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0899154894602932		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.0899154894602932 | validation: 0.11010524931997932]
	TIME [epoch: 5.83 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905198074697551		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.08905198074697551 | validation: 0.10473626791313739]
	TIME [epoch: 5.84 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725887989364452		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.08725887989364452 | validation: 0.10681450148873416]
	TIME [epoch: 5.83 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799690183971666		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.08799690183971666 | validation: 0.10290268060464883]
	TIME [epoch: 5.83 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08899862733241673		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.08899862733241673 | validation: 0.10558876837641672]
	TIME [epoch: 5.83 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09135641993045444		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.09135641993045444 | validation: 0.09777307017768255]
	TIME [epoch: 5.83 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035931390591559		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.09035931390591559 | validation: 0.1132156985565771]
	TIME [epoch: 5.83 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09206234150009658		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.09206234150009658 | validation: 0.09078855025004029]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1334.pth
	Model improved!!!
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997858873344651		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.0997858873344651 | validation: 0.120785526933564]
	TIME [epoch: 5.83 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09090284071053509		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.09090284071053509 | validation: 0.10344930543723592]
	TIME [epoch: 5.83 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09051370637852908		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.09051370637852908 | validation: 0.09906511197795709]
	TIME [epoch: 5.83 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09107043010765888		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.09107043010765888 | validation: 0.11579755466372217]
	TIME [epoch: 5.83 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861980696665192		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.08861980696665192 | validation: 0.09954940914065268]
	TIME [epoch: 5.83 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714643765641382		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.08714643765641382 | validation: 0.1129420443422424]
	TIME [epoch: 5.83 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09179493715227555		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.09179493715227555 | validation: 0.10965665072023245]
	TIME [epoch: 5.82 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08884324845058295		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.08884324845058295 | validation: 0.09529104816135951]
	TIME [epoch: 5.83 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08900409821513343		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.08900409821513343 | validation: 0.10604097700199772]
	TIME [epoch: 5.83 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813020880580785		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.08813020880580785 | validation: 0.09850477515207302]
	TIME [epoch: 5.83 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653213263194552		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.08653213263194552 | validation: 0.09993461683244892]
	TIME [epoch: 5.82 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842642058473374		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.08842642058473374 | validation: 0.10544243234132078]
	TIME [epoch: 5.83 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08606044482231621		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.08606044482231621 | validation: 0.09941489468015098]
	TIME [epoch: 5.83 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877899993882347		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.0877899993882347 | validation: 0.09917485346956162]
	TIME [epoch: 5.84 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08834484983035906		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.08834484983035906 | validation: 0.10110448842054945]
	TIME [epoch: 5.83 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08624127056603964		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.08624127056603964 | validation: 0.10068546224859364]
	TIME [epoch: 5.83 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714466673573788		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.08714466673573788 | validation: 0.11329053079538975]
	TIME [epoch: 5.84 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09052997604851803		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.09052997604851803 | validation: 0.09518152774455528]
	TIME [epoch: 5.83 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09152529101436148		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.09152529101436148 | validation: 0.11540545735054089]
	TIME [epoch: 5.84 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08876910378435982		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.08876910378435982 | validation: 0.10184731094717439]
	TIME [epoch: 5.83 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865169539639511		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.0865169539639511 | validation: 0.10395144034872494]
	TIME [epoch: 5.83 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778278813398376		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.08778278813398376 | validation: 0.1104255376228846]
	TIME [epoch: 5.83 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08980350036135214		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.08980350036135214 | validation: 0.09725719891092967]
	TIME [epoch: 5.83 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660556681600291		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.08660556681600291 | validation: 0.11287319338619065]
	TIME [epoch: 5.84 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054123886287561		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.09054123886287561 | validation: 0.09907215266546837]
	TIME [epoch: 5.83 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08858897149841828		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.08858897149841828 | validation: 0.10905938257403053]
	TIME [epoch: 5.83 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08512842917516857		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.08512842917516857 | validation: 0.11088700150087383]
	TIME [epoch: 5.83 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08730923518519823		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.08730923518519823 | validation: 0.08921488079761312]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1362.pth
	Model improved!!!
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545717116547724		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.08545717116547724 | validation: 0.10335684321893744]
	TIME [epoch: 5.84 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08834586928953925		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.08834586928953925 | validation: 0.09960200771991437]
	TIME [epoch: 5.83 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08810134116815625		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.08810134116815625 | validation: 0.11250299177213202]
	TIME [epoch: 5.82 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08699321431125867		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.08699321431125867 | validation: 0.09671238545030669]
	TIME [epoch: 5.82 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08825122895875769		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.08825122895875769 | validation: 0.10080419200765381]
	TIME [epoch: 5.84 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862499634512609		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.0862499634512609 | validation: 0.09775492123081773]
	TIME [epoch: 5.83 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0837910478253861		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.0837910478253861 | validation: 0.10757113213834357]
	TIME [epoch: 5.83 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08606893796410266		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.08606893796410266 | validation: 0.09841129369100934]
	TIME [epoch: 5.82 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08656301826073054		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.08656301826073054 | validation: 0.11046181835607682]
	TIME [epoch: 5.83 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328595321895574		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.08328595321895574 | validation: 0.09954404471121416]
	TIME [epoch: 5.83 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442611068622256		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.08442611068622256 | validation: 0.10285778602244716]
	TIME [epoch: 5.83 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08772292548932754		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.08772292548932754 | validation: 0.10456185299686599]
	TIME [epoch: 5.83 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08547140904352968		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.08547140904352968 | validation: 0.10554528285293396]
	TIME [epoch: 5.83 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08605616796799201		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.08605616796799201 | validation: 0.10762837168426476]
	TIME [epoch: 5.82 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08589221338156516		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.08589221338156516 | validation: 0.09458502601091223]
	TIME [epoch: 5.83 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0933021991769412		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.0933021991769412 | validation: 0.1122241739393281]
	TIME [epoch: 5.82 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636296427485718		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.08636296427485718 | validation: 0.10027910328480974]
	TIME [epoch: 5.84 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08831269695035769		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.08831269695035769 | validation: 0.10401122138053645]
	TIME [epoch: 5.83 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08424438061937348		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.08424438061937348 | validation: 0.10273471365170522]
	TIME [epoch: 5.82 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08667029685454346		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.08667029685454346 | validation: 0.10033242918317356]
	TIME [epoch: 5.83 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771768085799998		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.08771768085799998 | validation: 0.10703267220244016]
	TIME [epoch: 5.83 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0836392225703624		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.0836392225703624 | validation: 0.09474250742398013]
	TIME [epoch: 5.83 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493935685177886		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.08493935685177886 | validation: 0.10668191582811715]
	TIME [epoch: 5.82 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08404430732010532		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.08404430732010532 | validation: 0.09405387725000423]
	TIME [epoch: 5.83 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0866925186225857		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.0866925186225857 | validation: 0.10946229429854336]
	TIME [epoch: 5.83 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608819288921225		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.08608819288921225 | validation: 0.10304729408008395]
	TIME [epoch: 5.82 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0863827996337863		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.0863827996337863 | validation: 0.10486794960535073]
	TIME [epoch: 5.83 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08578878004113914		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.08578878004113914 | validation: 0.0920097316988453]
	TIME [epoch: 5.83 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08701036018020274		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.08701036018020274 | validation: 0.10002251340174287]
	TIME [epoch: 5.83 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0828381415867467		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.0828381415867467 | validation: 0.10756604430069439]
	TIME [epoch: 5.83 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870434367510376		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.0870434367510376 | validation: 0.10602954360271527]
	TIME [epoch: 5.83 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08500277549524345		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.08500277549524345 | validation: 0.09510717842869539]
	TIME [epoch: 5.83 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08612275937841166		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.08612275937841166 | validation: 0.10809249794358014]
	TIME [epoch: 5.83 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08806197545978371		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.08806197545978371 | validation: 0.09305663397141467]
	TIME [epoch: 5.83 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08340460497114574		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.08340460497114574 | validation: 0.11082282644317515]
	TIME [epoch: 5.83 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08819536384709074		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.08819536384709074 | validation: 0.09810678316923403]
	TIME [epoch: 5.82 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550498734379748		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.08550498734379748 | validation: 0.09607385796601178]
	TIME [epoch: 5.84 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835695872142364		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.0835695872142364 | validation: 0.10674696597131805]
	TIME [epoch: 5.83 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845631378654376		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.0845631378654376 | validation: 0.09214110685212117]
	TIME [epoch: 5.83 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666181298380105		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.08666181298380105 | validation: 0.10709469276886407]
	TIME [epoch: 5.83 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08618315986327246		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.08618315986327246 | validation: 0.10936431375603646]
	TIME [epoch: 5.83 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08505548418723842		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.08505548418723842 | validation: 0.09370808783947789]
	TIME [epoch: 5.83 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711824838385972		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.08711824838385972 | validation: 0.1042816367474718]
	TIME [epoch: 5.83 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362824014296287		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.08362824014296287 | validation: 0.09568716056827108]
	TIME [epoch: 5.83 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08366338886506945		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.08366338886506945 | validation: 0.10001379550090453]
	TIME [epoch: 5.83 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08244233387661044		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.08244233387661044 | validation: 0.10455083230597761]
	TIME [epoch: 5.83 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08511527991631575		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.08511527991631575 | validation: 0.09558671101751394]
	TIME [epoch: 5.84 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692082526193248		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.08692082526193248 | validation: 0.10565501978071921]
	TIME [epoch: 5.85 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08769131106832372		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.08769131106832372 | validation: 0.09683059535407756]
	TIME [epoch: 5.83 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08412308133959544		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.08412308133959544 | validation: 0.10484080829631957]
	TIME [epoch: 5.83 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0844010962730741		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.0844010962730741 | validation: 0.09555065017039159]
	TIME [epoch: 5.83 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.082955772059161		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.082955772059161 | validation: 0.10080100590279505]
	TIME [epoch: 5.83 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08483014814319939		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.08483014814319939 | validation: 0.10277088036589335]
	TIME [epoch: 5.83 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397564812025218		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.08397564812025218 | validation: 0.09951186314316869]
	TIME [epoch: 5.83 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08383371210972808		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.08383371210972808 | validation: 0.10929462542385543]
	TIME [epoch: 5.83 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08394145655876756		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.08394145655876756 | validation: 0.09343796311771385]
	TIME [epoch: 5.83 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710347500166336		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.08710347500166336 | validation: 0.10643962033288412]
	TIME [epoch: 5.83 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08384562835543839		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.08384562835543839 | validation: 0.10291063858898926]
	TIME [epoch: 5.82 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359743892720087		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.08359743892720087 | validation: 0.0967221250173364]
	TIME [epoch: 5.83 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846142135504047		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.0846142135504047 | validation: 0.1021484374956188]
	TIME [epoch: 5.82 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08360564782963505		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.08360564782963505 | validation: 0.09854276650186407]
	TIME [epoch: 5.83 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345053850968565		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.08345053850968565 | validation: 0.11044986683954552]
	TIME [epoch: 5.83 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306876548386076		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.08306876548386076 | validation: 0.09653917034351948]
	TIME [epoch: 5.82 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08230705113432916		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.08230705113432916 | validation: 0.09927874773229733]
	TIME [epoch: 5.83 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180280869067592		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.08180280869067592 | validation: 0.09636816037488191]
	TIME [epoch: 5.82 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08475388261718805		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.08475388261718805 | validation: 0.10621891681810017]
	TIME [epoch: 5.82 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115002136237458		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.08115002136237458 | validation: 0.09931016098480464]
	TIME [epoch: 5.83 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08254772177753222		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.08254772177753222 | validation: 0.09832744255817316]
	TIME [epoch: 5.83 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435358388728732		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.08435358388728732 | validation: 0.09393007889688502]
	TIME [epoch: 5.82 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142723871510665		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.08142723871510665 | validation: 0.10256378403460512]
	TIME [epoch: 5.83 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306112964520528		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.08306112964520528 | validation: 0.09894330084664325]
	TIME [epoch: 5.83 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817683897702506		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.0817683897702506 | validation: 0.1136382602727724]
	TIME [epoch: 5.82 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08395640692088666		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.08395640692088666 | validation: 0.0867936158022819]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1435.pth
	Model improved!!!
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896543098483722		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.0896543098483722 | validation: 0.10181938461477134]
	TIME [epoch: 5.79 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269874957183618		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.08269874957183618 | validation: 0.10306772388857813]
	TIME [epoch: 5.78 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08173199110443957		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.08173199110443957 | validation: 0.09526051546753389]
	TIME [epoch: 5.8 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083221515480242		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.08083221515480242 | validation: 0.10012497360310413]
	TIME [epoch: 5.79 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128121191072447		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.08128121191072447 | validation: 0.10599604174917095]
	TIME [epoch: 5.8 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256917312998523		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.08256917312998523 | validation: 0.09035422748312892]
	TIME [epoch: 5.8 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08208138257361913		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.08208138257361913 | validation: 0.09453610632057435]
	TIME [epoch: 5.82 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08233106701632024		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.08233106701632024 | validation: 0.099806965313611]
	TIME [epoch: 5.83 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08013348988100986		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.08013348988100986 | validation: 0.09230570009642747]
	TIME [epoch: 5.82 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08393108688172735		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.08393108688172735 | validation: 0.10070541506637119]
	TIME [epoch: 5.83 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128155850748427		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.08128155850748427 | validation: 0.10741879211053705]
	TIME [epoch: 5.83 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245924236092449		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.08245924236092449 | validation: 0.09757813345158839]
	TIME [epoch: 5.83 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193205264743261		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.08193205264743261 | validation: 0.10183341702486531]
	TIME [epoch: 5.83 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815089070055939		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.0815089070055939 | validation: 0.09646496741507983]
	TIME [epoch: 5.83 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115964543295193		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.08115964543295193 | validation: 0.09696795937437691]
	TIME [epoch: 5.82 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0799180176397633		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.0799180176397633 | validation: 0.09298897495455989]
	TIME [epoch: 5.82 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811523110508025		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.0811523110508025 | validation: 0.09734876071223603]
	TIME [epoch: 5.82 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257264120499176		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.08257264120499176 | validation: 0.08884298783938542]
	TIME [epoch: 5.82 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139838474533873		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.08139838474533873 | validation: 0.09937496249911949]
	TIME [epoch: 5.81 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243190190846375		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.08243190190846375 | validation: 0.09080154285627401]
	TIME [epoch: 5.82 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08276851058418683		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.08276851058418683 | validation: 0.09609334609548692]
	TIME [epoch: 5.82 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954934511589665		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.07954934511589665 | validation: 0.09131928768264723]
	TIME [epoch: 5.81 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08030045743690127		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.08030045743690127 | validation: 0.09496123718793856]
	TIME [epoch: 5.81 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935697546616556		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.07935697546616556 | validation: 0.10163943567053871]
	TIME [epoch: 5.81 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08143152676693718		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.08143152676693718 | validation: 0.09637479292290814]
	TIME [epoch: 5.81 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0820317330227321		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.0820317330227321 | validation: 0.1089794431826451]
	TIME [epoch: 5.82 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913053501166097		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.07913053501166097 | validation: 0.09448920744882466]
	TIME [epoch: 5.81 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827375573314129		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.0827375573314129 | validation: 0.10244606277368601]
	TIME [epoch: 5.82 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081279117023718		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.08081279117023718 | validation: 0.10325533990564868]
	TIME [epoch: 5.81 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08168142122964271		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.08168142122964271 | validation: 0.09559959785248676]
	TIME [epoch: 5.82 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08025751802803122		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.08025751802803122 | validation: 0.1043777542446799]
	TIME [epoch: 5.81 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07984135041602175		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.07984135041602175 | validation: 0.10594106287906904]
	TIME [epoch: 5.81 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08022761281534498		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.08022761281534498 | validation: 0.08706436092286678]
	TIME [epoch: 5.81 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08530551698215369		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.08530551698215369 | validation: 0.09693084993183348]
	TIME [epoch: 5.82 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936013560374248		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.07936013560374248 | validation: 0.0934473735037391]
	TIME [epoch: 5.82 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08181113660537996		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.08181113660537996 | validation: 0.09905841249385436]
	TIME [epoch: 5.82 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935792718952199		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.07935792718952199 | validation: 0.08958247675095306]
	TIME [epoch: 5.82 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103780066758998		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.08103780066758998 | validation: 0.1062460906720201]
	TIME [epoch: 5.81 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08402227477750479		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.08402227477750479 | validation: 0.09216264273517288]
	TIME [epoch: 5.82 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810423048374922		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.0810423048374922 | validation: 0.09699774656922275]
	TIME [epoch: 5.81 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08020485340670046		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.08020485340670046 | validation: 0.09668376301637727]
	TIME [epoch: 5.82 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928632755000112		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.07928632755000112 | validation: 0.09593184578073999]
	TIME [epoch: 5.82 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951745756768895		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.07951745756768895 | validation: 0.09662573761838192]
	TIME [epoch: 5.82 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040143253510813		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.08040143253510813 | validation: 0.11175850908346946]
	TIME [epoch: 5.82 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101031604318099		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.08101031604318099 | validation: 0.092503688978489]
	TIME [epoch: 5.82 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07899035803615716		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.07899035803615716 | validation: 0.0946486230468644]
	TIME [epoch: 5.82 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101484175682738		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.08101484175682738 | validation: 0.10471161219107326]
	TIME [epoch: 5.81 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08244973334083731		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.08244973334083731 | validation: 0.08758012456653405]
	TIME [epoch: 5.81 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077639733464746		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.08077639733464746 | validation: 0.10349304491621028]
	TIME [epoch: 5.81 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08028510923047655		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.08028510923047655 | validation: 0.10515858170631608]
	TIME [epoch: 5.81 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07987470363624223		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.07987470363624223 | validation: 0.09490107208119578]
	TIME [epoch: 5.81 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942750703554957		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.07942750703554957 | validation: 0.104624914232451]
	TIME [epoch: 5.81 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819149326747557		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.07819149326747557 | validation: 0.09165153845335916]
	TIME [epoch: 5.82 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07918950489481243		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.07918950489481243 | validation: 0.09829026539454168]
	TIME [epoch: 5.81 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710558269494765		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.07710558269494765 | validation: 0.09699299547319563]
	TIME [epoch: 5.81 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998042326961177		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.07998042326961177 | validation: 0.09277549533679241]
	TIME [epoch: 5.81 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980648281979782		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.07980648281979782 | validation: 0.09771700680841675]
	TIME [epoch: 5.81 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760109016076391		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.07760109016076391 | validation: 0.09301093052436553]
	TIME [epoch: 5.8 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188843609254512		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.08188843609254512 | validation: 0.1081020267012685]
	TIME [epoch: 5.82 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08058371588378492		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.08058371588378492 | validation: 0.09874295773985603]
	TIME [epoch: 5.81 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07833048777717483		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.07833048777717483 | validation: 0.09511180801837651]
	TIME [epoch: 5.82 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07801779870153765		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.07801779870153765 | validation: 0.10703745521769199]
	TIME [epoch: 5.82 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815308969302551		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.0815308969302551 | validation: 0.08824287676665493]
	TIME [epoch: 5.8 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07781848869400754		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.07781848869400754 | validation: 0.09192528809386108]
	TIME [epoch: 5.8 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632318835395789		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.07632318835395789 | validation: 0.09313117766066246]
	TIME [epoch: 5.81 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07957357674375913		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.07957357674375913 | validation: 0.09882458061294865]
	TIME [epoch: 5.83 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950679575323256		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.07950679575323256 | validation: 0.10223009349279205]
	TIME [epoch: 5.83 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07783555852328236		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.07783555852328236 | validation: 0.09817215491051197]
	TIME [epoch: 5.82 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837975941073119		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.07837975941073119 | validation: 0.09174247128077435]
	TIME [epoch: 5.84 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075133904125269		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.08075133904125269 | validation: 0.10384872923390885]
	TIME [epoch: 5.83 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08296902252316414		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.08296902252316414 | validation: 0.09170418906613226]
	TIME [epoch: 5.84 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785600376604912		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.0785600376604912 | validation: 0.09211696497162009]
	TIME [epoch: 5.84 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764377489769995		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.0764377489769995 | validation: 0.104548129305927]
	TIME [epoch: 5.83 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965301321739993		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.07965301321739993 | validation: 0.09006712761814972]
	TIME [epoch: 5.83 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954457347358712		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.07954457347358712 | validation: 0.09500339550401532]
	TIME [epoch: 5.83 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771128487633642		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.0771128487633642 | validation: 0.09498342037115698]
	TIME [epoch: 5.79 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785814499557143		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.0785814499557143 | validation: 0.09425219347395536]
	TIME [epoch: 5.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958620948045178		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.07958620948045178 | validation: 0.09590078862012352]
	TIME [epoch: 5.78 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07746811880924513		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.07746811880924513 | validation: 0.08991209748535828]
	TIME [epoch: 5.83 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769369295772441		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.0769369295772441 | validation: 0.09034795734643555]
	TIME [epoch: 5.83 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07621480156245632		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.07621480156245632 | validation: 0.09054478379041585]
	TIME [epoch: 5.84 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928943784304707		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.07928943784304707 | validation: 0.09656488460301614]
	TIME [epoch: 5.84 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07857076453319069		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.07857076453319069 | validation: 0.09505927389996295]
	TIME [epoch: 5.83 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0767762069983339		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.0767762069983339 | validation: 0.09451272449113253]
	TIME [epoch: 5.83 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712584088578109		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.07712584088578109 | validation: 0.08609564462627378]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1520.pth
	Model improved!!!
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07743467991868175		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.07743467991868175 | validation: 0.08773591347553786]
	TIME [epoch: 5.8 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760339635513945		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.07760339635513945 | validation: 0.09515558084847645]
	TIME [epoch: 5.79 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07738061704653473		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.07738061704653473 | validation: 0.0911898030004833]
	TIME [epoch: 5.79 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07619230150848263		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.07619230150848263 | validation: 0.09699029856212146]
	TIME [epoch: 5.79 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480865046916482		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.07480865046916482 | validation: 0.10100186347548588]
	TIME [epoch: 5.78 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819477247580986		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.07819477247580986 | validation: 0.08292163201959622]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1526.pth
	Model improved!!!
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922305915333525		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.07922305915333525 | validation: 0.09734233320598669]
	TIME [epoch: 5.83 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681374415276064		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.07681374415276064 | validation: 0.09561721635208675]
	TIME [epoch: 5.83 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730255333895411		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.07730255333895411 | validation: 0.09691937411940031]
	TIME [epoch: 5.82 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07826832862412123		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.07826832862412123 | validation: 0.0912228974517128]
	TIME [epoch: 5.83 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965092609656341		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.07965092609656341 | validation: 0.09273817356546776]
	TIME [epoch: 5.83 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07966952199630166		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.07966952199630166 | validation: 0.1002075384339165]
	TIME [epoch: 5.83 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07900675903852548		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.07900675903852548 | validation: 0.08848257659340941]
	TIME [epoch: 5.82 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787201866054416		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.07787201866054416 | validation: 0.09422388253815026]
	TIME [epoch: 5.83 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0750306628713307		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.0750306628713307 | validation: 0.09063195083194897]
	TIME [epoch: 5.82 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07526915810401491		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.07526915810401491 | validation: 0.09455408290268591]
	TIME [epoch: 5.83 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710005002038559		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.07710005002038559 | validation: 0.09675731238672752]
	TIME [epoch: 5.83 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784168685598624		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.0784168685598624 | validation: 0.09257062494650647]
	TIME [epoch: 5.83 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07591273101238133		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.07591273101238133 | validation: 0.09654219516561989]
	TIME [epoch: 5.82 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730014543199072		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.07730014543199072 | validation: 0.08992810302028431]
	TIME [epoch: 5.82 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07702534719257824		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.07702534719257824 | validation: 0.09486800572017277]
	TIME [epoch: 5.83 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07716335895146915		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.07716335895146915 | validation: 0.08830993167330463]
	TIME [epoch: 5.83 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532040751962141		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.07532040751962141 | validation: 0.08833303123367789]
	TIME [epoch: 5.82 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708749745392841		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.07708749745392841 | validation: 0.10044367680301908]
	TIME [epoch: 5.83 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903112139874056		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.07903112139874056 | validation: 0.09173445494966889]
	TIME [epoch: 5.84 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07831915680994768		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.07831915680994768 | validation: 0.10481668209870994]
	TIME [epoch: 5.83 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07776223493543732		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.07776223493543732 | validation: 0.09308734123447084]
	TIME [epoch: 5.83 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609559311244461		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.07609559311244461 | validation: 0.08951150415127374]
	TIME [epoch: 5.83 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07717887745771328		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.07717887745771328 | validation: 0.09762755008984027]
	TIME [epoch: 5.83 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738952466913961		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.0738952466913961 | validation: 0.09538967462627979]
	TIME [epoch: 5.83 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07590218524154903		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.07590218524154903 | validation: 0.08689959385809433]
	TIME [epoch: 5.83 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763276899622706		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.0763276899622706 | validation: 0.09598829705457844]
	TIME [epoch: 5.83 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07617263547400709		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.07617263547400709 | validation: 0.0926028911359941]
	TIME [epoch: 5.82 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719003295655713		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.07719003295655713 | validation: 0.09066084512792022]
	TIME [epoch: 5.82 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599807265623452		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.07599807265623452 | validation: 0.0864873644719756]
	TIME [epoch: 5.84 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731256713659998		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.07731256713659998 | validation: 0.09935257045219509]
	TIME [epoch: 5.83 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07722435422561039		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.07722435422561039 | validation: 0.09323962680426551]
	TIME [epoch: 5.83 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838394367535098		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.07838394367535098 | validation: 0.09755381528204873]
	TIME [epoch: 5.82 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388269181785756		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.07388269181785756 | validation: 0.09241732832301801]
	TIME [epoch: 5.83 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497978592769299		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.07497978592769299 | validation: 0.09385090460698178]
	TIME [epoch: 5.83 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753371392410111		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.0753371392410111 | validation: 0.09768281115505036]
	TIME [epoch: 5.83 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449341145734999		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.07449341145734999 | validation: 0.08790890593493585]
	TIME [epoch: 5.84 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07664995607799849		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.07664995607799849 | validation: 0.08948068439525741]
	TIME [epoch: 5.83 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07422507837530602		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.07422507837530602 | validation: 0.08746753033781761]
	TIME [epoch: 5.83 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07597726928083029		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.07597726928083029 | validation: 0.09394959686186076]
	TIME [epoch: 5.83 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267593688602075		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.07267593688602075 | validation: 0.08741158887809819]
	TIME [epoch: 5.83 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532985106365844		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.07532985106365844 | validation: 0.09762576113244446]
	TIME [epoch: 5.83 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07597206823157054		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.07597206823157054 | validation: 0.09055297596996857]
	TIME [epoch: 5.83 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07753133519020707		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.07753133519020707 | validation: 0.10011880160921867]
	TIME [epoch: 5.83 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075637007018251		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.075637007018251 | validation: 0.09356373405849844]
	TIME [epoch: 5.83 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07451792955102937		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.07451792955102937 | validation: 0.09317810005307986]
	TIME [epoch: 5.82 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732671512382278		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.07732671512382278 | validation: 0.09575043131976181]
	TIME [epoch: 5.84 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07453556154230244		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.07453556154230244 | validation: 0.09310973503323579]
	TIME [epoch: 5.82 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07548428612005735		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.07548428612005735 | validation: 0.0993393577686798]
	TIME [epoch: 5.83 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325196284764811		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.07325196284764811 | validation: 0.08070895026053408]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1575.pth
	Model improved!!!
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07808444414191713		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.07808444414191713 | validation: 0.09088269789051313]
	TIME [epoch: 5.79 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07662006470776307		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.07662006470776307 | validation: 0.0949952873154012]
	TIME [epoch: 5.79 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439187070490987		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.07439187070490987 | validation: 0.08971769159887713]
	TIME [epoch: 5.77 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500261774348455		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.07500261774348455 | validation: 0.08589813478698795]
	TIME [epoch: 5.78 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557098650604233		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.07557098650604233 | validation: 0.08492891173585147]
	TIME [epoch: 5.79 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749641887590984		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.0749641887590984 | validation: 0.097076564155682]
	TIME [epoch: 5.78 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07548219272715784		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.07548219272715784 | validation: 0.09583802746141996]
	TIME [epoch: 5.79 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07596632172397079		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.07596632172397079 | validation: 0.08910574820842909]
	TIME [epoch: 5.78 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07512279927841933		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.07512279927841933 | validation: 0.08945971735687519]
	TIME [epoch: 5.78 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07749466992991752		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.07749466992991752 | validation: 0.09869293796422891]
	TIME [epoch: 5.78 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764572442863915		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.0764572442863915 | validation: 0.08708643911789268]
	TIME [epoch: 5.78 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07384683633839223		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.07384683633839223 | validation: 0.08749149639669353]
	TIME [epoch: 5.79 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241318947549244		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.07241318947549244 | validation: 0.09195596980518621]
	TIME [epoch: 5.79 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468280810327865		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.07468280810327865 | validation: 0.09131596706018705]
	TIME [epoch: 5.78 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513090708663739		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.07513090708663739 | validation: 0.09390546110084141]
	TIME [epoch: 5.78 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317087422544176		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.07317087422544176 | validation: 0.08346762493125688]
	TIME [epoch: 5.78 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07617028117255481		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.07617028117255481 | validation: 0.10133168548176158]
	TIME [epoch: 5.78 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649745282468436		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.07649745282468436 | validation: 0.0925583219866303]
	TIME [epoch: 5.78 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07416760966072505		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.07416760966072505 | validation: 0.08874678550709854]
	TIME [epoch: 5.78 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07408424873574816		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.07408424873574816 | validation: 0.09202200168357066]
	TIME [epoch: 5.78 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282055641618032		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.07282055641618032 | validation: 0.09725974879137514]
	TIME [epoch: 5.79 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07518848549001794		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.07518848549001794 | validation: 0.09410083687481775]
	TIME [epoch: 5.78 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07578496848974188		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.07578496848974188 | validation: 0.08915547268973217]
	TIME [epoch: 5.79 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449119766658073		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.07449119766658073 | validation: 0.08865430015873396]
	TIME [epoch: 5.78 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07290848306200409		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.07290848306200409 | validation: 0.08755881728878753]
	TIME [epoch: 5.78 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347380535911949		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.07347380535911949 | validation: 0.09584358702301969]
	TIME [epoch: 5.83 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07560034284556999		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.07560034284556999 | validation: 0.0882026896096995]
	TIME [epoch: 5.83 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726511541939662		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.0726511541939662 | validation: 0.0892071963638061]
	TIME [epoch: 5.84 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223147796722111		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.07223147796722111 | validation: 0.09217445761847393]
	TIME [epoch: 5.83 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316167259801253		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.07316167259801253 | validation: 0.09012836247654038]
	TIME [epoch: 5.83 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298643742197264		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.07298643742197264 | validation: 0.09606168914715726]
	TIME [epoch: 5.83 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07353011396710202		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.07353011396710202 | validation: 0.08819821306155594]
	TIME [epoch: 5.83 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07393051257859376		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.07393051257859376 | validation: 0.09784910299966154]
	TIME [epoch: 5.84 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07379789939565987		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.07379789939565987 | validation: 0.09359814722927351]
	TIME [epoch: 5.84 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07483504037131354		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.07483504037131354 | validation: 0.09668151178004722]
	TIME [epoch: 5.83 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0721984786849552		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.0721984786849552 | validation: 0.09598900071053923]
	TIME [epoch: 5.83 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07432941019184704		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.07432941019184704 | validation: 0.09046984304537696]
	TIME [epoch: 5.84 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719413384035159		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.0719413384035159 | validation: 0.08582626160114849]
	TIME [epoch: 5.83 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07254686305892032		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.07254686305892032 | validation: 0.09650100950611196]
	TIME [epoch: 5.84 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458056279057255		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.07458056279057255 | validation: 0.08933049572929525]
	TIME [epoch: 5.83 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07206523551246168		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.07206523551246168 | validation: 0.08678984574735521]
	TIME [epoch: 5.83 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07552457472588524		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.07552457472588524 | validation: 0.08545766823104825]
	TIME [epoch: 5.83 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532755469595095		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.07532755469595095 | validation: 0.09327671587556532]
	TIME [epoch: 5.83 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.074270599170981		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.074270599170981 | validation: 0.08216519161907472]
	TIME [epoch: 5.83 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724661981857808		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.0724661981857808 | validation: 0.09543487158327757]
	TIME [epoch: 5.83 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282902237193868		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.07282902237193868 | validation: 0.0856219763650139]
	TIME [epoch: 5.83 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07438612598928977		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.07438612598928977 | validation: 0.08626273665391729]
	TIME [epoch: 5.83 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07496559053031585		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.07496559053031585 | validation: 0.10068595373717143]
	TIME [epoch: 5.84 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07408877962930413		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.07408877962930413 | validation: 0.0884371706832559]
	TIME [epoch: 5.84 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07518554311813638		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.07518554311813638 | validation: 0.08380591818808754]
	TIME [epoch: 5.83 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215521196494888		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.07215521196494888 | validation: 0.09081334984141363]
	TIME [epoch: 5.83 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07462461340866687		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.07462461340866687 | validation: 0.08969694826830207]
	TIME [epoch: 5.83 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306721889856622		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.07306721889856622 | validation: 0.09075057023929588]
	TIME [epoch: 5.84 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07406704523286942		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.07406704523286942 | validation: 0.08922094146571631]
	TIME [epoch: 5.83 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07245890877409633		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.07245890877409633 | validation: 0.09031309196299228]
	TIME [epoch: 5.83 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295793839199265		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.07295793839199265 | validation: 0.09580459885294777]
	TIME [epoch: 5.84 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737434042351835		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.0737434042351835 | validation: 0.08843858870119642]
	TIME [epoch: 5.84 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574812276355783		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.07574812276355783 | validation: 0.09572729208546565]
	TIME [epoch: 5.83 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07453865791949772		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.07453865791949772 | validation: 0.09021467574031693]
	TIME [epoch: 5.85 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07541168856178802		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.07541168856178802 | validation: 0.08528494584532756]
	TIME [epoch: 5.86 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177267375955772		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.07177267375955772 | validation: 0.09325669283564492]
	TIME [epoch: 5.83 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07328251317224686		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.07328251317224686 | validation: 0.08551763094976936]
	TIME [epoch: 5.83 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419452956432881		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.07419452956432881 | validation: 0.08782306169298726]
	TIME [epoch: 5.83 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145981012733468		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.07145981012733468 | validation: 0.09104550756693996]
	TIME [epoch: 5.84 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07262515577279223		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.07262515577279223 | validation: 0.08045797618575823]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1640.pth
	Model improved!!!
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269530974661281		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.07269530974661281 | validation: 0.09156638262249643]
	TIME [epoch: 5.77 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325803404232284		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.07325803404232284 | validation: 0.08472230106402166]
	TIME [epoch: 5.78 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07230475620685389		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.07230475620685389 | validation: 0.08661341370829906]
	TIME [epoch: 5.78 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114857497384781		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.07114857497384781 | validation: 0.08505244704683523]
	TIME [epoch: 5.78 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07051773349225464		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.07051773349225464 | validation: 0.0882803198298171]
	TIME [epoch: 5.83 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07287474697929056		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.07287474697929056 | validation: 0.09780893644130208]
	TIME [epoch: 5.83 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07270100699206858		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.07270100699206858 | validation: 0.08346350116830135]
	TIME [epoch: 5.84 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07416448765802289		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.07416448765802289 | validation: 0.08580846835958808]
	TIME [epoch: 5.84 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07197622039305121		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.07197622039305121 | validation: 0.08632999716551994]
	TIME [epoch: 5.84 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749638958809116		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.0749638958809116 | validation: 0.0869718668309271]
	TIME [epoch: 5.84 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07199943590634873		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.07199943590634873 | validation: 0.09382485522462182]
	TIME [epoch: 5.84 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366880185748151		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.07366880185748151 | validation: 0.08429537049679336]
	TIME [epoch: 5.84 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175583953765051		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.07175583953765051 | validation: 0.09193743787865452]
	TIME [epoch: 5.83 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07374512826518177		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.07374512826518177 | validation: 0.09226693000958508]
	TIME [epoch: 5.84 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083558251440504		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.07083558251440504 | validation: 0.09028331578720925]
	TIME [epoch: 5.83 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065110782646242		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.07065110782646242 | validation: 0.08698663594350282]
	TIME [epoch: 5.83 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.071688553079564		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.071688553079564 | validation: 0.08705664301958305]
	TIME [epoch: 5.85 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298658072549154		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.07298658072549154 | validation: 0.09024210428704982]
	TIME [epoch: 5.84 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711218616599831		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.0711218616599831 | validation: 0.08858751669753802]
	TIME [epoch: 5.84 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708375242060114		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.0708375242060114 | validation: 0.09533844704589142]
	TIME [epoch: 5.83 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001477890177168		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.07001477890177168 | validation: 0.08664276340754012]
	TIME [epoch: 5.83 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984967083496162		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.06984967083496162 | validation: 0.09444146145082954]
	TIME [epoch: 5.83 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085721702432352		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.07085721702432352 | validation: 0.09151797347514157]
	TIME [epoch: 5.83 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263169547980547		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.07263169547980547 | validation: 0.08790435975134529]
	TIME [epoch: 5.83 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07461843829210996		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.07461843829210996 | validation: 0.08952797021792963]
	TIME [epoch: 5.83 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248414614950137		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.07248414614950137 | validation: 0.0878953595082485]
	TIME [epoch: 5.84 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724908751129456		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.0724908751129456 | validation: 0.08338200789950304]
	TIME [epoch: 5.84 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07219814993856608		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.07219814993856608 | validation: 0.09109308531870462]
	TIME [epoch: 5.84 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07150373055369312		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.07150373055369312 | validation: 0.09294264284179803]
	TIME [epoch: 5.84 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902419104058029		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.06902419104058029 | validation: 0.08272518237849406]
	TIME [epoch: 5.84 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06970808185258047		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.06970808185258047 | validation: 0.08952720318549517]
	TIME [epoch: 5.83 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07037052056207689		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.07037052056207689 | validation: 0.09055231994436662]
	TIME [epoch: 5.84 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468257084828839		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.07468257084828839 | validation: 0.0831355555138206]
	TIME [epoch: 5.83 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250119023086404		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.07250119023086404 | validation: 0.0853181980472611]
	TIME [epoch: 5.84 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936622514091764		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.06936622514091764 | validation: 0.08884925625728546]
	TIME [epoch: 5.83 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196648179031913		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.07196648179031913 | validation: 0.09111559103819786]
	TIME [epoch: 5.84 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07011409449429765		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.07011409449429765 | validation: 0.0888908639525163]
	TIME [epoch: 5.83 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07195789604375134		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.07195789604375134 | validation: 0.09003385218290372]
	TIME [epoch: 5.84 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090518417405643		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.07090518417405643 | validation: 0.09138077524822925]
	TIME [epoch: 5.83 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161532714695638		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.07161532714695638 | validation: 0.08926749397679119]
	TIME [epoch: 5.85 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06834410847607353		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.06834410847607353 | validation: 0.08978248183189713]
	TIME [epoch: 5.85 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161691331740473		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.07161691331740473 | validation: 0.08449478048809739]
	TIME [epoch: 5.83 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07214593571739247		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.07214593571739247 | validation: 0.09218085878406002]
	TIME [epoch: 5.83 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07233606164238643		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.07233606164238643 | validation: 0.08415558560077782]
	TIME [epoch: 5.84 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728793997784642		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.0728793997784642 | validation: 0.08961486415911406]
	TIME [epoch: 5.83 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07228474726508453		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.07228474726508453 | validation: 0.09035241479186602]
	TIME [epoch: 5.79 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718646064687823		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.0718646064687823 | validation: 0.07965541626144243]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1687.pth
	Model improved!!!
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07165507965279734		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.07165507965279734 | validation: 0.09307011009371284]
	TIME [epoch: 5.83 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07138065964457357		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.07138065964457357 | validation: 0.08630934799037882]
	TIME [epoch: 5.83 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696720202481217		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.0696720202481217 | validation: 0.08323644280246281]
	TIME [epoch: 5.83 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116142219843385		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.07116142219843385 | validation: 0.08878668068533856]
	TIME [epoch: 5.81 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916388063033205		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.06916388063033205 | validation: 0.08937335554596051]
	TIME [epoch: 5.82 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173300161693125		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.07173300161693125 | validation: 0.0864371763717677]
	TIME [epoch: 5.82 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113662782387567		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.07113662782387567 | validation: 0.09036802488822303]
	TIME [epoch: 5.83 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07332895530548378		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.07332895530548378 | validation: 0.08880977712577554]
	TIME [epoch: 5.79 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07129563973710128		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.07129563973710128 | validation: 0.08855045370520534]
	TIME [epoch: 5.79 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07104324029391638		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.07104324029391638 | validation: 0.0872296127695014]
	TIME [epoch: 5.79 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07254780527755077		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.07254780527755077 | validation: 0.08724785972772586]
	TIME [epoch: 5.79 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07089854885786602		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.07089854885786602 | validation: 0.08922556573526756]
	TIME [epoch: 5.79 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686948794341949		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.0686948794341949 | validation: 0.08683269396346438]
	TIME [epoch: 5.79 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07091633768179326		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.07091633768179326 | validation: 0.08722549489642162]
	TIME [epoch: 5.83 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699530512838268		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.0699530512838268 | validation: 0.08278202021094216]
	TIME [epoch: 5.82 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937036081340249		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.06937036081340249 | validation: 0.09212577179329216]
	TIME [epoch: 5.83 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016230440936225		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.07016230440936225 | validation: 0.0847049594049255]
	TIME [epoch: 5.83 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07027999857799337		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.07027999857799337 | validation: 0.09594168181408316]
	TIME [epoch: 5.84 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705164670199942		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.0705164670199942 | validation: 0.08720947398199841]
	TIME [epoch: 5.83 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07135173185480854		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.07135173185480854 | validation: 0.0920707636858566]
	TIME [epoch: 5.83 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07147865465894508		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.07147865465894508 | validation: 0.08592600154920797]
	TIME [epoch: 5.83 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145509572401845		[learning rate: 2.8134e-05]
	Learning Rate: 2.8134e-05
	LOSS [training: 0.07145509572401845 | validation: 0.09744244978984258]
	TIME [epoch: 5.82 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06927330629038383		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.06927330629038383 | validation: 0.08559648983378654]
	TIME [epoch: 5.83 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920875557100063		[learning rate: 2.7935e-05]
	Learning Rate: 2.79353e-05
	LOSS [training: 0.06920875557100063 | validation: 0.08693130928744665]
	TIME [epoch: 5.82 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0706038064144021		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.0706038064144021 | validation: 0.09077175973242983]
	TIME [epoch: 5.83 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06995368966937358		[learning rate: 2.7738e-05]
	Learning Rate: 2.77381e-05
	LOSS [training: 0.06995368966937358 | validation: 0.08563973059210414]
	TIME [epoch: 5.83 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865658015734757		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.06865658015734757 | validation: 0.08576293034480192]
	TIME [epoch: 5.84 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06922683413136273		[learning rate: 2.7542e-05]
	Learning Rate: 2.75423e-05
	LOSS [training: 0.06922683413136273 | validation: 0.09273388805485835]
	TIME [epoch: 5.83 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07167558291321076		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.07167558291321076 | validation: 0.0865824102685522]
	TIME [epoch: 5.83 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07206916416943296		[learning rate: 2.7348e-05]
	Learning Rate: 2.73478e-05
	LOSS [training: 0.07206916416943296 | validation: 0.08887232296349498]
	TIME [epoch: 5.83 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196570976771662		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.07196570976771662 | validation: 0.08171882913628761]
	TIME [epoch: 5.8 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117430624798037		[learning rate: 2.7155e-05]
	Learning Rate: 2.71548e-05
	LOSS [training: 0.07117430624798037 | validation: 0.08446775258337252]
	TIME [epoch: 5.81 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0685647184939062		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.0685647184939062 | validation: 0.08869662896664493]
	TIME [epoch: 5.81 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700916773552633		[learning rate: 2.6963e-05]
	Learning Rate: 2.69631e-05
	LOSS [training: 0.0700916773552633 | validation: 0.08875241419341989]
	TIME [epoch: 5.81 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148820747903101		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.07148820747903101 | validation: 0.08992816761713611]
	TIME [epoch: 5.81 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07098780514417431		[learning rate: 2.6773e-05]
	Learning Rate: 2.67727e-05
	LOSS [training: 0.07098780514417431 | validation: 0.0928877253431838]
	TIME [epoch: 5.81 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155448606517514		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.07155448606517514 | validation: 0.08964528535876286]
	TIME [epoch: 5.81 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07038362505033394		[learning rate: 2.6584e-05]
	Learning Rate: 2.65837e-05
	LOSS [training: 0.07038362505033394 | validation: 0.08581679275116179]
	TIME [epoch: 5.81 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800987898871717		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.06800987898871717 | validation: 0.08912100764760193]
	TIME [epoch: 5.81 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06875898766240979		[learning rate: 2.6396e-05]
	Learning Rate: 2.6396e-05
	LOSS [training: 0.06875898766240979 | validation: 0.08643143374490306]
	TIME [epoch: 5.8 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265140890535206		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.07265140890535206 | validation: 0.08718660615150281]
	TIME [epoch: 5.81 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800838978581009		[learning rate: 2.621e-05]
	Learning Rate: 2.62097e-05
	LOSS [training: 0.06800838978581009 | validation: 0.08484605236386064]
	TIME [epoch: 5.82 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988903443169658		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.06988903443169658 | validation: 0.09001431178193439]
	TIME [epoch: 5.81 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889290887886505		[learning rate: 2.6025e-05]
	Learning Rate: 2.60246e-05
	LOSS [training: 0.06889290887886505 | validation: 0.08578042420679544]
	TIME [epoch: 5.81 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06777374876021704		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.06777374876021704 | validation: 0.08589202510718898]
	TIME [epoch: 5.81 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095136560166199		[learning rate: 2.5841e-05]
	Learning Rate: 2.58409e-05
	LOSS [training: 0.07095136560166199 | validation: 0.09100351753028885]
	TIME [epoch: 5.8 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936948592585669		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.06936948592585669 | validation: 0.08500017566922158]
	TIME [epoch: 5.81 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06973663884316667		[learning rate: 2.5658e-05]
	Learning Rate: 2.56585e-05
	LOSS [training: 0.06973663884316667 | validation: 0.08831994156838723]
	TIME [epoch: 5.81 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075001350078484		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.07075001350078484 | validation: 0.08997038992299056]
	TIME [epoch: 5.8 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960078201748676		[learning rate: 2.5477e-05]
	Learning Rate: 2.54773e-05
	LOSS [training: 0.06960078201748676 | validation: 0.09393660647772861]
	TIME [epoch: 5.8 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686327795886335		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.0686327795886335 | validation: 0.08670078245102825]
	TIME [epoch: 5.8 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968909576273515		[learning rate: 2.5297e-05]
	Learning Rate: 2.52975e-05
	LOSS [training: 0.06968909576273515 | validation: 0.08431256806131436]
	TIME [epoch: 5.81 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752447599023706		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.06752447599023706 | validation: 0.08495050001975307]
	TIME [epoch: 5.82 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06842649982211274		[learning rate: 2.5119e-05]
	Learning Rate: 2.51189e-05
	LOSS [training: 0.06842649982211274 | validation: 0.09104295151491143]
	TIME [epoch: 5.82 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058615738789509		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.07058615738789509 | validation: 0.08086930724290166]
	TIME [epoch: 5.81 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773051213961337		[learning rate: 2.4942e-05]
	Learning Rate: 2.49415e-05
	LOSS [training: 0.06773051213961337 | validation: 0.09218777140575807]
	TIME [epoch: 5.81 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075073055408014		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.07075073055408014 | validation: 0.09267142836851938]
	TIME [epoch: 5.81 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963556500373011		[learning rate: 2.4765e-05]
	Learning Rate: 2.47655e-05
	LOSS [training: 0.06963556500373011 | validation: 0.08951526113726614]
	TIME [epoch: 5.81 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020011944060038		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.07020011944060038 | validation: 0.08592977436428545]
	TIME [epoch: 5.81 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868156762514277		[learning rate: 2.4591e-05]
	Learning Rate: 2.45906e-05
	LOSS [training: 0.06868156762514277 | validation: 0.09031842103813002]
	TIME [epoch: 5.81 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06912934078682849		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.06912934078682849 | validation: 0.08630930221420616]
	TIME [epoch: 5.81 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06954589394871481		[learning rate: 2.4417e-05]
	Learning Rate: 2.4417e-05
	LOSS [training: 0.06954589394871481 | validation: 0.08362282368862137]
	TIME [epoch: 5.82 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701748019886299		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.0701748019886299 | validation: 0.08845453406625971]
	TIME [epoch: 5.78 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06931477075913348		[learning rate: 2.4245e-05]
	Learning Rate: 2.42446e-05
	LOSS [training: 0.06931477075913348 | validation: 0.08572919283240876]
	TIME [epoch: 5.79 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688502457052689		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.0688502457052689 | validation: 0.08679657467959365]
	TIME [epoch: 5.77 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06927202729149298		[learning rate: 2.4073e-05]
	Learning Rate: 2.40735e-05
	LOSS [training: 0.06927202729149298 | validation: 0.08797264544604474]
	TIME [epoch: 5.78 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800270205783726		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.06800270205783726 | validation: 0.08975494961795111]
	TIME [epoch: 5.78 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06853154030664663		[learning rate: 2.3904e-05]
	Learning Rate: 2.39035e-05
	LOSS [training: 0.06853154030664663 | validation: 0.0913365147674459]
	TIME [epoch: 5.78 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07139389081531033		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.07139389081531033 | validation: 0.09017218253532427]
	TIME [epoch: 5.78 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987458497543637		[learning rate: 2.3735e-05]
	Learning Rate: 2.37347e-05
	LOSS [training: 0.06987458497543637 | validation: 0.0861097318293521]
	TIME [epoch: 5.79 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783473572833916		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.06783473572833916 | validation: 0.09077564711962463]
	TIME [epoch: 5.78 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841962322485862		[learning rate: 2.3567e-05]
	Learning Rate: 2.35672e-05
	LOSS [training: 0.06841962322485862 | validation: 0.08921206578004165]
	TIME [epoch: 5.79 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06755697959142798		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.06755697959142798 | validation: 0.08331008607380282]
	TIME [epoch: 5.79 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06812112771501945		[learning rate: 2.3401e-05]
	Learning Rate: 2.34008e-05
	LOSS [training: 0.06812112771501945 | validation: 0.08620339567676129]
	TIME [epoch: 5.8 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720652004748211		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.06720652004748211 | validation: 0.09190434889789063]
	TIME [epoch: 5.79 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910368986852426		[learning rate: 2.3236e-05]
	Learning Rate: 2.32356e-05
	LOSS [training: 0.06910368986852426 | validation: 0.08863352614826467]
	TIME [epoch: 5.8 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06935645730612972		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.06935645730612972 | validation: 0.08398183376706332]
	TIME [epoch: 5.81 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723894978277293		[learning rate: 2.3072e-05]
	Learning Rate: 2.30716e-05
	LOSS [training: 0.0723894978277293 | validation: 0.08466287249798576]
	TIME [epoch: 5.81 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761876428856328		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.06761876428856328 | validation: 0.08633577379025698]
	TIME [epoch: 5.81 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07046166167223412		[learning rate: 2.2909e-05]
	Learning Rate: 2.29087e-05
	LOSS [training: 0.07046166167223412 | validation: 0.08782416782621126]
	TIME [epoch: 5.82 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06775164956446356		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.06775164956446356 | validation: 0.08578984768084902]
	TIME [epoch: 5.81 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190530702582106		[learning rate: 2.2747e-05]
	Learning Rate: 2.27469e-05
	LOSS [training: 0.07190530702582106 | validation: 0.08994284032345337]
	TIME [epoch: 5.82 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917455035409657		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.06917455035409657 | validation: 0.08662223326931928]
	TIME [epoch: 5.81 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06942136996701048		[learning rate: 2.2586e-05]
	Learning Rate: 2.25864e-05
	LOSS [training: 0.06942136996701048 | validation: 0.08369041348375608]
	TIME [epoch: 5.82 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688782621157125		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.0688782621157125 | validation: 0.08428455361582116]
	TIME [epoch: 5.8 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06986246630959314		[learning rate: 2.2427e-05]
	Learning Rate: 2.24269e-05
	LOSS [training: 0.06986246630959314 | validation: 0.08377909787714959]
	TIME [epoch: 5.81 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06953616761807493		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.06953616761807493 | validation: 0.08501785393680081]
	TIME [epoch: 5.79 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936917802168976		[learning rate: 2.2269e-05]
	Learning Rate: 2.22686e-05
	LOSS [training: 0.06936917802168976 | validation: 0.08412138112291694]
	TIME [epoch: 5.81 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06991047576181428		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.06991047576181428 | validation: 0.08867368181317364]
	TIME [epoch: 5.81 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936029019112051		[learning rate: 2.2111e-05]
	Learning Rate: 2.21114e-05
	LOSS [training: 0.06936029019112051 | validation: 0.08249823298172002]
	TIME [epoch: 5.82 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07077699112026804		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.07077699112026804 | validation: 0.0861462946734356]
	TIME [epoch: 5.83 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06732658317408234		[learning rate: 2.1955e-05]
	Learning Rate: 2.19553e-05
	LOSS [training: 0.06732658317408234 | validation: 0.07677072801528954]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1779.pth
	Model improved!!!
EPOCH 1780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911974264930394		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.06911974264930394 | validation: 0.08847486651971297]
	TIME [epoch: 5.78 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887700671217144		[learning rate: 2.18e-05]
	Learning Rate: 2.18003e-05
	LOSS [training: 0.06887700671217144 | validation: 0.0860871152644786]
	TIME [epoch: 5.78 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0681184618861605		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.0681184618861605 | validation: 0.08222976066523388]
	TIME [epoch: 5.78 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06749984693741304		[learning rate: 2.1646e-05]
	Learning Rate: 2.16464e-05
	LOSS [training: 0.06749984693741304 | validation: 0.0922966100173302]
	TIME [epoch: 5.78 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0673668587418672		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.0673668587418672 | validation: 0.08819623999406076]
	TIME [epoch: 5.78 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877195080283209		[learning rate: 2.1494e-05]
	Learning Rate: 2.14935e-05
	LOSS [training: 0.06877195080283209 | validation: 0.0896927286878624]
	TIME [epoch: 5.78 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738775586373213		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.06738775586373213 | validation: 0.08294737505858801]
	TIME [epoch: 5.79 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737526457224469		[learning rate: 2.1342e-05]
	Learning Rate: 2.13418e-05
	LOSS [training: 0.06737526457224469 | validation: 0.08259243603282929]
	TIME [epoch: 5.78 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788078307988397		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.06788078307988397 | validation: 0.0930976172896288]
	TIME [epoch: 5.78 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054417507381876		[learning rate: 2.1191e-05]
	Learning Rate: 2.11911e-05
	LOSS [training: 0.07054417507381876 | validation: 0.08754436244110689]
	TIME [epoch: 5.78 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874166312990655		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.06874166312990655 | validation: 0.08001982931523816]
	TIME [epoch: 5.78 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690173187536892		[learning rate: 2.1042e-05]
	Learning Rate: 2.10415e-05
	LOSS [training: 0.0690173187536892 | validation: 0.08305389870426906]
	TIME [epoch: 5.78 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865970343526163		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.06865970343526163 | validation: 0.08625458629195126]
	TIME [epoch: 5.79 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689584396004794		[learning rate: 2.0893e-05]
	Learning Rate: 2.0893e-05
	LOSS [training: 0.0689584396004794 | validation: 0.0833503583694803]
	TIME [epoch: 5.78 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674153418473456		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.0674153418473456 | validation: 0.08183786503773124]
	TIME [epoch: 5.79 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859730312128391		[learning rate: 2.0745e-05]
	Learning Rate: 2.07455e-05
	LOSS [training: 0.06859730312128391 | validation: 0.0848754833547093]
	TIME [epoch: 5.79 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738677550317139		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.06738677550317139 | validation: 0.08408716016545492]
	TIME [epoch: 5.78 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068224684375583		[learning rate: 2.0599e-05]
	Learning Rate: 2.0599e-05
	LOSS [training: 0.068224684375583 | validation: 0.08434853778639512]
	TIME [epoch: 5.79 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06771100354279783		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.06771100354279783 | validation: 0.08824520251472623]
	TIME [epoch: 5.79 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06751645922438897		[learning rate: 2.0454e-05]
	Learning Rate: 2.04536e-05
	LOSS [training: 0.06751645922438897 | validation: 0.08023042039171138]
	TIME [epoch: 5.79 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07007233581232977		[learning rate: 2.0381e-05]
	Learning Rate: 2.03812e-05
	LOSS [training: 0.07007233581232977 | validation: 0.08380424521595649]
	TIME [epoch: 5.78 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06808272345577758		[learning rate: 2.0309e-05]
	Learning Rate: 2.03092e-05
	LOSS [training: 0.06808272345577758 | validation: 0.08536256711445564]
	TIME [epoch: 5.81 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638654818217467		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.06638654818217467 | validation: 0.08201026192291982]
	TIME [epoch: 5.79 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687667319403596		[learning rate: 2.0166e-05]
	Learning Rate: 2.01658e-05
	LOSS [training: 0.0687667319403596 | validation: 0.08791720662657194]
	TIME [epoch: 5.78 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0685229961284387		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.0685229961284387 | validation: 0.09041183845700868]
	TIME [epoch: 5.78 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652607337601585		[learning rate: 2.0023e-05]
	Learning Rate: 2.00234e-05
	LOSS [training: 0.06652607337601585 | validation: 0.08920020674338539]
	TIME [epoch: 5.78 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666726864766449		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.0666726864766449 | validation: 0.08986093664398535]
	TIME [epoch: 5.78 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936113897745083		[learning rate: 1.9882e-05]
	Learning Rate: 1.98821e-05
	LOSS [training: 0.06936113897745083 | validation: 0.08513304320026766]
	TIME [epoch: 5.81 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06659965246667962		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.06659965246667962 | validation: 0.0851934347855693]
	TIME [epoch: 5.81 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668172358015111		[learning rate: 1.9742e-05]
	Learning Rate: 1.97417e-05
	LOSS [training: 0.0668172358015111 | validation: 0.08569014751949275]
	TIME [epoch: 5.8 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06880152897535459		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.06880152897535459 | validation: 0.08313488416567218]
	TIME [epoch: 5.81 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992993708534978		[learning rate: 1.9602e-05]
	Learning Rate: 1.96023e-05
	LOSS [training: 0.06992993708534978 | validation: 0.08922775433798334]
	TIME [epoch: 5.82 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06836553689063815		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.06836553689063815 | validation: 0.08386102020030357]
	TIME [epoch: 5.82 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876565591021994		[learning rate: 1.9464e-05]
	Learning Rate: 1.94639e-05
	LOSS [training: 0.06876565591021994 | validation: 0.0870443637927038]
	TIME [epoch: 5.82 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897395944184802		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.06897395944184802 | validation: 0.08182507048235443]
	TIME [epoch: 5.82 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06834917840182154		[learning rate: 1.9327e-05]
	Learning Rate: 1.93265e-05
	LOSS [training: 0.06834917840182154 | validation: 0.09199062519818624]
	TIME [epoch: 5.82 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920000335811453		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.06920000335811453 | validation: 0.08631573722806746]
	TIME [epoch: 5.81 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813220130530398		[learning rate: 1.919e-05]
	Learning Rate: 1.91901e-05
	LOSS [training: 0.06813220130530398 | validation: 0.0853750909010656]
	TIME [epoch: 5.82 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06765762783737174		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.06765762783737174 | validation: 0.0902455485625096]
	TIME [epoch: 5.82 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06757067055556461		[learning rate: 1.9055e-05]
	Learning Rate: 1.90546e-05
	LOSS [training: 0.06757067055556461 | validation: 0.08412689859523168]
	TIME [epoch: 5.83 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852889858377846		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.06852889858377846 | validation: 0.08487492054006109]
	TIME [epoch: 5.82 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773878098777643		[learning rate: 1.892e-05]
	Learning Rate: 1.89201e-05
	LOSS [training: 0.06773878098777643 | validation: 0.09082219710706312]
	TIME [epoch: 5.84 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571422885357212		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.06571422885357212 | validation: 0.08059783047764336]
	TIME [epoch: 5.84 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791566080752091		[learning rate: 1.8787e-05]
	Learning Rate: 1.87865e-05
	LOSS [training: 0.06791566080752091 | validation: 0.09122724033633839]
	TIME [epoch: 5.84 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06724703345506788		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.06724703345506788 | validation: 0.0856225453774028]
	TIME [epoch: 5.83 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804616345516995		[learning rate: 1.8654e-05]
	Learning Rate: 1.86539e-05
	LOSS [training: 0.06804616345516995 | validation: 0.07872060751994839]
	TIME [epoch: 5.83 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06863678205794643		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.06863678205794643 | validation: 0.08392186381650847]
	TIME [epoch: 5.83 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843573666857627		[learning rate: 1.8522e-05]
	Learning Rate: 1.85222e-05
	LOSS [training: 0.06843573666857627 | validation: 0.08448628196548902]
	TIME [epoch: 5.84 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862727757008993		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.06862727757008993 | validation: 0.0863527974407772]
	TIME [epoch: 5.83 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624812919027342		[learning rate: 1.8391e-05]
	Learning Rate: 1.83914e-05
	LOSS [training: 0.06624812919027342 | validation: 0.08644579700302717]
	TIME [epoch: 5.82 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06699811586541088		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.06699811586541088 | validation: 0.07789949782097966]
	TIME [epoch: 5.82 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862169821411886		[learning rate: 1.8262e-05]
	Learning Rate: 1.82616e-05
	LOSS [training: 0.06862169821411886 | validation: 0.08702235865032266]
	TIME [epoch: 5.78 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683102608766772		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.0683102608766772 | validation: 0.09115078532469306]
	TIME [epoch: 5.79 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761971213855861		[learning rate: 1.8133e-05]
	Learning Rate: 1.81327e-05
	LOSS [training: 0.06761971213855861 | validation: 0.08735608751173674]
	TIME [epoch: 5.79 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795593313890436		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.06795593313890436 | validation: 0.08146605764216812]
	TIME [epoch: 5.78 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06849162165202768		[learning rate: 1.8005e-05]
	Learning Rate: 1.80047e-05
	LOSS [training: 0.06849162165202768 | validation: 0.08579707348425404]
	TIME [epoch: 5.82 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575243763909115		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.06575243763909115 | validation: 0.09110563896257241]
	TIME [epoch: 5.78 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07005230383548275		[learning rate: 1.7878e-05]
	Learning Rate: 1.78775e-05
	LOSS [training: 0.07005230383548275 | validation: 0.08518637675632319]
	TIME [epoch: 5.79 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695802731456922		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.06695802731456922 | validation: 0.0856109592068968]
	TIME [epoch: 5.78 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06767019470374061		[learning rate: 1.7751e-05]
	Learning Rate: 1.77513e-05
	LOSS [training: 0.06767019470374061 | validation: 0.08038694508013394]
	TIME [epoch: 5.79 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668496050291148		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.0668496050291148 | validation: 0.08281071134742027]
	TIME [epoch: 5.78 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833249831184553		[learning rate: 1.7626e-05]
	Learning Rate: 1.7626e-05
	LOSS [training: 0.06833249831184553 | validation: 0.0926190285528414]
	TIME [epoch: 5.79 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07046654495126131		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.07046654495126131 | validation: 0.08277322129578746]
	TIME [epoch: 5.79 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06777414829140885		[learning rate: 1.7502e-05]
	Learning Rate: 1.75016e-05
	LOSS [training: 0.06777414829140885 | validation: 0.08213006476868792]
	TIME [epoch: 5.8 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06730286007698329		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.06730286007698329 | validation: 0.08576898955514624]
	TIME [epoch: 5.78 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06632978988706519		[learning rate: 1.7378e-05]
	Learning Rate: 1.7378e-05
	LOSS [training: 0.06632978988706519 | validation: 0.08400574402334833]
	TIME [epoch: 5.79 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737530480785696		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.06737530480785696 | validation: 0.08442886105465709]
	TIME [epoch: 5.78 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648860723766499		[learning rate: 1.7255e-05]
	Learning Rate: 1.72553e-05
	LOSS [training: 0.0648860723766499 | validation: 0.08865733236450991]
	TIME [epoch: 5.78 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540379790663582		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.06540379790663582 | validation: 0.08519498251134676]
	TIME [epoch: 5.79 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06707980980501503		[learning rate: 1.7134e-05]
	Learning Rate: 1.71335e-05
	LOSS [training: 0.06707980980501503 | validation: 0.08824040686093498]
	TIME [epoch: 5.79 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06736197172134127		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.06736197172134127 | validation: 0.08321179232868382]
	TIME [epoch: 5.79 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652063624935456		[learning rate: 1.7013e-05]
	Learning Rate: 1.70125e-05
	LOSS [training: 0.06652063624935456 | validation: 0.08178305601215764]
	TIME [epoch: 5.78 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788168268541013		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.06788168268541013 | validation: 0.08567185291469123]
	TIME [epoch: 5.79 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06710659947913458		[learning rate: 1.6892e-05]
	Learning Rate: 1.68924e-05
	LOSS [training: 0.06710659947913458 | validation: 0.08463778605284736]
	TIME [epoch: 5.79 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659410736293487		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.0659410736293487 | validation: 0.08054000207811118]
	TIME [epoch: 5.82 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06747784535866504		[learning rate: 1.6773e-05]
	Learning Rate: 1.67732e-05
	LOSS [training: 0.06747784535866504 | validation: 0.08774265532038093]
	TIME [epoch: 5.81 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752343989703692		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.06752343989703692 | validation: 0.08380348633481616]
	TIME [epoch: 5.82 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761034215931812		[learning rate: 1.6655e-05]
	Learning Rate: 1.66548e-05
	LOSS [training: 0.06761034215931812 | validation: 0.08086368499507351]
	TIME [epoch: 5.82 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676945155799169		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.0676945155799169 | validation: 0.08127421405216927]
	TIME [epoch: 5.82 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807404177621589		[learning rate: 1.6537e-05]
	Learning Rate: 1.65372e-05
	LOSS [training: 0.06807404177621589 | validation: 0.08997733216286462]
	TIME [epoch: 5.82 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06912788701290813		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.06912788701290813 | validation: 0.0827181328431404]
	TIME [epoch: 5.82 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06649358154254557		[learning rate: 1.642e-05]
	Learning Rate: 1.64204e-05
	LOSS [training: 0.06649358154254557 | validation: 0.08177135292290363]
	TIME [epoch: 5.82 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06789592127024757		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.06789592127024757 | validation: 0.0805567952269498]
	TIME [epoch: 5.81 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754374988824063		[learning rate: 1.6305e-05]
	Learning Rate: 1.63045e-05
	LOSS [training: 0.06754374988824063 | validation: 0.08947652429160806]
	TIME [epoch: 5.82 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727529656872627		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.06727529656872627 | validation: 0.08253227503420006]
	TIME [epoch: 5.82 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584665575009806		[learning rate: 1.6189e-05]
	Learning Rate: 1.61894e-05
	LOSS [training: 0.06584665575009806 | validation: 0.08477601222434089]
	TIME [epoch: 5.81 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06656345703728941		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.06656345703728941 | validation: 0.07862923482468977]
	TIME [epoch: 5.78 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06787140921066752		[learning rate: 1.6075e-05]
	Learning Rate: 1.60751e-05
	LOSS [training: 0.06787140921066752 | validation: 0.08365952936838288]
	TIME [epoch: 5.78 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608293010871448		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.06608293010871448 | validation: 0.08395076775925703]
	TIME [epoch: 5.8 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06559616469140177		[learning rate: 1.5962e-05]
	Learning Rate: 1.59616e-05
	LOSS [training: 0.06559616469140177 | validation: 0.08214480245881754]
	TIME [epoch: 5.78 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895801634674259		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.06895801634674259 | validation: 0.08285110841430798]
	TIME [epoch: 5.78 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591274218861447		[learning rate: 1.5849e-05]
	Learning Rate: 1.58489e-05
	LOSS [training: 0.06591274218861447 | validation: 0.08142334487444275]
	TIME [epoch: 5.78 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661875930065501		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.0661875930065501 | validation: 0.07690694594415082]
	TIME [epoch: 5.78 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06769833380342051		[learning rate: 1.5737e-05]
	Learning Rate: 1.5737e-05
	LOSS [training: 0.06769833380342051 | validation: 0.08488200139307234]
	TIME [epoch: 5.77 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06627171520049649		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.06627171520049649 | validation: 0.07998337799539734]
	TIME [epoch: 5.78 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06542217713179495		[learning rate: 1.5626e-05]
	Learning Rate: 1.56259e-05
	LOSS [training: 0.06542217713179495 | validation: 0.08488751356708263]
	TIME [epoch: 5.78 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06710766732885773		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.06710766732885773 | validation: 0.08479748875733883]
	TIME [epoch: 5.78 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661123087788809		[learning rate: 1.5516e-05]
	Learning Rate: 1.55156e-05
	LOSS [training: 0.0661123087788809 | validation: 0.08090256705678706]
	TIME [epoch: 5.77 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790498700838156		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.06790498700838156 | validation: 0.0907137682600028]
	TIME [epoch: 5.78 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06530700965133024		[learning rate: 1.5406e-05]
	Learning Rate: 1.54061e-05
	LOSS [training: 0.06530700965133024 | validation: 0.08885564883831328]
	TIME [epoch: 5.79 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871754402449379		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.06871754402449379 | validation: 0.0835562589038354]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_1_v_mmd1_20250503_135224/states/model_phi1_4a_distortion_v2_1_v_mmd1_1880.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7882.443 seconds.
