Args:
Namespace(name='model_phi1_4a_distortion_v2_4_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2958598207

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.602817588034602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.602817588034602 | validation: 6.710912307214727]
	TIME [epoch: 158 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.102587042283552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.102587042283552 | validation: 6.204197974645793]
	TIME [epoch: 0.805 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.4232250381123785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4232250381123785 | validation: 5.9723775025009935]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.367301172532568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.367301172532568 | validation: 6.020337344112111]
	TIME [epoch: 0.705 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.166328204664438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.166328204664438 | validation: 6.096019626350154]
	TIME [epoch: 0.706 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.137788455339733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.137788455339733 | validation: 5.844096467445803]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.848773539944605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.848773539944605 | validation: 5.778712222715699]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6484935172593795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6484935172593795 | validation: 5.472824066965185]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3462540208284555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3462540208284555 | validation: 5.273155850431612]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9667744288291304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9667744288291304 | validation: 4.3716287829941525]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9697894056470573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9697894056470573 | validation: 5.333104187048697]
	TIME [epoch: 0.704 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.866037305787672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.866037305787672 | validation: 4.680821624663509]
	TIME [epoch: 0.703 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.24837158928702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.24837158928702 | validation: 4.0709853728677095]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2187761662670926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2187761662670926 | validation: 3.594766507897585]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7888147709285613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7888147709285613 | validation: 2.1299508520310435]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9817610703874595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9817610703874595 | validation: 3.295524633547445]
	TIME [epoch: 0.706 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5741461523841576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5741461523841576 | validation: 1.9899496646360881]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3223163524695996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3223163524695996 | validation: 2.599247802750954]
	TIME [epoch: 0.704 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0826482598888836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0826482598888836 | validation: 1.3085455684894296]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.028206667529529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.028206667529529 | validation: 2.5070216229728306]
	TIME [epoch: 0.705 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.122396449816485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.122396449816485 | validation: 1.3407055373114973]
	TIME [epoch: 0.701 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8523253083425761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8523253083425761 | validation: 2.1591239422710435]
	TIME [epoch: 0.7 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8794860688993265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8794860688993265 | validation: 1.1737786240785955]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7372315664328144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7372315664328144 | validation: 1.9469066815671938]
	TIME [epoch: 0.703 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.769128320741642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.769128320741642 | validation: 1.2279160166067762]
	TIME [epoch: 0.699 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6930797654710121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6930797654710121 | validation: 1.8571539762988332]
	TIME [epoch: 0.699 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7397596623526819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7397596623526819 | validation: 1.1024753944756864]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6588096654679962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6588096654679962 | validation: 1.7988843840628777]
	TIME [epoch: 0.702 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6866071698815095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6866071698815095 | validation: 1.1228296872678543]
	TIME [epoch: 0.7 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6074773734991865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6074773734991865 | validation: 1.6564930494951065]
	TIME [epoch: 0.701 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6170372687616419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6170372687616419 | validation: 1.1025465162714976]
	TIME [epoch: 0.698 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5928192213503791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5928192213503791 | validation: 1.6642334966486834]
	TIME [epoch: 0.699 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6248439326337927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6248439326337927 | validation: 1.117781128186926]
	TIME [epoch: 0.698 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5545658192428085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5545658192428085 | validation: 1.5244124533213712]
	TIME [epoch: 0.7 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5522383102567683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5522383102567683 | validation: 1.0576516191125218]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5345236515999165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5345236515999165 | validation: 1.606891997957291]
	TIME [epoch: 0.704 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.566401262683626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.566401262683626 | validation: 1.0381945609409484]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5016314356485037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5016314356485037 | validation: 1.498022748862176]
	TIME [epoch: 0.704 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5135001212372625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5135001212372625 | validation: 1.0508841563459175]
	TIME [epoch: 0.702 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.487487068998358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.487487068998358 | validation: 1.4728580326915721]
	TIME [epoch: 0.701 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5129920115137572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5129920115137572 | validation: 1.0178277323435851]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4865649626885358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4865649626885358 | validation: 1.4643012280015175]
	TIME [epoch: 0.707 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.48520342259627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.48520342259627 | validation: 0.9961164181874431]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4441753328447857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4441753328447857 | validation: 1.412682547707033]
	TIME [epoch: 0.706 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4554526600424647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4554526600424647 | validation: 0.9933789797498949]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4073593922351637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4073593922351637 | validation: 1.2729802904051513]
	TIME [epoch: 0.705 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4053624300761083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4053624300761083 | validation: 1.0803860092955657]
	TIME [epoch: 0.704 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.411623770702434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.411623770702434 | validation: 1.257579317427338]
	TIME [epoch: 0.7 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4240015760578677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4240015760578677 | validation: 1.170659107218825]
	TIME [epoch: 0.701 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4079086578338182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4079086578338182 | validation: 1.1153670329308707]
	TIME [epoch: 0.701 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.380859078144405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.380859078144405 | validation: 1.1909747872441352]
	TIME [epoch: 0.701 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.357485211787021		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.357485211787021 | validation: 0.899871622881197]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.340057785002437		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.340057785002437 | validation: 1.3810166457403092]
	TIME [epoch: 0.703 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3728954350172708		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.3728954350172708 | validation: 0.460076085789234]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6025357314746347		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.6025357314746347 | validation: 1.8234411383804812]
	TIME [epoch: 0.702 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6408734520278905		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.6408734520278905 | validation: 1.2975941552819887]
	TIME [epoch: 0.701 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3518779638355296		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.3518779638355296 | validation: 0.7808792794624421]
	TIME [epoch: 0.7 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3346880262239638		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3346880262239638 | validation: 1.0982846001091129]
	TIME [epoch: 0.699 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3056983279368626		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.3056983279368626 | validation: 1.1374998131722052]
	TIME [epoch: 0.699 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3254612113714523		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.3254612113714523 | validation: 1.0639749327217634]
	TIME [epoch: 0.699 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.288295784801134		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.288295784801134 | validation: 0.924594639776458]
	TIME [epoch: 0.698 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.27269682540515		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.27269682540515 | validation: 1.0368570531838714]
	TIME [epoch: 0.698 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2479126081247947		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.2479126081247947 | validation: 0.9518279517400183]
	TIME [epoch: 0.698 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2498981865157486		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.2498981865157486 | validation: 0.9953798811869667]
	TIME [epoch: 0.703 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2615190905613058		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.2615190905613058 | validation: 0.9243214900671192]
	TIME [epoch: 0.699 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283896847474662		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.283896847474662 | validation: 1.1926726880682048]
	TIME [epoch: 0.699 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3104283282064009		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.3104283282064009 | validation: 0.9184556698928521]
	TIME [epoch: 0.696 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3244992486259315		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3244992486259315 | validation: 1.2393538117519503]
	TIME [epoch: 0.697 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3108401158134881		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3108401158134881 | validation: 0.8773214319145457]
	TIME [epoch: 0.696 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2403839006742436		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.2403839006742436 | validation: 0.9673390157488866]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2332005272114737		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.2332005272114737 | validation: 0.8749526934271658]
	TIME [epoch: 0.696 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2120885311499936		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.2120885311499936 | validation: 0.9101117487497181]
	TIME [epoch: 0.696 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199457647808498		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.199457647808498 | validation: 0.8626882971018252]
	TIME [epoch: 0.697 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019397961136267		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.2019397961136267 | validation: 0.8794798093174685]
	TIME [epoch: 0.695 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2030436562965934		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2030436562965934 | validation: 0.934874317416687]
	TIME [epoch: 0.695 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2044392739130392		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2044392739130392 | validation: 0.8779304395075616]
	TIME [epoch: 0.696 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2139678975496582		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.2139678975496582 | validation: 1.0674676605751579]
	TIME [epoch: 0.695 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.276343423639288		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.276343423639288 | validation: 0.8957225903532525]
	TIME [epoch: 1.08 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2864412968058383		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.2864412968058383 | validation: 1.0255409195811143]
	TIME [epoch: 0.701 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2346658079949588		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2346658079949588 | validation: 0.7646531210846219]
	TIME [epoch: 0.699 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2019506750922702		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.2019506750922702 | validation: 1.3563940785968178]
	TIME [epoch: 0.701 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3192739812349146		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3192739812349146 | validation: 0.580829994004028]
	TIME [epoch: 0.7 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7201277991470745		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7201277991470745 | validation: 1.3333439271798577]
	TIME [epoch: 0.705 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339194996814672		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.339194996814672 | validation: 1.2636883987719323]
	TIME [epoch: 0.7 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2667422146682403		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.2667422146682403 | validation: 0.8918129447451698]
	TIME [epoch: 0.699 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.208866747906242		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.208866747906242 | validation: 0.8150738902552347]
	TIME [epoch: 0.699 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2195292520799104		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.2195292520799104 | validation: 0.9035446915880773]
	TIME [epoch: 0.699 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1822203887409217		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.1822203887409217 | validation: 0.9644503151177393]
	TIME [epoch: 0.7 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1964727962098198		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.1964727962098198 | validation: 0.8471773101767653]
	TIME [epoch: 0.699 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.185772433155968		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.185772433155968 | validation: 0.824540275687631]
	TIME [epoch: 0.699 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.178893448091885		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.178893448091885 | validation: 0.8955867017576701]
	TIME [epoch: 0.701 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1808788309923852		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.1808788309923852 | validation: 0.8517925944402901]
	TIME [epoch: 0.702 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16947883870024		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.16947883870024 | validation: 0.8777517364066502]
	TIME [epoch: 0.701 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164366818412038		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.164366818412038 | validation: 0.8354265176652161]
	TIME [epoch: 0.701 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1833778772621915		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.1833778772621915 | validation: 0.9365609814065872]
	TIME [epoch: 0.7 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187401310790843		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.187401310790843 | validation: 0.8535092787967946]
	TIME [epoch: 0.7 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911408974759676		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.1911408974759676 | validation: 0.9440161858869687]
	TIME [epoch: 0.699 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2233280578274548		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2233280578274548 | validation: 0.9582088543831322]
	TIME [epoch: 0.699 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2418685662541675		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.2418685662541675 | validation: 0.9222744063215625]
	TIME [epoch: 0.7 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194781622562913		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.194781622562913 | validation: 0.8612983838189314]
	TIME [epoch: 0.701 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1752060480208417		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.1752060480208417 | validation: 0.8641523656893604]
	TIME [epoch: 0.703 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1735511851773865		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1735511851773865 | validation: 0.7538646946690593]
	TIME [epoch: 0.702 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162700563968885		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.162700563968885 | validation: 0.9672570863995811]
	TIME [epoch: 0.701 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17122168387113		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.17122168387113 | validation: 0.6299453288290663]
	TIME [epoch: 0.701 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966594841436826		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.1966594841436826 | validation: 1.3227861349886005]
	TIME [epoch: 0.7 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.318114425908558		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.318114425908558 | validation: 0.694918560801697]
	TIME [epoch: 0.7 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.189222743926936		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.189222743926936 | validation: 0.8370177642329666]
	TIME [epoch: 0.701 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1648406590393936		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.1648406590393936 | validation: 0.9584968763966304]
	TIME [epoch: 0.699 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911403958401043		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.1911403958401043 | validation: 0.81095390733351]
	TIME [epoch: 0.7 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1776626779952215		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.1776626779952215 | validation: 0.8015941875619275]
	TIME [epoch: 0.701 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1800155523380693		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.1800155523380693 | validation: 1.0431386619949805]
	TIME [epoch: 0.699 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2177348516030049		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2177348516030049 | validation: 0.8586265035639038]
	TIME [epoch: 0.7 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2102964452194358		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.2102964452194358 | validation: 0.9019622096425665]
	TIME [epoch: 0.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183595614685741		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.183595614685741 | validation: 0.8187296172207393]
	TIME [epoch: 0.7 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1560674345786282		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1560674345786282 | validation: 0.9159754549179558]
	TIME [epoch: 0.699 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1514066384498503		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1514066384498503 | validation: 0.6970239510337888]
	TIME [epoch: 0.699 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1609904224266019		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.1609904224266019 | validation: 1.0177546833807198]
	TIME [epoch: 0.699 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1848843492298091		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1848843492298091 | validation: 0.6224926035898264]
	TIME [epoch: 0.7 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1903445570907594		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.1903445570907594 | validation: 1.1190613125213276]
	TIME [epoch: 0.702 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2049855758028776		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2049855758028776 | validation: 0.7067257644174151]
	TIME [epoch: 0.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1562687972366277		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1562687972366277 | validation: 0.8305646327075625]
	TIME [epoch: 0.702 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1502224503975729		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1502224503975729 | validation: 0.9067406288682147]
	TIME [epoch: 0.706 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15574425788052		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.15574425788052 | validation: 0.7248025685877266]
	TIME [epoch: 0.702 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690118763473836		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1690118763473836 | validation: 1.0290919829883147]
	TIME [epoch: 0.702 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2006361863086963		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2006361863086963 | validation: 0.757403872538374]
	TIME [epoch: 0.704 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188904947546143		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.188904947546143 | validation: 0.8963135298932641]
	TIME [epoch: 0.701 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1738148301535778		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.1738148301535778 | validation: 0.9331434930627864]
	TIME [epoch: 0.7 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1528895179797904		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1528895179797904 | validation: 0.5795826679803394]
	TIME [epoch: 0.701 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1909264108693898		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1909264108693898 | validation: 1.2479374986904412]
	TIME [epoch: 0.702 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2673555078196188		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2673555078196188 | validation: 0.802505410699418]
	TIME [epoch: 0.701 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1217754645038098		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.1217754645038098 | validation: 0.6257433416788603]
	TIME [epoch: 0.7 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1705054745392711		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.1705054745392711 | validation: 1.0072366256556748]
	TIME [epoch: 0.701 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1738235922350273		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.1738235922350273 | validation: 0.8831590068734638]
	TIME [epoch: 0.701 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391599411030986		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1391599411030986 | validation: 0.6845433182494882]
	TIME [epoch: 0.702 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324626619481273		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1324626619481273 | validation: 0.8416126282648325]
	TIME [epoch: 0.701 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1297533044397718		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.1297533044397718 | validation: 0.8556216557815436]
	TIME [epoch: 0.701 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129900346530697		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.129900346530697 | validation: 0.7292834237863461]
	TIME [epoch: 0.701 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1340855003982815		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.1340855003982815 | validation: 0.9231901667691494]
	TIME [epoch: 0.702 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146510389009202		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.146510389009202 | validation: 0.7517956956285838]
	TIME [epoch: 0.702 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1332623777116564		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.1332623777116564 | validation: 0.8108940450350162]
	TIME [epoch: 0.7 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1040872915671482		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.1040872915671482 | validation: 0.776436689032719]
	TIME [epoch: 0.701 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785089618686112		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0785089618686112 | validation: 0.6439238669206392]
	TIME [epoch: 0.7 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0639913566918877		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.0639913566918877 | validation: 1.2394159490113594]
	TIME [epoch: 0.7 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2255123163597876		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.2255123163597876 | validation: 0.5317212859537932]
	TIME [epoch: 0.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5713405774338767		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.5713405774338767 | validation: 1.0482845025474319]
	TIME [epoch: 0.7 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173399643880246		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.1173399643880246 | validation: 1.1069771130260644]
	TIME [epoch: 0.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1411537837806522		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1411537837806522 | validation: 0.7327385122502146]
	TIME [epoch: 0.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0734386947094285		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.0734386947094285 | validation: 0.6488321855525515]
	TIME [epoch: 0.701 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0663103123043003		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.0663103123043003 | validation: 0.7313384331473025]
	TIME [epoch: 0.701 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0370448201103915		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.0370448201103915 | validation: 0.735390892367513]
	TIME [epoch: 0.701 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025822121285149		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.025822121285149 | validation: 0.6461186328922439]
	TIME [epoch: 0.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0234316557260559		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.0234316557260559 | validation: 0.6730773304738191]
	TIME [epoch: 0.701 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004986213257531		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.004986213257531 | validation: 0.6802370631246186]
	TIME [epoch: 0.701 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9877493309739239		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9877493309739239 | validation: 0.6292195959931237]
	TIME [epoch: 0.7 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.988804026878345		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.988804026878345 | validation: 0.6957436639768031]
	TIME [epoch: 0.7 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9556994322242494		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.9556994322242494 | validation: 0.5635898215855074]
	TIME [epoch: 0.7 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453722104536025		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.9453722104536025 | validation: 1.153845636306784]
	TIME [epoch: 0.7 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1545023725246606		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1545023725246606 | validation: 0.6530809204823816]
	TIME [epoch: 0.699 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6234078769464224		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.6234078769464224 | validation: 1.0452228601656015]
	TIME [epoch: 0.7 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025403555829136		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.025403555829136 | validation: 0.9942898939111218]
	TIME [epoch: 0.7 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016035687540588		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.016035687540588 | validation: 0.565747823343634]
	TIME [epoch: 0.701 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300406267772999		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9300406267772999 | validation: 0.5603230427617738]
	TIME [epoch: 0.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9150726045530331		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.9150726045530331 | validation: 0.6906032906563331]
	TIME [epoch: 0.701 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916565440440931		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8916565440440931 | validation: 0.629327863252784]
	TIME [epoch: 0.703 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775222104780835		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.8775222104780835 | validation: 0.5604092618117128]
	TIME [epoch: 0.702 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496676488652913		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8496676488652913 | validation: 0.5959120907972196]
	TIME [epoch: 0.7 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293963652622449		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8293963652622449 | validation: 0.5517949706243048]
	TIME [epoch: 0.704 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210187502801328		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8210187502801328 | validation: 0.7500761931224393]
	TIME [epoch: 0.7 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648537708830619		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8648537708830619 | validation: 0.4944586854257571]
	TIME [epoch: 0.701 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1392317420418687		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1392317420418687 | validation: 1.3888328387232673]
	TIME [epoch: 0.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574594809287169		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1574594809287169 | validation: 0.5520703912517331]
	TIME [epoch: 0.7 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808231412425777		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7808231412425777 | validation: 0.4797735343506435]
	TIME [epoch: 0.7 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449443150025322		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.8449443150025322 | validation: 0.824918871611966]
	TIME [epoch: 0.702 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367049650455818		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.8367049650455818 | validation: 0.5887188125626894]
	TIME [epoch: 0.701 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775013092556898		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7775013092556898 | validation: 0.5006010636247373]
	TIME [epoch: 0.702 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890345986820549		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7890345986820549 | validation: 0.6830062723594983]
	TIME [epoch: 0.703 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772160536571182		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7772160536571182 | validation: 0.5470925309693168]
	TIME [epoch: 0.702 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517384302768336		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7517384302768336 | validation: 0.6324429256853836]
	TIME [epoch: 0.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784841407484063		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7784841407484063 | validation: 0.5492442085419482]
	TIME [epoch: 0.701 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529918788500964		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8529918788500964 | validation: 0.8818825453835735]
	TIME [epoch: 0.7 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580755577434233		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8580755577434233 | validation: 0.5591935468502197]
	TIME [epoch: 0.699 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183792921465274		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9183792921465274 | validation: 1.0625455237153467]
	TIME [epoch: 0.7 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130010621259291		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.9130010621259291 | validation: 0.6320844354449089]
	TIME [epoch: 0.7 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343846423962248		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7343846423962248 | validation: 0.518359919157635]
	TIME [epoch: 0.7 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686731227789798		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7686731227789798 | validation: 0.6271542125575927]
	TIME [epoch: 0.7 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.723807287728067		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.723807287728067 | validation: 0.5493570267140683]
	TIME [epoch: 0.7 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7408412036337271		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7408412036337271 | validation: 0.6210606971446911]
	TIME [epoch: 0.7 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217598764321622		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7217598764321622 | validation: 0.4755671756301557]
	TIME [epoch: 0.701 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363619174454114		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7363619174454114 | validation: 0.7522546011408786]
	TIME [epoch: 0.702 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703996385328403		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7703996385328403 | validation: 0.468528533496864]
	TIME [epoch: 0.701 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847013186161378		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7847013186161378 | validation: 0.7278789325209496]
	TIME [epoch: 0.7 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598855896503498		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7598855896503498 | validation: 0.5003846613745341]
	TIME [epoch: 0.701 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123638658069201		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7123638658069201 | validation: 0.6242711550666791]
	TIME [epoch: 0.699 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792221835182661		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6792221835182661 | validation: 0.5220512939810918]
	TIME [epoch: 0.699 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703560729352135		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6703560729352135 | validation: 0.6166018428648213]
	TIME [epoch: 0.701 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746659669232996		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6746659669232996 | validation: 0.49877161581309687]
	TIME [epoch: 0.701 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872057021102806		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6872057021102806 | validation: 0.790893084550163]
	TIME [epoch: 0.7 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8005398343769154		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8005398343769154 | validation: 0.48674582344440265]
	TIME [epoch: 0.7 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850259439320425		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8850259439320425 | validation: 0.8131881532604798]
	TIME [epoch: 0.701 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946766414485396		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7946766414485396 | validation: 0.514295127564615]
	TIME [epoch: 0.701 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822385775586385		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6822385775586385 | validation: 0.5010268049563072]
	TIME [epoch: 166 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694122597066524		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6694122597066524 | validation: 0.6304740941369703]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667019291537007		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.667019291537007 | validation: 0.4890259632834866]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522359683380534		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6522359683380534 | validation: 0.6257629807777499]
	TIME [epoch: 1.37 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678617963457819		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6678617963457819 | validation: 0.550623814854091]
	TIME [epoch: 1.37 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042490914947527		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7042490914947527 | validation: 0.6789556053639197]
	TIME [epoch: 1.37 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085398983529632		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7085398983529632 | validation: 0.473463557842303]
	TIME [epoch: 1.37 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370499267271472		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7370499267271472 | validation: 0.8727134019623041]
	TIME [epoch: 1.37 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8043855656807146		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.8043855656807146 | validation: 0.4834886077686491]
	TIME [epoch: 1.37 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773785134595269		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6773785134595269 | validation: 0.5472594166959205]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387212138205806		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6387212138205806 | validation: 0.5512287019140175]
	TIME [epoch: 1.37 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241641798726807		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6241641798726807 | validation: 0.4941431803169756]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280492265981226		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6280492265981226 | validation: 0.597600176565343]
	TIME [epoch: 1.37 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343002530968195		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6343002530968195 | validation: 0.4927505505569639]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685840377422935		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.685840377422935 | validation: 0.7547792563900608]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774211680760936		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7774211680760936 | validation: 0.4770937810396279]
	TIME [epoch: 1.38 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758864201627927		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.758864201627927 | validation: 0.6744572160425966]
	TIME [epoch: 1.37 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704169411553883		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6704169411553883 | validation: 0.5076748601446232]
	TIME [epoch: 1.37 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167394845197879		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6167394845197879 | validation: 0.5021882471646414]
	TIME [epoch: 1.37 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6050033094146464		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6050033094146464 | validation: 0.6055020036165573]
	TIME [epoch: 1.37 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6192965328663259		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6192965328663259 | validation: 0.495422576378458]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310513640436836		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6310513640436836 | validation: 0.7088353387941315]
	TIME [epoch: 1.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806474384041448		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6806474384041448 | validation: 0.4636520169773486]
	TIME [epoch: 1.37 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980411613338822		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6980411613338822 | validation: 0.7030017292468511]
	TIME [epoch: 1.37 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719711050068425		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.719711050068425 | validation: 0.475454133517504]
	TIME [epoch: 1.37 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480670851193945		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6480670851193945 | validation: 0.5281109220248349]
	TIME [epoch: 1.37 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6026406756581341		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6026406756581341 | validation: 0.5557381422684372]
	TIME [epoch: 1.37 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598132357269463		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.598132357269463 | validation: 0.4923051155957854]
	TIME [epoch: 1.37 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860998185563879		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.5860998185563879 | validation: 0.5604232065530229]
	TIME [epoch: 1.37 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5875616688856449		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.5875616688856449 | validation: 0.47077817003618194]
	TIME [epoch: 1.37 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6096083826169474		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6096083826169474 | validation: 0.7389053905882421]
	TIME [epoch: 1.37 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179395811238354		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7179395811238354 | validation: 0.4647520414894629]
	TIME [epoch: 1.37 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788437417186501		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.788437417186501 | validation: 0.7181785499123734]
	TIME [epoch: 1.37 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754608851414766		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6754608851414766 | validation: 0.48673418236678395]
	TIME [epoch: 1.37 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58724425974369		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.58724425974369 | validation: 0.4425705344010917]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014839288665421		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6014839288665421 | validation: 0.5905940616759849]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6012151869018822		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6012151869018822 | validation: 0.46725125295979875]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843260777881775		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.5843260777881775 | validation: 0.5550140167276781]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5758414272983415		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.5758414272983415 | validation: 0.47333058396945304]
	TIME [epoch: 1.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815082966892753		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.5815082966892753 | validation: 0.5747586868957528]
	TIME [epoch: 1.37 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5963296969271931		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.5963296969271931 | validation: 0.46989005239288634]
	TIME [epoch: 1.37 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676041802681826		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6676041802681826 | validation: 0.7182023311633379]
	TIME [epoch: 1.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908518393728396		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6908518393728396 | validation: 0.46799159375063815]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698473873577624		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6698473873577624 | validation: 0.6739265087227189]
	TIME [epoch: 1.37 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6257304357766214		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6257304357766214 | validation: 0.4875191899390087]
	TIME [epoch: 1.37 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5535404970133844		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.5535404970133844 | validation: 0.4813578482147479]
	TIME [epoch: 1.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5650775418869955		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.5650775418869955 | validation: 0.5417715154472148]
	TIME [epoch: 1.37 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.552341976482158		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.552341976482158 | validation: 0.4701270585619772]
	TIME [epoch: 1.37 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567625311612995		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5567625311612995 | validation: 0.6237417652013757]
	TIME [epoch: 1.37 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224179481886465		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6224179481886465 | validation: 0.4443056270026913]
	TIME [epoch: 1.37 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292364751513852		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7292364751513852 | validation: 0.6740186347163958]
	TIME [epoch: 1.37 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346566080946228		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6346566080946228 | validation: 0.44967950137404966]
	TIME [epoch: 1.37 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5446425233173983		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5446425233173983 | validation: 0.4593497206284525]
	TIME [epoch: 1.37 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276385161769873		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.5276385161769873 | validation: 0.538943564482784]
	TIME [epoch: 1.37 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530096215531699		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.530096215531699 | validation: 0.44234995573377384]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371616597715563		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5371616597715563 | validation: 0.5784316433563931]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719863839111036		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.5719863839111036 | validation: 0.4253191578012798]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6542742372490684		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6542742372490684 | validation: 0.6775643870817173]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689376280654711		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6689376280654711 | validation: 0.4187850163171183]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553618108140463		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.553618108140463 | validation: 0.490257264863285]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5161153724918575		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5161153724918575 | validation: 0.4687213336203008]
	TIME [epoch: 1.37 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506862806503096		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.506862806503096 | validation: 0.44232276457441455]
	TIME [epoch: 1.37 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111746120197502		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5111746120197502 | validation: 0.5453748245064268]
	TIME [epoch: 1.37 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137314262889981		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5137314262889981 | validation: 0.40209304218202063]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614495376838647		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.5614495376838647 | validation: 0.7555842918118214]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297484769462781		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7297484769462781 | validation: 0.451904999049995]
	TIME [epoch: 1.37 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647048520412836		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.647048520412836 | validation: 0.47591137100835434]
	TIME [epoch: 1.37 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5196637398207692		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5196637398207692 | validation: 0.5242331609966498]
	TIME [epoch: 1.37 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5096071272783376		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.5096071272783376 | validation: 0.4182246323032135]
	TIME [epoch: 1.37 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518817435372529		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.518817435372529 | validation: 0.5711032433731191]
	TIME [epoch: 1.37 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5325467247157005		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5325467247157005 | validation: 0.39878167466385617]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415283914242821		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5415283914242821 | validation: 0.602125785177903]
	TIME [epoch: 1.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581666473986802		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5581666473986802 | validation: 0.40628286839137306]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.531930068766344		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.531930068766344 | validation: 0.5593407559960215]
	TIME [epoch: 1.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5329285108692818		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5329285108692818 | validation: 0.41928591983486163]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185403940703518		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5185403940703518 | validation: 0.5166157542588027]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5119489914922446		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5119489914922446 | validation: 0.45947296084772554]
	TIME [epoch: 1.37 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5180660888924217		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5180660888924217 | validation: 0.5085708854868761]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026846767091167		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.5026846767091167 | validation: 0.4439714084093428]
	TIME [epoch: 1.37 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48718127878825834		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.48718127878825834 | validation: 0.4765900145059421]
	TIME [epoch: 1.37 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4617731075150541		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.4617731075150541 | validation: 0.41764608135880854]
	TIME [epoch: 1.37 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46430594169122147		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.46430594169122147 | validation: 0.597401960682835]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5292848225304075		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5292848225304075 | validation: 0.46710704883040305]
	TIME [epoch: 1.37 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512603493278731		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.8512603493278731 | validation: 0.7676856109948678]
	TIME [epoch: 1.37 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769947070079438		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6769947070079438 | validation: 0.5563298107912121]
	TIME [epoch: 1.37 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49799879796960933		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.49799879796960933 | validation: 0.4067117844044666]
	TIME [epoch: 1.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4948967224275085		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4948967224275085 | validation: 0.48434759959928153]
	TIME [epoch: 1.37 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4822119407854826		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.4822119407854826 | validation: 0.45202104445638813]
	TIME [epoch: 1.37 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889294722757417		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.4889294722757417 | validation: 0.49545887282754286]
	TIME [epoch: 1.37 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47265769067681845		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.47265769067681845 | validation: 0.3936648760153375]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4723155448508934		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.4723155448508934 | validation: 0.503639115412174]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47748322174604063		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.47748322174604063 | validation: 0.4015620559976185]
	TIME [epoch: 1.37 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49908425772986337		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.49908425772986337 | validation: 0.5527700588778791]
	TIME [epoch: 1.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504672012424537		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.504672012424537 | validation: 0.3768231036229388]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148546430810675		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.5148546430810675 | validation: 0.5387847006177718]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48243435401711593		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.48243435401711593 | validation: 0.38744858833565393]
	TIME [epoch: 1.37 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4504792352509193		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.4504792352509193 | validation: 0.48298345779696294]
	TIME [epoch: 1.37 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4525064854507792		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.4525064854507792 | validation: 0.3773339660951422]
	TIME [epoch: 1.37 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4415111009560228		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.4415111009560228 | validation: 0.5169177057438089]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46177629015947236		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.46177629015947236 | validation: 0.3623661486182832]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5008532466043882		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5008532466043882 | validation: 0.5818546191572046]
	TIME [epoch: 1.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323784804353248		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5323784804353248 | validation: 0.35057231014700163]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011830959202714		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5011830959202714 | validation: 0.49769407613150335]
	TIME [epoch: 1.37 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4463907505827598		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.4463907505827598 | validation: 0.3714346264323076]
	TIME [epoch: 1.37 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211436058886541		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.4211436058886541 | validation: 0.48551624253248654]
	TIME [epoch: 1.37 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4229644670879587		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.4229644670879587 | validation: 0.41165669495709256]
	TIME [epoch: 1.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.431376833090746		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.431376833090746 | validation: 0.5251998491213407]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45615516922012606		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.45615516922012606 | validation: 0.362351542191968]
	TIME [epoch: 1.37 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4570866230870202		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.4570866230870202 | validation: 0.5840755193902695]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5313174753918588		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5313174753918588 | validation: 0.4434154842234226]
	TIME [epoch: 1.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105692292250633		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6105692292250633 | validation: 0.47294978226391376]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286328869434544		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.4286328869434544 | validation: 0.38916148964719066]
	TIME [epoch: 1.37 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3917749548753784		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.3917749548753784 | validation: 0.38354364203196495]
	TIME [epoch: 1.37 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3940237691980126		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.3940237691980126 | validation: 0.4478911400548765]
	TIME [epoch: 1.37 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4057242041409522		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.4057242041409522 | validation: 0.347382292235046]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43590971907448317		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.43590971907448317 | validation: 0.6198197292122579]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5358328775509247		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.5358328775509247 | validation: 0.3204887052000912]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5292700039957974		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.5292700039957974 | validation: 0.5641240256119648]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.483299991750528		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.483299991750528 | validation: 0.35211227155880814]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38708435672920344		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.38708435672920344 | validation: 0.3681707401838602]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.378483402782849		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.378483402782849 | validation: 0.4517487995868898]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40701658307811484		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.40701658307811484 | validation: 0.34341513494147285]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255588878845156		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.4255588878845156 | validation: 0.5339234173577213]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544062132115923		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4544062132115923 | validation: 0.36393477940171903]
	TIME [epoch: 1.37 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4931766315739031		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.4931766315739031 | validation: 0.47611103665569854]
	TIME [epoch: 1.37 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4065645752155878		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.4065645752155878 | validation: 0.3464371965573826]
	TIME [epoch: 1.37 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3738536681412555		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3738536681412555 | validation: 0.42645001972038243]
	TIME [epoch: 1.37 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3849860294932146		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3849860294932146 | validation: 0.3643646439064066]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.427761757946955		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.427761757946955 | validation: 0.649720096757969]
	TIME [epoch: 1.37 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163003454825259		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5163003454825259 | validation: 0.3184041035494246]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38375640387113147		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.38375640387113147 | validation: 0.42773652673175794]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3858459297033124		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3858459297033124 | validation: 0.3574190985925791]
	TIME [epoch: 1.37 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41368818887017245		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.41368818887017245 | validation: 0.5029688446625749]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415262613424531		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.415262613424531 | validation: 0.30854651983964065]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42073359692395373		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.42073359692395373 | validation: 0.5038146297660947]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41999486082761606		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.41999486082761606 | validation: 0.3072409634974436]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987503833056401		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.3987503833056401 | validation: 0.4582204200586084]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3927379867454042		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3927379867454042 | validation: 0.3053564155522295]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37626904666141975		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.37626904666141975 | validation: 0.448519268555272]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3773081678436876		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3773081678436876 | validation: 0.3059560965791678]
	TIME [epoch: 1.37 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37238387571973114		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.37238387571973114 | validation: 0.47149206253404713]
	TIME [epoch: 1.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3985592213039621		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.3985592213039621 | validation: 0.3133661876449727]
	TIME [epoch: 1.37 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4169244012131077		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.4169244012131077 | validation: 0.5085913886154173]
	TIME [epoch: 1.37 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241664440754805		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.4241664440754805 | validation: 0.32070844269775006]
	TIME [epoch: 1.37 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39293057090591776		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.39293057090591776 | validation: 0.4045814319358744]
	TIME [epoch: 1.37 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35248902886832123		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.35248902886832123 | validation: 0.35259688590657623]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33712501083753127		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.33712501083753127 | validation: 0.3771144082032877]
	TIME [epoch: 1.37 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.338688421664664		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.338688421664664 | validation: 0.39856113821508926]
	TIME [epoch: 1.37 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35838428673906353		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.35838428673906353 | validation: 0.4200192246908026]
	TIME [epoch: 1.37 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40013001500100787		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.40013001500100787 | validation: 0.6128310070651986]
	TIME [epoch: 1.37 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4707654109777538		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.4707654109777538 | validation: 0.2974493323724369]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3786575022324858		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.3786575022324858 | validation: 0.5232946545085849]
	TIME [epoch: 1.38 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43373914703351246		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.43373914703351246 | validation: 0.33077060847290807]
	TIME [epoch: 1.37 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47272749233537265		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.47272749233537265 | validation: 0.4497416716454029]
	TIME [epoch: 1.37 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609911256537248		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.3609911256537248 | validation: 0.3415513296698757]
	TIME [epoch: 1.37 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339010342336438		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.339010342336438 | validation: 0.35843676704079486]
	TIME [epoch: 1.37 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3301372099220481		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.3301372099220481 | validation: 0.36646287828772706]
	TIME [epoch: 1.37 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3179310270956663		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.3179310270956663 | validation: 0.3322667025890894]
	TIME [epoch: 1.37 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32217252521930034		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.32217252521930034 | validation: 0.3800599725217102]
	TIME [epoch: 1.37 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3286322450867148		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3286322450867148 | validation: 0.286047826648116]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519748316492162		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.3519748316492162 | validation: 0.5503188594434947]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48505188699731605		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.48505188699731605 | validation: 0.25798125641292974]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4580019633885547		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.4580019633885547 | validation: 0.4487709786345293]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3741230186632094		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.3741230186632094 | validation: 0.313994443617321]
	TIME [epoch: 1.37 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193891948949033		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.3193891948949033 | validation: 0.33019314615635537]
	TIME [epoch: 1.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109299190114123		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.3109299190114123 | validation: 0.3559063038756476]
	TIME [epoch: 1.37 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30943741396250324		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.30943741396250324 | validation: 0.31224901041842007]
	TIME [epoch: 1.37 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173526349857434		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3173526349857434 | validation: 0.3742323728842494]
	TIME [epoch: 1.37 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111770693549748		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.3111770693549748 | validation: 0.2843325102744657]
	TIME [epoch: 1.37 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3417313902401962		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.3417313902401962 | validation: 0.5138638780830602]
	TIME [epoch: 1.37 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41308041791658184		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.41308041791658184 | validation: 0.352801515774533]
	TIME [epoch: 1.37 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4571382598438515		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4571382598438515 | validation: 0.4138108070699068]
	TIME [epoch: 1.37 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3218925279067854		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.3218925279067854 | validation: 0.32182222447894726]
	TIME [epoch: 1.37 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31105158915476716		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.31105158915476716 | validation: 0.46763151516065743]
	TIME [epoch: 1.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3860750047810144		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.3860750047810144 | validation: 0.3501309282949513]
	TIME [epoch: 1.37 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4318639941632202		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.4318639941632202 | validation: 0.530220374071765]
	TIME [epoch: 1.37 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43559659508908594		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.43559659508908594 | validation: 0.31565628863002104]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35914637597535204		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.35914637597535204 | validation: 0.3441123413392074]
	TIME [epoch: 1.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30890564604546866		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.30890564604546866 | validation: 0.38126642623230816]
	TIME [epoch: 1.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062466258924827		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.3062466258924827 | validation: 0.2732669123043287]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31884428107357476		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.31884428107357476 | validation: 0.40329865299967466]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33110266138063554		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.33110266138063554 | validation: 0.25721547176974785]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34510138024978093		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.34510138024978093 | validation: 0.44582685656831345]
	TIME [epoch: 1.37 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3605443992751792		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3605443992751792 | validation: 0.25655628416915005]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32934677088073144		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.32934677088073144 | validation: 0.39323011785732476]
	TIME [epoch: 1.38 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.321535754490679		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.321535754490679 | validation: 0.26692956639483967]
	TIME [epoch: 1.37 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31269009862116903		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.31269009862116903 | validation: 0.4072186804580771]
	TIME [epoch: 1.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3223442240311367		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3223442240311367 | validation: 0.24674997184900677]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35158166632748744		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.35158166632748744 | validation: 0.43252600421939164]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35996849663210173		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.35996849663210173 | validation: 0.2657682119743908]
	TIME [epoch: 1.37 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30548116773263045		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.30548116773263045 | validation: 0.353599032412474]
	TIME [epoch: 1.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29617921008364734		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.29617921008364734 | validation: 0.27587057432703216]
	TIME [epoch: 1.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29437980604566133		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.29437980604566133 | validation: 0.359711931965363]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056234116690013		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.3056234116690013 | validation: 0.3055252136582322]
	TIME [epoch: 1.37 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32787098967180855		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.32787098967180855 | validation: 0.4419110708903413]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458435584663937		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.3458435584663937 | validation: 0.26106987833513223]
	TIME [epoch: 1.37 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35500973270647845		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.35500973270647845 | validation: 0.41329395163807353]
	TIME [epoch: 1.37 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3358104203062513		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.3358104203062513 | validation: 0.28157594021569965]
	TIME [epoch: 1.37 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3358930561134		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.3358930561134 | validation: 0.5007084041144695]
	TIME [epoch: 1.37 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3915377232539207		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3915377232539207 | validation: 0.2942233647350939]
	TIME [epoch: 1.37 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28633309044113575		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.28633309044113575 | validation: 0.2879885788622539]
	TIME [epoch: 1.37 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276455167099684		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.276455167099684 | validation: 0.32508685249314884]
	TIME [epoch: 1.37 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807980441485961		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2807980441485961 | validation: 0.2824590268491133]
	TIME [epoch: 1.37 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2898807763984647		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2898807763984647 | validation: 0.35487886117297385]
	TIME [epoch: 1.37 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295700343545407		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.295700343545407 | validation: 0.22750584675296068]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3250961686909847		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3250961686909847 | validation: 0.47535309648881524]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4175954972698628		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4175954972698628 | validation: 0.25198640671799766]
	TIME [epoch: 1.37 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3352525644656272		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.3352525644656272 | validation: 0.34592040417883213]
	TIME [epoch: 1.37 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28822003656690637		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.28822003656690637 | validation: 0.300469783743912]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27683919183705286		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.27683919183705286 | validation: 0.2865585836798434]
	TIME [epoch: 1.37 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26887722182223384		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.26887722182223384 | validation: 0.3207251390775407]
	TIME [epoch: 1.37 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2715355456014499		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2715355456014499 | validation: 0.25213208938853154]
	TIME [epoch: 1.37 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28787978250538615		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.28787978250538615 | validation: 0.38693767357744435]
	TIME [epoch: 1.37 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3158066524259781		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3158066524259781 | validation: 0.22110032570908553]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36336344085457467		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.36336344085457467 | validation: 0.4811460959678021]
	TIME [epoch: 1.37 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093502536785226		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4093502536785226 | validation: 0.2625945661952427]
	TIME [epoch: 1.37 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2712577844039258		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2712577844039258 | validation: 0.2859689322768183]
	TIME [epoch: 1.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194229111906714		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3194229111906714 | validation: 0.3926668993147641]
	TIME [epoch: 1.37 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3069333950221588		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3069333950221588 | validation: 0.2520569634264748]
	TIME [epoch: 1.37 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2680971487242881		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.2680971487242881 | validation: 0.2973771368525901]
	TIME [epoch: 1.37 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26510175516607754		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.26510175516607754 | validation: 0.29564341217052664]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720254607508227		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2720254607508227 | validation: 0.3313147529753921]
	TIME [epoch: 1.37 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27966124759161665		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.27966124759161665 | validation: 0.2637537404083663]
	TIME [epoch: 1.37 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3094386983507671		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3094386983507671 | validation: 0.4507580479155599]
	TIME [epoch: 1.37 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36131062406627446		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.36131062406627446 | validation: 0.24781372169475993]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3419179404332536		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3419179404332536 | validation: 0.3489231340239247]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2819175159747486		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.2819175159747486 | validation: 0.26001522175902453]
	TIME [epoch: 1.37 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28213627161938065		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.28213627161938065 | validation: 0.38914511259512097]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3164522602651356		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3164522602651356 | validation: 0.25433973680901606]
	TIME [epoch: 1.37 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28211581096740307		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.28211581096740307 | validation: 0.33695006797913524]
	TIME [epoch: 1.37 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2721654941636369		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.2721654941636369 | validation: 0.23401087990560063]
	TIME [epoch: 1.37 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743299922972839		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2743299922972839 | validation: 0.3532570437178173]
	TIME [epoch: 1.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2770491622952464		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.2770491622952464 | validation: 0.21782360065349643]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28860941012352825		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.28860941012352825 | validation: 0.37296297233505576]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996057217554177		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2996057217554177 | validation: 0.22279951259038244]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27415404923474457		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.27415404923474457 | validation: 0.32442883334869504]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661536062590346		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.2661536062590346 | validation: 0.23672836849394507]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600807143024844		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.2600807143024844 | validation: 0.33199420579704375]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2804015358858511		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.2804015358858511 | validation: 0.2211544138521847]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789546030927131		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.2789546030927131 | validation: 0.35329993893920597]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.290389687585812		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.290389687585812 | validation: 0.245736343455668]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27537295379487636		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.27537295379487636 | validation: 0.33387119762333706]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752067313506489		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2752067313506489 | validation: 0.2558044036606237]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26660843228411724		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.26660843228411724 | validation: 0.31632436847158685]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26859107078799543		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.26859107078799543 | validation: 0.25421586078861386]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25966290959951166		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.25966290959951166 | validation: 0.3265917486125587]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26635762100823107		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.26635762100823107 | validation: 0.211641998611118]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781581614160729		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.2781581614160729 | validation: 0.4419955736723846]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748054689589213		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.3748054689589213 | validation: 0.27155052356425263]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.389619370977094		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.389619370977094 | validation: 0.30549517695224643]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2545985381727837		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2545985381727837 | validation: 0.3186607692347325]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25216194053840174		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.25216194053840174 | validation: 0.2501192315432916]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27200511567875457		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.27200511567875457 | validation: 0.3232233302242906]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258094288694724		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.258094288694724 | validation: 0.2407910919270826]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24406772558488782		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.24406772558488782 | validation: 0.2879411987051232]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24934030103874008		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.24934030103874008 | validation: 0.27239080935228505]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549754762877653		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.2549754762877653 | validation: 0.3222649682176024]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662673665039101		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2662673665039101 | validation: 0.23538258084594005]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2802312371158964		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.2802312371158964 | validation: 0.3918032243337721]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102862193095341		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.3102862193095341 | validation: 0.22490145020008293]
	TIME [epoch: 1.37 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25777675176725795		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.25777675176725795 | validation: 0.31815280585781675]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515706610939138		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.2515706610939138 | validation: 0.24527742831167731]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26693110501721684		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.26693110501721684 | validation: 0.3384491620103016]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611938016409273		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2611938016409273 | validation: 0.22961441573029853]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539682151540756		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2539682151540756 | validation: 0.32604455577242164]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25250861319116086		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.25250861319116086 | validation: 0.2127156330615163]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2513434732739895		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.2513434732739895 | validation: 0.32100583324509646]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591836999185415		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2591836999185415 | validation: 0.22566629860979567]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25165970696120266		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.25165970696120266 | validation: 0.28903360477275414]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23922635042828697		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.23922635042828697 | validation: 0.2152280573208546]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250948326299857		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.250948326299857 | validation: 0.36093598379868425]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840975446308389		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2840975446308389 | validation: 0.18028107300166318]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28771267451618887		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.28771267451618887 | validation: 0.3589919153928417]
	TIME [epoch: 1.37 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28970928061838214		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.28970928061838214 | validation: 0.24702991547993622]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26262382621154234		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.26262382621154234 | validation: 0.35378344712019116]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2882281261453614		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2882281261453614 | validation: 0.2831415715625876]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575015037008501		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.2575015037008501 | validation: 0.2662304257533245]
	TIME [epoch: 1.37 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23456497093557788		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.23456497093557788 | validation: 0.2611739612019083]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22888348808064415		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.22888348808064415 | validation: 0.24849247180530132]
	TIME [epoch: 1.37 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23127825303911934		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.23127825303911934 | validation: 0.25330225914873755]
	TIME [epoch: 1.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372915874265189		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2372915874265189 | validation: 0.2467377110403383]
	TIME [epoch: 1.37 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23748671073059766		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.23748671073059766 | validation: 0.29096382476193233]
	TIME [epoch: 1.37 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2431361120234746		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.2431361120234746 | validation: 0.21952096212412409]
	TIME [epoch: 1.37 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529056497241971		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.2529056497241971 | validation: 0.3755287077071959]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29428691058847145		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.29428691058847145 | validation: 0.20150918436429388]
	TIME [epoch: 1.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3177898982539898		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.3177898982539898 | validation: 0.37223105754848895]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2845383810075592		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.2845383810075592 | validation: 0.24601610622103234]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23742428494060888		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.23742428494060888 | validation: 0.27097387624830765]
	TIME [epoch: 1.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2309747060527171		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.2309747060527171 | validation: 0.2612455879178606]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22300056872186852		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.22300056872186852 | validation: 0.25933598667185154]
	TIME [epoch: 1.37 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23105190475078757		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.23105190475078757 | validation: 0.2421855966543668]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23656450322910286		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.23656450322910286 | validation: 0.2649098121203761]
	TIME [epoch: 1.37 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22877266318332168		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.22877266318332168 | validation: 0.2544562499852104]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22771871298420665		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.22771871298420665 | validation: 0.2702163169705091]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23463416004595722		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.23463416004595722 | validation: 0.224249237647806]
	TIME [epoch: 1.37 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22952471050718914		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.22952471050718914 | validation: 0.3465109625788053]
	TIME [epoch: 1.37 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265044463366296		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.265044463366296 | validation: 0.1642385280356735]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290726713845763		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.3290726713845763 | validation: 0.43514369077763454]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37417470227596283		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.37417470227596283 | validation: 0.32911716571770255]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280379900499602		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.280379900499602 | validation: 0.21026217822081306]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30427628929376227		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.30427628929376227 | validation: 0.30138408075521594]
	TIME [epoch: 170 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518159916627331		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.2518159916627331 | validation: 0.2929262047192715]
	TIME [epoch: 2.73 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250994068972279		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.250994068972279 | validation: 0.22508461415191122]
	TIME [epoch: 2.72 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22842297637677372		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.22842297637677372 | validation: 0.2610201245373391]
	TIME [epoch: 2.72 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22447925633622753		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.22447925633622753 | validation: 0.24937833675774732]
	TIME [epoch: 2.72 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23042633891991549		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.23042633891991549 | validation: 0.2617116350353786]
	TIME [epoch: 2.73 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2242554196971307		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.2242554196971307 | validation: 0.22943280697677643]
	TIME [epoch: 2.73 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22003538538894055		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.22003538538894055 | validation: 0.24659024849286057]
	TIME [epoch: 2.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22240491791002154		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.22240491791002154 | validation: 0.2167964806448031]
	TIME [epoch: 2.73 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2239352005322428		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.2239352005322428 | validation: 0.332026424134345]
	TIME [epoch: 2.72 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504125424216739		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.2504125424216739 | validation: 0.18896732484330936]
	TIME [epoch: 2.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754038126855907		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.2754038126855907 | validation: 0.3630795625070209]
	TIME [epoch: 2.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2741254013617867		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.2741254013617867 | validation: 0.23173504099972742]
	TIME [epoch: 2.72 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23429979999560016		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.23429979999560016 | validation: 0.24070125318301946]
	TIME [epoch: 2.72 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23085277449870922		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.23085277449870922 | validation: 0.29802228513128376]
	TIME [epoch: 2.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23930687120005278		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.23930687120005278 | validation: 0.20893248691768662]
	TIME [epoch: 2.73 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2231354727890907		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.2231354727890907 | validation: 0.27794734143277994]
	TIME [epoch: 2.73 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21902319902360695		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.21902319902360695 | validation: 0.2074173107359294]
	TIME [epoch: 2.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.224508642021644		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.224508642021644 | validation: 0.30981934787775994]
	TIME [epoch: 2.73 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24250985486582102		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.24250985486582102 | validation: 0.19418051714588078]
	TIME [epoch: 2.72 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562511205718904		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.2562511205718904 | validation: 0.3252855076786605]
	TIME [epoch: 2.73 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2589527119597752		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.2589527119597752 | validation: 0.21388830986271612]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2334498632605566		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.2334498632605566 | validation: 0.25441283245279717]
	TIME [epoch: 2.73 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22639352884847275		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.22639352884847275 | validation: 0.24402496234602664]
	TIME [epoch: 2.72 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22934944910979027		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.22934944910979027 | validation: 0.25625151295701976]
	TIME [epoch: 2.73 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24840204764924786		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.24840204764924786 | validation: 0.26767878715049215]
	TIME [epoch: 2.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22793447346492962		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.22793447346492962 | validation: 0.2475733901672501]
	TIME [epoch: 2.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2162760942499426		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.2162760942499426 | validation: 0.2216168818416521]
	TIME [epoch: 2.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21522610716823912		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.21522610716823912 | validation: 0.27673467934477103]
	TIME [epoch: 2.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22792311445827046		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.22792311445827046 | validation: 0.18590335147074946]
	TIME [epoch: 2.73 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2454656764710936		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.2454656764710936 | validation: 0.35612804046962154]
	TIME [epoch: 2.73 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29442771922742805		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.29442771922742805 | validation: 0.23851220810048385]
	TIME [epoch: 2.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336029141483095		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.2336029141483095 | validation: 0.24580432235781516]
	TIME [epoch: 2.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24447417394597737		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.24447417394597737 | validation: 0.29390217096447624]
	TIME [epoch: 2.73 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24150394481073256		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.24150394481073256 | validation: 0.19963211447668897]
	TIME [epoch: 2.72 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23009298549550397		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.23009298549550397 | validation: 0.308895584893332]
	TIME [epoch: 2.72 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2478610072597017		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.2478610072597017 | validation: 0.20575476312372076]
	TIME [epoch: 2.72 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24083721034802097		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.24083721034802097 | validation: 0.2849060344616344]
	TIME [epoch: 2.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22909361507335732		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.22909361507335732 | validation: 0.21987543722858086]
	TIME [epoch: 2.72 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22029188628501245		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.22029188628501245 | validation: 0.2473905600228739]
	TIME [epoch: 2.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21819402462850998		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.21819402462850998 | validation: 0.21925428778086978]
	TIME [epoch: 2.73 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21616519327748077		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.21616519327748077 | validation: 0.24070834258341955]
	TIME [epoch: 2.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21298747196457307		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.21298747196457307 | validation: 0.24052613869754033]
	TIME [epoch: 2.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21526519556592874		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.21526519556592874 | validation: 0.2158640720955913]
	TIME [epoch: 2.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22334320581000738		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.22334320581000738 | validation: 0.32520540887738836]
	TIME [epoch: 2.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526566710985306		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.2526566710985306 | validation: 0.20502332845751506]
	TIME [epoch: 2.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2264286587343316		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.2264286587343316 | validation: 0.2968268766063447]
	TIME [epoch: 2.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23273459660516424		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.23273459660516424 | validation: 0.19899828183853263]
	TIME [epoch: 2.72 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24318435264071825		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.24318435264071825 | validation: 0.30893315141647054]
	TIME [epoch: 2.73 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24636138457650625		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.24636138457650625 | validation: 0.20791224643471257]
	TIME [epoch: 2.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24514595565969163		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.24514595565969163 | validation: 0.25799890483740795]
	TIME [epoch: 2.73 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21997878286295539		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.21997878286295539 | validation: 0.22173260910117243]
	TIME [epoch: 2.73 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21330578524312674		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.21330578524312674 | validation: 0.20431834917050107]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21709272040335426		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.21709272040335426 | validation: 0.2980119629083472]
	TIME [epoch: 2.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23466067587965314		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.23466067587965314 | validation: 0.19186382089213447]
	TIME [epoch: 2.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22086176513828487		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.22086176513828487 | validation: 0.25357283748857684]
	TIME [epoch: 2.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20919532537754648		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.20919532537754648 | validation: 0.20561714575093804]
	TIME [epoch: 2.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20986900327837213		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.20986900327837213 | validation: 0.24760371488654143]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.209012793895059		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.209012793895059 | validation: 0.22067565590153893]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2129859333271491		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.2129859333271491 | validation: 0.29357058136897946]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.238841987287991		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.238841987287991 | validation: 0.2591007751864253]
	TIME [epoch: 2.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645862247981067		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.2645862247981067 | validation: 0.24397470555962478]
	TIME [epoch: 2.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22927296299315025		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.22927296299315025 | validation: 0.20835763861454712]
	TIME [epoch: 2.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2067694366277016		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.2067694366277016 | validation: 0.29334352453469253]
	TIME [epoch: 2.73 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23022407090176764		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.23022407090176764 | validation: 0.1892755966957744]
	TIME [epoch: 2.73 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2199615430517306		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.2199615430517306 | validation: 0.2617705034016245]
	TIME [epoch: 2.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22021067374850348		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.22021067374850348 | validation: 0.1946510409836202]
	TIME [epoch: 2.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24339797309202632		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.24339797309202632 | validation: 0.32673828747354333]
	TIME [epoch: 2.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25064802317662344		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.25064802317662344 | validation: 0.20199556060370885]
	TIME [epoch: 2.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21490346051120984		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.21490346051120984 | validation: 0.22461172393524267]
	TIME [epoch: 2.73 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20344184480937053		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.20344184480937053 | validation: 0.2601430469930434]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21432396527293507		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.21432396527293507 | validation: 0.1729287277698612]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490085207750815		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.2490085207750815 | validation: 0.28515319594413485]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23889477703020723		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.23889477703020723 | validation: 0.22142496908800213]
	TIME [epoch: 2.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20950532533490415		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.20950532533490415 | validation: 0.21622104020376925]
	TIME [epoch: 2.73 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22195683953853584		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.22195683953853584 | validation: 0.23012481819440317]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20689158708621874		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.20689158708621874 | validation: 0.235941480371153]
	TIME [epoch: 2.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20075441180655212		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.20075441180655212 | validation: 0.20937228281630055]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20443462492909648		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.20443462492909648 | validation: 0.24792229412641384]
	TIME [epoch: 2.74 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20388793205205352		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.20388793205205352 | validation: 0.2046501509811145]
	TIME [epoch: 2.74 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013668370283097		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.2013668370283097 | validation: 0.27365072312275046]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22537453698260015		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.22537453698260015 | validation: 0.1854424334495523]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2521041270069826		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.2521041270069826 | validation: 0.31086158693011884]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2446269465363949		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.2446269465363949 | validation: 0.22351371459694766]
	TIME [epoch: 2.74 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2118107256506916		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.2118107256506916 | validation: 0.21143570163160563]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2106365725639515		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.2106365725639515 | validation: 0.24880139763782175]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21171100178105007		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.21171100178105007 | validation: 0.18646134382552404]
	TIME [epoch: 2.73 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22306512570855838		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.22306512570855838 | validation: 0.2616184671096774]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21080636708553752		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.21080636708553752 | validation: 0.20537812844341113]
	TIME [epoch: 2.73 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20339482783176557		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.20339482783176557 | validation: 0.2526839784319317]
	TIME [epoch: 2.73 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20418426832560627		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.20418426832560627 | validation: 0.189522372173075]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20947857492144217		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.20947857492144217 | validation: 0.26557889585473243]
	TIME [epoch: 2.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2133174366106879		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.2133174366106879 | validation: 0.18264438344663386]
	TIME [epoch: 2.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22460572216502808		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.22460572216502808 | validation: 0.3068955462229557]
	TIME [epoch: 2.73 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2378301404710816		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.2378301404710816 | validation: 0.2125875720239991]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22458716152403832		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.22458716152403832 | validation: 0.22047619704416285]
	TIME [epoch: 2.73 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21645395652989494		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.21645395652989494 | validation: 0.2754277591157061]
	TIME [epoch: 2.73 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22229634910158452		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.22229634910158452 | validation: 0.2096227329961926]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_4_v_mmd1_20250503_135322/states/model_phi1_4a_distortion_v2_4_v_mmd1_598.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1351.089 seconds.
