Args:
Namespace(name='model_phi1_4a_distortion_v1_11_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_11/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_11/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.03956755, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 978364907

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.116416832662045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.116416832662045 | validation: 7.485830814924798]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.000106277951357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.000106277951357 | validation: 7.661243643685826]
	TIME [epoch: 0.764 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.523285762614032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.523285762614032 | validation: 7.225422797896439]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.380085052804301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.380085052804301 | validation: 7.559603232935135]
	TIME [epoch: 0.696 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.279986501338267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.279986501338267 | validation: 7.3889078242070925]
	TIME [epoch: 0.697 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.17382016476588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.17382016476588 | validation: 7.142999430402712]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.911638897044069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.911638897044069 | validation: 7.129261597498047]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.6614116581986265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6614116581986265 | validation: 7.33767828903337]
	TIME [epoch: 0.692 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.402478993867638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.402478993867638 | validation: 7.407423949366294]
	TIME [epoch: 0.694 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.98870713449323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.98870713449323 | validation: 7.285093009914703]
	TIME [epoch: 0.691 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.467432069771004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.467432069771004 | validation: 7.25990334504186]
	TIME [epoch: 0.694 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.953998527016514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.953998527016514 | validation: 6.730157082000781]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.819185381417951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.819185381417951 | validation: 7.193409459997125]
	TIME [epoch: 0.698 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.37011944560625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.37011944560625 | validation: 7.093123775154922]
	TIME [epoch: 0.694 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.20244166493251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.20244166493251 | validation: 6.778173089309755]
	TIME [epoch: 0.696 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.243256959559367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.243256959559367 | validation: 6.995825474037296]
	TIME [epoch: 0.696 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.057456036344127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.057456036344127 | validation: 6.856676665762442]
	TIME [epoch: 0.695 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.945513829981963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.945513829981963 | validation: 6.754429126914601]
	TIME [epoch: 0.695 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.872949521741614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.872949521741614 | validation: 6.7757804110489595]
	TIME [epoch: 0.695 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.836904410429973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.836904410429973 | validation: 6.4740715939147]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.84539659514759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.84539659514759 | validation: 6.665718582475112]
	TIME [epoch: 0.695 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.87834927096598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.87834927096598 | validation: 6.402935410817236]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.666165046676447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.666165046676447 | validation: 6.389771577280438]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.499025698719504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.499025698719504 | validation: 6.307681741892368]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.416744040898279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.416744040898279 | validation: 6.138054364901574]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.364767005261504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.364767005261504 | validation: 6.278207163953905]
	TIME [epoch: 0.693 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5399158253842655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5399158253842655 | validation: 6.048750599960029]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.684011657253599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.684011657253599 | validation: 6.031023211908476]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3211830113926135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3211830113926135 | validation: 6.053442259412768]
	TIME [epoch: 0.692 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.238720183173928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.238720183173928 | validation: 5.875935790087081]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.31136081733647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.31136081733647 | validation: 5.995581609674004]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.205013471867504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.205013471867504 | validation: 5.802272716332856]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0725500865533375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0725500865533375 | validation: 5.769784632319178]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.966976629543649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.966976629543649 | validation: 5.7787542383633115]
	TIME [epoch: 0.692 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.95961044373894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.95961044373894 | validation: 5.671364659075362]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.032139601153991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032139601153991 | validation: 5.742281089261394]
	TIME [epoch: 0.694 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.981560316828056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.981560316828056 | validation: 5.595053848781865]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.837888197643789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.837888197643789 | validation: 5.595249784909271]
	TIME [epoch: 0.695 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8115802702955426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8115802702955426 | validation: 5.59916109152698]
	TIME [epoch: 0.691 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8087757152382897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8087757152382897 | validation: 5.5514560619118924]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.873144127066327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.873144127066327 | validation: 5.562960436420841]
	TIME [epoch: 0.695 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.79632778509702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.79632778509702 | validation: 5.50105463799093]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7557656131498276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7557656131498276 | validation: 5.510074346792027]
	TIME [epoch: 0.692 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7414946384065066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7414946384065066 | validation: 5.464642896273661]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.737943992191479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737943992191479 | validation: 5.487387867064717]
	TIME [epoch: 0.695 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7228456160909706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7228456160909706 | validation: 5.416233250704187]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7047964716391992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7047964716391992 | validation: 5.450274578513604]
	TIME [epoch: 0.693 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6953018888073417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6953018888073417 | validation: 5.3922669737739515]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6770855575647317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6770855575647317 | validation: 5.400456581688989]
	TIME [epoch: 0.694 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.647402949234364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.647402949234364 | validation: 5.365429309518103]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.633275821830792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.633275821830792 | validation: 5.350239909767423]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6193707204361836		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.6193707204361836 | validation: 5.330202319567511]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6027112744714227		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.6027112744714227 | validation: 5.317757048020383]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5954935240092754		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.5954935240092754 | validation: 5.304944293768862]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5832751078733396		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.5832751078733396 | validation: 5.29653955749108]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.569270185767715		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.569270185767715 | validation: 5.268437610527244]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5632514024890973		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.5632514024890973 | validation: 5.266826242875785]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5537056268236427		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.5537056268236427 | validation: 5.227225530334812]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5540594771239933		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.5540594771239933 | validation: 5.253677813242856]
	TIME [epoch: 0.698 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.54689910933019		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.54689910933019 | validation: 5.192071697851396]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.542564069565862		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.542564069565862 | validation: 5.195715029549126]
	TIME [epoch: 0.696 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5099220633248316		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.5099220633248316 | validation: 5.162748082745328]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.493755531802349		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.493755531802349 | validation: 5.134177441821085]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4827117351477965		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.4827117351477965 | validation: 5.129586607325503]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4817285357397134		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.4817285357397134 | validation: 5.0897980323699406]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.491021095849295		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.491021095849295 | validation: 5.098781005579409]
	TIME [epoch: 0.698 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4864504161031253		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.4864504161031253 | validation: 4.992157055255548]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.49013633650424		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.49013633650424 | validation: 4.920788755487721]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4036804739599757		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.4036804739599757 | validation: 4.683250110562611]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.254022223198694		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.254022223198694 | validation: 4.14599029340387]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0183641897414573		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.0183641897414573 | validation: 4.076159410618878]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7309988404545504		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.7309988404545504 | validation: 4.494674069427771]
	TIME [epoch: 0.696 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1441750955298926		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.1441750955298926 | validation: 3.9620393084444374]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.846700408795002		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.846700408795002 | validation: 3.889989661670173]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.647300981319852		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.647300981319852 | validation: 4.041808841421508]
	TIME [epoch: 0.695 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7120739428924674		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.7120739428924674 | validation: 3.8090098352930895]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.600308230927078		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.600308230927078 | validation: 3.787297505985862]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5684374899730553		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.5684374899730553 | validation: 3.7966358453625326]
	TIME [epoch: 0.695 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.567287454781397		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.567287454781397 | validation: 3.720670752301115]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5355457052393082		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.5355457052393082 | validation: 3.6755719508132616]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.514092439561756		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.514092439561756 | validation: 3.6662581430943413]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.49760414223002		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.49760414223002 | validation: 3.5849491565408727]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.493240421330818		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.493240421330818 | validation: 3.623444644766235]
	TIME [epoch: 0.696 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.526177552732305		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.526177552732305 | validation: 3.531742127775134]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6687953248433054		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.6687953248433054 | validation: 3.594765470673535]
	TIME [epoch: 0.695 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5468596806207033		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.5468596806207033 | validation: 3.4121535480301612]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4285122525808687		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.4285122525808687 | validation: 3.3608890184473617]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.426407856701189		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.426407856701189 | validation: 3.3753716997416334]
	TIME [epoch: 0.7 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4088058093937583		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.4088058093937583 | validation: 3.2621792920975126]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3788822042636957		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.3788822042636957 | validation: 3.258820753932809]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.342934379299322		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.342934379299322 | validation: 3.1356190277320724]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3338343766013203		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.3338343766013203 | validation: 3.1826872976976226]
	TIME [epoch: 0.696 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3334135440820165		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.3334135440820165 | validation: 3.110968194223442]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.380286653260076		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.380286653260076 | validation: 3.0863680259659487]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2943396870784767		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.2943396870784767 | validation: 2.9437848226415597]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2574574805963556		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.2574574805963556 | validation: 2.896234477211918]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.220087582812028		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.220087582812028 | validation: 2.8391507399605302]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1911408858434602		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.1911408858434602 | validation: 2.6989303959158577]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2216204618154665		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.2216204618154665 | validation: 3.2487849083542977]
	TIME [epoch: 0.696 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4851436148840698		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.4851436148840698 | validation: 2.639565481092826]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.294694370285804		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.294694370285804 | validation: 2.6198565618690832]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1103552122705485		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.1103552122705485 | validation: 2.592613140794743]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1048864616759877		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.1048864616759877 | validation: 2.333265258600617]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1029997574137793		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.1029997574137793 | validation: 2.5721576708045215]
	TIME [epoch: 0.698 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.125796600366261		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.125796600366261 | validation: 2.279738045309923]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.12596491900733		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.12596491900733 | validation: 2.415196271557955]
	TIME [epoch: 0.697 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.03348353681184		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.03348353681184 | validation: 1.9095583004027123]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8836459331594357		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.8836459331594357 | validation: 1.71821854733807]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7398432959738137		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.7398432959738137 | validation: 1.3306945954499838]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6454734666559228		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6454734666559228 | validation: 3.0065590046386355]
	TIME [epoch: 0.695 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4046566740243085		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.4046566740243085 | validation: 1.0815161907577935]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5754602063952279		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5754602063952279 | validation: 1.5069853271365004]
	TIME [epoch: 0.697 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5226338862038602		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5226338862038602 | validation: 1.1842830085527798]
	TIME [epoch: 0.698 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4381437250122668		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.4381437250122668 | validation: 2.226838185116552]
	TIME [epoch: 0.694 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.853595177807751		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.853595177807751 | validation: 1.7004261202053224]
	TIME [epoch: 0.698 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.837678318524358		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.837678318524358 | validation: 1.6703765150411145]
	TIME [epoch: 0.693 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5207128316788812		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.5207128316788812 | validation: 1.1027532886593299]
	TIME [epoch: 0.692 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3454672749697398		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3454672749697398 | validation: 1.2660611556675332]
	TIME [epoch: 0.692 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.352982271463518		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.352982271463518 | validation: 1.3393294451262685]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.472509580598811		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.472509580598811 | validation: 1.653515633210952]
	TIME [epoch: 0.693 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4976667743458896		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.4976667743458896 | validation: 1.0104989015433632]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3030574216407527		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.3030574216407527 | validation: 1.2077633113953796]
	TIME [epoch: 0.696 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2689278391986472		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.2689278391986472 | validation: 1.2295721648132703]
	TIME [epoch: 0.695 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2952522153704058		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2952522153704058 | validation: 1.3857008057917068]
	TIME [epoch: 0.694 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.306241740945353		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.306241740945353 | validation: 1.0911483201145513]
	TIME [epoch: 0.694 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415937934052124		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.415937934052124 | validation: 1.396632114588368]
	TIME [epoch: 0.695 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.277856948844595		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.277856948844595 | validation: 1.0771419133168327]
	TIME [epoch: 0.696 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439455306732602		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.1439455306732602 | validation: 1.0422958468698267]
	TIME [epoch: 0.694 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108074086891864		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.1108074086891864 | validation: 0.9842221617812357]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1739142947785763		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.1739142947785763 | validation: 1.5632200332343338]
	TIME [epoch: 0.694 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3162135631135403		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.3162135631135403 | validation: 0.9125075269791417]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137921157674652		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.137921157674652 | validation: 1.1526803480516141]
	TIME [epoch: 0.693 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097116282491334		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.097116282491334 | validation: 1.0185727685921473]
	TIME [epoch: 0.693 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1382216421766613		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.1382216421766613 | validation: 1.358180667506883]
	TIME [epoch: 0.691 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1637572375648568		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.1637572375648568 | validation: 0.8964970263261975]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0651397260143347		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.0651397260143347 | validation: 1.1042009802968635]
	TIME [epoch: 0.697 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017181728758658		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.017181728758658 | validation: 0.993447321248482]
	TIME [epoch: 0.697 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199992562516023		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0199992562516023 | validation: 1.1118125506559715]
	TIME [epoch: 0.694 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746814985969871		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.1746814985969871 | validation: 1.0846142898732765]
	TIME [epoch: 0.694 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2559145562446619		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2559145562446619 | validation: 1.2259655599737038]
	TIME [epoch: 0.694 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0340298799587575		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.0340298799587575 | validation: 0.8286043088527424]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9866060865998229		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.9866060865998229 | validation: 1.0700021968074702]
	TIME [epoch: 0.696 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.963053171451769		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.963053171451769 | validation: 0.803021467295173]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959040443627101		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.959040443627101 | validation: 1.124766027281937]
	TIME [epoch: 0.696 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9848621562856377		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.9848621562856377 | validation: 0.9965412615392687]
	TIME [epoch: 0.699 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0018646589168145		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.0018646589168145 | validation: 1.0397955349623929]
	TIME [epoch: 0.697 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9875933141975597		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.9875933141975597 | validation: 0.9258279022443192]
	TIME [epoch: 0.695 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9394207437307353		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.9394207437307353 | validation: 1.058326905609391]
	TIME [epoch: 0.696 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9384131307225028		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9384131307225028 | validation: 0.873479575471436]
	TIME [epoch: 0.696 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091525245892836		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9091525245892836 | validation: 1.1230657125528174]
	TIME [epoch: 0.697 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.910412163943714		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.910412163943714 | validation: 0.8213804254068183]
	TIME [epoch: 0.699 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8809327974715288		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8809327974715288 | validation: 1.1061428933510828]
	TIME [epoch: 0.702 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8952348888571489		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8952348888571489 | validation: 0.7322512032973792]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450870887875923		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9450870887875923 | validation: 1.2100498667548063]
	TIME [epoch: 0.696 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9296517798554272		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.9296517798554272 | validation: 0.7020177047041171]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596325409484159		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8596325409484159 | validation: 0.9262759357118346]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401085527394417		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8401085527394417 | validation: 0.6743674677004394]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297709518299816		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8297709518299816 | validation: 0.9015842608678089]
	TIME [epoch: 0.696 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877532795457728		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7877532795457728 | validation: 0.6236791829042743]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781527317190141		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7781527317190141 | validation: 1.1672855040869767]
	TIME [epoch: 0.695 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993856668035861		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8993856668035861 | validation: 0.88272300380233]
	TIME [epoch: 0.695 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542380321460763		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9542380321460763 | validation: 1.1263231683445472]
	TIME [epoch: 0.698 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013502090580529		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.013502090580529 | validation: 0.9574173438415703]
	TIME [epoch: 0.694 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448266732984031		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.9448266732984031 | validation: 0.8439099408995467]
	TIME [epoch: 0.695 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812131601604361		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7812131601604361 | validation: 0.7948478051731677]
	TIME [epoch: 0.693 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.742774595132281		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.742774595132281 | validation: 0.922951247259563]
	TIME [epoch: 0.694 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421688117596088		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7421688117596088 | validation: 0.6986268322896416]
	TIME [epoch: 0.694 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227375554030667		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8227375554030667 | validation: 1.083944503108094]
	TIME [epoch: 0.694 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586802377205273		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8586802377205273 | validation: 0.7917106688051837]
	TIME [epoch: 0.696 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784646865124074		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7784646865124074 | validation: 0.7741995142177753]
	TIME [epoch: 0.697 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986820126801609		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7986820126801609 | validation: 0.9535993941997781]
	TIME [epoch: 0.694 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069401619003546		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8069401619003546 | validation: 0.6778897214736281]
	TIME [epoch: 0.694 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085337092793553		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7085337092793553 | validation: 0.7338815229986752]
	TIME [epoch: 0.694 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675332101043558		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.675332101043558 | validation: 0.9783265272944114]
	TIME [epoch: 0.694 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808868953506946		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7808868953506946 | validation: 0.7496837261675305]
	TIME [epoch: 0.695 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986121144127003		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8986121144127003 | validation: 1.072921728810394]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775187553997593		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7775187553997593 | validation: 0.5913631386269796]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6252214640889957		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6252214640889957 | validation: 0.810204734879071]
	TIME [epoch: 0.695 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333640592963408		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6333640592963408 | validation: 0.6257749662257514]
	TIME [epoch: 0.694 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847584968721057		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6847584968721057 | validation: 1.1031440934972199]
	TIME [epoch: 0.697 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779516381594674		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.779516381594674 | validation: 0.49853328531695934]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321998765380997		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7321998765380997 | validation: 0.9122493683301036]
	TIME [epoch: 0.693 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550227191168959		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6550227191168959 | validation: 0.5730166494586542]
	TIME [epoch: 0.692 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440086213546959		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6440086213546959 | validation: 0.8714988820037505]
	TIME [epoch: 0.692 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266460714744683		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6266460714744683 | validation: 0.5605932826131629]
	TIME [epoch: 0.69 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605747225609228		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.605747225609228 | validation: 0.9409252658950935]
	TIME [epoch: 0.691 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6474456736528458		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6474456736528458 | validation: 0.562162169065145]
	TIME [epoch: 0.691 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123558183729423		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7123558183729423 | validation: 0.9946332432666992]
	TIME [epoch: 0.692 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491459158706966		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7491459158706966 | validation: 0.8358142059520509]
	TIME [epoch: 0.691 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940021278729657		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8940021278729657 | validation: 0.8696137784708884]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639924745801956		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8639924745801956 | validation: 0.7672653055415348]
	TIME [epoch: 0.693 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932338515561273		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5932338515561273 | validation: 0.5507675985597823]
	TIME [epoch: 0.695 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5255103039584177		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.5255103039584177 | validation: 0.6730290518750539]
	TIME [epoch: 0.693 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716202671021928		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5716202671021928 | validation: 0.8458940515754979]
	TIME [epoch: 0.693 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685668521572685		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6685668521572685 | validation: 0.7024230763593334]
	TIME [epoch: 0.694 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250041783457581		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7250041783457581 | validation: 0.8581206727687452]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293089169069223		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6293089169069223 | validation: 0.5772215473978418]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5507710495794119		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.5507710495794119 | validation: 0.7732662043580842]
	TIME [epoch: 0.692 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531729223743141		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.5531729223743141 | validation: 0.6025463242636567]
	TIME [epoch: 0.692 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5903933106963016		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5903933106963016 | validation: 0.8821753521286706]
	TIME [epoch: 0.693 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618212200252259		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.618212200252259 | validation: 0.5494936927858058]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604657610254344		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.604657610254344 | validation: 0.7734693549161037]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5577973145337475		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5577973145337475 | validation: 0.6344016712695357]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5612143069800452		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.5612143069800452 | validation: 0.7653884032192947]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589743655204683		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.589743655204683 | validation: 0.7190981365762321]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5937894551746155		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5937894551746155 | validation: 0.6391798985137038]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540503032008063		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6540503032008063 | validation: 0.8444475889198639]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6452586610656915		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6452586610656915 | validation: 0.44797225028198967]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193046397781194		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.5193046397781194 | validation: 0.6199776111558921]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44458945267005107		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.44458945267005107 | validation: 0.5430918983963007]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44477836405753607		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.44477836405753607 | validation: 0.6851885444442158]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793446082064303		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5793446082064303 | validation: 0.9770791349634161]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460167310785881		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7460167310785881 | validation: 0.48275987292519124]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5601592476916677		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.5601592476916677 | validation: 0.7735256667039394]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4851410886671171		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.4851410886671171 | validation: 0.4993930066615724]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5572511034579664		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5572511034579664 | validation: 0.8450328134502091]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5704047037902036		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.5704047037902036 | validation: 0.35725711756794776]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.511659338577215		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.511659338577215 | validation: 0.7255102234436493]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399355092370216		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4399355092370216 | validation: 0.4964132945825894]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4791328132860025		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.4791328132860025 | validation: 0.7727877558467768]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005655913800878		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5005655913800878 | validation: 0.5646152512040556]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5533648244520363		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.5533648244520363 | validation: 0.7742776133378069]
	TIME [epoch: 1.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312778856220568		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6312778856220568 | validation: 0.6629466446456743]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496744797117628		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.5496744797117628 | validation: 0.5932974228545519]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874434321112409		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4874434321112409 | validation: 0.6517894805244112]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4732345977187696		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4732345977187696 | validation: 0.5010144475937747]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4855324758295066		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.4855324758295066 | validation: 0.672429329165542]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4956835842938405		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4956835842938405 | validation: 0.5610763319943354]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4622467638200976		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.4622467638200976 | validation: 0.6995953536620224]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510026972494377		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.510026972494377 | validation: 0.4975372068240887]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42574492548750736		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.42574492548750736 | validation: 0.5858596083080038]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4360047244685964		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.4360047244685964 | validation: 0.6029346295912849]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45250964660729465		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.45250964660729465 | validation: 0.5947982386186101]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5012551209930745		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5012551209930745 | validation: 0.850607900124153]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881137155171449		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.5881137155171449 | validation: 0.40270423701030955]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5426615736394339		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.5426615736394339 | validation: 0.7055872247977361]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41682832369135947		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.41682832369135947 | validation: 0.39003264615022754]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4268641959395923		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.4268641959395923 | validation: 0.7464840672528291]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4855692189295829		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.4855692189295829 | validation: 0.30843563145762287]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4609658240336135		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4609658240336135 | validation: 0.6954985467345737]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4062897255115604		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.4062897255115604 | validation: 0.45655974736760474]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40636760077259515		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.40636760077259515 | validation: 0.6495349830479246]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43461697748406425		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.43461697748406425 | validation: 0.5410705470193896]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215842505622486		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5215842505622486 | validation: 0.6290128401420976]
	TIME [epoch: 1.37 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5182355797791296		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.5182355797791296 | validation: 0.5341540606377744]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44097679645618293		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.44097679645618293 | validation: 0.5155593665060735]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38797013197741875		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.38797013197741875 | validation: 0.5402941336652454]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40903049406028147		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.40903049406028147 | validation: 0.49295097357856454]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42707136518009736		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.42707136518009736 | validation: 0.561622209264263]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4323586782989807		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4323586782989807 | validation: 0.4731271860680492]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4144579701411233		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.4144579701411233 | validation: 0.5367942342834006]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38636869136346114		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.38636869136346114 | validation: 0.5315311801933507]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381782584692602		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.381782584692602 | validation: 0.461910930441827]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4747357680163687		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.4747357680163687 | validation: 0.7956158996670013]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261516165375089		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5261516165375089 | validation: 0.3333203652155396]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4322808519390904		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.4322808519390904 | validation: 0.6585153582384156]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39366768282724723		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.39366768282724723 | validation: 0.3259480238391131]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38717338786857625		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.38717338786857625 | validation: 0.6285331006457834]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3821031984080788		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.3821031984080788 | validation: 0.2931838700090574]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.352689589269431		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.352689589269431 | validation: 0.6323322644672907]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.361642105471247		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.361642105471247 | validation: 0.36200862881557]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36354404884958513		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.36354404884958513 | validation: 0.6747122052082088]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42237686569789235		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.42237686569789235 | validation: 0.40545327833245415]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4965688618253347		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.4965688618253347 | validation: 0.5688557742294135]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4238009943425476		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4238009943425476 | validation: 0.6031081072468943]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.463261503077551		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.463261503077551 | validation: 0.40044543632623797]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3977160988851227		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.3977160988851227 | validation: 0.4972234315469544]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36202029394170937		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.36202029394170937 | validation: 0.45233442881121805]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34155777081896693		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.34155777081896693 | validation: 0.45165131971206063]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380605638384771		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.380605638384771 | validation: 0.6058164860599018]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40995628551025465		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.40995628551025465 | validation: 0.37210969419572465]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37840487614688145		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.37840487614688145 | validation: 0.596245012752504]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3522065026964614		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.3522065026964614 | validation: 0.3122067732098517]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35375947178275846		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.35375947178275846 | validation: 0.6052372898712837]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34900448959288627		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.34900448959288627 | validation: 0.27947583031854606]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35327501240273806		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.35327501240273806 | validation: 0.5833977775359165]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37210322173709814		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.37210322173709814 | validation: 0.2760585357122231]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34436427876440545		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.34436427876440545 | validation: 0.5880442699315133]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34150451555790184		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.34150451555790184 | validation: 0.29889036219114534]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414396957468545		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.4414396957468545 | validation: 0.455186341837738]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2912246664228057		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.2912246664228057 | validation: 0.3020665400339208]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3356987007590492		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.3356987007590492 | validation: 0.6520308933903916]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43930402078016484		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.43930402078016484 | validation: 0.45146384926104305]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42561938743358396		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.42561938743358396 | validation: 0.5307688845704117]
	TIME [epoch: 1.87 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5223179823015617		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5223179823015617 | validation: 0.5125275823006009]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3815797206309644		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3815797206309644 | validation: 0.3444805898693384]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2760447137927771		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.2760447137927771 | validation: 0.3620865725060995]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31700000086479824		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.31700000086479824 | validation: 0.5989403352322967]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3958458864898165		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3958458864898165 | validation: 0.3695211945127456]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34210700803016264		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.34210700803016264 | validation: 0.4401594365426322]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313205762816925		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.313205762816925 | validation: 0.43942724038064007]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32718843169224565		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.32718843169224565 | validation: 0.369300171836649]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3333347165627214		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.3333347165627214 | validation: 0.46910307212486074]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34983490601833694		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.34983490601833694 | validation: 0.3910001978776574]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183506555634935		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.3183506555634935 | validation: 0.39290920687573566]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122407390146082		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.3122407390146082 | validation: 0.4178404115387746]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30560851645737863		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.30560851645737863 | validation: 0.36750719273920984]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3646005166364117		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.3646005166364117 | validation: 0.5981418324314849]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3610648380878334		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.3610648380878334 | validation: 0.2836935906919919]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29807481264224756		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.29807481264224756 | validation: 0.5086896157488267]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905388274208908		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.2905388274208908 | validation: 0.23366288851415828]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3300643256858833		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3300643256858833 | validation: 0.559548130363389]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32480750387926166		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.32480750387926166 | validation: 0.2457996101019628]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756322710871302		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.2756322710871302 | validation: 0.45253775270376906]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603585311843239		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.2603585311843239 | validation: 0.26391091075404743]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687360084201826		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.2687360084201826 | validation: 0.5204768511679045]
	TIME [epoch: 1.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944722752858386		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.2944722752858386 | validation: 0.25344613815879147]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007412628798662		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.3007412628798662 | validation: 0.5193492450736027]
	TIME [epoch: 1.37 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31749244535712684		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.31749244535712684 | validation: 0.36388167571058805]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34876004520764986		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.34876004520764986 | validation: 0.5148795821877219]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4459755718430995		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.4459755718430995 | validation: 0.4501590533527132]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36058490028648493		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.36058490028648493 | validation: 0.29995550502690155]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27805387467091985		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.27805387467091985 | validation: 0.35989768293159136]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26210008841836385		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.26210008841836385 | validation: 0.36997935821007694]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28207918145397426		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.28207918145397426 | validation: 0.36423567444554733]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116227546547468		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.3116227546547468 | validation: 0.48950587989125993]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34159043294184044		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.34159043294184044 | validation: 0.2619215481644756]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30923430387806966		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.30923430387806966 | validation: 0.48264437833190676]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2790215629154392		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.2790215629154392 | validation: 0.19785883716944444]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2712138016534808		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2712138016534808 | validation: 0.5016967209507449]
	TIME [epoch: 1.37 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30396602723414573		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.30396602723414573 | validation: 0.20074411798920339]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2371512889459464		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.2371512889459464 | validation: 0.3420221191835858]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20457315546550114		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.20457315546550114 | validation: 0.28711960978286327]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24908076187624748		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.24908076187624748 | validation: 0.4414694487618489]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957089950889307		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.2957089950889307 | validation: 0.4243498437691088]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37530735664930315		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.37530735664930315 | validation: 0.3796816654973312]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482667433393497		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3482667433393497 | validation: 0.32842635231850936]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2720695987388047		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2720695987388047 | validation: 0.38368703421319794]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25124050473528886		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.25124050473528886 | validation: 0.30657228166815653]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32641920787145445		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.32641920787145445 | validation: 0.42820474657363905]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27467227103833664		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.27467227103833664 | validation: 0.22638376032493623]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27701430215764467		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.27701430215764467 | validation: 0.39517200106050715]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2278742441845787		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.2278742441845787 | validation: 0.2673758514327273]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2301786572505953		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.2301786572505953 | validation: 0.38237914957570546]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2287095218014876		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.2287095218014876 | validation: 0.26916036005674365]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23227259504772468		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.23227259504772468 | validation: 0.4272779518665412]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32640075388632617		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.32640075388632617 | validation: 0.3916250019355194]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37066528158517287		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.37066528158517287 | validation: 0.3059215262885353]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30642183077463087		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.30642183077463087 | validation: 0.45102929960725835]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31475116106792284		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.31475116106792284 | validation: 0.19774318271809796]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25078829234561956		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.25078829234561956 | validation: 0.2371506823374151]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17876686458477245		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.17876686458477245 | validation: 0.28845434825157973]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669197070390243		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.2669197070390243 | validation: 0.34708849434647016]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26346078359856906		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.26346078359856906 | validation: 0.3167139568474013]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26883853376968353		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.26883853376968353 | validation: 0.3125789127188025]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739921926768533		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.2739921926768533 | validation: 0.49014383465189726]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3064393993318923		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.3064393993318923 | validation: 0.2197579032491781]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21729192003839992		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.21729192003839992 | validation: 0.32279590978250194]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19484429573886108		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.19484429573886108 | validation: 0.2202102515776209]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130990027776687		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2130990027776687 | validation: 0.39768915278292766]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22696649364026605		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.22696649364026605 | validation: 0.28463306626688856]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22228579337122206		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.22228579337122206 | validation: 0.3108956731225607]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.273528188141549		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.273528188141549 | validation: 0.3622064148715594]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31837349055159486		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.31837349055159486 | validation: 0.3073736980766938]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24481678207702165		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.24481678207702165 | validation: 0.25957517079600356]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071350682869337		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.2071350682869337 | validation: 0.31586517277016707]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21435892014321098		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.21435892014321098 | validation: 0.27402920069841435]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.256207722629718		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.256207722629718 | validation: 0.4300612784772937]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25736604852804135		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.25736604852804135 | validation: 0.20917717921617307]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21645134429613144		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.21645134429613144 | validation: 0.35278706760855416]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20208376828599237		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.20208376828599237 | validation: 0.1897076541196861]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1920003476500167		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.1920003476500167 | validation: 0.40992977381379475]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2240303881407839		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.2240303881407839 | validation: 0.15979225195393046]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.242112312127211		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.242112312127211 | validation: 0.40923683925453114]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24076539852607806		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.24076539852607806 | validation: 0.14446770948872642]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20036331136658447		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.20036331136658447 | validation: 0.3188871267522426]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18782691689841766		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.18782691689841766 | validation: 0.13862287600173467]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18949256589485827		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.18949256589485827 | validation: 0.3498712079644546]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20545635184155095		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.20545635184155095 | validation: 0.17827852813621783]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1913670380954694		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.1913670380954694 | validation: 0.3479385503582402]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19415146536002342		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.19415146536002342 | validation: 0.2562406905836184]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23098247191166393		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.23098247191166393 | validation: 0.48928997858178136]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3542699395125308		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.3542699395125308 | validation: 0.37368101248219626]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3295228257153926		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.3295228257153926 | validation: 0.2506415300905149]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25735977061246323		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.25735977061246323 | validation: 0.33446279012026087]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2501994258711347		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2501994258711347 | validation: 0.19377004928365638]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18057198187420498		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.18057198187420498 | validation: 0.21730089645981027]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1730664384164983		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1730664384164983 | validation: 0.32987629514063144]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24126052075702334		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.24126052075702334 | validation: 0.26155276871773475]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24435407214402918		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.24435407214402918 | validation: 0.30134438692938553]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20643212350954193		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.20643212350954193 | validation: 0.20554831861333836]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18936356754389064		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.18936356754389064 | validation: 0.28986173135685656]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18939775362187233		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.18939775362187233 | validation: 0.23380178543318264]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19605873391772066		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.19605873391772066 | validation: 0.3034127050623545]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20090735183376318		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.20090735183376318 | validation: 0.23697013384270768]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19637651098387632		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.19637651098387632 | validation: 0.2585991426315286]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20942845226805495		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.20942845226805495 | validation: 0.26144419631556387]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21928392024190005		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.21928392024190005 | validation: 0.2514697209811457]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062232652654867		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.2062232652654867 | validation: 0.2767734768604351]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001068367778309		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.2001068367778309 | validation: 0.19546774888918214]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19583469889781974		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.19583469889781974 | validation: 0.2883608831390447]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19935258473114545		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.19935258473114545 | validation: 0.1964187948977835]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18622414945747587		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.18622414945747587 | validation: 0.255726919299892]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18164622936990543		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.18164622936990543 | validation: 0.22476977058070213]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17304443926385127		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.17304443926385127 | validation: 0.22782718389315343]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19910758907438983		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.19910758907438983 | validation: 0.43620681328325034]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28481287805823874		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.28481287805823874 | validation: 0.1910670547161989]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20919020681860198		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.20919020681860198 | validation: 0.2239346124324281]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14459995947424922		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.14459995947424922 | validation: 0.19082540239143322]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14798862881021677		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.14798862881021677 | validation: 0.20697854150480635]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17406938497270466		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.17406938497270466 | validation: 0.29407249290172616]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21456548996676394		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.21456548996676394 | validation: 0.23194169954176658]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21948861281308304		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.21948861281308304 | validation: 0.24756935556560464]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1858372651131855		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.1858372651131855 | validation: 0.2647480127011038]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17371255011660605		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.17371255011660605 | validation: 0.184771795946314]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19156427209903434		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.19156427209903434 | validation: 0.3410402104927311]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22082220568170077		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.22082220568170077 | validation: 0.1526367291407641]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955652341500007		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.1955652341500007 | validation: 0.3190129788871089]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16691264413190837		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.16691264413190837 | validation: 0.14263166468367705]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13508870772213477		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.13508870772213477 | validation: 0.21372413965051829]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563753753401678		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.13563753753401678 | validation: 0.14001366957410896]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14543582383092854		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.14543582383092854 | validation: 0.4983055654178549]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823115506351414		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2823115506351414 | validation: 0.24444413473637672]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22351679363732493		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.22351679363732493 | validation: 0.17047174130816223]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678257126748019		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.11678257126748019 | validation: 0.2081273865452142]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18961835977816763		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.18961835977816763 | validation: 0.3446137456715514]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28019920415461347		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.28019920415461347 | validation: 0.3068229021448963]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26664772194578473		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.26664772194578473 | validation: 0.18990884884570522]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15931588474015101		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.15931588474015101 | validation: 0.16743071464104856]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13668239880935434		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.13668239880935434 | validation: 0.1920323774825313]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14621984212030686		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.14621984212030686 | validation: 0.2281407388346239]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1796182859917138		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.1796182859917138 | validation: 0.25711565265456354]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19582149147277114		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.19582149147277114 | validation: 0.19729067918786103]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1779142228133818		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.1779142228133818 | validation: 0.22602171536334248]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15728831046763003		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.15728831046763003 | validation: 0.16791956436377428]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153793284470334		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.153793284470334 | validation: 0.25264862227349677]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16406521400861648		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.16406521400861648 | validation: 0.17023836942061618]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15541564332587698		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.15541564332587698 | validation: 0.25979178340417514]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15758435299265453		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.15758435299265453 | validation: 0.15103829410078534]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15532274851647457		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.15532274851647457 | validation: 0.23625545395033606]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056454461928104		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.15056454461928104 | validation: 0.1679428939720891]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14410108200337823		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.14410108200337823 | validation: 0.2335781779501522]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15570758832657552		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.15570758832657552 | validation: 0.20754684910956414]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16736061413805586		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.16736061413805586 | validation: 0.19597888887905737]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22663439709489372		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.22663439709489372 | validation: 0.30273415942286036]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23326774991552554		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.23326774991552554 | validation: 0.1349381119191457]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416431618387348		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.1416431618387348 | validation: 0.15969803944666092]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090495236821405		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.1090495236821405 | validation: 0.16695793085268695]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11166384312271617		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.11166384312271617 | validation: 0.1610891444425737]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389399176030505		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1389399176030505 | validation: 0.28879052422389506]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1833188329019654		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.1833188329019654 | validation: 0.17309360477948996]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17907493341978545		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.17907493341978545 | validation: 0.24629272508837208]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16077385204695632		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.16077385204695632 | validation: 0.18582440742278786]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14700535732237252		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.14700535732237252 | validation: 0.17160520008008381]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784977113271327		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1784977113271327 | validation: 0.27640115363127016]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1976213172134439		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.1976213172134439 | validation: 0.1385761596251864]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145220714461443		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.145220714461443 | validation: 0.15878925599574545]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564396672123406		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.11564396672123406 | validation: 0.1746679529715828]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315789864837186		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.12315789864837186 | validation: 0.16921713762860216]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631049630083112		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.1631049630083112 | validation: 0.2902677989307521]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20316155452756757		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.20316155452756757 | validation: 0.15640508029842373]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.143953642968659		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.143953642968659 | validation: 0.18066841510407913]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11820824190186364		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.11820824190186364 | validation: 0.14990854868318962]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12537169970159076		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.12537169970159076 | validation: 0.1673803968790383]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427528875793608		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.1427528875793608 | validation: 0.22675787039450448]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1703523800202571		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.1703523800202571 | validation: 0.1730181277136137]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16928417104711002		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.16928417104711002 | validation: 0.1979632259380274]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14308877469245596		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.14308877469245596 | validation: 0.1425806586276968]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11677829012377508		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.11677829012377508 | validation: 0.14211859644555846]
	TIME [epoch: 1.36 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12364281374971552		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.12364281374971552 | validation: 0.24127108439954173]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15577406383784173		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.15577406383784173 | validation: 0.16780256256599868]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17258432255314607		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.17258432255314607 | validation: 0.24828934852699516]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15421161003885495		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.15421161003885495 | validation: 0.11163311998075223]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12226425930307565		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.12226425930307565 | validation: 0.1873190433531333]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10869757502446245		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.10869757502446245 | validation: 0.12692403837797922]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11211022551636025		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.11211022551636025 | validation: 0.20158321796360845]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12258178161802191		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.12258178161802191 | validation: 0.1597207294634783]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366917626953407		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.1366917626953407 | validation: 0.20239920770823186]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162335922767056		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.162335922767056 | validation: 0.20500121984604994]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1820916066113095		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1820916066113095 | validation: 0.15466286717438524]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15731827936045809		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.15731827936045809 | validation: 0.21619257371983452]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14495810922357982		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.14495810922357982 | validation: 0.10589725321107409]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18368608082586782		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.18368608082586782 | validation: 0.19380995729237685]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11615499401868257		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.11615499401868257 | validation: 0.13313569157158525]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10319659881337287		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.10319659881337287 | validation: 0.15741148974091762]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635556055826396		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.11635556055826396 | validation: 0.18457434895353642]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458804733129517		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.1458804733129517 | validation: 0.17429697281324893]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538875377314556		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1538875377314556 | validation: 0.17400544919785343]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14856714117818118		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.14856714117818118 | validation: 0.1950986905984562]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325836824495292		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1325836824495292 | validation: 0.136445844008473]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12034018808873391		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.12034018808873391 | validation: 0.17895415483548072]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11378539309748531		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.11378539309748531 | validation: 0.11580052913805763]
	TIME [epoch: 1.36 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176856100367825		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.11176856100367825 | validation: 0.17693892809498446]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448438756193152		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.11448438756193152 | validation: 0.13860153060137842]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13702519548218234		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.13702519548218234 | validation: 0.20463584410492777]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13737082871810752		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.13737082871810752 | validation: 0.13894888125846516]
	TIME [epoch: 1.36 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11870792201214542		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.11870792201214542 | validation: 0.13714365474846277]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11392914376423835		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.11392914376423835 | validation: 0.16998360089995532]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13057424337078777		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.13057424337078777 | validation: 0.13033447415526594]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397936782360049		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.1397936782360049 | validation: 0.1940606156536442]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13041719338654578		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.13041719338654578 | validation: 0.1310008516810211]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10644169064713532		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.10644169064713532 | validation: 0.11789393827643409]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0980750282189879		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.0980750282189879 | validation: 0.18630527678934722]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259249714970787		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1259249714970787 | validation: 0.1479302299319072]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15018203425030802		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.15018203425030802 | validation: 0.2587320307200995]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15491746707680615		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.15491746707680615 | validation: 0.10281501144397819]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10018487551981364		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.10018487551981364 | validation: 0.1289203461615514]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965991787692786		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.07965991787692786 | validation: 0.09151347033060332]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08753169334086831		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.08753169334086831 | validation: 0.20833362870489164]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12225030019163541		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.12225030019163541 | validation: 0.1420832837219547]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12880134410138733		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.12880134410138733 | validation: 0.17411078395692764]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12807705574579178		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.12807705574579178 | validation: 0.15445393815038203]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279059405772538		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1279059405772538 | validation: 0.13075286910268108]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314078663740944		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.13314078663740944 | validation: 0.18682466998394132]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13143876634068524		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.13143876634068524 | validation: 0.10557277972536383]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11912499217257896		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.11912499217257896 | validation: 0.1434642563077329]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178710815928857		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.09178710815928857 | validation: 0.10609109879101708]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08259939101207493		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.08259939101207493 | validation: 0.10901944377841587]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10536361788976699		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.10536361788976699 | validation: 0.222685307793068]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14927369563161022		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.14927369563161022 | validation: 0.14068173241250714]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394250571369078		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.1394250571369078 | validation: 0.15675715395583534]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750931548872532		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.09750931548872532 | validation: 0.11394167105236884]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08804133294532389		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.08804133294532389 | validation: 0.09663488103053645]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09935194334987743		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09935194334987743 | validation: 0.1633108510161929]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12018444641918731		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.12018444641918731 | validation: 0.14011126989927036]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12842547061617185		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.12842547061617185 | validation: 0.14461800779939646]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11345623973497607		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.11345623973497607 | validation: 0.14338414403690367]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10798005799030708		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10798005799030708 | validation: 0.17141780216730193]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628465838536883		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.2628465838536883 | validation: 0.15331367515706398]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067846648011333		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.11067846648011333 | validation: 0.09784466100844989]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07441700676454148		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.07441700676454148 | validation: 0.0993363847643105]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07389481774725729		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07389481774725729 | validation: 0.10223959405675945]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07992746682554212		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.07992746682554212 | validation: 0.12410277072476905]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10382354542337593		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.10382354542337593 | validation: 0.17614128737811768]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13379055916697752		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.13379055916697752 | validation: 0.16100964391645656]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13246003035777168		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.13246003035777168 | validation: 0.12173846483267775]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10678675481907733		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.10678675481907733 | validation: 0.12453274718241465]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08533833900813395		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.08533833900813395 | validation: 0.10389175106496747]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08117969221028794		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.08117969221028794 | validation: 0.12783629562080553]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599917829382564		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.08599917829382564 | validation: 0.1169051882131142]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09826341414040446		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.09826341414040446 | validation: 0.1970418212355303]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275753175802188		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1275753175802188 | validation: 0.11592714797671336]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11807184301551518		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.11807184301551518 | validation: 0.12834105591073267]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912877633167684		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.0912877633167684 | validation: 0.10725245311558429]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293607898720299		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1293607898720299 | validation: 0.17958825150205596]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096235309584436		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.11096235309584436 | validation: 0.11012065733968637]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09313565486379158		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.09313565486379158 | validation: 0.11006046231158376]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208941759141331		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.07208941759141331 | validation: 0.10099505935662989]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614329534174901		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.08614329534174901 | validation: 0.11069373587335757]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12917076550409434		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.12917076550409434 | validation: 0.20926842911364416]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14618861393455146		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.14618861393455146 | validation: 0.11904369384168856]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0984061559869276		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.0984061559869276 | validation: 0.0962090132697484]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07444927911456453		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.07444927911456453 | validation: 0.11313400057544026]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07353845560131873		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.07353845560131873 | validation: 0.08890194526810803]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09392340469127529		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.09392340469127529 | validation: 0.23827316841950708]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14424974518134406		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.14424974518134406 | validation: 0.11729287234885742]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10412476308351266		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.10412476308351266 | validation: 0.10203069370370532]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077669230893607		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.08077669230893607 | validation: 0.12467402970394366]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110804396167133		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.10110804396167133 | validation: 0.11459699470959112]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10428159632211495		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.10428159632211495 | validation: 0.13193135996523317]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925941454497254		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.09925941454497254 | validation: 0.13933753834350113]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10343907715470767		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.10343907715470767 | validation: 0.09191870030926844]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09997884977224886		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.09997884977224886 | validation: 0.13783898688638252]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009228616480099		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.08009228616480099 | validation: 0.08503212360632]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07750455694530425		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.07750455694530425 | validation: 0.12897298383865316]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07630631695930007		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.07630631695930007 | validation: 0.07983759715767191]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10777808541438033		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.10777808541438033 | validation: 0.15995438879791213]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09746309558573325		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.09746309558573325 | validation: 0.08271864889873587]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462635247248962		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.08462635247248962 | validation: 0.08752221016926594]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696420526160224		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.0696420526160224 | validation: 0.12586279849712959]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0946380740847022		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.0946380740847022 | validation: 0.13648218708031354]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13919561286497362		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.13919561286497362 | validation: 0.16200585177812887]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955088217632116		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.11955088217632116 | validation: 0.10160822740542433]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545695671104401		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.07545695671104401 | validation: 0.07057744396503107]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071519666468772		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.07071519666468772 | validation: 0.20720617886518933]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13172050097911783		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.13172050097911783 | validation: 0.10472607563967672]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09778416265274213		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.09778416265274213 | validation: 0.08516390002105141]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925820704366822		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.06925820704366822 | validation: 0.10872063146361173]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08856789458781543		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.08856789458781543 | validation: 0.10941890995690512]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09162692669112957		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.09162692669112957 | validation: 0.1445882447049868]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10045923987521964		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.10045923987521964 | validation: 0.11330298042025032]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09444947023663312		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.09444947023663312 | validation: 0.11429732779106368]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08206802087034143		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.08206802087034143 | validation: 0.0902466795370006]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07464553500009316		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.07464553500009316 | validation: 0.10157854771829022]
	TIME [epoch: 2.67 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07355762010002768		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.07355762010002768 | validation: 0.07378140248267949]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07499967352038016		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.07499967352038016 | validation: 0.11223975997708113]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07487789985750867		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.07487789985750867 | validation: 0.085291946009133]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811210469679544		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.07811210469679544 | validation: 0.11711598380929722]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08043602623418217		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.08043602623418217 | validation: 0.12630641326411027]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930180989938342		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.07930180989938342 | validation: 0.1006869602057504]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09009231240994035		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.09009231240994035 | validation: 0.16173028928690353]
	TIME [epoch: 2.67 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964529106168609		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.10964529106168609 | validation: 0.10482519492042744]
	TIME [epoch: 2.68 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10876599453646993		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.10876599453646993 | validation: 0.10607066050296235]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07573122769455408		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.07573122769455408 | validation: 0.08850225625052766]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918589396757241		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.06918589396757241 | validation: 0.08939577688479397]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216817907445337		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.07216817907445337 | validation: 0.12318932231276472]
	TIME [epoch: 2.68 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08781323664178593		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.08781323664178593 | validation: 0.07973712510917837]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09005170709736597		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.09005170709736597 | validation: 0.10105513915815945]
	TIME [epoch: 2.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193911284487762		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.08193911284487762 | validation: 0.0947108671264232]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07776416335000196		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.07776416335000196 | validation: 0.09581155301822208]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09628088320402768		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.09628088320402768 | validation: 0.170594889231115]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10853029903906328		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.10853029903906328 | validation: 0.07721942534241441]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07382945439247525		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.07382945439247525 | validation: 0.08576882307732907]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056562491853399496		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.056562491853399496 | validation: 0.0760561223003372]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694271029000461		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.05694271029000461 | validation: 0.08562501399923188]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682170984561494		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.0682170984561494 | validation: 0.1070736988566716]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115049013792625		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.08115049013792625 | validation: 0.12293617597254586]
	TIME [epoch: 2.67 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1010493372352991		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.1010493372352991 | validation: 0.10851526160398924]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962157391810926		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.0962157391810926 | validation: 0.1161546130740083]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753348381401913		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0753348381401913 | validation: 0.08180896013357368]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715481723112385		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.06715481723112385 | validation: 0.11542508773929577]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855362789035828		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.06855362789035828 | validation: 0.058859023069022846]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07459148382246054		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.07459148382246054 | validation: 0.12637818472645657]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647257164788633		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.07647257164788633 | validation: 0.05853413501087809]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779914268713546		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07779914268713546 | validation: 0.11874807939883487]
	TIME [epoch: 2.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07097239135806485		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07097239135806485 | validation: 0.07268384807256667]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059141177705850645		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.059141177705850645 | validation: 0.07118939046485662]
	TIME [epoch: 2.67 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05466336185397284		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.05466336185397284 | validation: 0.09995200747206846]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449741962003896		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.07449741962003896 | validation: 0.11737011693195797]
	TIME [epoch: 2.67 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111696892738725		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.1111696892738725 | validation: 0.16207881191692006]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12083137931528129		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.12083137931528129 | validation: 0.10068620901537986]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07729499489429352		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.07729499489429352 | validation: 0.09405498348080747]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15862090333889162		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.15862090333889162 | validation: 0.09792853558816943]
	TIME [epoch: 2.67 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07912854857264016		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07912854857264016 | validation: 0.06684296555914176]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05147786912594654		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.05147786912594654 | validation: 0.06783803525501798]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05920243246488985		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.05920243246488985 | validation: 0.07773337869986054]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608650464657109		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.05608650464657109 | validation: 0.08120674924445949]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06145147037464543		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.06145147037464543 | validation: 0.10470154254520495]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872492373650429		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.07872492373650429 | validation: 0.13002080900851384]
	TIME [epoch: 2.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431393125150393		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.09431393125150393 | validation: 0.10179301210694516]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08831586861571916		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.08831586861571916 | validation: 0.08852262626069485]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07210280260028318		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.07210280260028318 | validation: 0.07277161795405962]
	TIME [epoch: 2.68 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061460461128059064		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.061460461128059064 | validation: 0.08209169323674387]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694903274677604		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.05694903274677604 | validation: 0.07138944558982929]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05936843299456987		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.05936843299456987 | validation: 0.09285401304642615]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06509962287731823		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.06509962287731823 | validation: 0.07991070847494404]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06821874863537841		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.06821874863537841 | validation: 0.13956681486406539]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328745274635338		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.08328745274635338 | validation: 0.07261038842176297]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07403029082953505		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07403029082953505 | validation: 0.08703426854485576]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06170462621419544		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.06170462621419544 | validation: 0.08095554638136103]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546842569954352		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.06546842569954352 | validation: 0.08850995783925975]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299943834303373		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07299943834303373 | validation: 0.1111119131946206]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07802434660859836		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07802434660859836 | validation: 0.08949074181767074]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07550622304218996		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07550622304218996 | validation: 0.09962167110716845]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06826260018071277		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.06826260018071277 | validation: 0.06932950351557612]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05889920804319109		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.05889920804319109 | validation: 0.07026692271572411]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410040371981653		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.05410040371981653 | validation: 0.08579258588368854]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420326231472622		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.05420326231472622 | validation: 0.06820091117621149]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748568287920823		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.0748568287920823 | validation: 0.14840032339768663]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832772852892835		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.0832772852892835 | validation: 0.06880955847585012]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07183686548568007		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.07183686548568007 | validation: 0.07377405308684874]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057174253318839414		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.057174253318839414 | validation: 0.09203273939492924]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06762968286981716		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.06762968286981716 | validation: 0.12508135815467844]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376203245950863		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.08376203245950863 | validation: 0.08605338907785857]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06796029896324626		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.06796029896324626 | validation: 0.08654413540168454]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06106706044978737		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06106706044978737 | validation: 0.07021023416010018]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05625976737453061		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.05625976737453061 | validation: 0.0713602308063548]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06173564991179273		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06173564991179273 | validation: 0.0957064790114722]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06578524718602127		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.06578524718602127 | validation: 0.08613377093540535]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06811525851168042		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.06811525851168042 | validation: 0.09370699697598564]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491105345879786		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.06491105345879786 | validation: 0.08978064205361211]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05870603722688694		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.05870603722688694 | validation: 0.07046112744133025]
	TIME [epoch: 2.67 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057760182774116445		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.057760182774116445 | validation: 0.0859618553040224]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684897469768277		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.05684897469768277 | validation: 0.07667954192184406]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055229297900411665		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.055229297900411665 | validation: 0.08057714077893148]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142279160357001		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06142279160357001 | validation: 0.08528742513078008]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172864460190308		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.07172864460190308 | validation: 0.38629542888595475]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23264904003433196		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.23264904003433196 | validation: 0.28439932541516505]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16894051121651632		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.16894051121651632 | validation: 0.16432898263388052]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13031807015595553		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.13031807015595553 | validation: 0.10669326518063944]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570768060586346		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.06570768060586346 | validation: 0.07110391724487584]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056541360830954686		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.056541360830954686 | validation: 0.06931481247036342]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653492666562223		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.0653492666562223 | validation: 0.07991109922263526]
	TIME [epoch: 2.67 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058797324926843794		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.058797324926843794 | validation: 0.07612425967821736]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058022729930987615		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.058022729930987615 | validation: 0.07136441631949546]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060059719799976645		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.060059719799976645 | validation: 0.07754416295502514]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057795719488022035		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.057795719488022035 | validation: 0.07411835704864703]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05818095445530992		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.05818095445530992 | validation: 0.06947608496701177]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054285141454982595		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.054285141454982595 | validation: 0.07016045269136013]
	TIME [epoch: 2.67 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05550594975791311		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.05550594975791311 | validation: 0.0761756945340841]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055594703526197886		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.055594703526197886 | validation: 0.0741654519549957]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057144306175270415		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.057144306175270415 | validation: 0.07680137535026084]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059598914699371786		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.059598914699371786 | validation: 0.07527687284661755]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644741372644874		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.0644741372644874 | validation: 0.08505826292758634]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551311066504535		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.06551311066504535 | validation: 0.0679635178823431]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05995708584058258		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05995708584058258 | validation: 0.06984406624457146]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054154518135664916		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.054154518135664916 | validation: 0.06405763101896371]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088843685786662		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.05088843685786662 | validation: 0.08371880365758819]
	TIME [epoch: 2.67 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053386810740835226		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.053386810740835226 | validation: 0.07655829127859942]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056839888476238894		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.056839888476238894 | validation: 0.07419114721412179]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700636373377476		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.05700636373377476 | validation: 0.077933792672921]
	TIME [epoch: 2.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365092349101016		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06365092349101016 | validation: 0.08689247516842673]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679121322144681		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.06679121322144681 | validation: 0.0783559027767884]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06291051245168341		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06291051245168341 | validation: 0.07988694844938135]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519006613449193		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.05519006613449193 | validation: 0.05748631549038766]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265125241316877		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.05265125241316877 | validation: 0.10770499909592286]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06189199136893406		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.06189199136893406 | validation: 0.0475322460287703]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06367849208038318		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.06367849208038318 | validation: 0.07947137159302994]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241380185003942		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.05241380185003942 | validation: 0.054109928575954286]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050085012826957306		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.050085012826957306 | validation: 0.08581685687577587]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057420651079263405		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.057420651079263405 | validation: 0.06294380352310648]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455825533220138		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.05455825533220138 | validation: 0.09218582099940775]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0597866034947178		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0597866034947178 | validation: 0.0942219633175254]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712826698851806		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0712826698851806 | validation: 0.08045026950352074]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07491482081169831		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.07491482081169831 | validation: 0.08722138059180284]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05922422111099672		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.05922422111099672 | validation: 0.058261769135173495]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04502184860183025		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.04502184860183025 | validation: 0.05211218017965452]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265654577152948		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.04265654577152948 | validation: 0.06419667175468084]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04361862805114027		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.04361862805114027 | validation: 0.049231772082814554]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04498071994337548		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.04498071994337548 | validation: 0.06629896968838014]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696628819996118		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.04696628819996118 | validation: 0.06459424956542391]
	TIME [epoch: 2.69 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05752254157633702		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.05752254157633702 | validation: 0.09359828808322396]
	TIME [epoch: 2.69 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693773720194114		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.0693773720194114 | validation: 0.08442836089245373]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181115901974212		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.07181115901974212 | validation: 0.09126578232755282]
	TIME [epoch: 2.69 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05915311131876159		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.05915311131876159 | validation: 0.06169748844895404]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046889341483007135		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.046889341483007135 | validation: 0.06402984659906823]
	TIME [epoch: 2.69 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043758357155101034		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.043758357155101034 | validation: 0.05405202725742229]
	TIME [epoch: 2.69 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04261046611798326		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.04261046611798326 | validation: 0.06451397177921862]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534477966744314		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.04534477966744314 | validation: 0.07524817416662267]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045374018999227335		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.045374018999227335 | validation: 0.06358717249212485]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05327247160909801		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.05327247160909801 | validation: 0.1041047900398683]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093084953823367		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07093084953823367 | validation: 0.07032815313726923]
	TIME [epoch: 2.69 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07260331577036387		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.07260331577036387 | validation: 0.08730334273389496]
	TIME [epoch: 2.69 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052540382924672085		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.052540382924672085 | validation: 0.06397470135931353]
	TIME [epoch: 2.69 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045690005841982066		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.045690005841982066 | validation: 0.05820863008700148]
	TIME [epoch: 2.69 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0467720915272667		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.0467720915272667 | validation: 0.07874855569053299]
	TIME [epoch: 2.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435637982638701		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05435637982638701 | validation: 0.06876176590811695]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628593734433058		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.06628593734433058 | validation: 0.09641429083322088]
	TIME [epoch: 2.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057880251434638		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.057880251434638 | validation: 0.06525199796820783]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050875343446882576		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.050875343446882576 | validation: 0.062350199023265045]
	TIME [epoch: 2.69 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04893569989598241		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.04893569989598241 | validation: 0.06693667532889278]
	TIME [epoch: 2.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974001768027532		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.04974001768027532 | validation: 0.05703270103884202]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047772611703141975		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.047772611703141975 | validation: 0.0765847726901111]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0479583187291961		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.0479583187291961 | validation: 0.05636197503357936]
	TIME [epoch: 2.68 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04742419584957993		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.04742419584957993 | validation: 0.06409615044713006]
	TIME [epoch: 2.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994328280322865		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.04994328280322865 | validation: 0.08229620489928442]
	TIME [epoch: 2.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064117415535232		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.05064117415535232 | validation: 0.06250607310134058]
	TIME [epoch: 2.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205604586029192		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.05205604586029192 | validation: 0.08237139418789122]
	TIME [epoch: 2.69 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639650937650682		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.05639650937650682 | validation: 0.06690201582807644]
	TIME [epoch: 2.69 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08399176885363176		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08399176885363176 | validation: 0.086907658282698]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360179855033389		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.05360179855033389 | validation: 0.0616444531059451]
	TIME [epoch: 2.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424676450020146		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04424676450020146 | validation: 0.049947287711130085]
	TIME [epoch: 2.69 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041024489884766265		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.041024489884766265 | validation: 0.054005273488583874]
	TIME [epoch: 2.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039747295650880754		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.039747295650880754 | validation: 0.047796927461890876]
	TIME [epoch: 2.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913108343912195		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.03913108343912195 | validation: 0.05734915009075218]
	TIME [epoch: 2.69 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03991551697793651		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.03991551697793651 | validation: 0.05060403538314167]
	TIME [epoch: 2.69 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909813160365849		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.03909813160365849 | validation: 0.06172485001014341]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03913799877523014		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.03913799877523014 | validation: 0.05847447442575446]
	TIME [epoch: 2.69 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042684885035169595		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.042684885035169595 | validation: 0.08519337789821019]
	TIME [epoch: 2.69 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473828046256183		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05473828046256183 | validation: 0.09984848716439301]
	TIME [epoch: 2.69 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470229096850126		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.08470229096850126 | validation: 0.10408809163322237]
	TIME [epoch: 2.69 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912626690502343		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.08912626690502343 | validation: 0.06140459732324002]
	TIME [epoch: 2.69 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044498648562244905		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.044498648562244905 | validation: 0.04605946699567425]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527956558463745		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04527956558463745 | validation: 0.06704686061517819]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04976606643697256		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.04976606643697256 | validation: 0.04622781839433218]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461951765055647		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.04461951765055647 | validation: 0.05705626653861026]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03947762811232588		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.03947762811232588 | validation: 0.05534828629109567]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039885825492747114		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.039885825492747114 | validation: 0.06241214296729003]
	TIME [epoch: 2.67 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558432638650901		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.04558432638650901 | validation: 0.07506045919367121]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047126109823704646		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.047126109823704646 | validation: 0.06932529263762251]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053221439257088506		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.053221439257088506 | validation: 0.06786085490603484]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05313574274411681		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05313574274411681 | validation: 0.07196197116189583]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05024447461451567		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.05024447461451567 | validation: 0.0600097497025457]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046064331699985854		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.046064331699985854 | validation: 0.06157146026053359]
	TIME [epoch: 2.68 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086807944319828		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.04086807944319828 | validation: 0.05619714843930447]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040871514574624214		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.040871514574624214 | validation: 0.06850518334939906]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0408643844930608		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.0408643844930608 | validation: 0.056151553638792255]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214858334122063		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.04214858334122063 | validation: 0.06046312448760145]
	TIME [epoch: 2.67 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044788679832037784		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.044788679832037784 | validation: 0.06819014485248609]
	TIME [epoch: 2.67 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460844584451168		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0460844584451168 | validation: 0.06022153154644882]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030359860028507		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.05030359860028507 | validation: 0.06319875862568763]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049757836925329306		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.049757836925329306 | validation: 0.061525021454330156]
	TIME [epoch: 2.67 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723027013972319		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.04723027013972319 | validation: 0.0664933432621824]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047619832773699196		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.047619832773699196 | validation: 0.06948529505687949]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049998464421610524		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.049998464421610524 | validation: 0.05428838793578]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048239264818345284		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.048239264818345284 | validation: 0.0724269650048933]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04667476914320492		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.04667476914320492 | validation: 0.04743446143144915]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045832929057211994		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.045832929057211994 | validation: 0.07091219604089015]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04568543849084542		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.04568543849084542 | validation: 0.05105651793548707]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0472225070271908		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0472225070271908 | validation: 0.06157066465501926]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236198003398011		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.04236198003398011 | validation: 0.053149816809312384]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038459902199151345		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.038459902199151345 | validation: 0.047227472212941934]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041676878066121886		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.041676878066121886 | validation: 0.064310055065243]
	TIME [epoch: 2.68 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504355223989256		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.04504355223989256 | validation: 0.057066060796815345]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971842809493		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.04971842809493 | validation: 0.0844975126189691]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054688383741597256		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.054688383741597256 | validation: 0.06914090662616774]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765591370355949		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.04765591370355949 | validation: 0.05616899113763675]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047135188262132435		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.047135188262132435 | validation: 0.06769586218101481]
	TIME [epoch: 2.67 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750059779649117		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.04750059779649117 | validation: 0.05454146831419119]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04333138411925865		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.04333138411925865 | validation: 0.049085899137615956]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039993998993486796		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.039993998993486796 | validation: 0.06365576724922915]
	TIME [epoch: 2.68 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327060939163996		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.04327060939163996 | validation: 0.04848449435380826]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137069373507895		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.04137069373507895 | validation: 0.05757579344917799]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03925864256781374		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.03925864256781374 | validation: 0.04644790799691631]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042178790820519806		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.042178790820519806 | validation: 0.05410914023824658]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717335440586448		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.03717335440586448 | validation: 0.04614139305231876]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03682290151611125		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.03682290151611125 | validation: 0.05326034593016315]
	TIME [epoch: 2.68 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03451162104895761		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.03451162104895761 | validation: 0.04417154721416447]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037616107201773955		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.037616107201773955 | validation: 0.06261640068782985]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04068200471815032		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04068200471815032 | validation: 0.07358503097492891]
	TIME [epoch: 2.68 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456686956759844		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.06456686956759844 | validation: 0.11571879197822255]
	TIME [epoch: 2.68 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329199009955968		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.08329199009955968 | validation: 0.06604170401580921]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048499818307977664		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.048499818307977664 | validation: 0.04574735281084289]
	TIME [epoch: 2.68 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036832203748247125		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.036832203748247125 | validation: 0.059962146412408435]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03924255229447648		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.03924255229447648 | validation: 0.04804552110480788]
	TIME [epoch: 2.68 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039799211719605376		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.039799211719605376 | validation: 0.05401262694209963]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867953274187622		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.03867953274187622 | validation: 0.06185870758487327]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042160423872401216		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.042160423872401216 | validation: 0.04892811916426283]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863191215511432		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.04863191215511432 | validation: 0.07684448738217207]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04925358181395801		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04925358181395801 | validation: 0.04899634480369178]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03794135184588185		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.03794135184588185 | validation: 0.04052832892975227]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621522882255091		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.03621522882255091 | validation: 0.07110114377177365]
	TIME [epoch: 2.67 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03940817113927076		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.03940817113927076 | validation: 0.04406661927869746]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040656450919787625		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.040656450919787625 | validation: 0.061900017100058026]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265319352981161		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.04265319352981161 | validation: 0.07151158571476997]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050318378727544445		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.050318378727544445 | validation: 0.06597348883751275]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055907251611910985		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.055907251611910985 | validation: 0.06353998218672922]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04374687281386274		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.04374687281386274 | validation: 0.05669019722518507]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03519621329256135		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.03519621329256135 | validation: 0.04396913329721474]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036762142378697196		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.036762142378697196 | validation: 0.05865573621190036]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035434334221190314		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.035434334221190314 | validation: 0.04220013331083397]
	TIME [epoch: 2.68 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03612300575271554		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.03612300575271554 | validation: 0.05994330705009504]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03853478985632114		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.03853478985632114 | validation: 0.045196682232756816]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685533355612269		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.03685533355612269 | validation: 0.05580444806631279]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037852328690399915		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.037852328690399915 | validation: 0.04489703223588444]
	TIME [epoch: 2.68 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03738457764919272		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.03738457764919272 | validation: 0.07312603997030977]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978732355020791		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.04978732355020791 | validation: 0.07834021795001933]
	TIME [epoch: 2.68 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636900221198033		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.0636900221198033 | validation: 0.07520102481594709]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224337699896449		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05224337699896449 | validation: 0.055120211245278654]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036500804855114435		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.036500804855114435 | validation: 0.04991486661997623]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036123248717337915		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.036123248717337915 | validation: 0.05707000830526681]
	TIME [epoch: 2.68 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037158640379222875		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.037158640379222875 | validation: 0.04384307065392681]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03810813449439757		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.03810813449439757 | validation: 0.05887705504368057]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035553397660451375		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.035553397660451375 | validation: 0.047411491585922165]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782460198027506		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.03782460198027506 | validation: 0.06028070714218739]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036658378294077185		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.036658378294077185 | validation: 0.0596531260774117]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04034217437263788		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.04034217437263788 | validation: 0.05202146979776411]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372940364970503		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04372940364970503 | validation: 0.06412610678481112]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04486474867741788		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.04486474867741788 | validation: 0.05199148477183449]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04230495864613298		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.04230495864613298 | validation: 0.0440403695611373]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037674539405022314		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.037674539405022314 | validation: 0.0569513695324621]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035534196542367245		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.035534196542367245 | validation: 0.04515585282076443]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04009794028568075		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.04009794028568075 | validation: 0.05187859862492916]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036590075173661286		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.036590075173661286 | validation: 0.04214051565583052]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03642237074596747		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.03642237074596747 | validation: 0.060215657576134445]
	TIME [epoch: 2.68 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600851564537166		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03600851564537166 | validation: 0.05195393353048622]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03696032266776624		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.03696032266776624 | validation: 0.04978592096302218]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039329755044888545		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.039329755044888545 | validation: 0.061448655074945194]
	TIME [epoch: 2.68 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303177996829016		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.04303177996829016 | validation: 0.05727732964101692]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04247465953515448		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04247465953515448 | validation: 0.05214434367418478]
	TIME [epoch: 2.68 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993875241719275		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.03993875241719275 | validation: 0.059468261654832134]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039166582118501755		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.039166582118501755 | validation: 0.046274392327247484]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03610442051803576		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.03610442051803576 | validation: 0.05454147122185517]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035059722180990016		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.035059722180990016 | validation: 0.0499450591979433]
	TIME [epoch: 2.69 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03394553912747104		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.03394553912747104 | validation: 0.04992127342522184]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03447854561915781		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.03447854561915781 | validation: 0.0621667830837635]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038473469851970005		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.038473469851970005 | validation: 0.05357025317156882]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04175786965873469		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.04175786965873469 | validation: 0.052429144154092694]
	TIME [epoch: 2.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038117426206375024		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.038117426206375024 | validation: 0.05032507834725036]
	TIME [epoch: 2.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03742218146479512		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03742218146479512 | validation: 0.05623988169047605]
	TIME [epoch: 2.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857419175395742		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.03857419175395742 | validation: 0.04695641995707816]
	TIME [epoch: 2.69 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043046606962027466		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.043046606962027466 | validation: 0.05932601893188071]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705485511573066		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.03705485511573066 | validation: 0.045162371763932854]
	TIME [epoch: 2.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037787246099852324		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.037787246099852324 | validation: 0.05568076918552637]
	TIME [epoch: 2.69 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040803641661052374		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.040803641661052374 | validation: 0.05452286496104192]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400136163858824		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.0400136163858824 | validation: 0.06319751497475423]
	TIME [epoch: 2.69 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137469827173999		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04137469827173999 | validation: 0.06164944097784957]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957082804870247		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03957082804870247 | validation: 0.04767879323287386]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036650695123636494		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.036650695123636494 | validation: 0.04661838802222558]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034158605123926086		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.034158605123926086 | validation: 0.04899238138462963]
	TIME [epoch: 2.69 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032226016972766836		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.032226016972766836 | validation: 0.045008791318097256]
	TIME [epoch: 2.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298281597673107		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.03298281597673107 | validation: 0.05607040359259235]
	TIME [epoch: 2.69 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034958034190225334		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.034958034190225334 | validation: 0.04326763382111598]
	TIME [epoch: 2.69 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0360383786271964		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0360383786271964 | validation: 0.07011884372085136]
	TIME [epoch: 2.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909805128668974		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.03909805128668974 | validation: 0.04936658536345238]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045604607267560236		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.045604607267560236 | validation: 0.06509029640493848]
	TIME [epoch: 2.69 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038435594504626414		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.038435594504626414 | validation: 0.05129134073994999]
	TIME [epoch: 2.69 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03436178817093802		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.03436178817093802 | validation: 0.04616241125142878]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035307477426834856		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.035307477426834856 | validation: 0.051206506118004105]
	TIME [epoch: 2.69 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036098544577840536		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.036098544577840536 | validation: 0.04522082289798277]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03544704079218973		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.03544704079218973 | validation: 0.050208501900301974]
	TIME [epoch: 2.69 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217959192742188		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03217959192742188 | validation: 0.04753952251407412]
	TIME [epoch: 2.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217228794589677		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.03217228794589677 | validation: 0.05415330566982806]
	TIME [epoch: 2.69 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564174332839957		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.03564174332839957 | validation: 0.058168361924081896]
	TIME [epoch: 2.69 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036906148779795575		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.036906148779795575 | validation: 0.059436830488378715]
	TIME [epoch: 2.69 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03798820082675678		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03798820082675678 | validation: 0.04950800012701839]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651304649130365		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.03651304649130365 | validation: 0.05535308280689996]
	TIME [epoch: 2.69 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03370948279451914		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.03370948279451914 | validation: 0.04447381749762491]
	TIME [epoch: 2.68 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03343922384264963		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03343922384264963 | validation: 0.04257237315492053]
	TIME [epoch: 2.69 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168467138724558		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03168467138724558 | validation: 0.041956240569694395]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031314660308306166		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.031314660308306166 | validation: 0.05387343966452]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032697859838952466		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.032697859838952466 | validation: 0.041278952263714856]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037601552160773155		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.037601552160773155 | validation: 0.05610627975097304]
	TIME [epoch: 2.69 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804262910574534		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.03804262910574534 | validation: 0.04089455488143173]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892155518658456		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03892155518658456 | validation: 0.05431544841944819]
	TIME [epoch: 2.69 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545374005370631		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03545374005370631 | validation: 0.06713227840140255]
	TIME [epoch: 2.69 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045542295819662744		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.045542295819662744 | validation: 0.05590645604250656]
	TIME [epoch: 2.69 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04255434470736754		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.04255434470736754 | validation: 0.05326499597611304]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03274220501486338		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.03274220501486338 | validation: 0.048965689943910554]
	TIME [epoch: 2.69 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032118798394270606		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.032118798394270606 | validation: 0.04119725836210706]
	TIME [epoch: 2.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03358180193152326		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.03358180193152326 | validation: 0.04544155643390936]
	TIME [epoch: 2.69 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129262786711138		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03129262786711138 | validation: 0.04453551123085312]
	TIME [epoch: 2.69 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03267218336862593		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.03267218336862593 | validation: 0.04511915897494628]
	TIME [epoch: 2.69 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033305151422405324		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.033305151422405324 | validation: 0.07474719805815225]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541091939028741		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.04541091939028741 | validation: 0.050307433733885715]
	TIME [epoch: 2.69 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038787951627884676		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.038787951627884676 | validation: 0.04545244730672489]
	TIME [epoch: 2.69 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568845725082686		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.03568845725082686 | validation: 0.05214449158237932]
	TIME [epoch: 2.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036525975480825264		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.036525975480825264 | validation: 0.03827635696352492]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03196282375836007		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.03196282375836007 | validation: 0.04225016020950428]
	TIME [epoch: 2.69 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030405365186790555		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.030405365186790555 | validation: 0.051565291501027836]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363335950821078		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.03363335950821078 | validation: 0.03959417538261001]
	TIME [epoch: 2.69 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03409142322924492		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03409142322924492 | validation: 0.049475073251810865]
	TIME [epoch: 2.69 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035706858488446944		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.035706858488446944 | validation: 0.050235527942273366]
	TIME [epoch: 2.69 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037412920453435375		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.037412920453435375 | validation: 0.05596703014485939]
	TIME [epoch: 2.69 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836905806436669		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.03836905806436669 | validation: 0.04455968439832202]
	TIME [epoch: 2.69 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344673079004627		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.0344673079004627 | validation: 0.04185618966654238]
	TIME [epoch: 2.69 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032656570761680666		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.032656570761680666 | validation: 0.04458114584386935]
	TIME [epoch: 2.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03205828189373215		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03205828189373215 | validation: 0.0484805653745425]
	TIME [epoch: 2.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419181995957792		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03419181995957792 | validation: 0.041641092785803295]
	TIME [epoch: 2.69 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030869179203561404		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.030869179203561404 | validation: 0.04487231888836346]
	TIME [epoch: 2.69 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03228662101720563		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.03228662101720563 | validation: 0.04360332916993104]
	TIME [epoch: 2.69 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02938167355987261		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.02938167355987261 | validation: 0.05179727278178606]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032113884758161695		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.032113884758161695 | validation: 0.03905099090782752]
	TIME [epoch: 2.69 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154871801169451		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03154871801169451 | validation: 0.05471465141171138]
	TIME [epoch: 2.69 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03226461196463523		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03226461196463523 | validation: 0.0497642716018149]
	TIME [epoch: 2.69 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406832296311443		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.03406832296311443 | validation: 0.050149132218500626]
	TIME [epoch: 2.69 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038024660640219414		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.038024660640219414 | validation: 0.05563674557767369]
	TIME [epoch: 2.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0393088131072116		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.0393088131072116 | validation: 0.046326958132612985]
	TIME [epoch: 2.69 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441289491621508		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.03441289491621508 | validation: 0.03714314589708774]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03390852486908181		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.03390852486908181 | validation: 0.07219329117159702]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03987895536897827		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.03987895536897827 | validation: 0.04440861382692485]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030986073866152975		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.030986073866152975 | validation: 0.04079678725196954]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029969798928840722		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.029969798928840722 | validation: 0.046138150582645046]
	TIME [epoch: 2.68 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032754192677186725		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.032754192677186725 | validation: 0.035557956168058534]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321358397201448		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03321358397201448 | validation: 0.04315985723710192]
	TIME [epoch: 2.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034446248994215986		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.034446248994215986 | validation: 0.04776301696531635]
	TIME [epoch: 2.69 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03398811796222574		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.03398811796222574 | validation: 0.054420462814609175]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566497118902493		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.03566497118902493 | validation: 0.042590531210616234]
	TIME [epoch: 2.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03356290198835895		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.03356290198835895 | validation: 0.04176841410572734]
	TIME [epoch: 2.69 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03284912819469306		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.03284912819469306 | validation: 0.04698254224930616]
	TIME [epoch: 2.69 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030127389028397664		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.030127389028397664 | validation: 0.044114367015656464]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02883422197411495		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.02883422197411495 | validation: 0.04148601147216452]
	TIME [epoch: 2.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028959512130011196		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.028959512130011196 | validation: 0.04185129308649106]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029768898904233475		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.029768898904233475 | validation: 0.04104345004667784]
	TIME [epoch: 2.69 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027623110293527743		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.027623110293527743 | validation: 0.041347380291512775]
	TIME [epoch: 2.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02721553014333436		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.02721553014333436 | validation: 0.0501098522609978]
	TIME [epoch: 2.69 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03152729740626792		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03152729740626792 | validation: 0.052997253494981925]
	TIME [epoch: 2.69 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032619999269465835		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.032619999269465835 | validation: 0.05505998928751859]
	TIME [epoch: 2.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03582560320823265		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.03582560320823265 | validation: 0.05054958872833907]
	TIME [epoch: 2.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033810813059162566		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.033810813059162566 | validation: 0.04624478951580964]
	TIME [epoch: 2.69 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448419900819399		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.03448419900819399 | validation: 0.054213295946853285]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03266922957152823		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03266922957152823 | validation: 0.0338129212218789]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204240906570553		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03204240906570553 | validation: 0.049962948822749556]
	TIME [epoch: 2.69 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03080951789542633		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.03080951789542633 | validation: 0.043915571288403614]
	TIME [epoch: 2.69 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028594012939350277		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.028594012939350277 | validation: 0.04731509179927784]
	TIME [epoch: 2.69 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028450190313334466		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.028450190313334466 | validation: 0.03909704360776753]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029297002935164615		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.029297002935164615 | validation: 0.04803310122745537]
	TIME [epoch: 2.69 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03004690340447724		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03004690340447724 | validation: 0.03848995143503884]
	TIME [epoch: 2.69 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030241369975538515		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.030241369975538515 | validation: 0.04833232706524187]
	TIME [epoch: 2.68 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030448776310048737		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.030448776310048737 | validation: 0.052489422135784086]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033339152158271775		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.033339152158271775 | validation: 0.05127912813120086]
	TIME [epoch: 2.69 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03635033057825399		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.03635033057825399 | validation: 0.04006067740652668]
	TIME [epoch: 2.69 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034036758841172306		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.034036758841172306 | validation: 0.04261915948725168]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0298584392250517		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.0298584392250517 | validation: 0.046011621338595224]
	TIME [epoch: 2.69 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029772744268259734		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.029772744268259734 | validation: 0.04220914221662937]
	TIME [epoch: 2.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029866036284546746		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.029866036284546746 | validation: 0.04111518493010812]
	TIME [epoch: 2.69 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028656012179255976		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.028656012179255976 | validation: 0.037240602750835954]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03171909551275943		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03171909551275943 | validation: 0.048836642600371516]
	TIME [epoch: 2.69 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032975235280932436		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.032975235280932436 | validation: 0.041398833696079274]
	TIME [epoch: 2.69 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02998052577742652		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.02998052577742652 | validation: 0.04064607434908943]
	TIME [epoch: 2.69 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028561102566491102		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.028561102566491102 | validation: 0.04936080152935882]
	TIME [epoch: 2.69 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223490629255983		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.03223490629255983 | validation: 0.03880206621581729]
	TIME [epoch: 2.69 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031165434088442395		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.031165434088442395 | validation: 0.0579058281073192]
	TIME [epoch: 2.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03323760194896358		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03323760194896358 | validation: 0.04495646711948819]
	TIME [epoch: 2.69 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031814914240320666		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.031814914240320666 | validation: 0.05347895536555032]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03019842661936655		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.03019842661936655 | validation: 0.04286744950058265]
	TIME [epoch: 2.69 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029492858872720316		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.029492858872720316 | validation: 0.03447277321279007]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029856720987713192		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.029856720987713192 | validation: 0.040510188455189194]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029493502063303173		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.029493502063303173 | validation: 0.04574540875349697]
	TIME [epoch: 2.68 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02745912971529858		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.02745912971529858 | validation: 0.04992262651038512]
	TIME [epoch: 2.69 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02951762987173578		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.02951762987173578 | validation: 0.04580868151541654]
	TIME [epoch: 2.69 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029884183740661208		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.029884183740661208 | validation: 0.044791075435738674]
	TIME [epoch: 2.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028348846559721853		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.028348846559721853 | validation: 0.04052011783548816]
	TIME [epoch: 2.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256656548161918		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.03256656548161918 | validation: 0.04697511347211506]
	TIME [epoch: 2.69 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035819767065371697		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.035819767065371697 | validation: 0.04637986872249333]
	TIME [epoch: 2.69 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902547105334832		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03902547105334832 | validation: 0.05012028706091279]
	TIME [epoch: 2.69 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030959199614800665		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.030959199614800665 | validation: 0.04065512216404078]
	TIME [epoch: 2.69 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028887083112079926		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.028887083112079926 | validation: 0.03797072144410473]
	TIME [epoch: 2.69 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031876554352059897		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.031876554352059897 | validation: 0.06696025340327687]
	TIME [epoch: 2.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0398480301294953		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0398480301294953 | validation: 0.05322289639834128]
	TIME [epoch: 2.69 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03527070571583402		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.03527070571583402 | validation: 0.04041945979813688]
	TIME [epoch: 2.69 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03090436072673569		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03090436072673569 | validation: 0.037761842625735786]
	TIME [epoch: 2.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027564129459658613		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.027564129459658613 | validation: 0.04018233088183069]
	TIME [epoch: 2.69 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027754264702965152		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.027754264702965152 | validation: 0.044851704508963336]
	TIME [epoch: 2.69 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02926138324663632		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.02926138324663632 | validation: 0.05751656298724524]
	TIME [epoch: 2.69 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03134659613337807		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03134659613337807 | validation: 0.05157192010978282]
	TIME [epoch: 2.69 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03257170094483856		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.03257170094483856 | validation: 0.0484562891589031]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029794174377032655		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.029794174377032655 | validation: 0.038215363575572064]
	TIME [epoch: 2.69 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02878281439715499		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.02878281439715499 | validation: 0.04117316619702193]
	TIME [epoch: 2.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201961063744989		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03201961063744989 | validation: 0.045516811994455224]
	TIME [epoch: 2.69 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03143676829094792		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.03143676829094792 | validation: 0.04054545813418425]
	TIME [epoch: 2.69 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030174161180108034		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.030174161180108034 | validation: 0.04171206759770142]
	TIME [epoch: 2.69 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030097422517730284		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.030097422517730284 | validation: 0.038590512085784526]
	TIME [epoch: 2.69 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028602398076972166		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.028602398076972166 | validation: 0.03650910761498031]
	TIME [epoch: 2.69 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02804823851061066		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.02804823851061066 | validation: 0.034178919729369164]
	TIME [epoch: 2.69 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02732436053419134		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.02732436053419134 | validation: 0.04224065270504828]
	TIME [epoch: 2.69 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0281531844888763		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.0281531844888763 | validation: 0.04018362599528561]
	TIME [epoch: 2.69 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028220472165155747		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.028220472165155747 | validation: 0.03575694971334603]
	TIME [epoch: 2.69 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028737946783813296		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.028737946783813296 | validation: 0.046704033795334]
	TIME [epoch: 2.69 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02975284330028517		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.02975284330028517 | validation: 0.036808345488704965]
	TIME [epoch: 2.69 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031182120911348928		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.031182120911348928 | validation: 0.04218963205898543]
	TIME [epoch: 179 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02975573411254917		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.02975573411254917 | validation: 0.04233910688122449]
	TIME [epoch: 5.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029222305919600764		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.029222305919600764 | validation: 0.03862180312301153]
	TIME [epoch: 5.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02659796836090895		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.02659796836090895 | validation: 0.04307111658261497]
	TIME [epoch: 5.75 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02789635578154681		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.02789635578154681 | validation: 0.043099674688570455]
	TIME [epoch: 5.75 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02654859645909773		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.02654859645909773 | validation: 0.03472908315841824]
	TIME [epoch: 5.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028075498063768824		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.028075498063768824 | validation: 0.04544050423561894]
	TIME [epoch: 5.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027934090631693245		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.027934090631693245 | validation: 0.04240777637656378]
	TIME [epoch: 5.75 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028732896017802836		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.028732896017802836 | validation: 0.04207549421555254]
	TIME [epoch: 5.75 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02950508198734284		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.02950508198734284 | validation: 0.05091456097058466]
	TIME [epoch: 5.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03257691493284285		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.03257691493284285 | validation: 0.042011008819081444]
	TIME [epoch: 5.74 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031586072941926224		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.031586072941926224 | validation: 0.04187358942193819]
	TIME [epoch: 5.75 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017933527982335		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.03017933527982335 | validation: 0.037808389636811074]
	TIME [epoch: 5.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027516307244442137		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.027516307244442137 | validation: 0.042303160437566746]
	TIME [epoch: 5.75 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02636524760818872		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.02636524760818872 | validation: 0.03653994173855981]
	TIME [epoch: 5.75 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04026291202270287		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.04026291202270287 | validation: 0.043047051897002066]
	TIME [epoch: 5.75 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03074848477311247		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03074848477311247 | validation: 0.04552464768731545]
	TIME [epoch: 5.75 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028304350700127278		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.028304350700127278 | validation: 0.04066104980949087]
	TIME [epoch: 5.75 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027714481521996275		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.027714481521996275 | validation: 0.04518443347492203]
	TIME [epoch: 5.75 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02838359211161274		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.02838359211161274 | validation: 0.04383581703462452]
	TIME [epoch: 5.76 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026169322790846244		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.026169322790846244 | validation: 0.0427747620097311]
	TIME [epoch: 5.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028038346313186146		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.028038346313186146 | validation: 0.03666173166140294]
	TIME [epoch: 5.74 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029796735937693285		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.029796735937693285 | validation: 0.050006604121146096]
	TIME [epoch: 5.78 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02982793941309926		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.02982793941309926 | validation: 0.05051468087987037]
	TIME [epoch: 5.75 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030449577898387474		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.030449577898387474 | validation: 0.04278764345039126]
	TIME [epoch: 5.75 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029619608988159227		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.029619608988159227 | validation: 0.04347479652201258]
	TIME [epoch: 5.74 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028747007842675105		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.028747007842675105 | validation: 0.04062332428643487]
	TIME [epoch: 5.75 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029235772122995735		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.029235772122995735 | validation: 0.037076267934425665]
	TIME [epoch: 5.75 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026377588898768706		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.026377588898768706 | validation: 0.04479177169380913]
	TIME [epoch: 5.75 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02631694544814347		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.02631694544814347 | validation: 0.03786323918419698]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026210179128744928		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.026210179128744928 | validation: 0.03917649219038852]
	TIME [epoch: 5.75 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026554276034239054		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.026554276034239054 | validation: 0.04177799734136292]
	TIME [epoch: 5.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028147007729809683		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.028147007729809683 | validation: 0.041151073840728]
	TIME [epoch: 5.75 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027455976823649423		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.027455976823649423 | validation: 0.045262455397802485]
	TIME [epoch: 5.74 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02985480536580223		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.02985480536580223 | validation: 0.03827141317405472]
	TIME [epoch: 5.75 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02952921136711021		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.02952921136711021 | validation: 0.04306356280393069]
	TIME [epoch: 5.75 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028453361270870916		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.028453361270870916 | validation: 0.03728780302315739]
	TIME [epoch: 5.75 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027068488993030576		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.027068488993030576 | validation: 0.040325237228779356]
	TIME [epoch: 5.74 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026581244437002414		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.026581244437002414 | validation: 0.04112765489481845]
	TIME [epoch: 5.75 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027504680297673927		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.027504680297673927 | validation: 0.032493562116791885]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028521663186077163		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.028521663186077163 | validation: 0.035275264037674815]
	TIME [epoch: 5.75 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027843220050189712		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.027843220050189712 | validation: 0.035072895045517985]
	TIME [epoch: 5.74 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027964839580467717		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.027964839580467717 | validation: 0.04156712921476596]
	TIME [epoch: 5.75 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027083262550823543		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.027083262550823543 | validation: 0.04535935925890899]
	TIME [epoch: 5.75 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029298759653314133		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.029298759653314133 | validation: 0.03659221001856353]
	TIME [epoch: 5.75 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033299973347370934		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.033299973347370934 | validation: 0.04137447264501835]
	TIME [epoch: 5.74 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029261414524916703		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.029261414524916703 | validation: 0.039164328127719666]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028707093973772572		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.028707093973772572 | validation: 0.043177328447422794]
	TIME [epoch: 5.75 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026931049371894604		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.026931049371894604 | validation: 0.04225023477119684]
	TIME [epoch: 5.75 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02646322667225133		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.02646322667225133 | validation: 0.04228933884651628]
	TIME [epoch: 5.74 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028709215985652167		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.028709215985652167 | validation: 0.04053412574660314]
	TIME [epoch: 5.75 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02793247107949865		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.02793247107949865 | validation: 0.04272336764695317]
	TIME [epoch: 5.75 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030516618929744065		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.030516618929744065 | validation: 0.049173483991009986]
	TIME [epoch: 5.75 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027916208017873076		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.027916208017873076 | validation: 0.039923783839075036]
	TIME [epoch: 5.74 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02471084047106275		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.02471084047106275 | validation: 0.03568387386102878]
	TIME [epoch: 5.75 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026096849627764502		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.026096849627764502 | validation: 0.035366457041898725]
	TIME [epoch: 5.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025524731745573143		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.025524731745573143 | validation: 0.037738875593487424]
	TIME [epoch: 5.75 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02572776098988877		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.02572776098988877 | validation: 0.04050452742249028]
	TIME [epoch: 5.74 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02819043572162075		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.02819043572162075 | validation: 0.035522557103623086]
	TIME [epoch: 5.75 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024497845706716773		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.024497845706716773 | validation: 0.04004542077243566]
	TIME [epoch: 5.75 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026194092364598066		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.026194092364598066 | validation: 0.047279841670933645]
	TIME [epoch: 5.76 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027473717011037485		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.027473717011037485 | validation: 0.05071660401234727]
	TIME [epoch: 5.75 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030867752810459805		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.030867752810459805 | validation: 0.038675298576912556]
	TIME [epoch: 5.76 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028461641725798483		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.028461641725798483 | validation: 0.030753594908807413]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1064.pth
	Model improved!!!
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02726386208728765		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.02726386208728765 | validation: 0.030901478532884233]
	TIME [epoch: 5.75 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02599190272766311		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.02599190272766311 | validation: 0.0369782937937024]
	TIME [epoch: 5.75 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0241070123116475		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.0241070123116475 | validation: 0.047056991823036354]
	TIME [epoch: 5.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02566106843620441		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.02566106843620441 | validation: 0.04154241728593716]
	TIME [epoch: 5.75 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026713681194329694		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.026713681194329694 | validation: 0.03741993593819241]
	TIME [epoch: 5.74 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025565957613340566		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.025565957613340566 | validation: 0.03734230573348154]
	TIME [epoch: 5.75 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025587254907890815		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.025587254907890815 | validation: 0.038189822779632836]
	TIME [epoch: 5.75 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027017200033671872		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.027017200033671872 | validation: 0.03982630653934759]
	TIME [epoch: 5.75 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0251039068204238		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.0251039068204238 | validation: 0.03944958679034094]
	TIME [epoch: 5.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027458620260524244		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.027458620260524244 | validation: 0.04830933867325393]
	TIME [epoch: 5.75 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02862115261132834		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.02862115261132834 | validation: 0.03993272632441269]
	TIME [epoch: 5.75 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029573754645325422		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.029573754645325422 | validation: 0.036342705283952]
	TIME [epoch: 5.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02781050035560174		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.02781050035560174 | validation: 0.039886691691460634]
	TIME [epoch: 5.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026393743591362116		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.026393743591362116 | validation: 0.04433396904712681]
	TIME [epoch: 5.76 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024726179316866947		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.024726179316866947 | validation: 0.032794934378560923]
	TIME [epoch: 5.75 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539127269458701		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.02539127269458701 | validation: 0.038605918210162196]
	TIME [epoch: 5.75 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026798712143856422		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.026798712143856422 | validation: 0.04457097933393857]
	TIME [epoch: 5.76 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02717400205907998		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02717400205907998 | validation: 0.03981644187961719]
	TIME [epoch: 5.75 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0266975270953529		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.0266975270953529 | validation: 0.042342545494872354]
	TIME [epoch: 5.75 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027552104992449804		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.027552104992449804 | validation: 0.03739965290778248]
	TIME [epoch: 5.75 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02642344410080602		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.02642344410080602 | validation: 0.04192297610523421]
	TIME [epoch: 5.75 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025895520529968838		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.025895520529968838 | validation: 0.04218448795156261]
	TIME [epoch: 5.74 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024798525907089867		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.024798525907089867 | validation: 0.04687384751264732]
	TIME [epoch: 5.75 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025409296856165296		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.025409296856165296 | validation: 0.038534188194607126]
	TIME [epoch: 5.75 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027720553726202316		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.027720553726202316 | validation: 0.03938025264848066]
	TIME [epoch: 5.75 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02710728650121893		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.02710728650121893 | validation: 0.03807830868676271]
	TIME [epoch: 5.75 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02687409328008418		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.02687409328008418 | validation: 0.04414929520133895]
	TIME [epoch: 5.75 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025811035765808614		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.025811035765808614 | validation: 0.04094010655192146]
	TIME [epoch: 5.76 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025257383846394726		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.025257383846394726 | validation: 0.03982051175252507]
	TIME [epoch: 5.75 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025328543349371894		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.025328543349371894 | validation: 0.03593247750474359]
	TIME [epoch: 5.75 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02413698093087825		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.02413698093087825 | validation: 0.03172831119793479]
	TIME [epoch: 5.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02590981864628727		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.02590981864628727 | validation: 0.03949495869002542]
	TIME [epoch: 5.75 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02752148485376111		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.02752148485376111 | validation: 0.04080328886347441]
	TIME [epoch: 5.75 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025583037675426922		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.025583037675426922 | validation: 0.03680057600145903]
	TIME [epoch: 5.75 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02532914358965756		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.02532914358965756 | validation: 0.04389596290125911]
	TIME [epoch: 5.75 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02625435845040789		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02625435845040789 | validation: 0.04411815152044948]
	TIME [epoch: 5.75 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025140145402551002		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.025140145402551002 | validation: 0.03724748246140304]
	TIME [epoch: 5.75 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0250725763192849		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.0250725763192849 | validation: 0.03933943050593186]
	TIME [epoch: 5.75 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02508999551960952		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.02508999551960952 | validation: 0.03823797901716472]
	TIME [epoch: 5.75 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025919554806503303		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.025919554806503303 | validation: 0.037386483567242834]
	TIME [epoch: 5.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024201152013862793		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.024201152013862793 | validation: 0.0405806254033744]
	TIME [epoch: 5.76 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647180315433767		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.02647180315433767 | validation: 0.040187716886998286]
	TIME [epoch: 5.75 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540476753775045		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.02540476753775045 | validation: 0.03395472312082589]
	TIME [epoch: 5.75 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0245211012538114		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.0245211012538114 | validation: 0.035653804530714116]
	TIME [epoch: 5.75 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02498529202661243		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.02498529202661243 | validation: 0.034408668159515335]
	TIME [epoch: 5.74 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024244988655417625		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.024244988655417625 | validation: 0.037629156891220664]
	TIME [epoch: 5.74 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02528738641843173		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.02528738641843173 | validation: 0.04144365842313107]
	TIME [epoch: 5.74 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024102692609996045		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.024102692609996045 | validation: 0.03505473492112232]
	TIME [epoch: 5.75 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02700078268652282		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.02700078268652282 | validation: 0.03547597397430718]
	TIME [epoch: 5.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02502446935106602		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.02502446935106602 | validation: 0.03647829129444583]
	TIME [epoch: 5.74 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026531853529805388		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.026531853529805388 | validation: 0.03893784548128783]
	TIME [epoch: 5.74 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025639613960646068		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.025639613960646068 | validation: 0.039520689027559]
	TIME [epoch: 5.74 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025631258304207993		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.025631258304207993 | validation: 0.03792988390239197]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02628512098280076		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.02628512098280076 | validation: 0.03768687359920088]
	TIME [epoch: 5.74 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02567466359264726		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02567466359264726 | validation: 0.03207989747232944]
	TIME [epoch: 5.73 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023292311714544215		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.023292311714544215 | validation: 0.037759376346563026]
	TIME [epoch: 5.73 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024478009087452807		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.024478009087452807 | validation: 0.040364462325293615]
	TIME [epoch: 5.73 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023830202643420062		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.023830202643420062 | validation: 0.035628115511457094]
	TIME [epoch: 5.74 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024181134117724948		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.024181134117724948 | validation: 0.0357008132186464]
	TIME [epoch: 5.74 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026105057820786533		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.026105057820786533 | validation: 0.035331079346540006]
	TIME [epoch: 5.74 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025083479287942422		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.025083479287942422 | validation: 0.04414846971257119]
	TIME [epoch: 5.74 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024166773167089213		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.024166773167089213 | validation: 0.03284041015157679]
	TIME [epoch: 5.74 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024053316412889825		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.024053316412889825 | validation: 0.03548848197003817]
	TIME [epoch: 5.74 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026323446827037734		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.026323446827037734 | validation: 0.03773221439641742]
	TIME [epoch: 5.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024122704622600882		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.024122704622600882 | validation: 0.03249024880408652]
	TIME [epoch: 5.73 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028216371106865746		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.028216371106865746 | validation: 0.039959783511664126]
	TIME [epoch: 5.74 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023898002524320103		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.023898002524320103 | validation: 0.03667850055975776]
	TIME [epoch: 5.74 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02495904531020945		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.02495904531020945 | validation: 0.036364305672934506]
	TIME [epoch: 5.74 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024438453150284346		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.024438453150284346 | validation: 0.0343820029709575]
	TIME [epoch: 5.74 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024368130687477924		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.024368130687477924 | validation: 0.03778949424278597]
	TIME [epoch: 5.74 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025901063594373795		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.025901063594373795 | validation: 0.035388371996335266]
	TIME [epoch: 5.74 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0255662041036459		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.0255662041036459 | validation: 0.03776052906431049]
	TIME [epoch: 5.74 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02336825433924528		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.02336825433924528 | validation: 0.03489218972371936]
	TIME [epoch: 5.74 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023867590354826426		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.023867590354826426 | validation: 0.03362194228380061]
	TIME [epoch: 5.75 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024890412568678185		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.024890412568678185 | validation: 0.04281406419299771]
	TIME [epoch: 5.73 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023782092527330986		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.023782092527330986 | validation: 0.036298054116446314]
	TIME [epoch: 5.74 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024694733202100947		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.024694733202100947 | validation: 0.033061427352684025]
	TIME [epoch: 5.74 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023901128514684542		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.023901128514684542 | validation: 0.03468927537294817]
	TIME [epoch: 5.74 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02500292649934619		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.02500292649934619 | validation: 0.03050441761759406]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1143.pth
	Model improved!!!
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02430089883826944		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02430089883826944 | validation: 0.03156861757639203]
	TIME [epoch: 5.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02442978665547397		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.02442978665547397 | validation: 0.036077100563692586]
	TIME [epoch: 5.74 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024232946669019012		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.024232946669019012 | validation: 0.03838969787933218]
	TIME [epoch: 5.74 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023649007376917865		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.023649007376917865 | validation: 0.03466065356280458]
	TIME [epoch: 5.74 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023996050814525296		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.023996050814525296 | validation: 0.0356331668787395]
	TIME [epoch: 5.74 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024837309810525336		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.024837309810525336 | validation: 0.03973842941417519]
	TIME [epoch: 5.75 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025605882799880755		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.025605882799880755 | validation: 0.03442290840355557]
	TIME [epoch: 5.74 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025292953753814497		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.025292953753814497 | validation: 0.03698142685662239]
	TIME [epoch: 5.74 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0238984455616339		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.0238984455616339 | validation: 0.031115780990763467]
	TIME [epoch: 5.74 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024276030679037195		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.024276030679037195 | validation: 0.03582990268973423]
	TIME [epoch: 5.74 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024656247795981898		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.024656247795981898 | validation: 0.040186839350844106]
	TIME [epoch: 5.75 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0255851972142191		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.0255851972142191 | validation: 0.03382379924324524]
	TIME [epoch: 5.74 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023136591888273546		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.023136591888273546 | validation: 0.033206222749207516]
	TIME [epoch: 5.74 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024559738756459934		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.024559738756459934 | validation: 0.03498526629537992]
	TIME [epoch: 5.74 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022699538349675624		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.022699538349675624 | validation: 0.043780067199916534]
	TIME [epoch: 5.75 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02492152750456799		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.02492152750456799 | validation: 0.041167526813733125]
	TIME [epoch: 5.75 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024184876210438473		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.024184876210438473 | validation: 0.03611287183370462]
	TIME [epoch: 5.74 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023941570184854415		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.023941570184854415 | validation: 0.03551656807553243]
	TIME [epoch: 5.74 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023712387510624468		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.023712387510624468 | validation: 0.03210857785174922]
	TIME [epoch: 5.74 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025000207964110652		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.025000207964110652 | validation: 0.03391848102692883]
	TIME [epoch: 5.74 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02370696202155526		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.02370696202155526 | validation: 0.03934556373271846]
	TIME [epoch: 5.75 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022725816046216357		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.022725816046216357 | validation: 0.031398156354226284]
	TIME [epoch: 5.74 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021974598332358736		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.021974598332358736 | validation: 0.0313098809542828]
	TIME [epoch: 5.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023729942533411208		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.023729942533411208 | validation: 0.04082280846865806]
	TIME [epoch: 5.74 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0243100473540684		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.0243100473540684 | validation: 0.03743296528275471]
	TIME [epoch: 5.74 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02475044237765092		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.02475044237765092 | validation: 0.03553910317609701]
	TIME [epoch: 5.75 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024856129748977943		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.024856129748977943 | validation: 0.03714004445034953]
	TIME [epoch: 5.75 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023356185655871047		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.023356185655871047 | validation: 0.03226582051737697]
	TIME [epoch: 5.74 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024155540966663783		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.024155540966663783 | validation: 0.029471513162714326]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1172.pth
	Model improved!!!
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024580299207022864		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.024580299207022864 | validation: 0.03318694996926654]
	TIME [epoch: 5.74 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02347764197484672		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02347764197484672 | validation: 0.03434820974222853]
	TIME [epoch: 5.74 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02246882679494331		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.02246882679494331 | validation: 0.033206372578388425]
	TIME [epoch: 5.74 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02413599466785522		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02413599466785522 | validation: 0.031432864157129016]
	TIME [epoch: 5.74 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024015175792311334		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.024015175792311334 | validation: 0.04288566146498512]
	TIME [epoch: 5.73 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024007975434716188		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.024007975434716188 | validation: 0.035511629574620375]
	TIME [epoch: 5.74 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02333644466896348		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.02333644466896348 | validation: 0.028616989006975957]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1179.pth
	Model improved!!!
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02440290933695601		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.02440290933695601 | validation: 0.03511207876566243]
	TIME [epoch: 5.74 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02323036492351831		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.02323036492351831 | validation: 0.036502989116092066]
	TIME [epoch: 5.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024682113032898603		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.024682113032898603 | validation: 0.030223745615819343]
	TIME [epoch: 5.74 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021611445227400435		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.021611445227400435 | validation: 0.034144872897009314]
	TIME [epoch: 5.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023584799088324494		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.023584799088324494 | validation: 0.03681740465677106]
	TIME [epoch: 5.73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022395672958957374		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.022395672958957374 | validation: 0.03157001200424005]
	TIME [epoch: 5.74 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02300055024123716		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.02300055024123716 | validation: 0.036575183765718014]
	TIME [epoch: 5.73 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022717843850195374		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.022717843850195374 | validation: 0.038315681720281304]
	TIME [epoch: 5.74 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0248694500641601		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.0248694500641601 | validation: 0.03335040236060582]
	TIME [epoch: 5.73 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0245955576337291		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.0245955576337291 | validation: 0.03287332721836941]
	TIME [epoch: 5.74 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022257004272493726		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.022257004272493726 | validation: 0.03109577932814425]
	TIME [epoch: 5.74 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021606964075143967		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.021606964075143967 | validation: 0.033395228865722236]
	TIME [epoch: 5.74 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02324315149138075		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.02324315149138075 | validation: 0.03648833208820034]
	TIME [epoch: 5.74 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023898837191637386		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.023898837191637386 | validation: 0.039025679998111795]
	TIME [epoch: 5.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023519643063031843		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.023519643063031843 | validation: 0.038216894071535605]
	TIME [epoch: 5.75 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023130055685738293		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.023130055685738293 | validation: 0.03163661188520672]
	TIME [epoch: 5.75 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02225066450017226		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.02225066450017226 | validation: 0.03612106740657597]
	TIME [epoch: 5.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02179803613060336		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.02179803613060336 | validation: 0.038062193449473196]
	TIME [epoch: 5.74 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02289990691076596		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.02289990691076596 | validation: 0.032299568224470034]
	TIME [epoch: 5.74 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023694882943712702		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.023694882943712702 | validation: 0.032008297067208105]
	TIME [epoch: 5.74 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023493688149622726		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.023493688149622726 | validation: 0.03718863781559291]
	TIME [epoch: 5.74 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022604868040630742		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.022604868040630742 | validation: 0.03785712433199933]
	TIME [epoch: 5.72 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02410387706925495		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02410387706925495 | validation: 0.03646435513173407]
	TIME [epoch: 5.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022867144402567093		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.022867144402567093 | validation: 0.03357511740610878]
	TIME [epoch: 5.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022330479772782307		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.022330479772782307 | validation: 0.035302297493619496]
	TIME [epoch: 5.72 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022541417045373234		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.022541417045373234 | validation: 0.03439953537704876]
	TIME [epoch: 5.72 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02344116811299338		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.02344116811299338 | validation: 0.03442295306628584]
	TIME [epoch: 5.72 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02414055823409765		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.02414055823409765 | validation: 0.039730067802040386]
	TIME [epoch: 5.72 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024557830901285972		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.024557830901285972 | validation: 0.03020068666259751]
	TIME [epoch: 5.72 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021732673452451983		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.021732673452451983 | validation: 0.029760738803126296]
	TIME [epoch: 5.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023335693006860333		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.023335693006860333 | validation: 0.029815472113027743]
	TIME [epoch: 5.72 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023118702802488695		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.023118702802488695 | validation: 0.032526251523362935]
	TIME [epoch: 5.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0231329022624372		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.0231329022624372 | validation: 0.035253271827416686]
	TIME [epoch: 5.72 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023556509841250774		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.023556509841250774 | validation: 0.026904032055644057]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1213.pth
	Model improved!!!
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02345226358649783		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.02345226358649783 | validation: 0.030177710395154756]
	TIME [epoch: 5.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02382634104078231		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.02382634104078231 | validation: 0.03325421269036005]
	TIME [epoch: 5.75 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022431595063277073		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.022431595063277073 | validation: 0.03524057433967487]
	TIME [epoch: 5.74 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023115697481014514		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.023115697481014514 | validation: 0.02986538356078936]
	TIME [epoch: 5.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02167284252910126		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.02167284252910126 | validation: 0.03297954003334791]
	TIME [epoch: 5.74 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236661099506351		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.02236661099506351 | validation: 0.039020638938368025]
	TIME [epoch: 5.74 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024018398253504235		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.024018398253504235 | validation: 0.03692177262507195]
	TIME [epoch: 5.74 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022915698141935562		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.022915698141935562 | validation: 0.033814804683350456]
	TIME [epoch: 5.75 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022353209348944664		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.022353209348944664 | validation: 0.03451856335712289]
	TIME [epoch: 5.74 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023381161454847695		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.023381161454847695 | validation: 0.030442066305581706]
	TIME [epoch: 5.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020902542290163705		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.020902542290163705 | validation: 0.03993738205379325]
	TIME [epoch: 5.74 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02233441685685811		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.02233441685685811 | validation: 0.03420289514779789]
	TIME [epoch: 5.74 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021981730960940464		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.021981730960940464 | validation: 0.032034064973032954]
	TIME [epoch: 5.74 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02223581379553199		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.02223581379553199 | validation: 0.037649161039454294]
	TIME [epoch: 5.74 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022149668741237853		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.022149668741237853 | validation: 0.030077343469651175]
	TIME [epoch: 5.74 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021839922228692396		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.021839922228692396 | validation: 0.035624649719188384]
	TIME [epoch: 5.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02237726349352813		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02237726349352813 | validation: 0.03123423645480118]
	TIME [epoch: 5.74 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024619334736162807		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.024619334736162807 | validation: 0.037367985376790515]
	TIME [epoch: 5.75 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023104457925283275		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.023104457925283275 | validation: 0.029371311102270026]
	TIME [epoch: 5.74 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022522101419090244		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.022522101419090244 | validation: 0.036253451920380354]
	TIME [epoch: 5.74 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023319176537285876		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.023319176537285876 | validation: 0.031028663692668247]
	TIME [epoch: 5.74 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02184615694088591		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.02184615694088591 | validation: 0.030439682309229246]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02327497142311579		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02327497142311579 | validation: 0.03619187837808169]
	TIME [epoch: 5.75 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023344391055252274		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.023344391055252274 | validation: 0.03410951481795196]
	TIME [epoch: 5.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02125631066250721		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.02125631066250721 | validation: 0.031432152263372294]
	TIME [epoch: 5.74 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02204404779849456		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.02204404779849456 | validation: 0.03436949350867948]
	TIME [epoch: 5.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022742391640437083		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.022742391640437083 | validation: 0.03583060254470324]
	TIME [epoch: 5.74 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023032234756518215		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.023032234756518215 | validation: 0.03635288844147451]
	TIME [epoch: 5.75 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132457739721554		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02132457739721554 | validation: 0.03437012133752151]
	TIME [epoch: 5.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022437974773939783		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.022437974773939783 | validation: 0.03618602563042194]
	TIME [epoch: 5.74 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022375637608680318		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.022375637608680318 | validation: 0.03618050172715822]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02197351309767053		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.02197351309767053 | validation: 0.03409702093402246]
	TIME [epoch: 5.75 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022034842520206016		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.022034842520206016 | validation: 0.032319336633475214]
	TIME [epoch: 5.74 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132151199533296		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.02132151199533296 | validation: 0.03714477082904151]
	TIME [epoch: 5.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02078359718571654		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.02078359718571654 | validation: 0.03686896742591431]
	TIME [epoch: 5.74 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020128970241350966		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.020128970241350966 | validation: 0.030495689635876566]
	TIME [epoch: 5.74 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023623544454259994		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.023623544454259994 | validation: 0.03166602900457273]
	TIME [epoch: 5.74 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022286707648414502		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.022286707648414502 | validation: 0.036399383097365234]
	TIME [epoch: 5.75 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022502268672133226		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.022502268672133226 | validation: 0.03294908746666303]
	TIME [epoch: 5.75 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022660064965525174		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.022660064965525174 | validation: 0.0360582842663051]
	TIME [epoch: 5.74 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022584369714808757		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.022584369714808757 | validation: 0.033649036967037596]
	TIME [epoch: 5.74 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022158524746317552		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.022158524746317552 | validation: 0.03182290083992535]
	TIME [epoch: 5.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02293699791130359		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.02293699791130359 | validation: 0.035692800699018305]
	TIME [epoch: 5.74 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023201454228164015		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.023201454228164015 | validation: 0.029257938270498066]
	TIME [epoch: 5.75 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022329159270724482		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.022329159270724482 | validation: 0.03065692183771437]
	TIME [epoch: 5.74 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02129769485152538		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.02129769485152538 | validation: 0.028517286669751264]
	TIME [epoch: 5.75 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021700235048177398		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.021700235048177398 | validation: 0.0306756486283852]
	TIME [epoch: 5.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022793718396497794		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.022793718396497794 | validation: 0.031904021934964034]
	TIME [epoch: 5.74 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021911548451441273		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.021911548451441273 | validation: 0.0319186984344648]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020727504432464277		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.020727504432464277 | validation: 0.0249158850041615]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1263.pth
	Model improved!!!
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02074621515716429		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.02074621515716429 | validation: 0.032899141566715195]
	TIME [epoch: 5.74 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02299939275055615		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.02299939275055615 | validation: 0.036011400830325384]
	TIME [epoch: 5.74 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021208209120723347		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.021208209120723347 | validation: 0.03442970904458026]
	TIME [epoch: 5.74 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021804632715070396		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.021804632715070396 | validation: 0.03342893146423768]
	TIME [epoch: 5.74 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02265228818418019		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.02265228818418019 | validation: 0.030975690127426306]
	TIME [epoch: 5.75 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020810973917951437		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.020810973917951437 | validation: 0.03380035734113729]
	TIME [epoch: 5.75 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020891586173703482		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.020891586173703482 | validation: 0.030592695632436186]
	TIME [epoch: 5.75 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021209987746173713		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.021209987746173713 | validation: 0.030230661645958248]
	TIME [epoch: 5.75 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021794045017184294		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.021794045017184294 | validation: 0.03359454384651763]
	TIME [epoch: 5.76 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020591199858385093		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.020591199858385093 | validation: 0.03543130549764918]
	TIME [epoch: 5.76 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02291433776934023		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.02291433776934023 | validation: 0.03241484172655871]
	TIME [epoch: 5.75 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020374935808087836		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.020374935808087836 | validation: 0.031238825717203156]
	TIME [epoch: 5.75 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021714474469953168		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.021714474469953168 | validation: 0.029806158382623973]
	TIME [epoch: 5.75 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02092045892588515		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.02092045892588515 | validation: 0.028898979680303163]
	TIME [epoch: 5.75 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022672770468730983		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.022672770468730983 | validation: 0.031200427559530532]
	TIME [epoch: 5.75 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02218584702319423		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.02218584702319423 | validation: 0.037708498273019746]
	TIME [epoch: 5.75 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021345451423414624		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.021345451423414624 | validation: 0.03540033860583413]
	TIME [epoch: 5.75 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021610105423541085		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.021610105423541085 | validation: 0.03038491921904435]
	TIME [epoch: 5.75 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019580049090094656		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.019580049090094656 | validation: 0.03953018606396358]
	TIME [epoch: 5.76 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021364908256800326		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.021364908256800326 | validation: 0.03802118592766243]
	TIME [epoch: 5.75 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022054671797972124		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.022054671797972124 | validation: 0.03033616469629772]
	TIME [epoch: 5.76 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023047180272434194		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.023047180272434194 | validation: 0.03117121165310729]
	TIME [epoch: 5.75 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021287335528796917		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.021287335528796917 | validation: 0.03313367083388583]
	TIME [epoch: 5.76 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021782881594353622		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.021782881594353622 | validation: 0.02910626634079775]
	TIME [epoch: 5.75 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022130466436289225		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.022130466436289225 | validation: 0.03205767156043512]
	TIME [epoch: 5.76 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02194937100864744		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02194937100864744 | validation: 0.0430540991408114]
	TIME [epoch: 5.75 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023873316400483135		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.023873316400483135 | validation: 0.03366804785128485]
	TIME [epoch: 5.78 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02252805475130078		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02252805475130078 | validation: 0.030642292657667426]
	TIME [epoch: 5.75 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020645837965700808		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.020645837965700808 | validation: 0.03174710392211971]
	TIME [epoch: 5.75 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019898509400871386		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.019898509400871386 | validation: 0.034121490980505556]
	TIME [epoch: 5.76 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02070669264630343		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.02070669264630343 | validation: 0.034065001437751395]
	TIME [epoch: 5.75 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020225063932132636		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.020225063932132636 | validation: 0.03427459375243637]
	TIME [epoch: 5.75 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02042932580491067		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.02042932580491067 | validation: 0.03246544540560614]
	TIME [epoch: 5.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02064110421234238		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.02064110421234238 | validation: 0.03560894598467353]
	TIME [epoch: 5.75 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02143303971360391		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.02143303971360391 | validation: 0.031498092477115616]
	TIME [epoch: 5.76 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02155343252837691		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.02155343252837691 | validation: 0.029789673388927088]
	TIME [epoch: 5.75 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01959566395373456		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.01959566395373456 | validation: 0.028137383708856756]
	TIME [epoch: 5.75 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022072886875828753		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.022072886875828753 | validation: 0.037155027079104806]
	TIME [epoch: 5.75 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021049037019484837		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.021049037019484837 | validation: 0.03515473002221022]
	TIME [epoch: 5.75 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01986446542646977		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.01986446542646977 | validation: 0.03820373384607944]
	TIME [epoch: 5.75 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02148183791088749		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.02148183791088749 | validation: 0.03469364740993255]
	TIME [epoch: 5.75 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0207089926663284		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.0207089926663284 | validation: 0.03166611269149446]
	TIME [epoch: 5.74 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0202828124969521		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.0202828124969521 | validation: 0.032495216717491615]
	TIME [epoch: 5.75 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019674576163620287		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.019674576163620287 | validation: 0.034806161141114976]
	TIME [epoch: 5.75 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02135310290171182		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02135310290171182 | validation: 0.02726603416777781]
	TIME [epoch: 5.75 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020972059544252814		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.020972059544252814 | validation: 0.03131511443663685]
	TIME [epoch: 5.74 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020952145149518993		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.020952145149518993 | validation: 0.029876407045021848]
	TIME [epoch: 5.75 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020895869853979614		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.020895869853979614 | validation: 0.03502380489170719]
	TIME [epoch: 5.74 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020215351372975192		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.020215351372975192 | validation: 0.03206092273473656]
	TIME [epoch: 5.76 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02165256187267065		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02165256187267065 | validation: 0.029494666247926595]
	TIME [epoch: 5.74 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022167097809128462		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.022167097809128462 | validation: 0.028730042461681073]
	TIME [epoch: 5.76 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021151484433292636		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.021151484433292636 | validation: 0.030810279997623116]
	TIME [epoch: 5.74 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02184052069421421		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.02184052069421421 | validation: 0.0278418561252496]
	TIME [epoch: 5.75 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02137675016429148		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.02137675016429148 | validation: 0.03294994547501636]
	TIME [epoch: 5.74 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01922545712877764		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.01922545712877764 | validation: 0.03077518404439559]
	TIME [epoch: 5.75 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021048336803763057		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.021048336803763057 | validation: 0.030064392474184132]
	TIME [epoch: 5.75 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02097953932793891		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02097953932793891 | validation: 0.0286683553429406]
	TIME [epoch: 5.75 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022186756258553392		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.022186756258553392 | validation: 0.02951375768388518]
	TIME [epoch: 5.74 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020592036210966363		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.020592036210966363 | validation: 0.03174307444093168]
	TIME [epoch: 5.75 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0202235280877806		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.0202235280877806 | validation: 0.030578585845376396]
	TIME [epoch: 5.75 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02061373840091995		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.02061373840091995 | validation: 0.03411667818528246]
	TIME [epoch: 5.76 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206752855712429		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.0206752855712429 | validation: 0.030547222235756944]
	TIME [epoch: 5.75 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022116641712667327		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.022116641712667327 | validation: 0.029392745662250853]
	TIME [epoch: 5.75 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01941516779651297		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.01941516779651297 | validation: 0.03010845788595754]
	TIME [epoch: 5.74 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022076202826184336		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.022076202826184336 | validation: 0.0343916671663427]
	TIME [epoch: 5.75 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039044693067258		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.02039044693067258 | validation: 0.03391038749682542]
	TIME [epoch: 5.75 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02103023851822681		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.02103023851822681 | validation: 0.02717311726080157]
	TIME [epoch: 5.75 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021192959446603		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.021192959446603 | validation: 0.029919792325673468]
	TIME [epoch: 5.75 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02126059122380651		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.02126059122380651 | validation: 0.027493424278946323]
	TIME [epoch: 5.75 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020252161426363154		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.020252161426363154 | validation: 0.02875185190230556]
	TIME [epoch: 5.74 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02038193509687867		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02038193509687867 | validation: 0.029879870192161907]
	TIME [epoch: 5.76 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961933055874321		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.01961933055874321 | validation: 0.027424451282097154]
	TIME [epoch: 5.74 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02047339517191969		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.02047339517191969 | validation: 0.02790046071255068]
	TIME [epoch: 5.75 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020884190920197627		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.020884190920197627 | validation: 0.03226628203638043]
	TIME [epoch: 5.75 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021040014014490304		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.021040014014490304 | validation: 0.02748962317646292]
	TIME [epoch: 5.75 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020559485573100354		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.020559485573100354 | validation: 0.028693074176203627]
	TIME [epoch: 5.75 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020171462241067868		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.020171462241067868 | validation: 0.031407416296059025]
	TIME [epoch: 5.75 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019269850056710138		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.019269850056710138 | validation: 0.03345303621318296]
	TIME [epoch: 5.74 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020190410897879416		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.020190410897879416 | validation: 0.02527425857797705]
	TIME [epoch: 5.76 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020809672407544343		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.020809672407544343 | validation: 0.038573341191946324]
	TIME [epoch: 5.74 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020101189361079422		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.020101189361079422 | validation: 0.03360313907570924]
	TIME [epoch: 5.75 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02032776762558636		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.02032776762558636 | validation: 0.030871477293004025]
	TIME [epoch: 5.74 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020431052470331723		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.020431052470331723 | validation: 0.03119906266323579]
	TIME [epoch: 5.75 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019524077417780908		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.019524077417780908 | validation: 0.03175746126065626]
	TIME [epoch: 5.74 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020068844058074502		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.020068844058074502 | validation: 0.036380087521733474]
	TIME [epoch: 5.75 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020102758204859926		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.020102758204859926 | validation: 0.026366029840669738]
	TIME [epoch: 5.74 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02060819605193653		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.02060819605193653 | validation: 0.02879651980630147]
	TIME [epoch: 5.75 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020500299332819782		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.020500299332819782 | validation: 0.03360496843138629]
	TIME [epoch: 5.74 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02099592268953685		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.02099592268953685 | validation: 0.029419473900180872]
	TIME [epoch: 5.76 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02046167964971967		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.02046167964971967 | validation: 0.03484560997560308]
	TIME [epoch: 5.74 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020597868784885846		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.020597868784885846 | validation: 0.03629230778055982]
	TIME [epoch: 5.75 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020669764548395874		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.020669764548395874 | validation: 0.029354715833936985]
	TIME [epoch: 5.75 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020420853818798358		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.020420853818798358 | validation: 0.02763098442707669]
	TIME [epoch: 5.75 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019903210905128883		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.019903210905128883 | validation: 0.030694328635759206]
	TIME [epoch: 5.74 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02126692089052135		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.02126692089052135 | validation: 0.030201889953479145]
	TIME [epoch: 5.75 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236555353309666		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.02236555353309666 | validation: 0.03162068146028411]
	TIME [epoch: 5.75 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020757430400356727		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.020757430400356727 | validation: 0.028856008161007675]
	TIME [epoch: 5.76 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019420386092043943		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.019420386092043943 | validation: 0.02761703820086151]
	TIME [epoch: 5.74 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0212116960659874		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.0212116960659874 | validation: 0.026336885476588356]
	TIME [epoch: 5.75 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02083780485005888		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.02083780485005888 | validation: 0.0298179725910526]
	TIME [epoch: 5.74 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01984317891325769		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.01984317891325769 | validation: 0.02960863642620931]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_11_v_mmd4_20250519_185958/states/model_phi1_4a_distortion_v1_11_v_mmd4_1364.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4780.081 seconds.
