Args:
Namespace(name='model_phi1_4a_distortion_v1_3_v_mmd3', outdir='out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.048453342, 0.1, 1.0], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 952721835

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2247676074514526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2247676074514526 | validation: 3.5734919379717343]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.153813601328707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.153813601328707 | validation: 3.958355607535728]
	TIME [epoch: 0.585 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8024331577014725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8024331577014725 | validation: 3.5154426472859472]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0258841231430846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0258841231430846 | validation: 3.4662686015482183]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9904781696287346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9904781696287346 | validation: 3.5007720860744147]
	TIME [epoch: 0.574 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9186892378462312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9186892378462312 | validation: 3.4193610080250005]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8936842551985644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8936842551985644 | validation: 3.422858913219878]
	TIME [epoch: 0.573 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.835402418353926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.835402418353926 | validation: 3.435514700989805]
	TIME [epoch: 0.571 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8044675019078613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8044675019078613 | validation: 3.1831713313704557]
	TIME [epoch: 0.579 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.758576563606755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.758576563606755 | validation: 2.999096182968738]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7032302059195796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7032302059195796 | validation: 2.938735619840387]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6246725812191825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6246725812191825 | validation: 2.9117593220247118]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5486526827649034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5486526827649034 | validation: 2.8335124759253336]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.472156384481816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.472156384481816 | validation: 2.7432681638657486]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3952038145154813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3952038145154813 | validation: 2.6995281093339387]
	TIME [epoch: 0.575 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3010850320052287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3010850320052287 | validation: 2.598346766501287]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.310646271480441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.310646271480441 | validation: 3.3681138184891077]
	TIME [epoch: 0.574 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.989753861493185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.989753861493185 | validation: 3.0560150710971685]
	TIME [epoch: 0.57 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.547067266803697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.547067266803697 | validation: 2.5521465457242276]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1774796949150783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1774796949150783 | validation: 2.5619736484303015]
	TIME [epoch: 0.573 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.203675367424052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.203675367424052 | validation: 2.5483905826653412]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1743916363728055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1743916363728055 | validation: 2.4864532750204127]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.102682128167518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.102682128167518 | validation: 2.46866135890654]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0310782523737596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0310782523737596 | validation: 2.523030188541227]
	TIME [epoch: 0.577 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9748925871985783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9748925871985783 | validation: 2.4047598795481555]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9085876879471062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9085876879471062 | validation: 2.451455461765594]
	TIME [epoch: 0.572 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.874671615858394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.874671615858394 | validation: 2.288211045266178]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9053002100891872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9053002100891872 | validation: 2.68126250827566]
	TIME [epoch: 0.573 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.227310204944506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.227310204944506 | validation: 2.5233850532248674]
	TIME [epoch: 0.57 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9811016359634281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9811016359634281 | validation: 2.4226784873843865]
	TIME [epoch: 0.572 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9223455408780792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9223455408780792 | validation: 2.358881339848217]
	TIME [epoch: 0.57 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8023813450601167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8023813450601167 | validation: 2.5217977750313185]
	TIME [epoch: 0.57 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.846950423561587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.846950423561587 | validation: 2.3748170255216094]
	TIME [epoch: 0.571 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7952716259305153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7952716259305153 | validation: 2.3082637382848277]
	TIME [epoch: 0.57 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7421637994336945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7421637994336945 | validation: 2.3992415475909943]
	TIME [epoch: 0.569 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7447949488513366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7447949488513366 | validation: 2.305413545140041]
	TIME [epoch: 0.57 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7083727957296035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7083727957296035 | validation: 2.309109676351754]
	TIME [epoch: 0.57 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6812760381139995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6812760381139995 | validation: 2.3051061893046163]
	TIME [epoch: 0.57 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6604346315909992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6604346315909992 | validation: 2.2851307528911997]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6535456939336626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6535456939336626 | validation: 2.3106044847430556]
	TIME [epoch: 0.575 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.656369403699271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.656369403699271 | validation: 2.2898204798823083]
	TIME [epoch: 0.573 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7465455069918767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7465455069918767 | validation: 2.393181412968231]
	TIME [epoch: 0.57 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7794325467247993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7794325467247993 | validation: 2.322378478586772]
	TIME [epoch: 0.569 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7619411680634915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7619411680634915 | validation: 2.277713714438294]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.671685796384063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.671685796384063 | validation: 2.3669054737913586]
	TIME [epoch: 0.574 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7596658977998199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7596658977998199 | validation: 2.2986240911201947]
	TIME [epoch: 0.572 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6990749676002017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6990749676002017 | validation: 2.267262729048539]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6706329302283076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6706329302283076 | validation: 2.2891340563141784]
	TIME [epoch: 0.573 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6665320594234532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6665320594234532 | validation: 2.2630728892449214]
	TIME [epoch: 0.578 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6395947996050786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6395947996050786 | validation: 2.26131747904243]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6395052179709075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6395052179709075 | validation: 2.2662373240103846]
	TIME [epoch: 0.575 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6418675494296173		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.6418675494296173 | validation: 2.250591040542718]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6321540351776431		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.6321540351776431 | validation: 2.251444446315766]
	TIME [epoch: 0.571 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.634443670657229		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.634443670657229 | validation: 2.231962420458407]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6334503136373963		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.6334503136373963 | validation: 2.2637701718700707]
	TIME [epoch: 0.573 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6518955044131158		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.6518955044131158 | validation: 2.2550853427031696]
	TIME [epoch: 0.571 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7224186321481683		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.7224186321481683 | validation: 2.2686444616236727]
	TIME [epoch: 0.573 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6711489890611029		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.6711489890611029 | validation: 2.254358135959786]
	TIME [epoch: 0.571 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6663614631537045		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.6663614631537045 | validation: 2.2304402303901836]
	TIME [epoch: 0.569 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6265082136837437		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.6265082136837437 | validation: 2.229915000303502]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6315753645840887		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.6315753645840887 | validation: 2.2236407084981358]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6422536976634223		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.6422536976634223 | validation: 2.247106254184707]
	TIME [epoch: 0.572 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6596996488673281		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.6596996488673281 | validation: 2.2455168649892054]
	TIME [epoch: 0.571 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6955378779144092		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.6955378779144092 | validation: 2.222802127225923]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6233360653912237		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.6233360653912237 | validation: 2.212909813565583]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6273916926115037		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.6273916926115037 | validation: 2.199126887661735]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6449436108728401		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.6449436108728401 | validation: 2.2034288202836474]
	TIME [epoch: 0.573 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6399817483847738		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.6399817483847738 | validation: 2.1909554771158635]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6580433089493665		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.6580433089493665 | validation: 2.2128748693760985]
	TIME [epoch: 0.573 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6404622453076394		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.6404622453076394 | validation: 2.1701836976230227]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.632233658654203		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.632233658654203 | validation: 2.1701370105282627]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6106219744506558		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.6106219744506558 | validation: 2.1638079322703607]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.610389114513474		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.610389114513474 | validation: 2.1924532326627366]
	TIME [epoch: 0.571 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6528512652003995		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.6528512652003995 | validation: 2.2038622766128433]
	TIME [epoch: 0.571 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.698606739333405		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.698606739333405 | validation: 2.178670388800438]
	TIME [epoch: 0.569 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6262787808999994		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.6262787808999994 | validation: 2.139872952256341]
	TIME [epoch: 0.569 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6147771683418253		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.6147771683418253 | validation: 2.13897262500868]
	TIME [epoch: 0.572 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.606973574223827		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.606973574223827 | validation: 2.132460521432991]
	TIME [epoch: 0.571 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5967752727876738		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.5967752727876738 | validation: 2.1143408227705116]
	TIME [epoch: 0.576 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6014776533240218		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.6014776533240218 | validation: 2.116144175717649]
	TIME [epoch: 0.572 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5968472940599656		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.5968472940599656 | validation: 2.1180390452192697]
	TIME [epoch: 0.57 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6040484496068796		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.6040484496068796 | validation: 2.1749980725583145]
	TIME [epoch: 0.57 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7309956340373678		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7309956340373678 | validation: 2.254459442276181]
	TIME [epoch: 0.576 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.683916258724459		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.683916258724459 | validation: 2.16508184065422]
	TIME [epoch: 0.569 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6468179385762627		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.6468179385762627 | validation: 2.1122770362774754]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5904679367903465		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.5904679367903465 | validation: 2.108662636541856]
	TIME [epoch: 0.574 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6180759197488066		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.6180759197488066 | validation: 2.113786239995607]
	TIME [epoch: 0.573 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6172565844298799		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6172565844298799 | validation: 2.1331198510116116]
	TIME [epoch: 0.571 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6113268434601566		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.6113268434601566 | validation: 2.1027362552682063]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5963569340613006		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.5963569340613006 | validation: 2.094989417486866]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.590384606505993		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.590384606505993 | validation: 2.098925341774443]
	TIME [epoch: 0.572 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.592968001347737		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.592968001347737 | validation: 2.09658384664273]
	TIME [epoch: 0.914 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5915314025045808		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.5915314025045808 | validation: 2.0993564824224538]
	TIME [epoch: 0.572 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5898601019199439		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.5898601019199439 | validation: 2.0892201934931687]
	TIME [epoch: 0.585 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6009307304754976		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.6009307304754976 | validation: 2.1104677621647627]
	TIME [epoch: 0.571 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6165526854069816		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6165526854069816 | validation: 2.2293134178808063]
	TIME [epoch: 0.571 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7742901867788852		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.7742901867788852 | validation: 2.1603514019445025]
	TIME [epoch: 0.613 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6376684763212694		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.6376684763212694 | validation: 2.1782086832727994]
	TIME [epoch: 0.585 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6497745713945653		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.6497745713945653 | validation: 2.1089249046414733]
	TIME [epoch: 0.569 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6211262434401306		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6211262434401306 | validation: 2.0809890996766107]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5928946284936576		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.5928946284936576 | validation: 2.088582938364142]
	TIME [epoch: 0.57 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6091440123204037		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6091440123204037 | validation: 2.0978938433620127]
	TIME [epoch: 0.569 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6114167180415495		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.6114167180415495 | validation: 2.0833161511287366]
	TIME [epoch: 0.573 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5855344493946826		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.5855344493946826 | validation: 2.0760602403207047]
	TIME [epoch: 0.568 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5814897943393822		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.5814897943393822 | validation: 2.0679062202037755]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5847507986039433		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.5847507986039433 | validation: 2.0791100896097943]
	TIME [epoch: 0.572 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.585641462175892		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.585641462175892 | validation: 2.067534441850659]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.589674651195677		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.589674651195677 | validation: 2.063844434467272]
	TIME [epoch: 0.573 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5872374065008013		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.5872374065008013 | validation: 2.104102359615389]
	TIME [epoch: 0.571 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.604853176614181		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.604853176614181 | validation: 2.1180581088833415]
	TIME [epoch: 0.57 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.619240145436832		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.619240145436832 | validation: 2.0973162745585046]
	TIME [epoch: 0.569 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6057887096596648		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6057887096596648 | validation: 2.054621848426045]
	TIME [epoch: 0.57 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5743854808038635		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5743854808038635 | validation: 2.055318118066509]
	TIME [epoch: 0.571 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5865120210372867		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.5865120210372867 | validation: 2.0761279985615655]
	TIME [epoch: 0.572 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.59029036165671		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.59029036165671 | validation: 2.0679614894023897]
	TIME [epoch: 0.571 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5816352729270715		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.5816352729270715 | validation: 2.0612233754790457]
	TIME [epoch: 0.57 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5905831201248777		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.5905831201248777 | validation: 2.0971519119465234]
	TIME [epoch: 0.569 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.629261525316829		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.629261525316829 | validation: 2.1546539232490427]
	TIME [epoch: 0.568 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6292390061124866		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.6292390061124866 | validation: 2.060136612515013]
	TIME [epoch: 0.569 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5813798751385577		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5813798751385577 | validation: 2.0535693763493237]
	TIME [epoch: 0.569 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5732711665079595		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.5732711665079595 | validation: 2.085906792056368]
	TIME [epoch: 0.573 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5849016856487492		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5849016856487492 | validation: 2.0768768876284596]
	TIME [epoch: 0.572 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5755503861152427		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5755503861152427 | validation: 2.0669961146642932]
	TIME [epoch: 0.572 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5672832016125362		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.5672832016125362 | validation: 2.067634653872865]
	TIME [epoch: 0.579 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5719914653368627		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.5719914653368627 | validation: 2.085115865643203]
	TIME [epoch: 0.571 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5694599528649535		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.5694599528649535 | validation: 2.084534040486376]
	TIME [epoch: 0.571 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5799846350590259		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.5799846350590259 | validation: 2.1617856882338704]
	TIME [epoch: 0.573 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6188994598830988		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.6188994598830988 | validation: 2.0993957107888757]
	TIME [epoch: 0.573 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5839167052804413		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.5839167052804413 | validation: 2.0688186484391937]
	TIME [epoch: 0.571 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5969079431994815		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.5969079431994815 | validation: 2.1198433480331538]
	TIME [epoch: 0.572 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6050524086563311		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.6050524086563311 | validation: 2.120388000161821]
	TIME [epoch: 0.573 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5923393675562125		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.5923393675562125 | validation: 2.135666437894463]
	TIME [epoch: 0.572 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6371531977838067		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.6371531977838067 | validation: 2.119745235038772]
	TIME [epoch: 0.573 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.603554881819557		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.603554881819557 | validation: 2.0997839430522034]
	TIME [epoch: 0.571 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.586042475874279		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.586042475874279 | validation: 2.068892853170163]
	TIME [epoch: 0.57 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.562705103737728		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.562705103737728 | validation: 2.0687398716910956]
	TIME [epoch: 0.571 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5697046282597678		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.5697046282597678 | validation: 2.089990498558719]
	TIME [epoch: 0.571 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5727831037770035		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.5727831037770035 | validation: 2.0884016811295507]
	TIME [epoch: 0.571 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5703911858293595		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5703911858293595 | validation: 2.075745392842729]
	TIME [epoch: 0.57 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5771785775449485		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.5771785775449485 | validation: 2.099122063776752]
	TIME [epoch: 0.572 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.576143797880184		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.576143797880184 | validation: 2.109289327034486]
	TIME [epoch: 0.572 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.594893842061876		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.594893842061876 | validation: 2.0989905537071634]
	TIME [epoch: 0.574 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5819978184387349		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.5819978184387349 | validation: 2.101827562767645]
	TIME [epoch: 0.571 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5776922918228702		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.5776922918228702 | validation: 2.0976911636623194]
	TIME [epoch: 0.569 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5769877120601883		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.5769877120601883 | validation: 2.103388154681121]
	TIME [epoch: 0.57 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5732967092766381		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.5732967092766381 | validation: 2.095254306556581]
	TIME [epoch: 0.57 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.563385970776552		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.563385970776552 | validation: 2.0687030563760738]
	TIME [epoch: 0.572 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.555097545770467		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.555097545770467 | validation: 2.095975732492093]
	TIME [epoch: 0.57 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5582681403008851		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5582681403008851 | validation: 2.087949784095249]
	TIME [epoch: 0.571 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5766317800173837		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.5766317800173837 | validation: 2.120017345753572]
	TIME [epoch: 0.572 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6304533568635768		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.6304533568635768 | validation: 2.1245739802420487]
	TIME [epoch: 0.571 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5914505366442862		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.5914505366442862 | validation: 2.1280178409650823]
	TIME [epoch: 0.571 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5913545973589118		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.5913545973589118 | validation: 2.118559601945313]
	TIME [epoch: 0.572 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5836543612545106		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5836543612545106 | validation: 2.0878745689877904]
	TIME [epoch: 0.572 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5572306006210768		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.5572306006210768 | validation: 2.0879415248279036]
	TIME [epoch: 0.571 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5554828024056258		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.5554828024056258 | validation: 2.1001215905578006]
	TIME [epoch: 0.571 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5572570611901342		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.5572570611901342 | validation: 2.095451752071877]
	TIME [epoch: 0.572 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.555352050841392		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.555352050841392 | validation: 2.086077525797627]
	TIME [epoch: 0.572 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5575536564537669		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.5575536564537669 | validation: 2.1244001326673625]
	TIME [epoch: 0.571 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.584690571354394		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.584690571354394 | validation: 2.103443809589784]
	TIME [epoch: 0.572 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5921670725921881		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5921670725921881 | validation: 2.108801769241196]
	TIME [epoch: 0.572 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.598224024444038		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.598224024444038 | validation: 2.115979111673402]
	TIME [epoch: 0.581 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5568760838397935		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.5568760838397935 | validation: 2.097199665807739]
	TIME [epoch: 0.57 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.562330249406056		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.562330249406056 | validation: 2.139614325558895]
	TIME [epoch: 0.571 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.586517714837468		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.586517714837468 | validation: 2.097603081621072]
	TIME [epoch: 0.571 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.560880314595245		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.560880314595245 | validation: 2.0710092734649463]
	TIME [epoch: 0.571 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5490591924435588		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.5490591924435588 | validation: 2.09550355814912]
	TIME [epoch: 0.571 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5507088248470202		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.5507088248470202 | validation: 2.074064540740921]
	TIME [epoch: 0.571 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5502355815631341		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.5502355815631341 | validation: 2.0963173073845685]
	TIME [epoch: 0.572 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5632322972589947		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.5632322972589947 | validation: 2.105906347048272]
	TIME [epoch: 0.57 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5703669786263579		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.5703669786263579 | validation: 2.1459961397202028]
	TIME [epoch: 0.571 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6023415833801715		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.6023415833801715 | validation: 2.185923358699831]
	TIME [epoch: 0.57 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6064042837764059		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.6064042837764059 | validation: 2.1134889105795787]
	TIME [epoch: 0.571 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5601994679915887		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.5601994679915887 | validation: 2.1015636015932446]
	TIME [epoch: 0.57 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5679388312252107		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.5679388312252107 | validation: 2.124562407011559]
	TIME [epoch: 0.569 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.566246247170634		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.566246247170634 | validation: 2.105844891701757]
	TIME [epoch: 0.577 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5582229245618053		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.5582229245618053 | validation: 2.081730982310034]
	TIME [epoch: 0.569 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5477461310778702		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.5477461310778702 | validation: 2.099808932961519]
	TIME [epoch: 0.569 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5410319910037114		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.5410319910037114 | validation: 2.092375703316524]
	TIME [epoch: 0.572 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.543250558131151		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.543250558131151 | validation: 2.0927112782594426]
	TIME [epoch: 0.571 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5534587070966814		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.5534587070966814 | validation: 2.1102833715446843]
	TIME [epoch: 0.57 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5763240068232365		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.5763240068232365 | validation: 2.143418171318441]
	TIME [epoch: 0.57 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6084469992158439		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.6084469992158439 | validation: 2.1158087962596754]
	TIME [epoch: 0.57 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5486353171126417		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.5486353171126417 | validation: 2.1115783056729946]
	TIME [epoch: 0.57 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5587758534362812		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.5587758534362812 | validation: 2.185077926456741]
	TIME [epoch: 0.569 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6123314379855338		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.6123314379855338 | validation: 2.1147295568152944]
	TIME [epoch: 0.569 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.556248999514933		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.556248999514933 | validation: 2.101403619160482]
	TIME [epoch: 0.57 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.560368073302715		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.560368073302715 | validation: 2.1354278750824363]
	TIME [epoch: 0.576 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.568669318561491		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.568669318561491 | validation: 2.1098534372902544]
	TIME [epoch: 0.57 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5575099025227623		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.5575099025227623 | validation: 2.0849451591538486]
	TIME [epoch: 0.57 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5462040093878795		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.5462040093878795 | validation: 2.106051111212483]
	TIME [epoch: 0.57 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5592638060263049		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.5592638060263049 | validation: 2.0840486555071664]
	TIME [epoch: 0.57 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5404955059596892		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.5404955059596892 | validation: 2.0871422822040593]
	TIME [epoch: 0.57 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5258937296740878		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.5258937296740878 | validation: 2.076167981629618]
	TIME [epoch: 0.57 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.53230669271451		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.53230669271451 | validation: 2.1010118961272157]
	TIME [epoch: 0.571 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5445258340642674		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.5445258340642674 | validation: 2.173987443185066]
	TIME [epoch: 0.571 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5914542544386485		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.5914542544386485 | validation: 2.152750627776746]
	TIME [epoch: 0.571 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6010349236563963		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.6010349236563963 | validation: 2.088417437965805]
	TIME [epoch: 0.57 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5325802286136998		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.5325802286136998 | validation: 2.086045618637509]
	TIME [epoch: 0.571 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.535311085568578		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.535311085568578 | validation: 2.095767868373984]
	TIME [epoch: 0.571 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5381905547990204		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.5381905547990204 | validation: 2.0962338372550735]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5322663899187396		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.5322663899187396 | validation: 2.103553672548622]
	TIME [epoch: 1.13 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.545376553546404		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.545376553546404 | validation: 2.1869577809445793]
	TIME [epoch: 1.12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5822798361787853		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.5822798361787853 | validation: 2.098290961535423]
	TIME [epoch: 1.12 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.531024826585151		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.531024826585151 | validation: 2.136571589778608]
	TIME [epoch: 1.12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5697303305195407		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.5697303305195407 | validation: 2.169587832379299]
	TIME [epoch: 1.12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5736600432271743		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.5736600432271743 | validation: 2.141325964831176]
	TIME [epoch: 1.12 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.546605809211889		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.546605809211889 | validation: 2.087043884148207]
	TIME [epoch: 1.12 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.530425672414594		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.530425672414594 | validation: 2.1123973142972665]
	TIME [epoch: 1.12 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5379029758075995		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.5379029758075995 | validation: 2.123770823487405]
	TIME [epoch: 1.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5637784741317386		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.5637784741317386 | validation: 2.1305244633702176]
	TIME [epoch: 1.12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5810727099779558		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.5810727099779558 | validation: 2.0761736436471696]
	TIME [epoch: 1.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.521310770623121		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.521310770623121 | validation: 2.0726821808629254]
	TIME [epoch: 1.12 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.523707144109482		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.523707144109482 | validation: 2.0941413809640945]
	TIME [epoch: 1.12 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5278675565918207		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.5278675565918207 | validation: 2.078892103750712]
	TIME [epoch: 1.11 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5251776947494893		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.5251776947494893 | validation: 2.1075712299572666]
	TIME [epoch: 1.12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5498381534779262		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.5498381534779262 | validation: 2.2022037077893337]
	TIME [epoch: 1.12 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6069751363288327		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.6069751363288327 | validation: 2.1063174113959073]
	TIME [epoch: 1.12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5236811329672395		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.5236811329672395 | validation: 2.1485161535475465]
	TIME [epoch: 1.12 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5701289694215315		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.5701289694215315 | validation: 2.1564470054002243]
	TIME [epoch: 1.12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5744850415367573		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.5744850415367573 | validation: 2.143858523011682]
	TIME [epoch: 1.12 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5418885719593478		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.5418885719593478 | validation: 2.0958753445121405]
	TIME [epoch: 1.12 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.514494382216483		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.514494382216483 | validation: 2.0837162866404704]
	TIME [epoch: 1.12 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5287202353854816		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.5287202353854816 | validation: 2.09286133689513]
	TIME [epoch: 1.12 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5212062933317565		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.5212062933317565 | validation: 2.0767692974660283]
	TIME [epoch: 1.12 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5331526212377184		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.5331526212377184 | validation: 2.0977055443654558]
	TIME [epoch: 1.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5550484774314188		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.5550484774314188 | validation: 2.0767558797835]
	TIME [epoch: 1.12 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5270009747374804		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.5270009747374804 | validation: 2.079593319813356]
	TIME [epoch: 1.12 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5166846311978395		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.5166846311978395 | validation: 2.0759540661791562]
	TIME [epoch: 1.12 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5094879686469704		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.5094879686469704 | validation: 2.0664447646186748]
	TIME [epoch: 1.12 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5205825047047687		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.5205825047047687 | validation: 2.129328805348289]
	TIME [epoch: 1.12 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5434742297738853		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.5434742297738853 | validation: 2.0688345327703503]
	TIME [epoch: 1.12 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.518962522828931		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.518962522828931 | validation: 2.108523948124519]
	TIME [epoch: 1.11 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5335128388932975		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.5335128388932975 | validation: 2.086350047132408]
	TIME [epoch: 1.12 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5193707298001282		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.5193707298001282 | validation: 2.134550685158634]
	TIME [epoch: 1.12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5406472812293757		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.5406472812293757 | validation: 2.0744570655566537]
	TIME [epoch: 1.12 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5093669516409707		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.5093669516409707 | validation: 2.0768814878059003]
	TIME [epoch: 1.12 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.534074346343786		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.534074346343786 | validation: 2.1439322775698755]
	TIME [epoch: 1.12 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5794234192578305		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.5794234192578305 | validation: 2.077516262530402]
	TIME [epoch: 1.12 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5085418095695315		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.5085418095695315 | validation: 2.0966596914715327]
	TIME [epoch: 1.12 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5351710606921496		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.5351710606921496 | validation: 2.1676794125992536]
	TIME [epoch: 1.12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.567235090005255		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.567235090005255 | validation: 2.1300918554394586]
	TIME [epoch: 1.12 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5441894377526775		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.5441894377526775 | validation: 2.072252989178881]
	TIME [epoch: 1.12 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5060764984968666		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.5060764984968666 | validation: 2.066244876377359]
	TIME [epoch: 1.12 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5068178448269374		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.5068178448269374 | validation: 2.0836538881354394]
	TIME [epoch: 1.12 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5077056291286919		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.5077056291286919 | validation: 2.0656995208942024]
	TIME [epoch: 1.12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5098799767773938		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.5098799767773938 | validation: 2.0599063090497878]
	TIME [epoch: 1.12 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.504303608075922		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.504303608075922 | validation: 2.0737666915033373]
	TIME [epoch: 1.12 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5152595826607356		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.5152595826607356 | validation: 2.0688502106506492]
	TIME [epoch: 1.12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5194433486919756		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.5194433486919756 | validation: 2.1186026976031127]
	TIME [epoch: 1.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5455581483779262		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.5455581483779262 | validation: 2.2413767568222798]
	TIME [epoch: 1.12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5959761679222124		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.5959761679222124 | validation: 2.184939627523548]
	TIME [epoch: 1.12 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5553065668408765		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.5553065668408765 | validation: 2.065507608838371]
	TIME [epoch: 1.12 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.502288704542391		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.502288704542391 | validation: 2.0776616137892328]
	TIME [epoch: 1.12 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5210910391899517		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.5210910391899517 | validation: 2.141181099030994]
	TIME [epoch: 1.12 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5332296781565697		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.5332296781565697 | validation: 2.119640087794141]
	TIME [epoch: 1.12 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5234806950671769		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.5234806950671769 | validation: 2.0644626229747085]
	TIME [epoch: 1.12 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5116216456865164		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.5116216456865164 | validation: 2.062195070822703]
	TIME [epoch: 1.11 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.495632606564377		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.495632606564377 | validation: 2.075843505127118]
	TIME [epoch: 1.12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.493974228614033		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.493974228614033 | validation: 2.061097285287467]
	TIME [epoch: 1.12 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5017983492563662		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.5017983492563662 | validation: 2.0894843128038714]
	TIME [epoch: 1.12 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5197395983116595		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.5197395983116595 | validation: 2.064467370365359]
	TIME [epoch: 1.12 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.517967607949907		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.517967607949907 | validation: 2.0867700872604096]
	TIME [epoch: 1.12 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5261645794855292		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.5261645794855292 | validation: 2.0613994557497923]
	TIME [epoch: 1.11 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5038430220347017		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.5038430220347017 | validation: 2.161235673641469]
	TIME [epoch: 1.12 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.550880412836987		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.550880412836987 | validation: 2.095783489790393]
	TIME [epoch: 1.11 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5230029962580722		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.5230029962580722 | validation: 2.1017115612176167]
	TIME [epoch: 1.11 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5222040200185547		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.5222040200185547 | validation: 2.1573251220390186]
	TIME [epoch: 1.11 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5453253438696033		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.5453253438696033 | validation: 2.1139491384545908]
	TIME [epoch: 1.12 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.530097184341767		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.530097184341767 | validation: 2.0730094280212463]
	TIME [epoch: 1.11 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.503509900619148		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.503509900619148 | validation: 2.0671093913926977]
	TIME [epoch: 1.11 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4934707530875206		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.4934707530875206 | validation: 2.0639996170342063]
	TIME [epoch: 1.12 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.488988868508789		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.488988868508789 | validation: 2.0576717083370903]
	TIME [epoch: 1.12 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4833105576658678		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.4833105576658678 | validation: 2.0541478414263312]
	TIME [epoch: 1.12 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4840495735472308		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.4840495735472308 | validation: 2.05880300500997]
	TIME [epoch: 1.12 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.489054939586744		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.489054939586744 | validation: 2.063943370529063]
	TIME [epoch: 1.12 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4928515374707325		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.4928515374707325 | validation: 2.102179130762143]
	TIME [epoch: 1.12 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5254050262284875		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.5254050262284875 | validation: 2.219383865196833]
	TIME [epoch: 1.11 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5804996508678955		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.5804996508678955 | validation: 2.1700101800752285]
	TIME [epoch: 1.11 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5463698927729286		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.5463698927729286 | validation: 2.0681080895631445]
	TIME [epoch: 1.12 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4993086801777116		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.4993086801777116 | validation: 2.103881784480903]
	TIME [epoch: 1.12 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5276730528930855		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.5276730528930855 | validation: 2.107813548174604]
	TIME [epoch: 1.11 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5118778261621453		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.5118778261621453 | validation: 2.0871305293663003]
	TIME [epoch: 1.11 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4901802889403413		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.4901802889403413 | validation: 2.094614724382558]
	TIME [epoch: 1.12 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5179012991707004		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.5179012991707004 | validation: 2.1237361139056086]
	TIME [epoch: 1.12 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5203631712275882		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.5203631712275882 | validation: 2.1055736968731935]
	TIME [epoch: 1.12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5132669847086886		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.5132669847086886 | validation: 2.064869262262842]
	TIME [epoch: 1.11 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.497646711597728		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.497646711597728 | validation: 2.063518990660261]
	TIME [epoch: 1.11 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4954102065825365		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.4954102065825365 | validation: 2.068686388230812]
	TIME [epoch: 1.12 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4900547699588653		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.4900547699588653 | validation: 2.056335312013452]
	TIME [epoch: 1.12 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4843747871082804		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.4843747871082804 | validation: 2.0552763374958727]
	TIME [epoch: 1.12 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485497941609753		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.485497941609753 | validation: 2.0684099436456327]
	TIME [epoch: 1.12 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.49552917314213		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.49552917314213 | validation: 2.115295518580114]
	TIME [epoch: 1.12 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5288327926246275		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.5288327926246275 | validation: 2.058622653244597]
	TIME [epoch: 1.12 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4831542819762842		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.4831542819762842 | validation: 2.051964084352915]
	TIME [epoch: 1.13 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4800211909914416		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.4800211909914416 | validation: 2.055106828020908]
	TIME [epoch: 1.12 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4822671952629674		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.4822671952629674 | validation: 2.080835339846325]
	TIME [epoch: 1.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5025283988659117		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.5025283988659117 | validation: 2.158186277510937]
	TIME [epoch: 1.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5441098430482298		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.5441098430482298 | validation: 2.0902963010499627]
	TIME [epoch: 1.12 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4985570935216799		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.4985570935216799 | validation: 2.1101118074412093]
	TIME [epoch: 1.12 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.532185211672739		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.532185211672739 | validation: 2.130765088389702]
	TIME [epoch: 1.12 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5409703350801345		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.5409703350801345 | validation: 2.137965836472499]
	TIME [epoch: 1.12 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5206853372280653		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.5206853372280653 | validation: 2.068014910028694]
	TIME [epoch: 1.12 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4819627641718496		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.4819627641718496 | validation: 2.0788323565505293]
	TIME [epoch: 1.12 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5029740905775482		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.5029740905775482 | validation: 2.096113741914109]
	TIME [epoch: 1.12 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4928201116014517		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.4928201116014517 | validation: 2.072464293233627]
	TIME [epoch: 1.12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4870927885167948		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.4870927885167948 | validation: 2.0742238689772257]
	TIME [epoch: 1.12 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.492152603868679		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.492152603868679 | validation: 2.083200556462039]
	TIME [epoch: 1.12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4890685228999292		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.4890685228999292 | validation: 2.076573669178366]
	TIME [epoch: 1.12 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.492854832953444		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.492854832953444 | validation: 2.0772073084442093]
	TIME [epoch: 1.12 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.508794714015363		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.508794714015363 | validation: 2.0682757575175863]
	TIME [epoch: 1.12 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4805785739702662		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.4805785739702662 | validation: 2.0592764415445077]
	TIME [epoch: 1.12 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4770695154555893		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.4770695154555893 | validation: 2.0700387685131147]
	TIME [epoch: 1.12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.481089499973194		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.481089499973194 | validation: 2.068065487273965]
	TIME [epoch: 1.12 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4815387825884352		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.4815387825884352 | validation: 2.089744954014971]
	TIME [epoch: 1.12 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.497821307737753		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.497821307737753 | validation: 2.05778592178545]
	TIME [epoch: 1.12 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4805771331850566		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.4805771331850566 | validation: 2.0716174655239956]
	TIME [epoch: 1.12 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485354953126313		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.485354953126313 | validation: 2.0666075494331566]
	TIME [epoch: 1.12 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4802147812949755		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.4802147812949755 | validation: 2.0945126098906877]
	TIME [epoch: 1.12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4959419955882383		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.4959419955882383 | validation: 2.0666606416691837]
	TIME [epoch: 1.11 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4777869137799138		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.4777869137799138 | validation: 2.0503536487266607]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4713297012940691		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.4713297012940691 | validation: 2.053415146507484]
	TIME [epoch: 1.12 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.464486976784437		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.464486976784437 | validation: 2.0727916385084995]
	TIME [epoch: 1.12 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4752346845517128		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.4752346845517128 | validation: 2.0813807689922736]
	TIME [epoch: 1.12 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4866077822931303		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.4866077822931303 | validation: 2.1548195390445013]
	TIME [epoch: 1.12 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5280080590494942		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.5280080590494942 | validation: 2.102813691367327]
	TIME [epoch: 1.12 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.502779438809797		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.502779438809797 | validation: 2.1105743214614163]
	TIME [epoch: 1.12 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5379265732856635		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.5379265732856635 | validation: 2.110064001318161]
	TIME [epoch: 1.12 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5110609562510755		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.5110609562510755 | validation: 2.099024351177705]
	TIME [epoch: 1.12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5017640501446852		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.5017640501446852 | validation: 2.065839277459328]
	TIME [epoch: 1.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4789729880834717		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.4789729880834717 | validation: 2.0625299796971266]
	TIME [epoch: 1.11 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475805975025488		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.475805975025488 | validation: 2.066744822577428]
	TIME [epoch: 1.12 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4747474388506612		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.4747474388506612 | validation: 2.058271177835518]
	TIME [epoch: 1.12 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4675461293941492		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.4675461293941492 | validation: 2.055622810787704]
	TIME [epoch: 1.12 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.469825836038159		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.469825836038159 | validation: 2.065080679820063]
	TIME [epoch: 1.11 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4713095679272636		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.4713095679272636 | validation: 2.059604746682268]
	TIME [epoch: 1.12 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4789003696024126		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.4789003696024126 | validation: 2.0738265045453237]
	TIME [epoch: 1.12 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4902364179507777		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.4902364179507777 | validation: 2.073185725828977]
	TIME [epoch: 1.11 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.495821814427938		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.495821814427938 | validation: 2.088067265141626]
	TIME [epoch: 1.12 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4867415477310646		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.4867415477310646 | validation: 2.0773973597937583]
	TIME [epoch: 1.11 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4864256885004574		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.4864256885004574 | validation: 2.1007283348419485]
	TIME [epoch: 1.12 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5065765533700375		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.5065765533700375 | validation: 2.05640604445937]
	TIME [epoch: 1.12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4741651247783971		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.4741651247783971 | validation: 2.053011920641223]
	TIME [epoch: 1.11 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4688466795999957		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.4688466795999957 | validation: 2.0618534439599183]
	TIME [epoch: 1.12 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4718180782743873		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.4718180782743873 | validation: 2.0531755987151667]
	TIME [epoch: 1.12 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4730876625234581		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.4730876625234581 | validation: 2.0868763149259464]
	TIME [epoch: 1.12 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4880789493388256		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.4880789493388256 | validation: 2.063092464681335]
	TIME [epoch: 1.11 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4771956049868016		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.4771956049868016 | validation: 2.059343669948252]
	TIME [epoch: 1.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4686687585546705		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.4686687585546705 | validation: 2.0491606483229363]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4745013567424545		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.4745013567424545 | validation: 2.092677724248157]
	TIME [epoch: 1.12 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4833884767305225		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.4833884767305225 | validation: 2.0546143835634716]
	TIME [epoch: 1.12 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4665722321674475		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.4665722321674475 | validation: 2.047030809410568]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4684264516266279		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.4684264516266279 | validation: 2.0481192691974415]
	TIME [epoch: 1.12 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4694161624214994		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.4694161624214994 | validation: 2.0560458372328196]
	TIME [epoch: 1.12 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4753357333419541		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.4753357333419541 | validation: 2.0646865193320454]
	TIME [epoch: 1.12 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.486352363775543		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.486352363775543 | validation: 2.0600520212520093]
	TIME [epoch: 1.12 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4894462445206114		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.4894462445206114 | validation: 2.0834399867356748]
	TIME [epoch: 1.11 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4756942238972597		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.4756942238972597 | validation: 2.06403389301503]
	TIME [epoch: 1.12 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4745404823831754		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.4745404823831754 | validation: 2.0910711486569236]
	TIME [epoch: 1.12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.496347333676136		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.496347333676136 | validation: 2.055775501208435]
	TIME [epoch: 1.12 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.471951533316111		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.471951533316111 | validation: 2.0457008807508856]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4674006147404384		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.4674006147404384 | validation: 2.0488419933160102]
	TIME [epoch: 1.12 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.461235406129949		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.461235406129949 | validation: 2.0409716739244756]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4592299573700234		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.4592299573700234 | validation: 2.0657153775517068]
	TIME [epoch: 1.12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4604063831623781		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.4604063831623781 | validation: 2.087276477565646]
	TIME [epoch: 1.12 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4804653326895083		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.4804653326895083 | validation: 2.1326129802821723]
	TIME [epoch: 1.12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5095002119305456		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.5095002119305456 | validation: 2.087011715824112]
	TIME [epoch: 1.12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4805709481811398		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.4805709481811398 | validation: 2.084637374613427]
	TIME [epoch: 1.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4935945165482947		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.4935945165482947 | validation: 2.107570681709489]
	TIME [epoch: 1.12 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4904923612432763		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.4904923612432763 | validation: 2.0885125767640864]
	TIME [epoch: 1.12 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4869585077085794		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.4869585077085794 | validation: 2.061650551447425]
	TIME [epoch: 1.12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.470756097098561		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.470756097098561 | validation: 2.0601173878922068]
	TIME [epoch: 1.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4614531029064364		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.4614531029064364 | validation: 2.0586775001446775]
	TIME [epoch: 1.12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.463430790650685		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.463430790650685 | validation: 2.0478403753478776]
	TIME [epoch: 1.12 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.461889144506888		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.461889144506888 | validation: 2.048270468482138]
	TIME [epoch: 1.12 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4590419542429858		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.4590419542429858 | validation: 2.048011550649384]
	TIME [epoch: 1.12 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4610373699170132		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.4610373699170132 | validation: 2.047820735124016]
	TIME [epoch: 1.12 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4668030153484262		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.4668030153484262 | validation: 2.055960604593037]
	TIME [epoch: 1.12 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4776503870556235		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.4776503870556235 | validation: 2.0633881147007465]
	TIME [epoch: 1.12 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4686196046009037		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.4686196046009037 | validation: 2.0406100900070596]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4626424594730814		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.4626424594730814 | validation: 2.0454743418652357]
	TIME [epoch: 1.12 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4620645550275955		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.4620645550275955 | validation: 2.0750320757059617]
	TIME [epoch: 1.12 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.476007239284266		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.476007239284266 | validation: 2.0769456563071493]
	TIME [epoch: 1.12 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4838609297125922		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.4838609297125922 | validation: 2.1285387027478584]
	TIME [epoch: 1.12 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5034636886593837		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.5034636886593837 | validation: 2.075276230099472]
	TIME [epoch: 1.12 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4666060892616628		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.4666060892616628 | validation: 2.084840591304419]
	TIME [epoch: 1.12 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485611812342047		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.485611812342047 | validation: 2.1055657045982095]
	TIME [epoch: 1.12 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4954729430935785		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.4954729430935785 | validation: 2.086234852827266]
	TIME [epoch: 1.12 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.478924616028076		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.478924616028076 | validation: 2.0613891545239094]
	TIME [epoch: 1.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4611272609172468		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.4611272609172468 | validation: 2.062153084051516]
	TIME [epoch: 1.12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4621240245656695		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.4621240245656695 | validation: 2.050430472250011]
	TIME [epoch: 1.12 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4605638608574378		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.4605638608574378 | validation: 2.0416801967023313]
	TIME [epoch: 1.12 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4547002479165954		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.4547002479165954 | validation: 2.0564247706158114]
	TIME [epoch: 1.12 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4541378219192695		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.4541378219192695 | validation: 2.0603682535577414]
	TIME [epoch: 1.12 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4631966138854622		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.4631966138854622 | validation: 2.0651561073369322]
	TIME [epoch: 1.12 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4805776287906138		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.4805776287906138 | validation: 2.079776159082823]
	TIME [epoch: 1.11 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4839877866547073		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.4839877866547073 | validation: 2.053508920197458]
	TIME [epoch: 1.12 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4550863052616683		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.4550863052616683 | validation: 2.0463863804409486]
	TIME [epoch: 1.12 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4539879689498116		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.4539879689498116 | validation: 2.055487681551487]
	TIME [epoch: 1.12 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4525350628199645		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.4525350628199645 | validation: 2.0591712525312693]
	TIME [epoch: 1.12 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.463355371929756		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.463355371929756 | validation: 2.0997436205889084]
	TIME [epoch: 1.12 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4887831298941587		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.4887831298941587 | validation: 2.0634881885188756]
	TIME [epoch: 1.12 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.479717240477289		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.479717240477289 | validation: 2.0504218269443117]
	TIME [epoch: 1.12 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4547536159099246		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.4547536159099246 | validation: 2.055006723402142]
	TIME [epoch: 1.12 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4540450886976874		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.4540450886976874 | validation: 2.0665175557888227]
	TIME [epoch: 1.12 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.462683584263487		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.462683584263487 | validation: 2.0729328640047684]
	TIME [epoch: 1.12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4763533914765157		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.4763533914765157 | validation: 2.047378673544262]
	TIME [epoch: 1.12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4578171169226235		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.4578171169226235 | validation: 2.0528638859702664]
	TIME [epoch: 1.12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4510336636145982		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.4510336636145982 | validation: 2.052775592456284]
	TIME [epoch: 1.12 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4508397759579987		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.4508397759579987 | validation: 2.061946274534178]
	TIME [epoch: 1.12 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4651492160701824		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.4651492160701824 | validation: 2.052393999230732]
	TIME [epoch: 1.12 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4703587531210394		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.4703587531210394 | validation: 2.05658574601745]
	TIME [epoch: 1.12 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4611610946531357		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.4611610946531357 | validation: 2.042823257472273]
	TIME [epoch: 1.12 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4565869580296544		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.4565869580296544 | validation: 2.046131478636017]
	TIME [epoch: 1.12 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4513288476774764		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.4513288476774764 | validation: 2.041326176482205]
	TIME [epoch: 1.12 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4514593594421872		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.4514593594421872 | validation: 2.057357469271014]
	TIME [epoch: 1.12 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4567992584492038		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.4567992584492038 | validation: 2.1127923147592265]
	TIME [epoch: 1.12 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.49016486895177		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.49016486895177 | validation: 2.0583218536963095]
	TIME [epoch: 1.12 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4638237461629047		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.4638237461629047 | validation: 2.047159792351834]
	TIME [epoch: 1.12 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4540904260122358		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.4540904260122358 | validation: 2.053589184333515]
	TIME [epoch: 1.12 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.452741695848187		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.452741695848187 | validation: 2.0485237634005142]
	TIME [epoch: 1.12 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4671477984754213		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.4671477984754213 | validation: 2.0707201921806977]
	TIME [epoch: 1.12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4778198918991103		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.4778198918991103 | validation: 2.0510034070547802]
	TIME [epoch: 1.12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4559712429935658		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.4559712429935658 | validation: 2.0464696648946012]
	TIME [epoch: 1.12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.455481499466917		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.455481499466917 | validation: 2.0481598268853443]
	TIME [epoch: 1.12 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4517704713062427		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.4517704713062427 | validation: 2.0540803621186434]
	TIME [epoch: 1.12 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4563709618368603		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.4563709618368603 | validation: 2.041834899543918]
	TIME [epoch: 1.12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4650477127640844		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.4650477127640844 | validation: 2.0522183415482846]
	TIME [epoch: 1.12 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4606095408741127		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.4606095408741127 | validation: 2.046167682208593]
	TIME [epoch: 1.12 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4559488845630153		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.4559488845630153 | validation: 2.0614461011603282]
	TIME [epoch: 1.12 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4593808703713231		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.4593808703713231 | validation: 2.0632911876570414]
	TIME [epoch: 1.12 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4666816422503774		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.4666816422503774 | validation: 2.086320014283912]
	TIME [epoch: 1.12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4794696108070777		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.4794696108070777 | validation: 2.0481980692682047]
	TIME [epoch: 1.12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4512280933816155		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.4512280933816155 | validation: 2.048380975037444]
	TIME [epoch: 1.12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4506151040940605		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.4506151040940605 | validation: 2.0778140851887428]
	TIME [epoch: 1.12 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4642672986487526		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.4642672986487526 | validation: 2.041468778209694]
	TIME [epoch: 1.12 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4496284159735746		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.4496284159735746 | validation: 2.041565556571743]
	TIME [epoch: 1.12 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4546718177016706		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.4546718177016706 | validation: 2.044745318954211]
	TIME [epoch: 1.12 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.448026423180366		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.448026423180366 | validation: 2.041221682305033]
	TIME [epoch: 1.12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4483247427746915		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.4483247427746915 | validation: 2.0503937739983886]
	TIME [epoch: 1.12 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4532341461789937		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.4532341461789937 | validation: 2.076396505231054]
	TIME [epoch: 1.12 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4672480912257382		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.4672480912257382 | validation: 2.05198154203746]
	TIME [epoch: 1.12 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4554837461665497		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.4554837461665497 | validation: 2.0363747784761133]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4493819573593105		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.4493819573593105 | validation: 2.0391348170741384]
	TIME [epoch: 1.12 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4511476820879157		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.4511476820879157 | validation: 2.0371230592509675]
	TIME [epoch: 1.12 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.451391548294551		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.451391548294551 | validation: 2.053467392716847]
	TIME [epoch: 1.12 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4637055424162708		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.4637055424162708 | validation: 2.066104729942873]
	TIME [epoch: 1.12 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475737653194759		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.475737653194759 | validation: 2.0918888511486937]
	TIME [epoch: 1.12 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.480695815432481		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.480695815432481 | validation: 2.060056355405125]
	TIME [epoch: 1.12 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4547555377215438		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.4547555377215438 | validation: 2.0913902903013457]
	TIME [epoch: 1.12 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4776941393647287		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.4776941393647287 | validation: 2.091441168366625]
	TIME [epoch: 1.12 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4759775256341783		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.4759775256341783 | validation: 2.08368730252212]
	TIME [epoch: 1.12 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4616192360597347		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.4616192360597347 | validation: 2.057393617238861]
	TIME [epoch: 1.12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4516143617956851		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.4516143617956851 | validation: 2.039762115884606]
	TIME [epoch: 1.12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4471665982597113		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.4471665982597113 | validation: 2.0398083945759224]
	TIME [epoch: 1.12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4434123005921828		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.4434123005921828 | validation: 2.0461843284259085]
	TIME [epoch: 1.12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4433571109056942		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.4433571109056942 | validation: 2.0386484343204527]
	TIME [epoch: 1.12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4436697179280895		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.4436697179280895 | validation: 2.0407567463851346]
	TIME [epoch: 1.12 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4452183113734696		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.4452183113734696 | validation: 2.0376090260864235]
	TIME [epoch: 1.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4422234831160348		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.4422234831160348 | validation: 2.0446432817595914]
	TIME [epoch: 1.12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4413161825042207		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.4413161825042207 | validation: 2.038244769974183]
	TIME [epoch: 1.12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4448904569018908		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.4448904569018908 | validation: 2.04832097861591]
	TIME [epoch: 1.12 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4480850987267309		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.4480850987267309 | validation: 2.044870260917376]
	TIME [epoch: 1.12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4522043970551448		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.4522043970551448 | validation: 2.040790003728984]
	TIME [epoch: 1.12 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4514615174777072		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.4514615174777072 | validation: 2.044341887101799]
	TIME [epoch: 1.12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4453038093708506		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.4453038093708506 | validation: 2.0379280853454627]
	TIME [epoch: 1.12 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4521189195494901		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.4521189195494901 | validation: 2.054646721719237]
	TIME [epoch: 1.12 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4638156358318883		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.4638156358318883 | validation: 2.077256288424335]
	TIME [epoch: 1.12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.465332462256423		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.465332462256423 | validation: 2.048368597056866]
	TIME [epoch: 1.12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4480995489986868		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.4480995489986868 | validation: 2.0668734887983207]
	TIME [epoch: 1.12 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4517098923464606		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.4517098923464606 | validation: 2.0381507266231065]
	TIME [epoch: 1.12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4394207306117546		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.4394207306117546 | validation: 2.033792519722501]
	TIME [epoch: 1.12 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.443642057514082		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.443642057514082 | validation: 2.0388920891217084]
	TIME [epoch: 1.12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4414201084380347		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.4414201084380347 | validation: 2.0370034815475573]
	TIME [epoch: 1.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.441412026834694		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.441412026834694 | validation: 2.049145169050511]
	TIME [epoch: 1.12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4499754841392252		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.4499754841392252 | validation: 2.050911425558373]
	TIME [epoch: 1.12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4506614441595447		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.4506614441595447 | validation: 2.061457503481611]
	TIME [epoch: 1.11 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4617749750710953		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.4617749750710953 | validation: 2.0404750117240287]
	TIME [epoch: 1.12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.447600473724832		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.447600473724832 | validation: 2.0459603686249452]
	TIME [epoch: 1.12 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4411754031621415		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.4411754031621415 | validation: 2.0363047773355234]
	TIME [epoch: 1.12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.440525198473718		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.440525198473718 | validation: 2.049967739407294]
	TIME [epoch: 1.12 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4466910469823306		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.4466910469823306 | validation: 2.046046915757948]
	TIME [epoch: 1.12 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4517243649878295		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.4517243649878295 | validation: 2.0566338371415553]
	TIME [epoch: 1.12 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4481329143251427		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.4481329143251427 | validation: 2.045821033711298]
	TIME [epoch: 1.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4368998875883434		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.4368998875883434 | validation: 2.0422425027314053]
	TIME [epoch: 1.12 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.436281662120312		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.436281662120312 | validation: 2.041795949400322]
	TIME [epoch: 1.12 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4399326482306591		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.4399326482306591 | validation: 2.047390071432172]
	TIME [epoch: 1.12 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4516577451128894		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.4516577451128894 | validation: 2.0539828327677503]
	TIME [epoch: 1.12 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4582009092863855		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.4582009092863855 | validation: 2.080157267175256]
	TIME [epoch: 1.12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.468163460631207		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.468163460631207 | validation: 2.0485090726597006]
	TIME [epoch: 1.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.44443057615641		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.44443057615641 | validation: 2.0489313068403323]
	TIME [epoch: 1.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4462052342169096		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.4462052342169096 | validation: 2.056525723584956]
	TIME [epoch: 1.12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4478651833425544		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.4478651833425544 | validation: 2.0307237764177697]
	TIME [epoch: 1.11 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4411707772240814		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.4411707772240814 | validation: 2.038605280502163]
	TIME [epoch: 1.12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4392142508895578		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.4392142508895578 | validation: 2.0396496052160886]
	TIME [epoch: 1.12 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.434912086155729		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.434912086155729 | validation: 2.0374775029178958]
	TIME [epoch: 1.12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4352176307129652		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.4352176307129652 | validation: 2.0406611064051408]
	TIME [epoch: 1.12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4333479185185416		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.4333479185185416 | validation: 2.052349789086557]
	TIME [epoch: 1.12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4428504888297218		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.4428504888297218 | validation: 2.0538141572044943]
	TIME [epoch: 1.12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4455046108727623		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.4455046108727623 | validation: 2.073766767438723]
	TIME [epoch: 1.12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4645427341379773		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.4645427341379773 | validation: 2.043388363615063]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4500999735152533		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.4500999735152533 | validation: 2.043380353846499]
	TIME [epoch: 2.23 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4417345595375792		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.4417345595375792 | validation: 2.0370021925363533]
	TIME [epoch: 2.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4383313946722416		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.4383313946722416 | validation: 2.0481310661503422]
	TIME [epoch: 2.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4376538028219505		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.4376538028219505 | validation: 2.042510343273905]
	TIME [epoch: 2.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4410831258478		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.4410831258478 | validation: 2.0463749005967826]
	TIME [epoch: 2.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446330064921567		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.446330064921567 | validation: 2.0465604962424693]
	TIME [epoch: 2.22 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4377736958180214		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.4377736958180214 | validation: 2.0455603034148395]
	TIME [epoch: 2.21 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.439967750184594		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.439967750184594 | validation: 2.0434779537028884]
	TIME [epoch: 2.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4429291408213407		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.4429291408213407 | validation: 2.0601912965862677]
	TIME [epoch: 2.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4560600762564337		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.4560600762564337 | validation: 2.0442338099206956]
	TIME [epoch: 2.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4460516360624671		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.4460516360624671 | validation: 2.0360516354878686]
	TIME [epoch: 2.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4387046155303762		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.4387046155303762 | validation: 2.0333174347577234]
	TIME [epoch: 2.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4326031089140776		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.4326031089140776 | validation: 2.035460164954992]
	TIME [epoch: 2.22 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4369980607969262		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.4369980607969262 | validation: 2.0355668006842875]
	TIME [epoch: 2.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4366892849343338		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.4366892849343338 | validation: 2.0586682553702658]
	TIME [epoch: 2.22 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4435642352416693		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.4435642352416693 | validation: 2.040259236484576]
	TIME [epoch: 2.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4452631254489716		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.4452631254489716 | validation: 2.051799351161737]
	TIME [epoch: 2.22 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4455520102928574		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.4455520102928574 | validation: 2.0491018932535146]
	TIME [epoch: 2.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4422104587419204		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.4422104587419204 | validation: 2.0461787528393427]
	TIME [epoch: 2.22 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4425630380961831		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.4425630380961831 | validation: 2.043807083268525]
	TIME [epoch: 2.21 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4382202454117214		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.4382202454117214 | validation: 2.056214332790329]
	TIME [epoch: 2.22 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4432282249210031		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.4432282249210031 | validation: 2.0354592760775767]
	TIME [epoch: 2.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4275105630506215		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.4275105630506215 | validation: 2.033336999800118]
	TIME [epoch: 2.22 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.431959609748434		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.431959609748434 | validation: 2.039839287666639]
	TIME [epoch: 2.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4329568892419784		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.4329568892419784 | validation: 2.0340210032403774]
	TIME [epoch: 2.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4293230529645806		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.4293230529645806 | validation: 2.033975784133864]
	TIME [epoch: 2.22 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4344233169034128		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.4344233169034128 | validation: 2.0452212652686064]
	TIME [epoch: 2.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4448677240128132		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.4448677240128132 | validation: 2.072704032424015]
	TIME [epoch: 2.22 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4627132852947946		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.4627132852947946 | validation: 2.0401909367454967]
	TIME [epoch: 2.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4386488400855144		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.4386488400855144 | validation: 2.024519343305085]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.429824172174524		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.429824172174524 | validation: 2.033058600384302]
	TIME [epoch: 2.21 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4298490087899383		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.4298490087899383 | validation: 2.0288641175970654]
	TIME [epoch: 2.22 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4305639870728422		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.4305639870728422 | validation: 2.0358717429035518]
	TIME [epoch: 2.21 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4312186964234932		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.4312186964234932 | validation: 2.038377416984756]
	TIME [epoch: 2.22 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4360742245616809		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.4360742245616809 | validation: 2.049972462050385]
	TIME [epoch: 2.21 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4386375787813603		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.4386375787813603 | validation: 2.043718746101829]
	TIME [epoch: 2.22 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4442093316001001		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.4442093316001001 | validation: 2.0602840227251877]
	TIME [epoch: 2.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4505904534359786		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.4505904534359786 | validation: 2.0428313688581854]
	TIME [epoch: 2.22 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4392739390744231		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.4392739390744231 | validation: 2.0337274633552846]
	TIME [epoch: 2.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4375526014670827		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.4375526014670827 | validation: 2.0492200377409135]
	TIME [epoch: 2.21 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.434250405525222		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.434250405525222 | validation: 2.0348147607086293]
	TIME [epoch: 2.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4342900382417743		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.4342900382417743 | validation: 2.0385528248849214]
	TIME [epoch: 2.2 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4313546440446783		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.4313546440446783 | validation: 2.0341279223907747]
	TIME [epoch: 2.22 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4312805503235595		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.4312805503235595 | validation: 2.043790803842304]
	TIME [epoch: 2.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4410956072150771		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.4410956072150771 | validation: 2.0336899720580526]
	TIME [epoch: 2.22 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4281645055564842		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.4281645055564842 | validation: 2.0356984913446348]
	TIME [epoch: 2.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4294123953408713		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.4294123953408713 | validation: 2.0360118208866247]
	TIME [epoch: 2.22 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.429922324117449		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.429922324117449 | validation: 2.030785380528562]
	TIME [epoch: 2.21 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4314890029037306		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.4314890029037306 | validation: 2.034848567662841]
	TIME [epoch: 2.21 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4291129200714843		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.4291129200714843 | validation: 2.0390352206403537]
	TIME [epoch: 2.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4332549777038954		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.4332549777038954 | validation: 2.0359911796666617]
	TIME [epoch: 2.21 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4308949496271788		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.4308949496271788 | validation: 2.039478037557493]
	TIME [epoch: 2.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4306785941396454		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.4306785941396454 | validation: 2.043001929990427]
	TIME [epoch: 2.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4323246416023139		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.4323246416023139 | validation: 2.0297226280469047]
	TIME [epoch: 2.22 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4334613160428678		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.4334613160428678 | validation: 2.042707175121525]
	TIME [epoch: 2.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4305182334005508		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.4305182334005508 | validation: 2.056213257641001]
	TIME [epoch: 2.22 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4536029490510214		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.4536029490510214 | validation: 2.091316336360318]
	TIME [epoch: 2.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4756974882696394		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.4756974882696394 | validation: 2.052473598971492]
	TIME [epoch: 2.22 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4388011579202546		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.4388011579202546 | validation: 2.0646888112387187]
	TIME [epoch: 2.21 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4446737634068043		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.4446737634068043 | validation: 2.0497057608359888]
	TIME [epoch: 2.22 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4461057653748204		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.4461057653748204 | validation: 2.0424705747422744]
	TIME [epoch: 2.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4341293077139958		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 1.4341293077139958 | validation: 2.0372517186852144]
	TIME [epoch: 2.21 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.42970852079257		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 1.42970852079257 | validation: 2.0306253064417055]
	TIME [epoch: 2.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4321553141515755		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 1.4321553141515755 | validation: 2.036164562385087]
	TIME [epoch: 2.21 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4232475449058712		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.4232475449058712 | validation: 2.0357022399946816]
	TIME [epoch: 2.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4251337843274008		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.4251337843274008 | validation: 2.0312848134309642]
	TIME [epoch: 2.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4290920572110464		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.4290920572110464 | validation: 2.0355644060464098]
	TIME [epoch: 2.21 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.430665628742936		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.430665628742936 | validation: 2.047084366685794]
	TIME [epoch: 2.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4319513681345433		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 1.4319513681345433 | validation: 2.044939725182739]
	TIME [epoch: 2.22 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4310695644663225		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 1.4310695644663225 | validation: 2.049893933578054]
	TIME [epoch: 2.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4288589306333233		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.4288589306333233 | validation: 2.026989335486795]
	TIME [epoch: 2.22 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4273807441647717		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.4273807441647717 | validation: 2.029166723868579]
	TIME [epoch: 2.21 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4224908196760453		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.4224908196760453 | validation: 2.034869382426098]
	TIME [epoch: 2.22 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4265599157301052		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 1.4265599157301052 | validation: 2.0277370663029477]
	TIME [epoch: 2.21 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4287214339036307		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 1.4287214339036307 | validation: 2.0362806330795467]
	TIME [epoch: 2.22 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.424299838455774		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 1.424299838455774 | validation: 2.0306213670123174]
	TIME [epoch: 2.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.42229220248634		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 1.42229220248634 | validation: 2.0468792541985166]
	TIME [epoch: 2.22 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4291643007286592		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 1.4291643007286592 | validation: 2.035733744641754]
	TIME [epoch: 2.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4289200968437		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 1.4289200968437 | validation: 2.0531716392961252]
	TIME [epoch: 2.22 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4334521896374741		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 1.4334521896374741 | validation: 2.036621132178831]
	TIME [epoch: 2.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4361094811083703		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 1.4361094811083703 | validation: 2.0544200601251807]
	TIME [epoch: 2.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4380365264081842		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 1.4380365264081842 | validation: 2.033099192922197]
	TIME [epoch: 2.22 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4295764624202718		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 1.4295764624202718 | validation: 2.0426683286155822]
	TIME [epoch: 2.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4328655230118261		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 1.4328655230118261 | validation: 2.0198890285676896]
	TIME [epoch: 2.23 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4207836121670392		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 1.4207836121670392 | validation: 2.025669400477136]
	TIME [epoch: 2.21 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4211781898960636		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 1.4211781898960636 | validation: 2.0327169993409018]
	TIME [epoch: 2.22 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.424112393038263		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 1.424112393038263 | validation: 2.026533317338934]
	TIME [epoch: 2.21 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4263857084877498		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 1.4263857084877498 | validation: 2.042011995977539]
	TIME [epoch: 2.22 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4309434674562624		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 1.4309434674562624 | validation: 2.025801259377203]
	TIME [epoch: 2.21 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4312867966085707		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 1.4312867966085707 | validation: 2.039278495144952]
	TIME [epoch: 2.23 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.429894729316934		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.429894729316934 | validation: 2.035700792849107]
	TIME [epoch: 2.21 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4297235831916906		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 1.4297235831916906 | validation: 2.0433523524885726]
	TIME [epoch: 2.22 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4337778098948792		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 1.4337778098948792 | validation: 2.027765385070912]
	TIME [epoch: 2.21 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4265352078412727		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 1.4265352078412727 | validation: 2.0393781845289927]
	TIME [epoch: 2.22 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.42469731387181		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 1.42469731387181 | validation: 2.0352358018201597]
	TIME [epoch: 2.21 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4198668849385803		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 1.4198668849385803 | validation: 2.030759551999313]
	TIME [epoch: 2.22 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4198648950563777		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 1.4198648950563777 | validation: 2.0309366857748734]
	TIME [epoch: 2.22 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4179471852375145		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 1.4179471852375145 | validation: 2.026607351968925]
	TIME [epoch: 2.21 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4274856095946626		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.4274856095946626 | validation: 2.0324655568919274]
	TIME [epoch: 2.21 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.428979160234729		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 1.428979160234729 | validation: 2.0360622795245376]
	TIME [epoch: 2.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4328274981117803		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 1.4328274981117803 | validation: 2.0327093102527725]
	TIME [epoch: 2.22 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4322009073746325		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 1.4322009073746325 | validation: 2.055340294091595]
	TIME [epoch: 2.22 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4345934990162619		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 1.4345934990162619 | validation: 2.033446475652517]
	TIME [epoch: 2.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.435031373974353		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 1.435031373974353 | validation: 2.039919126215491]
	TIME [epoch: 2.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.426257260129562		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 1.426257260129562 | validation: 2.029629013714488]
	TIME [epoch: 2.22 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4176037370388768		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 1.4176037370388768 | validation: 2.0290915789333694]
	TIME [epoch: 2.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4220279364126498		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 1.4220279364126498 | validation: 2.0332063840664993]
	TIME [epoch: 2.22 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4247593784742307		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 1.4247593784742307 | validation: 2.0289592976069666]
	TIME [epoch: 2.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4184397634276154		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 1.4184397634276154 | validation: 2.025748836507123]
	TIME [epoch: 2.22 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4209727699193173		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 1.4209727699193173 | validation: 2.0222628352439123]
	TIME [epoch: 2.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4187499389425529		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 1.4187499389425529 | validation: 2.0293833183184917]
	TIME [epoch: 2.22 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4195633472756999		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 1.4195633472756999 | validation: 2.026157724526834]
	TIME [epoch: 2.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.425962585968126		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 1.425962585968126 | validation: 2.05100694505311]
	TIME [epoch: 2.21 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.442829148541203		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 1.442829148541203 | validation: 2.031421038658683]
	TIME [epoch: 2.21 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4244366448522896		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 1.4244366448522896 | validation: 2.0256823003034734]
	TIME [epoch: 2.21 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4239595486632441		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 1.4239595486632441 | validation: 2.029893117021155]
	TIME [epoch: 2.21 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4216159376852944		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 1.4216159376852944 | validation: 2.0272880350128353]
	TIME [epoch: 2.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4176556038751114		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 1.4176556038751114 | validation: 2.0263569811556503]
	TIME [epoch: 2.22 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4176640649708252		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 1.4176640649708252 | validation: 2.024849861539512]
	TIME [epoch: 2.21 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4183566674403971		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 1.4183566674403971 | validation: 2.0292102598663213]
	TIME [epoch: 2.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4206465973017937		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 1.4206465973017937 | validation: 2.0257309889611577]
	TIME [epoch: 2.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.419282427369796		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 1.419282427369796 | validation: 2.0430864901340438]
	TIME [epoch: 2.22 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4207829518519839		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 1.4207829518519839 | validation: 2.047956252790383]
	TIME [epoch: 2.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4360364316593799		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 1.4360364316593799 | validation: 2.073595553748229]
	TIME [epoch: 2.22 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4442385186719384		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 1.4442385186719384 | validation: 2.0359489855795694]
	TIME [epoch: 2.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.424584889645869		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 1.424584889645869 | validation: 2.0374545253430094]
	TIME [epoch: 2.21 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4286312123005576		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 1.4286312123005576 | validation: 2.0371473548571086]
	TIME [epoch: 2.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4245218993972373		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 1.4245218993972373 | validation: 2.034357636217693]
	TIME [epoch: 2.22 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4210143902015069		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 1.4210143902015069 | validation: 2.0213675802695654]
	TIME [epoch: 2.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4182944846485492		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 1.4182944846485492 | validation: 2.023050273517405]
	TIME [epoch: 2.21 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4158001541535146		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 1.4158001541535146 | validation: 2.0244163264126507]
	TIME [epoch: 2.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4187761581147071		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 1.4187761581147071 | validation: 2.02342673038292]
	TIME [epoch: 2.21 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4167505524773745		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 1.4167505524773745 | validation: 2.0208145409043987]
	TIME [epoch: 2.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4150975633887821		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 1.4150975633887821 | validation: 2.0219050960841023]
	TIME [epoch: 2.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4156300743559893		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 1.4156300743559893 | validation: 2.0252086652048384]
	TIME [epoch: 2.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4194954318489192		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 1.4194954318489192 | validation: 2.0342305138965835]
	TIME [epoch: 2.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130970351112546		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 1.4130970351112546 | validation: 2.020883767217039]
	TIME [epoch: 2.22 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.414814147094007		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 1.414814147094007 | validation: 2.043593805990014]
	TIME [epoch: 2.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4249216079704288		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 1.4249216079704288 | validation: 2.0314631189980425]
	TIME [epoch: 2.22 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4270111889570518		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 1.4270111889570518 | validation: 2.0287168654092045]
	TIME [epoch: 2.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.418868110337179		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 1.418868110337179 | validation: 2.026930622103208]
	TIME [epoch: 2.22 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415710804692538		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 1.415710804692538 | validation: 2.024682409047965]
	TIME [epoch: 2.21 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4192508771187162		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 1.4192508771187162 | validation: 2.0269568719769184]
	TIME [epoch: 2.22 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4171982136828292		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 1.4171982136828292 | validation: 2.027703189701354]
	TIME [epoch: 2.21 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415479034741668		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 1.415479034741668 | validation: 2.0220080321350804]
	TIME [epoch: 2.22 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.418348426623238		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 1.418348426623238 | validation: 2.0270130051381905]
	TIME [epoch: 2.21 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4216405147291442		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 1.4216405147291442 | validation: 2.0336681481018988]
	TIME [epoch: 2.23 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4267844677269184		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 1.4267844677269184 | validation: 2.050751143635043]
	TIME [epoch: 2.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4299398663541047		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 1.4299398663541047 | validation: 2.0227539103438383]
	TIME [epoch: 2.22 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415330208339268		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 1.415330208339268 | validation: 2.0229092470922767]
	TIME [epoch: 2.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4183593215921797		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 1.4183593215921797 | validation: 2.0376227086278305]
	TIME [epoch: 2.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4240888705701253		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 1.4240888705701253 | validation: 2.027334254592244]
	TIME [epoch: 2.22 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4137994260034952		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 1.4137994260034952 | validation: 2.0209092351498668]
	TIME [epoch: 2.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4106717362897032		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 1.4106717362897032 | validation: 2.0184642182912205]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4129681815090702		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 1.4129681815090702 | validation: 2.0223854903632605]
	TIME [epoch: 2.21 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4147374072838221		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 1.4147374072838221 | validation: 2.0222018922352776]
	TIME [epoch: 2.23 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4108230825913461		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 1.4108230825913461 | validation: 2.026700249854073]
	TIME [epoch: 2.21 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4152424494249356		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 1.4152424494249356 | validation: 2.0132119924394902]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4171569103508759		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 1.4171569103508759 | validation: 2.0222809438486635]
	TIME [epoch: 2.21 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4144336101010917		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 1.4144336101010917 | validation: 2.039964843217294]
	TIME [epoch: 2.22 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4266305555131913		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 1.4266305555131913 | validation: 2.02611290025341]
	TIME [epoch: 2.21 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4217201918735083		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 1.4217201918735083 | validation: 2.039993322986881]
	TIME [epoch: 2.21 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4202263368203283		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 1.4202263368203283 | validation: 2.0282976791112897]
	TIME [epoch: 2.21 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.416497388449177		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 1.416497388449177 | validation: 2.0220819631509763]
	TIME [epoch: 2.22 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4136748287056495		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 1.4136748287056495 | validation: 2.0217064244580323]
	TIME [epoch: 2.21 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130229733686204		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 1.4130229733686204 | validation: 2.021796592281519]
	TIME [epoch: 2.22 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4144519635745298		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 1.4144519635745298 | validation: 2.0294711066229305]
	TIME [epoch: 2.21 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4171676180377761		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 1.4171676180377761 | validation: 2.024641921973328]
	TIME [epoch: 2.22 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4155216298737474		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 1.4155216298737474 | validation: 2.032856383285626]
	TIME [epoch: 2.21 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4198308187136253		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 1.4198308187136253 | validation: 2.015047795891522]
	TIME [epoch: 2.22 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4115388600334962		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 1.4115388600334962 | validation: 2.0158740358931704]
	TIME [epoch: 2.21 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.410446894760826		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 1.410446894760826 | validation: 2.0183596969873463]
	TIME [epoch: 2.23 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412607624414535		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 1.412607624414535 | validation: 2.0217809851157065]
	TIME [epoch: 2.21 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4110641499770913		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 1.4110641499770913 | validation: 2.030395344399627]
	TIME [epoch: 2.22 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4171611006751084		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 1.4171611006751084 | validation: 2.0290939404582495]
	TIME [epoch: 2.21 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4151107093996473		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 1.4151107093996473 | validation: 2.0219695452387465]
	TIME [epoch: 2.23 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4191027276377504		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 1.4191027276377504 | validation: 2.03610868728479]
	TIME [epoch: 2.21 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4157523380623631		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 1.4157523380623631 | validation: 2.0178315568979857]
	TIME [epoch: 2.23 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412503376233056		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 1.412503376233056 | validation: 2.0257993882964267]
	TIME [epoch: 2.21 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4123451464366188		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 1.4123451464366188 | validation: 2.0184371572368898]
	TIME [epoch: 2.21 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4134816632105913		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 1.4134816632105913 | validation: 2.0241193482198345]
	TIME [epoch: 2.22 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4094332340003461		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 1.4094332340003461 | validation: 2.0204044380705937]
	TIME [epoch: 2.21 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4110739374146974		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 1.4110739374146974 | validation: 2.012289586685433]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4159604341508953		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 1.4159604341508953 | validation: 2.0265784788969756]
	TIME [epoch: 2.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4089663993320471		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 1.4089663993320471 | validation: 2.013255286336141]
	TIME [epoch: 2.21 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4096340696348		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 1.4096340696348 | validation: 2.024469502911486]
	TIME [epoch: 2.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4100981231728844		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 1.4100981231728844 | validation: 2.0122044995719746]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4074721426031922		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 1.4074721426031922 | validation: 2.018807372930965]
	TIME [epoch: 2.21 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4091827838524358		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 1.4091827838524358 | validation: 2.0277472164396664]
	TIME [epoch: 2.23 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4156652253512432		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 1.4156652253512432 | validation: 2.0485916133924054]
	TIME [epoch: 2.22 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4244850179873734		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 1.4244850179873734 | validation: 2.024810366977758]
	TIME [epoch: 2.22 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130774910877382		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 1.4130774910877382 | validation: 2.021182087180575]
	TIME [epoch: 2.21 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412410113941947		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 1.412410113941947 | validation: 2.0267356764260143]
	TIME [epoch: 2.22 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4081366031833125		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 1.4081366031833125 | validation: 2.013725990276617]
	TIME [epoch: 2.22 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4106545696796065		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 1.4106545696796065 | validation: 2.0185319215114057]
	TIME [epoch: 2.22 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4066641693147302		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 1.4066641693147302 | validation: 2.016485193424144]
	TIME [epoch: 2.23 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4098211063245705		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 1.4098211063245705 | validation: 2.0190167953710274]
	TIME [epoch: 2.21 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401965828057632		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 1.401965828057632 | validation: 2.0209465222755556]
	TIME [epoch: 2.22 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4035933754680425		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 1.4035933754680425 | validation: 2.0138457658608835]
	TIME [epoch: 2.27 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4072424523047775		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 1.4072424523047775 | validation: 2.0265469376875873]
	TIME [epoch: 2.21 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130368750022908		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 1.4130368750022908 | validation: 2.023335261178935]
	TIME [epoch: 2.21 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4132577164949516		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 1.4132577164949516 | validation: 2.0375693342442402]
	TIME [epoch: 2.22 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4175537503107944		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 1.4175537503107944 | validation: 2.014328344614568]
	TIME [epoch: 2.21 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4075812888684556		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 1.4075812888684556 | validation: 2.0236646746577467]
	TIME [epoch: 2.22 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4062621422448056		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 1.4062621422448056 | validation: 2.0114601690027407]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4111955324942558		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 1.4111955324942558 | validation: 2.0215907474946806]
	TIME [epoch: 2.21 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4047750474209164		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 1.4047750474209164 | validation: 2.019567516617744]
	TIME [epoch: 2.21 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4053033906916208		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 1.4053033906916208 | validation: 2.016187872415473]
	TIME [epoch: 2.22 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.407274675530814		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 1.407274675530814 | validation: 2.02153844048486]
	TIME [epoch: 2.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4087272619043198		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 1.4087272619043198 | validation: 2.0181697466036987]
	TIME [epoch: 2.22 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4086812682387335		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 1.4086812682387335 | validation: 2.0246467437784936]
	TIME [epoch: 2.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4106722504138116		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 1.4106722504138116 | validation: 2.019498869237777]
	TIME [epoch: 2.22 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.410355227881988		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 1.410355227881988 | validation: 2.0221634821743626]
	TIME [epoch: 2.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4092169218724258		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 1.4092169218724258 | validation: 2.012661591223417]
	TIME [epoch: 2.22 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4095981977986969		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 1.4095981977986969 | validation: 2.0140356057381235]
	TIME [epoch: 2.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4058962966186719		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 1.4058962966186719 | validation: 2.0067319590637966]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.408399031524217		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 1.408399031524217 | validation: 2.0197096884102708]
	TIME [epoch: 2.21 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4051348290679782		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 1.4051348290679782 | validation: 2.019834485035781]
	TIME [epoch: 2.22 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4083000376467039		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 1.4083000376467039 | validation: 2.0148109032330628]
	TIME [epoch: 2.21 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4082934274474612		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 1.4082934274474612 | validation: 2.0126580769549824]
	TIME [epoch: 2.23 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4130239314811137		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 1.4130239314811137 | validation: 2.021335969366173]
	TIME [epoch: 2.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4085264809909677		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 1.4085264809909677 | validation: 2.004364160986751]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4013930661207457		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 1.4013930661207457 | validation: 2.017225632984596]
	TIME [epoch: 2.21 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4011491136284415		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 1.4011491136284415 | validation: 2.0063889768800833]
	TIME [epoch: 2.22 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4067501232536836		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 1.4067501232536836 | validation: 2.01510131707751]
	TIME [epoch: 2.21 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4038444703905293		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 1.4038444703905293 | validation: 2.009487317148231]
	TIME [epoch: 2.22 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4034962033132474		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 1.4034962033132474 | validation: 2.0176647026152117]
	TIME [epoch: 2.21 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4027514176591314		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 1.4027514176591314 | validation: 2.013151501721739]
	TIME [epoch: 2.22 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4039184378613139		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 1.4039184378613139 | validation: 2.014510786225111]
	TIME [epoch: 2.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4093759128907795		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 1.4093759128907795 | validation: 2.0222388793753447]
	TIME [epoch: 2.23 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4110867104085583		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 1.4110867104085583 | validation: 2.0353242985087863]
	TIME [epoch: 2.21 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4114521665991344		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 1.4114521665991344 | validation: 2.013098267066373]
	TIME [epoch: 2.22 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4045541604713478		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 1.4045541604713478 | validation: 2.012396327268764]
	TIME [epoch: 2.22 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4008836396868816		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 1.4008836396868816 | validation: 2.0153434621758186]
	TIME [epoch: 2.21 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4028149922472022		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 1.4028149922472022 | validation: 2.0154614403074054]
	TIME [epoch: 2.22 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4029683784634097		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 1.4029683784634097 | validation: 2.0154253402249807]
	TIME [epoch: 2.21 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.406959478140982		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 1.406959478140982 | validation: 2.0151466184260167]
	TIME [epoch: 2.22 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4060792068476575		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 1.4060792068476575 | validation: 2.0154843253845343]
	TIME [epoch: 2.21 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4057766036395682		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 1.4057766036395682 | validation: 2.0150684944350465]
	TIME [epoch: 2.22 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.399856342502316		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 1.399856342502316 | validation: 2.0098287493977587]
	TIME [epoch: 2.21 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3977768725988184		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 1.3977768725988184 | validation: 2.0130415740661913]
	TIME [epoch: 2.23 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3973556663691193		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 1.3973556663691193 | validation: 2.011912052973071]
	TIME [epoch: 2.21 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4076026668030348		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 1.4076026668030348 | validation: 2.0188399133516577]
	TIME [epoch: 2.21 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4016287974088784		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 1.4016287974088784 | validation: 2.0157204778551066]
	TIME [epoch: 2.21 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4050403070725994		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 1.4050403070725994 | validation: 2.008337367161882]
	TIME [epoch: 2.22 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4049072046746376		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 1.4049072046746376 | validation: 2.01740387249879]
	TIME [epoch: 2.21 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.404902640373719		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 1.404902640373719 | validation: 2.0152209755542168]
	TIME [epoch: 2.22 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4019704451711925		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 1.4019704451711925 | validation: 2.0142233675765064]
	TIME [epoch: 2.21 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401200985547557		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 1.401200985547557 | validation: 2.0076936615486267]
	TIME [epoch: 2.23 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3985474210506597		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 1.3985474210506597 | validation: 2.0033980123490935]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.397267900165856		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 1.397267900165856 | validation: 2.008326678600266]
	TIME [epoch: 2.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3997967679253247		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 1.3997967679253247 | validation: 2.0264024634306685]
	TIME [epoch: 2.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4101826276598697		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 1.4101826276598697 | validation: 2.010052374924255]
	TIME [epoch: 2.22 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4125234364998218		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 1.4125234364998218 | validation: 2.0161996199057812]
	TIME [epoch: 2.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4041809766410567		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 1.4041809766410567 | validation: 2.0181331541467933]
	TIME [epoch: 2.22 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4010163471560693		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 1.4010163471560693 | validation: 2.006486184040321]
	TIME [epoch: 2.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3968813507916602		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 1.3968813507916602 | validation: 2.0114705158832495]
	TIME [epoch: 2.22 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3950146244846815		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 1.3950146244846815 | validation: 2.01348049532606]
	TIME [epoch: 2.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3996634665136591		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 1.3996634665136591 | validation: 2.0102221644087317]
	TIME [epoch: 2.22 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.395587696572489		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 1.395587696572489 | validation: 2.01083980185358]
	TIME [epoch: 2.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3998393797280622		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 1.3998393797280622 | validation: 2.0122209715371375]
	TIME [epoch: 2.22 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.401680157941518		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 1.401680157941518 | validation: 2.0083553954381483]
	TIME [epoch: 2.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3985013504986399		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 1.3985013504986399 | validation: 2.006712805011005]
	TIME [epoch: 2.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3956555160721846		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 1.3956555160721846 | validation: 2.016521834374317]
	TIME [epoch: 2.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4005006204154222		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 1.4005006204154222 | validation: 2.018825070786986]
	TIME [epoch: 2.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4009821430376217		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 1.4009821430376217 | validation: 2.0104853759471424]
	TIME [epoch: 2.21 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4035357946463005		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 1.4035357946463005 | validation: 2.0081492849609632]
	TIME [epoch: 2.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4004755431157583		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 1.4004755431157583 | validation: 2.0136287638116372]
	TIME [epoch: 2.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.397412213567021		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 1.397412213567021 | validation: 2.0022068746545125]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.394895235287846		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 1.394895235287846 | validation: 2.00700344500695]
	TIME [epoch: 2.21 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3978965823647442		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 1.3978965823647442 | validation: 2.005601022480468]
	TIME [epoch: 2.21 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3997017153426097		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 1.3997017153426097 | validation: 2.008261856314043]
	TIME [epoch: 2.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3988788520121416		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 1.3988788520121416 | validation: 2.0199559654448054]
	TIME [epoch: 2.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4028580696153763		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 1.4028580696153763 | validation: 2.01649330782736]
	TIME [epoch: 2.21 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4003130353165492		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 1.4003130353165492 | validation: 2.0094786883990237]
	TIME [epoch: 2.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3991317219649153		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 1.3991317219649153 | validation: 2.005369636649317]
	TIME [epoch: 2.21 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3969214322060142		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 1.3969214322060142 | validation: 2.003408900456353]
	TIME [epoch: 2.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3971420364176985		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 1.3971420364176985 | validation: 2.0076845263136365]
	TIME [epoch: 2.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3952402626505198		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 1.3952402626505198 | validation: 2.0118549847989384]
	TIME [epoch: 2.21 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.396878893614733		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 1.396878893614733 | validation: 2.0099038808207426]
	TIME [epoch: 2.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3950877217377409		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 1.3950877217377409 | validation: 2.015764794511992]
	TIME [epoch: 2.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4015479327152394		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 1.4015479327152394 | validation: 2.018374222134291]
	TIME [epoch: 2.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4027332206805811		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 1.4027332206805811 | validation: 2.007130254170801]
	TIME [epoch: 2.21 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3941534680629988		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 1.3941534680629988 | validation: 2.0097136171022467]
	TIME [epoch: 2.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3972779277857847		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 1.3972779277857847 | validation: 2.0060606136944656]
	TIME [epoch: 2.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.393171094061626		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 1.393171094061626 | validation: 2.004551241057882]
	TIME [epoch: 2.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3952756139216116		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 1.3952756139216116 | validation: 2.015350427951801]
	TIME [epoch: 2.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3978906122875714		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 1.3978906122875714 | validation: 2.005895026232527]
	TIME [epoch: 2.21 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.398273639355623		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 1.398273639355623 | validation: 2.0093780952400118]
	TIME [epoch: 2.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3984706780615295		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 1.3984706780615295 | validation: 2.0031108460373077]
	TIME [epoch: 2.21 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3978819761111267		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 1.3978819761111267 | validation: 2.0085850944677]
	TIME [epoch: 2.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3920651696043087		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 1.3920651696043087 | validation: 2.009890838078512]
	TIME [epoch: 2.21 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3944160418975773		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 1.3944160418975773 | validation: 2.000950819620548]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3939356647037573		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 1.3939356647037573 | validation: 2.001258462194354]
	TIME [epoch: 2.21 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3920133735059335		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 1.3920133735059335 | validation: 2.000616436044484]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_796.pth
	Model improved!!!
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3931611568220117		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 1.3931611568220117 | validation: 2.003056717124881]
	TIME [epoch: 2.21 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3932110071568715		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 1.3932110071568715 | validation: 1.9993865730330542]
	TIME [epoch: 2.21 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.39326465938901		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 1.39326465938901 | validation: 2.006337119783114]
	TIME [epoch: 2.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3916089668256768		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 1.3916089668256768 | validation: 2.0028465668213022]
	TIME [epoch: 2.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389437250695831		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 1.389437250695831 | validation: 2.0127362276754637]
	TIME [epoch: 2.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3955767461803619		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 1.3955767461803619 | validation: 2.0148672798951957]
	TIME [epoch: 2.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.407925456590245		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 1.407925456590245 | validation: 2.0022953544677953]
	TIME [epoch: 2.22 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3995379175997675		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 1.3995379175997675 | validation: 2.005759576390334]
	TIME [epoch: 2.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3894002769182152		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 1.3894002769182152 | validation: 2.001621083987866]
	TIME [epoch: 2.21 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3933947321279054		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 1.3933947321279054 | validation: 1.9994205849346907]
	TIME [epoch: 2.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.392391752642326		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 1.392391752642326 | validation: 2.004250893042016]
	TIME [epoch: 2.21 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3869114800986484		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 1.3869114800986484 | validation: 2.005964918032082]
	TIME [epoch: 2.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3944002889386768		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 1.3944002889386768 | validation: 2.005375291087757]
	TIME [epoch: 2.21 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3909263933095037		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 1.3909263933095037 | validation: 2.0050700023368573]
	TIME [epoch: 2.22 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3974359771787688		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 1.3974359771787688 | validation: 1.999547363901229]
	TIME [epoch: 2.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3926585864025058		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 1.3926585864025058 | validation: 2.0042076170931673]
	TIME [epoch: 2.22 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3956097840423882		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 1.3956097840423882 | validation: 2.006520415500866]
	TIME [epoch: 2.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3881099862780173		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 1.3881099862780173 | validation: 2.0076647719964926]
	TIME [epoch: 2.22 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.39106539883015		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 1.39106539883015 | validation: 2.009555700418124]
	TIME [epoch: 2.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3901921916765339		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 1.3901921916765339 | validation: 1.9989676922316797]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.386196573919687		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 1.386196573919687 | validation: 2.0027136539210146]
	TIME [epoch: 2.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3926625036812672		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 1.3926625036812672 | validation: 2.0135359150779317]
	TIME [epoch: 2.22 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4074500374823418		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 1.4074500374823418 | validation: 1.9955748588339646]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3937234506379963		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 1.3937234506379963 | validation: 1.9955813059910612]
	TIME [epoch: 2.21 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3895096517471073		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 1.3895096517471073 | validation: 2.0038179599471535]
	TIME [epoch: 2.22 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3924403106814092		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 1.3924403106814092 | validation: 2.009189612958219]
	TIME [epoch: 2.21 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3910547001790798		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 1.3910547001790798 | validation: 2.014806480099193]
	TIME [epoch: 2.22 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3942782995698688		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 1.3942782995698688 | validation: 1.9982284184131343]
	TIME [epoch: 2.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3866811060282849		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 1.3866811060282849 | validation: 2.003840102317785]
	TIME [epoch: 2.22 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3888520511124456		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 1.3888520511124456 | validation: 1.9950821540127883]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3884564766653857		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 1.3884564766653857 | validation: 2.0033086841744976]
	TIME [epoch: 2.21 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3828534598587692		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 1.3828534598587692 | validation: 2.002465613630259]
	TIME [epoch: 2.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3883292139407886		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 1.3883292139407886 | validation: 1.9980261532417132]
	TIME [epoch: 2.22 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3865022718730962		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 1.3865022718730962 | validation: 2.002523980667915]
	TIME [epoch: 2.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3875437413334413		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 1.3875437413334413 | validation: 1.9992012441505551]
	TIME [epoch: 2.22 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3888668712088441		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 1.3888668712088441 | validation: 1.9993372353259742]
	TIME [epoch: 2.21 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3879823079910967		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 1.3879823079910967 | validation: 2.0075302131983332]
	TIME [epoch: 2.22 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3898264788627979		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 1.3898264788627979 | validation: 2.0006350954531675]
	TIME [epoch: 2.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3887660276125746		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 1.3887660276125746 | validation: 2.0049518537531754]
	TIME [epoch: 2.22 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923139752215103		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 1.3923139752215103 | validation: 2.0126076396132073]
	TIME [epoch: 2.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.393915131204032		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 1.393915131204032 | validation: 2.0041635390936405]
	TIME [epoch: 2.22 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.391877937205785		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 1.391877937205785 | validation: 2.008045920089621]
	TIME [epoch: 2.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3931537620632777		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 1.3931537620632777 | validation: 2.0022264357782293]
	TIME [epoch: 2.21 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3876863524575331		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 1.3876863524575331 | validation: 1.9967173529556583]
	TIME [epoch: 2.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3873078560530814		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 1.3873078560530814 | validation: 1.9921812557157148]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3905855820163384		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 1.3905855820163384 | validation: 2.0055699855133207]
	TIME [epoch: 2.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389658878391528		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 1.389658878391528 | validation: 1.9976391160918756]
	TIME [epoch: 2.22 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3861084694580899		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 1.3861084694580899 | validation: 1.9982413003405193]
	TIME [epoch: 2.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3880924944066033		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 1.3880924944066033 | validation: 1.9987810277310303]
	TIME [epoch: 2.22 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3879486133199128		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 1.3879486133199128 | validation: 1.9994970398087035]
	TIME [epoch: 2.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3830766801069967		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 1.3830766801069967 | validation: 2.006310947159989]
	TIME [epoch: 2.22 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.385472625370996		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 1.385472625370996 | validation: 2.002237067430513]
	TIME [epoch: 2.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3861520928991669		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 1.3861520928991669 | validation: 1.9943924841726581]
	TIME [epoch: 2.22 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3867536225326007		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 1.3867536225326007 | validation: 2.0100709659233154]
	TIME [epoch: 2.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831249862809274		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 1.3831249862809274 | validation: 1.9993672959313118]
	TIME [epoch: 2.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3859918950907408		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 1.3859918950907408 | validation: 2.004119509261356]
	TIME [epoch: 2.22 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3858250066152502		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 1.3858250066152502 | validation: 2.007790896330545]
	TIME [epoch: 2.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.389582089730987		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 1.389582089730987 | validation: 2.0109455469282778]
	TIME [epoch: 2.22 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3906995728340037		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 1.3906995728340037 | validation: 2.0026101207378537]
	TIME [epoch: 2.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3880479053191521		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 1.3880479053191521 | validation: 1.9954238457169802]
	TIME [epoch: 2.22 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3813138297710652		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 1.3813138297710652 | validation: 2.006506247211352]
	TIME [epoch: 2.21 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.38571008632642		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 1.38571008632642 | validation: 1.9979622260652186]
	TIME [epoch: 2.22 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3875008561635498		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 1.3875008561635498 | validation: 1.999002042129526]
	TIME [epoch: 2.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3843128233837967		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 1.3843128233837967 | validation: 2.0017931154440656]
	TIME [epoch: 2.22 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383432298283385		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 1.383432298283385 | validation: 2.000662939151721]
	TIME [epoch: 2.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3812207582473657		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 1.3812207582473657 | validation: 1.999325320937209]
	TIME [epoch: 2.22 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383562732083457		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 1.383562732083457 | validation: 1.9940304713687773]
	TIME [epoch: 2.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3849126373931255		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 1.3849126373931255 | validation: 2.00176846462064]
	TIME [epoch: 2.22 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3828014116834857		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 1.3828014116834857 | validation: 2.0021425681264517]
	TIME [epoch: 2.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3832861486361756		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 1.3832861486361756 | validation: 1.9938737153887427]
	TIME [epoch: 2.22 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3836081472867203		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 1.3836081472867203 | validation: 2.00366181279893]
	TIME [epoch: 2.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3843364734885553		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 1.3843364734885553 | validation: 1.9970981610785934]
	TIME [epoch: 2.22 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3839925998115723		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 1.3839925998115723 | validation: 1.9950171754101094]
	TIME [epoch: 2.21 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3819241520158647		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 1.3819241520158647 | validation: 2.00774120782674]
	TIME [epoch: 2.22 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3858213928119267		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 1.3858213928119267 | validation: 1.993809679610039]
	TIME [epoch: 2.22 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384752302393043		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 1.384752302393043 | validation: 2.0094057671411414]
	TIME [epoch: 2.22 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3898670282403984		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 1.3898670282403984 | validation: 2.000237359779378]
	TIME [epoch: 2.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.384775182908434		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 1.384775182908434 | validation: 1.9993220960373457]
	TIME [epoch: 2.22 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3826952761643596		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 1.3826952761643596 | validation: 1.9968633578587622]
	TIME [epoch: 2.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3826714894381977		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 1.3826714894381977 | validation: 1.9994952680024227]
	TIME [epoch: 2.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3822650755172612		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 1.3822650755172612 | validation: 1.9981709465994122]
	TIME [epoch: 2.22 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3830952030584542		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 1.3830952030584542 | validation: 1.9993314214836806]
	TIME [epoch: 2.21 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831978336410071		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 1.3831978336410071 | validation: 2.005256405536026]
	TIME [epoch: 2.22 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3845844818234043		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 1.3845844818234043 | validation: 1.9906512769924327]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378491952274432		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 1.378491952274432 | validation: 2.0021403522981043]
	TIME [epoch: 2.21 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831730889779488		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 1.3831730889779488 | validation: 1.998945447230545]
	TIME [epoch: 2.22 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3830097037532199		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 1.3830097037532199 | validation: 1.9939452566949196]
	TIME [epoch: 2.24 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3816007024023231		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 1.3816007024023231 | validation: 2.002404969365521]
	TIME [epoch: 2.22 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378277758832374		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 1.378277758832374 | validation: 1.998713609620036]
	TIME [epoch: 2.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3788455868073595		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 1.3788455868073595 | validation: 1.9962630598887436]
	TIME [epoch: 2.22 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.383818464885542		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 1.383818464885542 | validation: 2.005407350851884]
	TIME [epoch: 2.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3852931031011075		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 1.3852931031011075 | validation: 2.001506931893472]
	TIME [epoch: 2.22 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3813175915961702		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 1.3813175915961702 | validation: 1.9918163939209486]
	TIME [epoch: 2.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.379119030704143		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 1.379119030704143 | validation: 1.991932274768059]
	TIME [epoch: 2.22 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3779181936302172		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 1.3779181936302172 | validation: 2.0007821356682975]
	TIME [epoch: 2.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.382466120628024		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 1.382466120628024 | validation: 1.9969908640208507]
	TIME [epoch: 2.21 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.375067812660114		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 1.375067812660114 | validation: 2.0032900570762977]
	TIME [epoch: 2.21 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3805266588434424		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 1.3805266588434424 | validation: 2.00098792215136]
	TIME [epoch: 2.22 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3798791574436842		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 1.3798791574436842 | validation: 2.0040347826001836]
	TIME [epoch: 2.2 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.381783038135426		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 1.381783038135426 | validation: 2.003304363832627]
	TIME [epoch: 2.22 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3797359175757868		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 1.3797359175757868 | validation: 1.9967728207924602]
	TIME [epoch: 2.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3786710736882943		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 1.3786710736882943 | validation: 1.998928264671009]
	TIME [epoch: 2.22 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3773820740895764		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 1.3773820740895764 | validation: 1.9936308037904706]
	TIME [epoch: 2.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3794110295511286		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 1.3794110295511286 | validation: 1.9916373339465618]
	TIME [epoch: 2.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3768955068484132		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 1.3768955068484132 | validation: 1.9977952715124598]
	TIME [epoch: 2.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3773223154637302		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 1.3773223154637302 | validation: 1.9988630803894818]
	TIME [epoch: 2.22 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3826210357089312		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 1.3826210357089312 | validation: 2.0050042136764823]
	TIME [epoch: 2.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3759403467875901		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 1.3759403467875901 | validation: 1.9927112683391195]
	TIME [epoch: 2.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.376046864327075		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 1.376046864327075 | validation: 1.9947101717681475]
	TIME [epoch: 2.23 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.374680646335678		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 1.374680646335678 | validation: 1.9991888697908664]
	TIME [epoch: 2.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3775968775599041		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 1.3775968775599041 | validation: 1.9955678103872387]
	TIME [epoch: 2.22 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37547079072281		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 1.37547079072281 | validation: 1.9965982826990256]
	TIME [epoch: 2.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3796575727626463		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 1.3796575727626463 | validation: 1.9950649947025845]
	TIME [epoch: 2.22 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3767492194834352		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 1.3767492194834352 | validation: 1.995740317306742]
	TIME [epoch: 2.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3770767802367088		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 1.3770767802367088 | validation: 1.9938531050009118]
	TIME [epoch: 2.22 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3796857384258032		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 1.3796857384258032 | validation: 2.0074807535447707]
	TIME [epoch: 2.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3793953190637553		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 1.3793953190637553 | validation: 1.9910249494728713]
	TIME [epoch: 2.21 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3764879677550017		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 1.3764879677550017 | validation: 2.0017202132110636]
	TIME [epoch: 2.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3772804727341998		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 1.3772804727341998 | validation: 1.992391935602929]
	TIME [epoch: 2.21 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3751784989772613		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 1.3751784989772613 | validation: 1.9901440407173319]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3733094187598203		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 1.3733094187598203 | validation: 1.9969121405078383]
	TIME [epoch: 2.21 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3761510631231948		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 1.3761510631231948 | validation: 1.992674962996064]
	TIME [epoch: 2.22 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3772457092622858		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 1.3772457092622858 | validation: 1.9899247349553897]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3756656072199196		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 1.3756656072199196 | validation: 1.994152046928817]
	TIME [epoch: 2.21 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3735987425954193		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 1.3735987425954193 | validation: 1.994082098694647]
	TIME [epoch: 2.22 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37570633497497		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 1.37570633497497 | validation: 1.9991825772210339]
	TIME [epoch: 2.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3753151176819967		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 1.3753151176819967 | validation: 1.9940944806643857]
	TIME [epoch: 2.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370028964726995		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 1.370028964726995 | validation: 1.9969359298873717]
	TIME [epoch: 2.22 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3730906331982693		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 1.3730906331982693 | validation: 1.9909050310624854]
	TIME [epoch: 2.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3793503100419984		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 1.3793503100419984 | validation: 1.9964881003744026]
	TIME [epoch: 2.22 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3745605429735002		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 1.3745605429735002 | validation: 1.9932951194057602]
	TIME [epoch: 2.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3778107429625785		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 1.3778107429625785 | validation: 1.9960584882284413]
	TIME [epoch: 2.22 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3787091333870587		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 1.3787091333870587 | validation: 1.9873532341938096]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3767301705609254		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 1.3767301705609254 | validation: 1.9958113732986626]
	TIME [epoch: 2.26 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3768088837118473		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 1.3768088837118473 | validation: 1.992469252303613]
	TIME [epoch: 2.22 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3743882162831509		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 1.3743882162831509 | validation: 1.9859372527691652]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3746532093113382		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 1.3746532093113382 | validation: 1.994727238513283]
	TIME [epoch: 2.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3731927745107504		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 1.3731927745107504 | validation: 1.9956371094922098]
	TIME [epoch: 2.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3724764618328582		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 1.3724764618328582 | validation: 1.9973464522309374]
	TIME [epoch: 2.22 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.374703487986638		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 1.374703487986638 | validation: 1.9920686818761102]
	TIME [epoch: 2.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3753984068600156		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 1.3753984068600156 | validation: 1.9910628253406424]
	TIME [epoch: 2.22 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3746276021251338		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 1.3746276021251338 | validation: 1.986525815912145]
	TIME [epoch: 2.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3781714097806803		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 1.3781714097806803 | validation: 1.995775462812398]
	TIME [epoch: 2.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.377155735050697		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 1.377155735050697 | validation: 1.9864621850568764]
	TIME [epoch: 2.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3790957007163769		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 1.3790957007163769 | validation: 1.9905220867399906]
	TIME [epoch: 2.2 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372565108918329		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 1.372565108918329 | validation: 1.9930929736508727]
	TIME [epoch: 2.23 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3782210057174078		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 1.3782210057174078 | validation: 1.9932306442939884]
	TIME [epoch: 2.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.375574248160928		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 1.375574248160928 | validation: 1.9926894547212328]
	TIME [epoch: 2.23 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.373858254509424		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 1.373858254509424 | validation: 1.9945913505336776]
	TIME [epoch: 2.21 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372674989755632		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 1.372674989755632 | validation: 1.9977314288665609]
	TIME [epoch: 2.22 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3707243190288665		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 1.3707243190288665 | validation: 1.99011232569815]
	TIME [epoch: 2.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3709256118960094		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 1.3709256118960094 | validation: 1.9892250414460353]
	TIME [epoch: 2.22 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3726650348160592		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 1.3726650348160592 | validation: 2.0031780261667165]
	TIME [epoch: 2.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372543032724545		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 1.372543032724545 | validation: 1.9840614415347901]
	TIME [epoch: 2.22 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.369889539518006		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 1.369889539518006 | validation: 2.002512634724326]
	TIME [epoch: 2.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3740700238344048		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 1.3740700238344048 | validation: 2.0018639671211136]
	TIME [epoch: 2.21 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378674690819562		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 1.378674690819562 | validation: 1.9899317169248816]
	TIME [epoch: 2.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.374691247932969		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 1.374691247932969 | validation: 1.9948495736281728]
	TIME [epoch: 2.21 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3670432033828446		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 1.3670432033828446 | validation: 1.9896127308183964]
	TIME [epoch: 2.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3701815111617923		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 1.3701815111617923 | validation: 1.995326024444143]
	TIME [epoch: 2.22 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3715273089078703		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 1.3715273089078703 | validation: 1.9928349941326287]
	TIME [epoch: 2.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.371595316928785		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 1.371595316928785 | validation: 1.9904288560036207]
	TIME [epoch: 2.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367831721013132		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 1.367831721013132 | validation: 2.003162654541842]
	TIME [epoch: 2.22 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3690764421208415		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 1.3690764421208415 | validation: 1.9877506775954168]
	TIME [epoch: 2.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3735671053299432		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 1.3735671053299432 | validation: 1.993112896409179]
	TIME [epoch: 2.22 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370467809116677		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 1.370467809116677 | validation: 1.9886059284543514]
	TIME [epoch: 2.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3720329351390086		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 1.3720329351390086 | validation: 2.000817907003033]
	TIME [epoch: 2.22 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3734934006604842		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 1.3734934006604842 | validation: 1.9901104428544611]
	TIME [epoch: 2.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3703246663524897		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 1.3703246663524897 | validation: 1.9857391858681812]
	TIME [epoch: 2.22 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.37164643134834		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 1.37164643134834 | validation: 2.0024988210630013]
	TIME [epoch: 2.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3721065324159718		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 1.3721065324159718 | validation: 1.9900542843989104]
	TIME [epoch: 2.22 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368464061774305		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 1.368464061774305 | validation: 1.9868465910627522]
	TIME [epoch: 2.21 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3679581850084734		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 1.3679581850084734 | validation: 1.997672823481937]
	TIME [epoch: 2.21 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3695238963765541		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 1.3695238963765541 | validation: 1.9945809766111369]
	TIME [epoch: 2.21 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368812921345771		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 1.368812921345771 | validation: 1.9897708016987128]
	TIME [epoch: 2.21 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3677253660675701		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 1.3677253660675701 | validation: 2.0013437345315004]
	TIME [epoch: 2.21 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3709175555690314		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 1.3709175555690314 | validation: 1.988796902294845]
	TIME [epoch: 2.21 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3707092918203188		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 1.3707092918203188 | validation: 1.9990496420269999]
	TIME [epoch: 2.22 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3712244147972306		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 1.3712244147972306 | validation: 1.9912295524700454]
	TIME [epoch: 2.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3682313487652384		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 1.3682313487652384 | validation: 1.9902165749498701]
	TIME [epoch: 2.22 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3721380308792896		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 1.3721380308792896 | validation: 1.9832925755191249]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3673498632024421		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 1.3673498632024421 | validation: 1.9882336131206715]
	TIME [epoch: 2.21 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3695763750996173		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 1.3695763750996173 | validation: 1.985650626065398]
	TIME [epoch: 2.21 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660151435467207		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 1.3660151435467207 | validation: 1.9937027945984036]
	TIME [epoch: 2.21 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3667886134439624		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 1.3667886134439624 | validation: 1.994711449682712]
	TIME [epoch: 2.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3678957589617073		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 1.3678957589617073 | validation: 1.9879781417657807]
	TIME [epoch: 2.22 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3704191451252012		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 1.3704191451252012 | validation: 1.9765833121849878]
	TIME [epoch: 2.2 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_983.pth
	Model improved!!!
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3683277916698142		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 1.3683277916698142 | validation: 1.9960211832425863]
	TIME [epoch: 2.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368307711940886		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 1.368307711940886 | validation: 1.994539013239642]
	TIME [epoch: 2.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363077863401444		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 1.363077863401444 | validation: 1.994027304434152]
	TIME [epoch: 2.22 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3641893143926596		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 1.3641893143926596 | validation: 1.9909277240686785]
	TIME [epoch: 2.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3648236627134105		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 1.3648236627134105 | validation: 1.981617695795643]
	TIME [epoch: 2.22 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3687808288526435		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 1.3687808288526435 | validation: 1.9925438552487096]
	TIME [epoch: 2.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3674272363027122		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 1.3674272363027122 | validation: 1.9906887423541193]
	TIME [epoch: 2.22 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660349566176364		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 1.3660349566176364 | validation: 1.9953680674708143]
	TIME [epoch: 2.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3661370187829602		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 1.3661370187829602 | validation: 1.9868814232579528]
	TIME [epoch: 2.21 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366727362252215		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 1.366727362252215 | validation: 1.9894061605948292]
	TIME [epoch: 2.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3656921415581185		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 1.3656921415581185 | validation: 1.9816824146943248]
	TIME [epoch: 2.22 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3647990823839689		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 1.3647990823839689 | validation: 1.986684340524211]
	TIME [epoch: 2.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3678368795955393		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 1.3678368795955393 | validation: 1.9854740722354756]
	TIME [epoch: 2.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3664870760504737		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 1.3664870760504737 | validation: 1.987160194678565]
	TIME [epoch: 2.22 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3723722149779274		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 1.3723722149779274 | validation: 1.9891936022017285]
	TIME [epoch: 2.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3671811916782708		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 1.3671811916782708 | validation: 1.9892523992255073]
	TIME [epoch: 2.22 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3673198785221488		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 1.3673198785221488 | validation: 1.9873954082819758]
	TIME [epoch: 2.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366324988081471		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 1.366324988081471 | validation: 1.9918037119541605]
	TIME [epoch: 178 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3683746220720385		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 1.3683746220720385 | validation: 1.99345812650949]
	TIME [epoch: 4.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3652971720633753		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 1.3652971720633753 | validation: 1.9886701664503557]
	TIME [epoch: 4.73 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363553358843925		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 1.363553358843925 | validation: 1.9884833619865547]
	TIME [epoch: 4.74 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3650896460682622		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 1.3650896460682622 | validation: 1.9923127837420318]
	TIME [epoch: 4.73 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3662011596394064		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 1.3662011596394064 | validation: 1.9787775624367936]
	TIME [epoch: 4.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36665658701479		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 1.36665658701479 | validation: 1.9984862468403042]
	TIME [epoch: 4.75 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3669689389162838		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 1.3669689389162838 | validation: 1.9841253661868283]
	TIME [epoch: 4.74 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3608561553553333		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 1.3608561553553333 | validation: 1.9928925467007859]
	TIME [epoch: 4.73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3678934277726293		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 1.3678934277726293 | validation: 1.9904874605692364]
	TIME [epoch: 4.76 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3648008857082992		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 1.3648008857082992 | validation: 1.9975067848481236]
	TIME [epoch: 4.74 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3660549072706238		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 1.3660549072706238 | validation: 1.9840476793232786]
	TIME [epoch: 4.77 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366820096337013		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 1.366820096337013 | validation: 1.9831256450952843]
	TIME [epoch: 4.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3668850909774721		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 1.3668850909774721 | validation: 1.9967356905961873]
	TIME [epoch: 4.74 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3658807854385315		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 1.3658807854385315 | validation: 1.983763735134735]
	TIME [epoch: 4.75 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.361883217519033		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 1.361883217519033 | validation: 1.984707613352763]
	TIME [epoch: 4.75 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3674827092011996		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 1.3674827092011996 | validation: 1.9898554558553958]
	TIME [epoch: 4.75 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3649210545181507		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 1.3649210545181507 | validation: 1.9894795105838838]
	TIME [epoch: 4.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362677994753642		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 1.362677994753642 | validation: 1.9853341372838398]
	TIME [epoch: 4.74 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3645598782331947		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 1.3645598782331947 | validation: 1.9911730874067783]
	TIME [epoch: 4.75 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3666754063152258		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 1.3666754063152258 | validation: 1.9998801877065298]
	TIME [epoch: 4.75 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363158963133739		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 1.363158963133739 | validation: 1.9892630660939927]
	TIME [epoch: 4.75 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3649661701628781		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 1.3649661701628781 | validation: 1.9846002888100704]
	TIME [epoch: 4.75 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3644401160444255		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 1.3644401160444255 | validation: 1.9897632617126888]
	TIME [epoch: 4.74 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3667802468413863		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 1.3667802468413863 | validation: 1.9959697329971704]
	TIME [epoch: 4.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3642528245789942		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 1.3642528245789942 | validation: 1.9856077486224502]
	TIME [epoch: 4.74 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.361228014957241		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 1.361228014957241 | validation: 1.9891509364384967]
	TIME [epoch: 4.74 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3612965606766216		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 1.3612965606766216 | validation: 1.9926745633316245]
	TIME [epoch: 4.73 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3649194256829316		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 1.3649194256829316 | validation: 1.9871076160948897]
	TIME [epoch: 4.73 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3613438034035557		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 1.3613438034035557 | validation: 1.9895178508910587]
	TIME [epoch: 4.73 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362003108050333		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 1.362003108050333 | validation: 1.991758837633877]
	TIME [epoch: 4.73 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360724581694097		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 1.360724581694097 | validation: 1.9888961410331856]
	TIME [epoch: 4.73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363398090062207		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 1.363398090062207 | validation: 1.985219627882485]
	TIME [epoch: 4.73 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3618448619953643		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 1.3618448619953643 | validation: 1.993155238275206]
	TIME [epoch: 4.73 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364076583557983		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 1.364076583557983 | validation: 1.9887664696100515]
	TIME [epoch: 4.74 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3596049912743005		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 1.3596049912743005 | validation: 1.9850474796978745]
	TIME [epoch: 4.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362219040496347		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 1.362219040496347 | validation: 1.9841155219604145]
	TIME [epoch: 4.73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3616240446086687		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 1.3616240446086687 | validation: 1.987357452138041]
	TIME [epoch: 4.73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3622106665636908		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 1.3622106665636908 | validation: 1.9792417158915505]
	TIME [epoch: 4.73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3610605373705982		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 1.3610605373705982 | validation: 1.9941946872043848]
	TIME [epoch: 4.73 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364084623335587		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 1.364084623335587 | validation: 1.9929997242010065]
	TIME [epoch: 4.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3616043897784011		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 1.3616043897784011 | validation: 1.979197286375883]
	TIME [epoch: 4.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3583437117637607		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 1.3583437117637607 | validation: 1.988602902814086]
	TIME [epoch: 4.73 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3633629600273034		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 1.3633629600273034 | validation: 1.9936462454848909]
	TIME [epoch: 4.73 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3617879557854764		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 1.3617879557854764 | validation: 1.9896736996230198]
	TIME [epoch: 4.73 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.361725855709961		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 1.361725855709961 | validation: 1.9843265480790735]
	TIME [epoch: 4.73 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3556306548738957		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 1.3556306548738957 | validation: 1.9905446429420124]
	TIME [epoch: 4.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3639102255277362		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 1.3639102255277362 | validation: 1.9888005347752165]
	TIME [epoch: 4.75 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3594625322672116		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 1.3594625322672116 | validation: 1.9887601495525986]
	TIME [epoch: 4.75 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3608841255988384		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 1.3608841255988384 | validation: 1.986115716362636]
	TIME [epoch: 4.75 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3590957648318513		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 1.3590957648318513 | validation: 1.9844694215498937]
	TIME [epoch: 4.74 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3580381674399198		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 1.3580381674399198 | validation: 1.9878179550967943]
	TIME [epoch: 4.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3604619477784599		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 1.3604619477784599 | validation: 1.989051393796827]
	TIME [epoch: 4.74 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.358311649692071		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 1.358311649692071 | validation: 1.9837916684325059]
	TIME [epoch: 4.75 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362649765294745		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 1.362649765294745 | validation: 1.9826717986603961]
	TIME [epoch: 4.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3616282541497629		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 1.3616282541497629 | validation: 1.9785613624350369]
	TIME [epoch: 4.74 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3605978798669884		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 1.3605978798669884 | validation: 1.9863385508456746]
	TIME [epoch: 4.74 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3581447892768768		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 1.3581447892768768 | validation: 1.9879604255767216]
	TIME [epoch: 4.74 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3584217465134396		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 1.3584217465134396 | validation: 1.9879877625738331]
	TIME [epoch: 4.74 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3627257945778768		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 1.3627257945778768 | validation: 1.9821678375861815]
	TIME [epoch: 4.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3540241650192788		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 1.3540241650192788 | validation: 1.983785821331955]
	TIME [epoch: 4.74 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3602884950019474		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 1.3602884950019474 | validation: 1.9892351200464728]
	TIME [epoch: 4.75 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3597999824115115		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 1.3597999824115115 | validation: 1.9919598228324134]
	TIME [epoch: 4.73 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.361663442492951		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 1.361663442492951 | validation: 1.9911821529455083]
	TIME [epoch: 4.74 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3627456905420534		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 1.3627456905420534 | validation: 1.987720185262392]
	TIME [epoch: 4.74 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3595469753765324		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 1.3595469753765324 | validation: 1.9893516946152539]
	TIME [epoch: 4.75 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360116607165407		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 1.360116607165407 | validation: 1.9861890564303246]
	TIME [epoch: 4.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3570764002442004		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 1.3570764002442004 | validation: 1.9899222457840935]
	TIME [epoch: 4.75 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3598280633252693		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 1.3598280633252693 | validation: 1.9895491269209922]
	TIME [epoch: 4.75 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3546550581383325		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 1.3546550581383325 | validation: 1.9823806512529267]
	TIME [epoch: 4.75 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3558201656481532		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 1.3558201656481532 | validation: 1.9887676261406]
	TIME [epoch: 4.75 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3577100419069237		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 1.3577100419069237 | validation: 1.9859278660272563]
	TIME [epoch: 4.73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3575266437053517		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 1.3575266437053517 | validation: 1.9884645246812687]
	TIME [epoch: 4.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.359957846198856		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 1.359957846198856 | validation: 1.9865044510155045]
	TIME [epoch: 4.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362182032024569		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 1.362182032024569 | validation: 1.9894271329113544]
	TIME [epoch: 4.74 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3604620330991706		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 1.3604620330991706 | validation: 1.9910788105757888]
	TIME [epoch: 4.73 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3565917957838418		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 1.3565917957838418 | validation: 1.9870901002170145]
	TIME [epoch: 4.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3610681963407405		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 1.3610681963407405 | validation: 1.990450713565964]
	TIME [epoch: 4.75 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360505222458491		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 1.360505222458491 | validation: 1.9887120699541598]
	TIME [epoch: 4.75 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3582914648646438		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 1.3582914648646438 | validation: 1.9819023682769705]
	TIME [epoch: 4.75 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3596841214774396		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 1.3596841214774396 | validation: 1.9877520967186797]
	TIME [epoch: 4.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3592399931460875		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 1.3592399931460875 | validation: 1.9873570831840306]
	TIME [epoch: 4.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3537701825663988		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 1.3537701825663988 | validation: 1.9909225869534002]
	TIME [epoch: 4.79 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.355713006580084		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 1.355713006580084 | validation: 1.985284743821225]
	TIME [epoch: 4.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd3_20250509_105628/states/model_phi1_4a_distortion_v1_3_v_mmd3_1084.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2790.956 seconds.
