Args:
Namespace(name='model_phi1_4a_distortion_v1_1_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 848198694

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.34923486257474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.34923486257474 | validation: 6.411701909000548]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.2387963923604275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2387963923604275 | validation: 5.594993018270226]
	TIME [epoch: 0.779 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.990023683362579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.990023683362579 | validation: 5.121995184798376]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.820478942776734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.820478942776734 | validation: 4.747764679676243]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.736168943656667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.736168943656667 | validation: 4.930094875272506]
	TIME [epoch: 0.708 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.70592151809618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.70592151809618 | validation: 4.4880796463935955]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.572801932781688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.572801932781688 | validation: 4.0613815051931885]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1997606664961635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1997606664961635 | validation: 4.3120653234426305]
	TIME [epoch: 0.705 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.274569905482929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.274569905482929 | validation: 3.9462533621968805]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.089228154005385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.089228154005385 | validation: 3.8368122459273026]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.056558822642015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.056558822642015 | validation: 3.6347643256156132]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.916997054398676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.916997054398676 | validation: 3.5112262537059244]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.859346759292776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.859346759292776 | validation: 3.4164961478608142]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.825254165000875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.825254165000875 | validation: 3.4892870238113973]
	TIME [epoch: 0.707 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.968630484257894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.968630484257894 | validation: 3.5831275355466943]
	TIME [epoch: 0.704 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.950027986613435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.950027986613435 | validation: 3.0892742733066196]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.634448422153629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.634448422153629 | validation: 3.1405682335287377]
	TIME [epoch: 0.706 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.716089111953001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.716089111953001 | validation: 2.988411683510867]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.564241714561777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.564241714561777 | validation: 2.769035400983662]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.472876374736791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.472876374736791 | validation: 2.663515298806895]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.440626467456699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.440626467456699 | validation: 2.79320393392708]
	TIME [epoch: 0.705 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4775731321174534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4775731321174534 | validation: 2.9503992064411797]
	TIME [epoch: 0.707 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.575633814546539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.575633814546539 | validation: 2.8634869531388776]
	TIME [epoch: 0.705 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.517129969805872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.517129969805872 | validation: 2.3088612833676483]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.249694890087202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.249694890087202 | validation: 2.3869279092847253]
	TIME [epoch: 0.706 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.277547200453916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.277547200453916 | validation: 2.486778996875257]
	TIME [epoch: 0.705 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.305439136579804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.305439136579804 | validation: 2.4792105756246405]
	TIME [epoch: 0.704 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.262140175733665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.262140175733665 | validation: 2.2424086989532213]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.163324125806285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163324125806285 | validation: 2.3397104177804526]
	TIME [epoch: 0.705 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.189280518630743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.189280518630743 | validation: 2.5369814079948947]
	TIME [epoch: 0.704 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.31666625048795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.31666625048795 | validation: 1.9795722905824045]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009126564898536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.009126564898536 | validation: 1.975396060237354]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003873622548533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.003873622548533 | validation: 2.1447007330572596]
	TIME [epoch: 0.707 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.091575688367556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.091575688367556 | validation: 2.674589154790593]
	TIME [epoch: 0.707 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.256167021094991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.256167021094991 | validation: 2.6125502073623506]
	TIME [epoch: 0.705 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3217968761276895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3217968761276895 | validation: 1.9552633252990432]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9577290205949236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9577290205949236 | validation: 2.5212448401572907]
	TIME [epoch: 0.708 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.182366967821964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182366967821964 | validation: 2.1207937944135153]
	TIME [epoch: 0.707 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.038506567144882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038506567144882 | validation: 1.9011329533327386]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8949954341676047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8949954341676047 | validation: 1.9327604904206617]
	TIME [epoch: 0.708 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.885149566051217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.885149566051217 | validation: 1.8682851546078343]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8784950289802715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8784950289802715 | validation: 1.922409335856809]
	TIME [epoch: 0.705 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.835683302266507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.835683302266507 | validation: 1.7882119625116666]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8116928535242884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8116928535242884 | validation: 1.9472054991481298]
	TIME [epoch: 0.705 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.854782003509697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.854782003509697 | validation: 2.204832954193044]
	TIME [epoch: 0.704 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02594059658872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.02594059658872 | validation: 1.7773154467238328]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7427280311217057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7427280311217057 | validation: 1.6778519520108313]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.706973991869489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.706973991869489 | validation: 1.6766500292455893]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.686581461232112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.686581461232112 | validation: 1.8132335983838102]
	TIME [epoch: 0.705 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7233332460422144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7233332460422144 | validation: 2.235943959803321]
	TIME [epoch: 0.703 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01970616917866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01970616917866 | validation: 1.9598737417452563]
	TIME [epoch: 0.703 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7573580628717185		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.7573580628717185 | validation: 1.6596014734935982]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6506746677517086		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.6506746677517086 | validation: 1.7484709554683293]
	TIME [epoch: 0.704 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6842409442720743		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.6842409442720743 | validation: 1.8615026124160359]
	TIME [epoch: 0.704 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7504174911362615		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.7504174911362615 | validation: 1.9439888843236117]
	TIME [epoch: 0.709 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7192942607807002		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.7192942607807002 | validation: 1.901233348095773]
	TIME [epoch: 0.703 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.779253926462825		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.779253926462825 | validation: 1.783701988688266]
	TIME [epoch: 0.704 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6300731422741426		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.6300731422741426 | validation: 1.6279485698685363]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5757320188389805		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.5757320188389805 | validation: 1.62152788939162]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5764537348494208		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.5764537348494208 | validation: 1.7092776802850524]
	TIME [epoch: 0.705 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5801831855839885		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.5801831855839885 | validation: 1.676082492353767]
	TIME [epoch: 0.706 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6073770904951425		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.6073770904951425 | validation: 1.8730047943446264]
	TIME [epoch: 0.705 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.645106064051047		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.645106064051047 | validation: 1.947197634476708]
	TIME [epoch: 0.704 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7519842840807387		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.7519842840807387 | validation: 1.6937779177695511]
	TIME [epoch: 0.705 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5337432248845575		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.5337432248845575 | validation: 1.7068027091925246]
	TIME [epoch: 0.704 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.556815333907457		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.556815333907457 | validation: 1.764261252946369]
	TIME [epoch: 0.704 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6191921431572003		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.6191921431572003 | validation: 1.8590247886373312]
	TIME [epoch: 0.703 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5673282860468545		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.5673282860468545 | validation: 1.626059700210432]
	TIME [epoch: 0.703 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4899790164236753		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.4899790164236753 | validation: 1.6188680204714332]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4899597709189516		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.4899597709189516 | validation: 1.7214516355025744]
	TIME [epoch: 0.707 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4924422996262368		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.4924422996262368 | validation: 1.670601220044083]
	TIME [epoch: 0.705 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.499938724662519		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.499938724662519 | validation: 1.727703235431119]
	TIME [epoch: 0.705 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4871149697100714		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.4871149697100714 | validation: 1.6800122321124578]
	TIME [epoch: 0.704 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5021513400880044		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.5021513400880044 | validation: 1.7593317734221081]
	TIME [epoch: 0.704 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.474514000201716		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.474514000201716 | validation: 1.6464910398222052]
	TIME [epoch: 0.704 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4542050823869945		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.4542050823869945 | validation: 1.6992051581418228]
	TIME [epoch: 0.706 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.43541385328635		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.43541385328635 | validation: 1.6530634757000529]
	TIME [epoch: 0.708 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.432001390497932		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.432001390497932 | validation: 1.7007797207324558]
	TIME [epoch: 0.703 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4260159163995048		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.4260159163995048 | validation: 1.6575434241098947]
	TIME [epoch: 0.703 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.425450442470333		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.425450442470333 | validation: 1.7574521213015666]
	TIME [epoch: 0.703 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.428778017594674		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.428778017594674 | validation: 1.7001083223043487]
	TIME [epoch: 0.704 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.438116876291031		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.438116876291031 | validation: 1.8133654243887083]
	TIME [epoch: 0.703 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.424175070378287		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.424175070378287 | validation: 1.6783206217802489]
	TIME [epoch: 0.705 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4039149989816133		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.4039149989816133 | validation: 1.7294742327114883]
	TIME [epoch: 0.705 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3715715373270703		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.3715715373270703 | validation: 1.6833942100720098]
	TIME [epoch: 0.706 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3625650977364456		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.3625650977364456 | validation: 1.6868569700529399]
	TIME [epoch: 0.704 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.355521236057515		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.355521236057515 | validation: 1.684633905295607]
	TIME [epoch: 0.706 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.346544380438286		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 3.346544380438286 | validation: 1.6963464563794248]
	TIME [epoch: 0.704 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3410688228215855		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.3410688228215855 | validation: 1.6890385788751727]
	TIME [epoch: 0.703 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.334831867963935		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.334831867963935 | validation: 1.7343580718130214]
	TIME [epoch: 0.702 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.326898228869992		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 3.326898228869992 | validation: 1.6815394168277402]
	TIME [epoch: 0.705 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.330010913338157		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 3.330010913338157 | validation: 1.8334272320474818]
	TIME [epoch: 0.704 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.343386257603381		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 3.343386257603381 | validation: 1.7771913532946968]
	TIME [epoch: 0.703 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.366272978473802		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.366272978473802 | validation: 1.8415446695028228]
	TIME [epoch: 0.702 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3219355476040504		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 3.3219355476040504 | validation: 1.7337141439371337]
	TIME [epoch: 0.703 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2959208215735885		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 3.2959208215735885 | validation: 1.7455599297198283]
	TIME [epoch: 0.703 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2751218641767643		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 3.2751218641767643 | validation: 1.7669658226532576]
	TIME [epoch: 0.705 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.265916628360402		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 3.265916628360402 | validation: 1.745282157136515]
	TIME [epoch: 0.702 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.256941702031586		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 3.256941702031586 | validation: 1.7770633510738287]
	TIME [epoch: 0.704 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2498390875994994		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 3.2498390875994994 | validation: 1.7776141861390806]
	TIME [epoch: 0.706 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2391838419494947		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 3.2391838419494947 | validation: 1.7969849223072893]
	TIME [epoch: 0.709 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.236841517454967		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 3.236841517454967 | validation: 1.8016886137502064]
	TIME [epoch: 0.706 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.224550705204749		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 3.224550705204749 | validation: 1.7855443481985067]
	TIME [epoch: 0.709 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.219327165573899		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 3.219327165573899 | validation: 1.8740232461628488]
	TIME [epoch: 0.708 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.214302079240672		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 3.214302079240672 | validation: 1.8140408520100466]
	TIME [epoch: 0.706 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2022425724097277		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 3.2022425724097277 | validation: 1.8714092620555627]
	TIME [epoch: 0.705 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1862016395992674		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 3.1862016395992674 | validation: 1.835175546486334]
	TIME [epoch: 0.706 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.171634660659163		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 3.171634660659163 | validation: 1.873891846540701]
	TIME [epoch: 0.704 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.163671843063863		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 3.163671843063863 | validation: 1.8757010029323902]
	TIME [epoch: 0.706 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1443760789854527		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 3.1443760789854527 | validation: 1.879426754396301]
	TIME [epoch: 0.712 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1319598862006375		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 3.1319598862006375 | validation: 1.8870336797923568]
	TIME [epoch: 0.707 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1195528298449866		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 3.1195528298449866 | validation: 1.9501553852447815]
	TIME [epoch: 0.705 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.092960970576653		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 3.092960970576653 | validation: 1.9093339521613304]
	TIME [epoch: 0.706 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.081361281994205		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 3.081361281994205 | validation: 2.040786759493441]
	TIME [epoch: 0.705 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.060643426122885		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 3.060643426122885 | validation: 1.9685195756856844]
	TIME [epoch: 0.705 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0141663313734135		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 3.0141663313734135 | validation: 2.019908099743428]
	TIME [epoch: 0.703 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9571476490918434		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.9571476490918434 | validation: 2.2662737543227727]
	TIME [epoch: 0.706 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7880360826153527		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.7880360826153527 | validation: 2.7101391021700043]
	TIME [epoch: 0.709 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.894117073727926		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.894117073727926 | validation: 2.4560457919140926]
	TIME [epoch: 0.706 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2716032438566356		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 3.2716032438566356 | validation: 2.109762578470558]
	TIME [epoch: 0.706 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.787962206418109		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.787962206418109 | validation: 2.362370261612091]
	TIME [epoch: 0.706 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.67406965765368		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.67406965765368 | validation: 2.338126337816563]
	TIME [epoch: 0.707 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.627323627213351		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.627323627213351 | validation: 2.0698219773082345]
	TIME [epoch: 0.706 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.50931740414105		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.50931740414105 | validation: 1.9292803231270863]
	TIME [epoch: 0.706 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4601069471359223		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.4601069471359223 | validation: 1.9999988327498899]
	TIME [epoch: 0.707 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4030788583609657		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.4030788583609657 | validation: 1.9180368265553653]
	TIME [epoch: 0.707 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3958613166596665		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.3958613166596665 | validation: 2.0416547633222826]
	TIME [epoch: 0.706 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.387680892194184		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.387680892194184 | validation: 1.8872754747577538]
	TIME [epoch: 0.705 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.426407387819055		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.426407387819055 | validation: 2.051059084893576]
	TIME [epoch: 0.707 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3671684907301396		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.3671684907301396 | validation: 1.9055236135110603]
	TIME [epoch: 0.706 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3306000263469167		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.3306000263469167 | validation: 1.9745988394962568]
	TIME [epoch: 0.705 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3020300006633363		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.3020300006633363 | validation: 1.9362233299682972]
	TIME [epoch: 0.707 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2680008596802774		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.2680008596802774 | validation: 1.9248478062750891]
	TIME [epoch: 0.707 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.244432716794593		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.244432716794593 | validation: 2.13787743369817]
	TIME [epoch: 0.707 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3086997001130767		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.3086997001130767 | validation: 2.0709116648752826]
	TIME [epoch: 0.706 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7770896484025935		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.7770896484025935 | validation: 1.9165182319143854]
	TIME [epoch: 0.706 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3334772570189064		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.3334772570189064 | validation: 2.489227599632498]
	TIME [epoch: 0.707 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.528953534900806		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.528953534900806 | validation: 1.9841385783654681]
	TIME [epoch: 0.708 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.18103498578491		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.18103498578491 | validation: 1.8915109883169152]
	TIME [epoch: 0.707 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2051002402481474		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.2051002402481474 | validation: 1.9488084435257282]
	TIME [epoch: 0.706 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.06897551501907		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.06897551501907 | validation: 2.0351575801570188]
	TIME [epoch: 0.706 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9924464894089715		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.9924464894089715 | validation: 1.9845967903111335]
	TIME [epoch: 0.706 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9795234272337712		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.9795234272337712 | validation: 2.3739857636990975]
	TIME [epoch: 0.706 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.173584577756194		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 2.173584577756194 | validation: 1.8773757444293744]
	TIME [epoch: 0.705 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9465050393777352		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.9465050393777352 | validation: 1.878843148317598]
	TIME [epoch: 0.706 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7015446349478265		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.7015446349478265 | validation: 1.7650583904613613]
	TIME [epoch: 0.705 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5943576958912444		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.5943576958912444 | validation: 1.8406531718472945]
	TIME [epoch: 0.704 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5445661247530433		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.5445661247530433 | validation: 1.6630981489033296]
	TIME [epoch: 0.706 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6683471915994765		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.6683471915994765 | validation: 1.8593129543657363]
	TIME [epoch: 0.705 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7282110231312755		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.7282110231312755 | validation: 1.517041479725634]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.431016709574794		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.431016709574794 | validation: 1.5822000540024845]
	TIME [epoch: 0.703 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3723012088029283		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3723012088029283 | validation: 1.4549178257223199]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3176213092485722		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.3176213092485722 | validation: 1.461688267826183]
	TIME [epoch: 0.708 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2994516710635629		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.2994516710635629 | validation: 1.43884898655788]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3056272791024157		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.3056272791024157 | validation: 1.4233021461172097]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5438338747406388		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.5438338747406388 | validation: 1.5302625654174349]
	TIME [epoch: 0.706 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4817806494852304		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.4817806494852304 | validation: 1.284682329042615]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.286459983116509		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.286459983116509 | validation: 1.4624051044054966]
	TIME [epoch: 0.706 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2396409733777372		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.2396409733777372 | validation: 1.2472583382040243]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207946458851268		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.207946458851268 | validation: 1.3377357135693302]
	TIME [epoch: 0.706 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1997200338373446		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.1997200338373446 | validation: 1.2755001891585505]
	TIME [epoch: 0.705 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.301075271257531		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.301075271257531 | validation: 1.3986825915509127]
	TIME [epoch: 0.706 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3408894964590496		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3408894964590496 | validation: 1.2152763912471014]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.205650979433662		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.205650979433662 | validation: 1.1962792408893528]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1080938067758381		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1080938067758381 | validation: 1.1926640401467015]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0962026322057603		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0962026322057603 | validation: 1.1794694168912225]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1103134737955587		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.1103134737955587 | validation: 1.1411241233169676]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1540821026376429		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1540821026376429 | validation: 1.2436668119392265]
	TIME [epoch: 0.703 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073131065217622		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2073131065217622 | validation: 1.0833708555968276]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2839172178853786		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2839172178853786 | validation: 1.2939849462405897]
	TIME [epoch: 0.706 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1537361747824386		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1537361747824386 | validation: 1.0656440428662017]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0435142696423594		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0435142696423594 | validation: 1.0404867043810149]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0960853969610336		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0960853969610336 | validation: 1.148383989840617]
	TIME [epoch: 0.705 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047264726077136		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.047264726077136 | validation: 1.0568273995751443]
	TIME [epoch: 0.706 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0061392976872272		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.0061392976872272 | validation: 1.0368168048448234]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.007872417713773		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.007872417713773 | validation: 1.091174332191872]
	TIME [epoch: 0.706 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0808069199550432		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.0808069199550432 | validation: 1.0212106308217954]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1713910947357864		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1713910947357864 | validation: 1.2704187954437933]
	TIME [epoch: 0.707 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130559446543184		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.130559446543184 | validation: 0.9901689117650394]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1587789569207254		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1587789569207254 | validation: 1.040825969269341]
	TIME [epoch: 0.71 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9637489284240093		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9637489284240093 | validation: 1.002348809167798]
	TIME [epoch: 0.705 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9562680199490998		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9562680199490998 | validation: 1.0135790310426878]
	TIME [epoch: 0.705 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9708112315439149		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.9708112315439149 | validation: 0.9998930273604126]
	TIME [epoch: 0.703 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975221380962512		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9975221380962512 | validation: 0.9989243221126346]
	TIME [epoch: 0.704 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0213826906038164		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0213826906038164 | validation: 0.9311425693231024]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0239781554379461		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.0239781554379461 | validation: 1.175652388288263]
	TIME [epoch: 0.704 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050578491320987		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.050578491320987 | validation: 0.9150078584003639]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1657412659957427		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1657412659957427 | validation: 0.9440794069808929]
	TIME [epoch: 0.706 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9141391780849373		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9141391780849373 | validation: 0.9660975387129818]
	TIME [epoch: 0.704 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243787694200972		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9243787694200972 | validation: 0.8884837658823115]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9820775135017644		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.9820775135017644 | validation: 0.9843067855297714]
	TIME [epoch: 0.708 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379913065049991		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9379913065049991 | validation: 0.8977364841266013]
	TIME [epoch: 0.707 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817694952079614		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8817694952079614 | validation: 0.8917309444756821]
	TIME [epoch: 0.706 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579812342634537		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.8579812342634537 | validation: 0.868313855763171]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8566445559900387		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8566445559900387 | validation: 0.9724947355580481]
	TIME [epoch: 0.704 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957461643677477		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8957461643677477 | validation: 0.8519792442553916]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1033516479533716		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1033516479533716 | validation: 1.0981247889220485]
	TIME [epoch: 0.707 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105991632783299		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.105991632783299 | validation: 0.8335046117091652]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0367605766194543		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0367605766194543 | validation: 1.0113920504150877]
	TIME [epoch: 0.703 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220397002459402		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9220397002459402 | validation: 0.8092588685585502]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9125938767834761		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9125938767834761 | validation: 0.9137871379549796]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764121404243116		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.8764121404243116 | validation: 0.789876903431308]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691647901174046		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.8691647901174046 | validation: 0.8649468166728318]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368590938807111		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8368590938807111 | validation: 0.7825550637395976]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8285643391631934		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8285643391631934 | validation: 0.8708732064406522]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8147525873984723		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8147525873984723 | validation: 0.7518237567148669]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746836196157125		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8746836196157125 | validation: 0.9188170725709078]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831826789732802		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8831826789732802 | validation: 0.7492755883100624]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9331734876180647		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.9331734876180647 | validation: 0.9178582754838349]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927565112290946		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.927565112290946 | validation: 0.836723360051958]
	TIME [epoch: 1.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0870825231249555		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.0870825231249555 | validation: 0.8128276641362255]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048763528539417		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.8048763528539417 | validation: 0.7422616493039377]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710255540910049		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7710255540910049 | validation: 0.7863698703496347]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942808689402426		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7942808689402426 | validation: 0.7260613920335717]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291604805165624		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8291604805165624 | validation: 0.8811336819554679]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.920218317584232		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.920218317584232 | validation: 0.7510401661900483]
	TIME [epoch: 1.38 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9340131448668371		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9340131448668371 | validation: 0.8006558117203172]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684698072301172		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7684698072301172 | validation: 0.6783280275645276]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030286607748624		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9030286607748624 | validation: 1.1022149105914278]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1338324732724812		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1338324732724812 | validation: 0.7194583863547592]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9876794533609673		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9876794533609673 | validation: 0.7190131489275982]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443123858069288		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7443123858069288 | validation: 0.7408432985055278]
	TIME [epoch: 1.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623927669310061		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7623927669310061 | validation: 0.6532404032405399]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832688263082963		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.832688263082963 | validation: 0.895187397305683]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723148983699991		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.8723148983699991 | validation: 0.67196075143104]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910543345310816		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.8910543345310816 | validation: 0.7686967184490467]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515420283559889		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7515420283559889 | validation: 0.6780002605458292]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326650383818415		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7326650383818415 | validation: 0.6660498090005356]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.727692688670646		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.727692688670646 | validation: 0.6550240620848076]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410722915393865		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7410722915393865 | validation: 0.6926642825367488]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007616098177708		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.8007616098177708 | validation: 0.7928875107132914]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267810117440395		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8267810117440395 | validation: 0.6427813226513379]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8384283785874564		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8384283785874564 | validation: 0.9260912081974908]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848116011911076		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8848116011911076 | validation: 0.6680673964290329]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0166160990363127		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.0166160990363127 | validation: 0.7212993167812567]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821085908900126		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7821085908900126 | validation: 0.6413424655496455]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239658893246872		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7239658893246872 | validation: 0.641664430948704]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060108067192736		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7060108067192736 | validation: 0.6338987167957241]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893729636533539		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6893729636533539 | validation: 0.620943696644354]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832600218323467		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6832600218323467 | validation: 0.5908897069419599]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789464465613319		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6789464465613319 | validation: 0.6585630706561452]
	TIME [epoch: 1.37 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033512565265739		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7033512565265739 | validation: 0.6092411421846716]
	TIME [epoch: 1.37 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828456414494068		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.828456414494068 | validation: 1.1303203093383858]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1359838462398006		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1359838462398006 | validation: 0.7943410696010731]
	TIME [epoch: 1.37 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1600756846276412		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1600756846276412 | validation: 0.6720853893115737]
	TIME [epoch: 1.37 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231352760263091		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8231352760263091 | validation: 0.9688834997214679]
	TIME [epoch: 1.37 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284575861336061		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9284575861336061 | validation: 0.5894462992725571]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250207307860369		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8250207307860369 | validation: 0.6267761030404323]
	TIME [epoch: 1.39 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897138397510629		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6897138397510629 | validation: 0.6984939897153164]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363291505808064		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7363291505808064 | validation: 0.5761394239288763]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426236420606089		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.7426236420606089 | validation: 0.625962181889888]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858255923542177		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6858255923542177 | validation: 0.5752968495484295]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818743024477104		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6818743024477104 | validation: 0.6349299945192866]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919440466545814		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6919440466545814 | validation: 0.5656090609330987]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332435589880814		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7332435589880814 | validation: 0.7404171857357413]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777131849103736		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.777131849103736 | validation: 0.5808080458186478]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657397771479799		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.8657397771479799 | validation: 0.7819477681829599]
	TIME [epoch: 1.37 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628792199190181		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7628792199190181 | validation: 0.5602127848320537]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266692967256263		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7266692967256263 | validation: 0.6430797665797997]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6876181202472429		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6876181202472429 | validation: 0.5460786660981296]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153081907254056		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7153081907254056 | validation: 0.6698785658977576]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897615321766988		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6897615321766988 | validation: 0.5468188469155809]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132353523338921		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.7132353523338921 | validation: 0.7025841640393381]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157713303398862		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7157713303398862 | validation: 0.5603778740752259]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801452972728871		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.801452972728871 | validation: 0.7819748568714943]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877645550748722		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7877645550748722 | validation: 0.5547633966775185]
	TIME [epoch: 1.37 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866819595202657		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.7866819595202657 | validation: 0.6199032500722886]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531274155832059		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6531274155832059 | validation: 0.5604522317255122]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6347592330793256		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6347592330793256 | validation: 0.5335186860161042]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415970335638883		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6415970335638883 | validation: 0.6364643757924839]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568191093838975		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6568191093838975 | validation: 0.5624956894810085]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614158985924686		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.7614158985924686 | validation: 0.7380351690016271]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499988888370536		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7499988888370536 | validation: 0.5489774534665081]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910051093559639		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7910051093559639 | validation: 0.6495194148686196]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005307818447898		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7005307818447898 | validation: 0.5764275926770357]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752736431655544		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.752736431655544 | validation: 0.6245351999551197]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671996802640893		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6671996802640893 | validation: 0.5227254623329517]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842454108862585		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6842454108862585 | validation: 0.7493326672917833]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537316890247914		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.7537316890247914 | validation: 0.5528178259536406]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037503551731545		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.8037503551731545 | validation: 0.5710421885509442]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315319501380657		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6315319501380657 | validation: 0.594055407169685]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508215694052214		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.6508215694052214 | validation: 0.56589726376106]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494221683717932		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6494221683717932 | validation: 0.5972188879837281]
	TIME [epoch: 1.37 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315312749784073		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6315312749784073 | validation: 0.5076066937513369]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018356073429441		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.7018356073429441 | validation: 0.8278296942364606]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209016296560581		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8209016296560581 | validation: 0.5610716895017562]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134785068731317		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.8134785068731317 | validation: 0.6117175914425194]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6131647317834598		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6131647317834598 | validation: 0.5584230162342111]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919446725611137		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5919446725611137 | validation: 0.5138875666033197]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.601842623078538		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.601842623078538 | validation: 0.6556082561951325]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484971337191313		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6484971337191313 | validation: 0.5459680397321964]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179042397620475		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8179042397620475 | validation: 0.6307032846835312]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630879079460897		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6630879079460897 | validation: 0.6137376822679219]
	TIME [epoch: 1.38 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556267307393489		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6556267307393489 | validation: 0.5503587389384198]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249242846105397		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6249242846105397 | validation: 0.6170398494667773]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397302154002309		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6397302154002309 | validation: 0.5110852881973121]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952382172451463		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6952382172451463 | validation: 0.7506399984329678]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432139334944421		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7432139334944421 | validation: 0.5134751687483049]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549625957888091		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7549625957888091 | validation: 0.5841651219218418]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077225162482056		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6077225162482056 | validation: 0.5084747266020738]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670796600793667		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5670796600793667 | validation: 0.5250948317433893]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5599006644986141		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5599006644986141 | validation: 0.5237466192829262]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643240364764254		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5643240364764254 | validation: 0.5627915783878172]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5642192195841494		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5642192195841494 | validation: 0.4910943283844098]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6072159930305588		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6072159930305588 | validation: 0.845279238219037]
	TIME [epoch: 1.38 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298240055165517		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8298240055165517 | validation: 0.5842629288821599]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645739537715688		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8645739537715688 | validation: 0.5575833766589496]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275766449033923		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6275766449033923 | validation: 0.8448694409185604]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236856860567648		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8236856860567648 | validation: 0.5329441233939611]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.70609085358833		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.70609085358833 | validation: 0.6783130333456908]
	TIME [epoch: 1.38 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752536044726211		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6752536044726211 | validation: 0.5417881966841698]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5849469825963494		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.5849469825963494 | validation: 0.5262405822936275]
	TIME [epoch: 1.38 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5801401911068865		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5801401911068865 | validation: 0.5945168500977224]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5962436718223372		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5962436718223372 | validation: 0.4898509696904369]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224910593300392		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6224910593300392 | validation: 0.6619119554717546]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289832758510471		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6289832758510471 | validation: 0.5027709319024842]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451779321710035		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6451779321710035 | validation: 0.6289932547584045]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.603209833407199		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.603209833407199 | validation: 0.4786310722185452]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5699298434485883		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.5699298434485883 | validation: 0.6240564311509685]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815886566013224		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.5815886566013224 | validation: 0.48729723149227966]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604462953584397		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.604462953584397 | validation: 0.6451030063594141]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6189866186335276		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6189866186335276 | validation: 0.5092948191657068]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7048660050024914		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.7048660050024914 | validation: 0.5794801508520588]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617781243314236		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.5617781243314236 | validation: 0.5352687429058147]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5538781726793216		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.5538781726793216 | validation: 0.49317418605129504]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679146776376965		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.5679146776376965 | validation: 0.5840729702310936]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727042087357502		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.5727042087357502 | validation: 0.49952248752709]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781775485820157		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6781775485820157 | validation: 0.6382644455930273]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063555616286799		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6063555616286799 | validation: 0.48083725981829684]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632715209301596		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5632715209301596 | validation: 0.5784815273939049]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5527412117666438		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.5527412117666438 | validation: 0.4671826772918971]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5646600885179862		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5646600885179862 | validation: 0.624671465459957]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5796799854991781		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.5796799854991781 | validation: 0.482042219375722]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194882267692357		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6194882267692357 | validation: 0.5936671597866915]
	TIME [epoch: 1.38 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670592884255116		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.5670592884255116 | validation: 0.4782913365285579]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5743727003520612		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5743727003520612 | validation: 0.6123152012273998]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5592178613742279		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5592178613742279 | validation: 0.4721817069477825]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5998686502952579		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.5998686502952579 | validation: 0.6040838811634668]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5665397199224578		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.5665397199224578 | validation: 0.4795841234392967]
	TIME [epoch: 1.37 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5803705013737203		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.5803705013737203 | validation: 0.5385985364680566]
	TIME [epoch: 1.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.508153779209338		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.508153779209338 | validation: 0.4852170324143149]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4864845577284642		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.4864845577284642 | validation: 0.5161111797499978]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48534728523531145		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.48534728523531145 | validation: 0.45712280948039674]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5057182357351456		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5057182357351456 | validation: 0.666823499948728]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342084445673392		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6342084445673392 | validation: 0.4962498755517684]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064879318646149		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.7064879318646149 | validation: 0.5568474819330448]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270762058505492		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5270762058505492 | validation: 0.5521601386681676]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5494642429243316		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5494642429243316 | validation: 0.4561367723217504]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.556165774242499		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.556165774242499 | validation: 0.5732461577830237]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132857234065382		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5132857234065382 | validation: 0.4434288281834297]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5319681890280795		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.5319681890280795 | validation: 0.575396035948386]
	TIME [epoch: 1.38 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5213296389819237		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5213296389819237 | validation: 0.44337951504188705]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5291158230783042		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5291158230783042 | validation: 0.5815434401152769]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.523978414942018		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.523978414942018 | validation: 0.46045040347971417]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5331174806660718		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5331174806660718 | validation: 0.5407079825496169]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48194238473675777		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.48194238473675777 | validation: 0.4278058311031922]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4918093126791541		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.4918093126791541 | validation: 0.565671964654232]
	TIME [epoch: 1.38 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5058735995866032		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.5058735995866032 | validation: 0.4411654891820688]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5568549655624063		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.5568549655624063 | validation: 0.6059887021564911]
	TIME [epoch: 1.38 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391936895491755		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5391936895491755 | validation: 0.44042777194675237]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5017604708837837		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5017604708837837 | validation: 0.57694020385285]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49745320683442884		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.49745320683442884 | validation: 0.45085893476733524]
	TIME [epoch: 1.38 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010295450683773		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5010295450683773 | validation: 0.5223062490302769]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47034740708018036		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.47034740708018036 | validation: 0.4405849860433292]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43262650404709113		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.43262650404709113 | validation: 0.48380652636620813]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4302196219489581		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.4302196219489581 | validation: 0.4129189136815968]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4626189597343573		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.4626189597343573 | validation: 0.5800056374319985]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5240501048202046		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5240501048202046 | validation: 0.4570985835386527]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269047251862241		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.6269047251862241 | validation: 0.5857524173163305]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5029972965906753		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5029972965906753 | validation: 0.4696568119148891]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656348782465329		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.4656348782465329 | validation: 0.41830220236583515]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4318589485267285		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4318589485267285 | validation: 0.5203061091561856]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4483929586740409		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.4483929586740409 | validation: 0.4077655283994894]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5033514192541463		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5033514192541463 | validation: 0.582375757525165]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5164909815106238		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5164909815106238 | validation: 0.4093613222550156]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035612940747416		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5035612940747416 | validation: 0.5041771271239726]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41809267141085976		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.41809267141085976 | validation: 0.4027402823824299]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39801162004534874		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.39801162004534874 | validation: 0.46459719767336866]
	TIME [epoch: 1.37 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39842225648377366		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.39842225648377366 | validation: 0.3905375753897536]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4365057111330786		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.4365057111330786 | validation: 0.6042027943933288]
	TIME [epoch: 1.37 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399959476531845		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5399959476531845 | validation: 0.4052165183048526]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5262880807777205		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5262880807777205 | validation: 0.4856442911690671]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.406307816137611		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.406307816137611 | validation: 0.4099630942299191]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3876237315382061		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3876237315382061 | validation: 0.42225808058622666]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380789896932197		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.380789896932197 | validation: 0.484916252235261]
	TIME [epoch: 1.37 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4129617355663478		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.4129617355663478 | validation: 0.39374716694438494]
	TIME [epoch: 1.37 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46373474807636245		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.46373474807636245 | validation: 0.5450123229902707]
	TIME [epoch: 1.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4566958806285314		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4566958806285314 | validation: 0.39116932418065337]
	TIME [epoch: 1.37 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5029782744382169		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5029782744382169 | validation: 0.6041675683434025]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5079763751542092		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5079763751542092 | validation: 0.3887853912148082]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3929703655620419		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.3929703655620419 | validation: 0.4894149862852153]
	TIME [epoch: 1.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39797734523832595		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.39797734523832595 | validation: 0.3949031411121302]
	TIME [epoch: 1.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38441071287836914		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.38441071287836914 | validation: 0.4508723669835541]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37196758595539053		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.37196758595539053 | validation: 0.38043302144956714]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39772972109923593		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.39772972109923593 | validation: 0.5295443318053221]
	TIME [epoch: 1.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4175168307956473		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4175168307956473 | validation: 0.3771177434006404]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45489944286735795		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.45489944286735795 | validation: 0.5380613631407574]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.421839861625178		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.421839861625178 | validation: 0.3699075281129241]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3778407227224275		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.3778407227224275 | validation: 0.46175129600013776]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36001252346577717		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.36001252346577717 | validation: 0.38076569724998244]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38588159271670314		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.38588159271670314 | validation: 0.527865690645257]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42443242090620986		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.42443242090620986 | validation: 0.3583679419567858]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39679916309352325		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.39679916309352325 | validation: 0.4952924517085349]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38871993357156315		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.38871993357156315 | validation: 0.3543911188646677]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3944077416321515		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.3944077416321515 | validation: 0.4729821546569115]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3809468463516494		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3809468463516494 | validation: 0.3587000039357078]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39217547967598193		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.39217547967598193 | validation: 0.48696423503220815]
	TIME [epoch: 1.37 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3805232752538861		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.3805232752538861 | validation: 0.3512029292748262]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36535847539905375		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.36535847539905375 | validation: 0.45417229421610134]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34821196376507024		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.34821196376507024 | validation: 0.35378417718937594]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34455135430047334		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.34455135430047334 | validation: 0.43602913580643743]
	TIME [epoch: 1.37 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3447066944804119		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.3447066944804119 | validation: 0.3419054972374045]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530234521537577		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3530234521537577 | validation: 0.4831910685467631]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37863005019119106		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.37863005019119106 | validation: 0.36443698828352206]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.435240993308374		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.435240993308374 | validation: 0.5328061278170717]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4165633049482058		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4165633049482058 | validation: 0.34855355582873665]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3225838702390581		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3225838702390581 | validation: 0.386359149274522]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28959821550148546		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.28959821550148546 | validation: 0.3903636727407343]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2944891195369692		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.2944891195369692 | validation: 0.3513311535785695]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30058424412234686		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.30058424412234686 | validation: 0.4390378678928304]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3235682876818731		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.3235682876818731 | validation: 0.33186823317116343]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319317438804209		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.4319317438804209 | validation: 0.5376191874675446]
	TIME [epoch: 1.37 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380463207654289		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.4380463207654289 | validation: 0.3259086510445818]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36307626033606566		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.36307626033606566 | validation: 0.4095641807013247]
	TIME [epoch: 1.37 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30812372350022316		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.30812372350022316 | validation: 0.33490401214004545]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30047712682722943		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.30047712682722943 | validation: 0.4020037579658093]
	TIME [epoch: 1.37 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30517211215367546		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.30517211215367546 | validation: 0.3384583305792672]
	TIME [epoch: 1.37 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31511548150024576		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.31511548150024576 | validation: 0.43337551710989763]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3177756132573944		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.3177756132573944 | validation: 0.3095371685420814]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34013317142455984		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.34013317142455984 | validation: 0.5240302566499223]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40023155079602846		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.40023155079602846 | validation: 0.31979981436996563]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37964733905747566		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.37964733905747566 | validation: 0.4417009712627321]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31935364259690213		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.31935364259690213 | validation: 0.32372296722596483]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2882369160178302		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.2882369160178302 | validation: 0.3873062734887008]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28643202761620573		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.28643202761620573 | validation: 0.3195458091048038]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27399422796256717		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.27399422796256717 | validation: 0.4012419913959077]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28835853580626764		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.28835853580626764 | validation: 0.3125417972287203]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227259380928622		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.3227259380928622 | validation: 0.46246189938051385]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35431187813554377		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.35431187813554377 | validation: 0.3028238261860878]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34526595780808095		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.34526595780808095 | validation: 0.4342962732687358]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3126393664555985		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.3126393664555985 | validation: 0.308923710813274]
	TIME [epoch: 1.37 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28617622014501576		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.28617622014501576 | validation: 0.3975674458616956]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690615624242738		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2690615624242738 | validation: 0.3085437011294418]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26432224305948543		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.26432224305948543 | validation: 0.39049298778450636]
	TIME [epoch: 1.39 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2602092788298628		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.2602092788298628 | validation: 0.31868864583162626]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2554397867337559		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.2554397867337559 | validation: 0.3854900135396783]
	TIME [epoch: 1.39 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27060956298212496		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.27060956298212496 | validation: 0.3009155104227374]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2892525728012648		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.2892525728012648 | validation: 0.45053396776633026]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34295522512203974		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.34295522512203974 | validation: 0.30300543979158123]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4064922355207652		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.4064922355207652 | validation: 0.3746310512378738]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2619777901886993		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2619777901886993 | validation: 0.32405624030541347]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23860138237815073		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.23860138237815073 | validation: 0.3209080697939099]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23066858007924032		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.23066858007924032 | validation: 0.335604319072625]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23126440185232383		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.23126440185232383 | validation: 0.30991549945200014]
	TIME [epoch: 1.37 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271055517749584		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2271055517749584 | validation: 0.3827047258766009]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2510267058439522		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.2510267058439522 | validation: 0.28477409892397276]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34375986487882354		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.34375986487882354 | validation: 0.5259968452075695]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4186821187970232		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.4186821187970232 | validation: 0.2888422612213921]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3073712539931765		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.3073712539931765 | validation: 0.3511383744045273]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25992867238018863		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.25992867238018863 | validation: 0.34200864457965485]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2246874956926229		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.2246874956926229 | validation: 0.30042199907791484]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180661326282416		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.2180661326282416 | validation: 0.3336379930914878]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22124040687725977		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.22124040687725977 | validation: 0.27766090082660755]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23973770878984432		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.23973770878984432 | validation: 0.42297742991306375]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2908933280202859		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2908933280202859 | validation: 0.27639751246702143]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37736189078990123		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.37736189078990123 | validation: 0.36984673871626805]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24802327806311142		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.24802327806311142 | validation: 0.2717309157327553]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2344216860794721		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2344216860794721 | validation: 0.3681245166472866]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22592703727030042		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.22592703727030042 | validation: 0.2729704009506642]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27329314868789284		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.27329314868789284 | validation: 0.40110174841719365]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2772878787730491		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2772878787730491 | validation: 0.2649118802498526]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27747205744473663		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.27747205744473663 | validation: 0.3844317969012154]
	TIME [epoch: 1.38 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24536489177795046		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.24536489177795046 | validation: 0.27538869025949414]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23105266182719092		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.23105266182719092 | validation: 0.35743418135545824]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23409479766468808		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.23409479766468808 | validation: 0.274602363224284]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2406117277037776		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.2406117277037776 | validation: 0.3922784595148734]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25932239789416933		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.25932239789416933 | validation: 0.2547473989766907]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806972475703897		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.2806972475703897 | validation: 0.35489835589966184]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23913104748905567		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.23913104748905567 | validation: 0.2753315421704442]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21532192626259167		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.21532192626259167 | validation: 0.3101293415289459]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2097789041622945		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.2097789041622945 | validation: 0.30377957467987227]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.197768366083419		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.197768366083419 | validation: 0.2929198114406767]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19308401279674775		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.19308401279674775 | validation: 0.31320418083692225]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19115377851906792		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.19115377851906792 | validation: 0.26196584644592774]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19889251934440516		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.19889251934440516 | validation: 0.38211391390977256]
	TIME [epoch: 1.38 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23827468655335574		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.23827468655335574 | validation: 0.24460420367887525]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142021209122749		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.3142021209122749 | validation: 0.3897812884420526]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775792796705726		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.2775792796705726 | validation: 0.23943072313851715]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2458292811699539		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.2458292811699539 | validation: 0.31482425718274654]
	TIME [epoch: 1.38 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20080655129123245		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.20080655129123245 | validation: 0.26763069379977494]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19875436368216895		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.19875436368216895 | validation: 0.3107692451244203]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19599820100050525		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.19599820100050525 | validation: 0.2653875924585642]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19630473679244817		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.19630473679244817 | validation: 0.3312716005274579]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19833777592504354		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.19833777592504354 | validation: 0.2458911105831461]
	TIME [epoch: 1.38 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24336784790398272		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.24336784790398272 | validation: 0.38358043235629447]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25747640643406405		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.25747640643406405 | validation: 0.23745604714586932]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30413282275110387		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.30413282275110387 | validation: 0.4138557963067898]
	TIME [epoch: 1.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572237690321578		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2572237690321578 | validation: 0.2511396582900824]
	TIME [epoch: 1.37 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17844922650273531		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.17844922650273531 | validation: 0.27749850880144594]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17156385571715613		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.17156385571715613 | validation: 0.302130769092612]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17762180717060339		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.17762180717060339 | validation: 0.2705188435147206]
	TIME [epoch: 176 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17956243175653763		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.17956243175653763 | validation: 0.28593753766182156]
	TIME [epoch: 2.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17928702425863724		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.17928702425863724 | validation: 0.2728402845664906]
	TIME [epoch: 2.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800880603928116		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1800880603928116 | validation: 0.28196051867939254]
	TIME [epoch: 2.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18141117676820187		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.18141117676820187 | validation: 0.2714940849884186]
	TIME [epoch: 2.71 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17621323022839278		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.17621323022839278 | validation: 0.24635368852528636]
	TIME [epoch: 2.71 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831898069148228		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.1831898069148228 | validation: 0.34191826237743905]
	TIME [epoch: 2.71 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23056183430731816		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.23056183430731816 | validation: 0.227502149059717]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3090072124573522		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.3090072124573522 | validation: 0.41146020187925186]
	TIME [epoch: 2.71 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28555109725272687		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.28555109725272687 | validation: 0.22969863998621887]
	TIME [epoch: 2.71 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19373152660810436		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.19373152660810436 | validation: 0.27216254296586184]
	TIME [epoch: 2.71 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17019412680116516		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.17019412680116516 | validation: 0.26666114365882676]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17448330550929064		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.17448330550929064 | validation: 0.2643936213745337]
	TIME [epoch: 2.71 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678780881120402		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1678780881120402 | validation: 0.2766270657715471]
	TIME [epoch: 2.71 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16637436948677078		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.16637436948677078 | validation: 0.25649421079565177]
	TIME [epoch: 2.71 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16602080504949435		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.16602080504949435 | validation: 0.3086809097356252]
	TIME [epoch: 2.71 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18910309507284367		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.18910309507284367 | validation: 0.22150500843977308]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25910128903565277		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.25910128903565277 | validation: 0.4003231126845515]
	TIME [epoch: 2.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929211698793228		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.2929211698793228 | validation: 0.21364810450982122]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22473516131369076		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.22473516131369076 | validation: 0.2686857657745072]
	TIME [epoch: 2.71 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15618262962554003		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.15618262962554003 | validation: 0.26809877568023505]
	TIME [epoch: 2.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15644408746107427		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.15644408746107427 | validation: 0.24606359293451813]
	TIME [epoch: 2.71 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560862075826557		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.1560862075826557 | validation: 0.29662200808609923]
	TIME [epoch: 2.71 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659996207501481		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1659996207501481 | validation: 0.21131019787834027]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1839793613425271		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.1839793613425271 | validation: 0.34137319909392594]
	TIME [epoch: 2.71 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22898200144002723		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.22898200144002723 | validation: 0.21142091941067753]
	TIME [epoch: 2.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2819001544213125		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.2819001544213125 | validation: 0.26850806447669984]
	TIME [epoch: 2.71 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14930636121396604		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.14930636121396604 | validation: 0.289569028250477]
	TIME [epoch: 2.71 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714859074350128		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1714859074350128 | validation: 0.20481176881907248]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22022109202553533		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.22022109202553533 | validation: 0.3018435256251888]
	TIME [epoch: 2.71 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18148186255676568		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.18148186255676568 | validation: 0.21322642673723724]
	TIME [epoch: 2.71 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17178243353566966		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.17178243353566966 | validation: 0.2873613480747258]
	TIME [epoch: 2.71 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16049219620598393		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.16049219620598393 | validation: 0.212586588077219]
	TIME [epoch: 2.71 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17050179952681516		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.17050179952681516 | validation: 0.3206067768346182]
	TIME [epoch: 2.71 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19800427119704694		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.19800427119704694 | validation: 0.2122482885241578]
	TIME [epoch: 2.71 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.177940860761938		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.177940860761938 | validation: 0.2832671809356076]
	TIME [epoch: 2.71 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681410837164846		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.1681410837164846 | validation: 0.21105815902594138]
	TIME [epoch: 2.71 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15699194243824022		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.15699194243824022 | validation: 0.2567473789987697]
	TIME [epoch: 2.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559140160211845		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.14559140160211845 | validation: 0.21298247758624356]
	TIME [epoch: 2.71 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1521230181196324		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.1521230181196324 | validation: 0.30793235310622014]
	TIME [epoch: 2.71 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17642819439010038		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.17642819439010038 | validation: 0.19334959947639585]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23429521279203167		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.23429521279203167 | validation: 0.3178167432463004]
	TIME [epoch: 2.71 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20413253304461473		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.20413253304461473 | validation: 0.193418426021222]
	TIME [epoch: 2.71 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741722079702435		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.1741722079702435 | validation: 0.25616369110918763]
	TIME [epoch: 2.71 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14776394610082366		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.14776394610082366 | validation: 0.22641338444034056]
	TIME [epoch: 2.71 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15557599957797072		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.15557599957797072 | validation: 0.26520119184680796]
	TIME [epoch: 2.71 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15889767681715763		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.15889767681715763 | validation: 0.21596404332264144]
	TIME [epoch: 2.71 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15597980752426355		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.15597980752426355 | validation: 0.25263621558498217]
	TIME [epoch: 2.71 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14941429480624627		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.14941429480624627 | validation: 0.21220778793332282]
	TIME [epoch: 2.71 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1393033098151484		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.1393033098151484 | validation: 0.23893028913582226]
	TIME [epoch: 2.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329543108064652		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.1329543108064652 | validation: 0.21735779657013615]
	TIME [epoch: 2.71 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12983803679907394		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.12983803679907394 | validation: 0.22184046945326258]
	TIME [epoch: 2.71 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417324854097826		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.13417324854097826 | validation: 0.27040452545075777]
	TIME [epoch: 2.71 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15297948299254263		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.15297948299254263 | validation: 0.21466197485749722]
	TIME [epoch: 2.71 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20110947483863043		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.20110947483863043 | validation: 0.4086882389307987]
	TIME [epoch: 2.71 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303898190646308		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.303898190646308 | validation: 0.19815951332861004]
	TIME [epoch: 2.71 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535755142196846		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.2535755142196846 | validation: 0.2481571149982377]
	TIME [epoch: 2.71 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726683494056832		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1726683494056832 | validation: 0.29850532416208947]
	TIME [epoch: 2.71 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18578125625783506		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.18578125625783506 | validation: 0.18206122877791653]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2239477093591772		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.2239477093591772 | validation: 0.24654993374016687]
	TIME [epoch: 2.71 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13747028247642287		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.13747028247642287 | validation: 0.24111450880379637]
	TIME [epoch: 2.71 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305010925628335		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.1305010925628335 | validation: 0.1947166401463316]
	TIME [epoch: 2.71 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122692306668583		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.14122692306668583 | validation: 0.2575775502134469]
	TIME [epoch: 2.71 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1340312348930062		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1340312348930062 | validation: 0.21511013757696654]
	TIME [epoch: 2.71 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13059020483902603		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.13059020483902603 | validation: 0.2534542398431988]
	TIME [epoch: 2.71 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13453388431816773		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.13453388431816773 | validation: 0.202632263155859]
	TIME [epoch: 2.71 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14156311517119866		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.14156311517119866 | validation: 0.2754459636875521]
	TIME [epoch: 2.71 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14742773734823517		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.14742773734823517 | validation: 0.17896237318411934]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1653653811898212		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1653653811898212 | validation: 0.30613987825489325]
	TIME [epoch: 2.71 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1720552068985333		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1720552068985333 | validation: 0.18003276364651316]
	TIME [epoch: 2.71 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825324480196142		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.1825324480196142 | validation: 0.2680880362698325]
	TIME [epoch: 2.71 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15342875984558046		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.15342875984558046 | validation: 0.1854159617838634]
	TIME [epoch: 2.71 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13594234433803398		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.13594234433803398 | validation: 0.24372642208762196]
	TIME [epoch: 2.71 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12347549624096903		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.12347549624096903 | validation: 0.19094411402724276]
	TIME [epoch: 2.71 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127728022264913		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.127728022264913 | validation: 0.24950166855480813]
	TIME [epoch: 2.71 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1324018894766085		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.1324018894766085 | validation: 0.17852649875790685]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441370833103288		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.1441370833103288 | validation: 0.2698881487293668]
	TIME [epoch: 2.71 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538432269047477		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.1538432269047477 | validation: 0.16545917693186274]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620221661961042		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.1620221661961042 | validation: 0.2575836985256887]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402924759590029		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1402924759590029 | validation: 0.1744368942140031]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13886730268514894		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.13886730268514894 | validation: 0.2556257576727851]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14365688392687612		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.14365688392687612 | validation: 0.19802699298949708]
	TIME [epoch: 2.74 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1733809365522795		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.1733809365522795 | validation: 0.3228847205066635]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20027786559033162		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.20027786559033162 | validation: 0.1857737307864622]
	TIME [epoch: 2.72 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12489008680302145		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.12489008680302145 | validation: 0.23818210103826473]
	TIME [epoch: 2.71 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1509348258872089		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1509348258872089 | validation: 0.19261756217365447]
	TIME [epoch: 2.71 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567486263981494		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.14567486263981494 | validation: 0.2222993671006952]
	TIME [epoch: 2.71 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12777070533911766		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.12777070533911766 | validation: 0.17526118252702802]
	TIME [epoch: 2.71 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12784811777007912		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.12784811777007912 | validation: 0.2436146597542103]
	TIME [epoch: 2.71 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13582834956184783		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.13582834956184783 | validation: 0.17288969410739335]
	TIME [epoch: 2.71 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15421837543925346		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.15421837543925346 | validation: 0.2802478857060224]
	TIME [epoch: 2.71 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16586361364135283		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.16586361364135283 | validation: 0.1649114755950071]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16103419522251655		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.16103419522251655 | validation: 0.23883916316274761]
	TIME [epoch: 2.74 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12777359351688095		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.12777359351688095 | validation: 0.17506361492599876]
	TIME [epoch: 2.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11763348120071389		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.11763348120071389 | validation: 0.22399453051640142]
	TIME [epoch: 2.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448678256390611		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.11448678256390611 | validation: 0.18514714576715174]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11379460894905939		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.11379460894905939 | validation: 0.23014888404788014]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13111632339986393		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.13111632339986393 | validation: 0.16508387026510485]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15893531907771016		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.15893531907771016 | validation: 0.22786282861204593]
	TIME [epoch: 2.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388429963662391		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.1388429963662391 | validation: 0.16494128425659704]
	TIME [epoch: 2.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1191928086451053		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.1191928086451053 | validation: 0.21519946611719953]
	TIME [epoch: 2.72 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11381247818994578		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.11381247818994578 | validation: 0.1748217638677414]
	TIME [epoch: 2.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12855534317806813		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.12855534317806813 | validation: 0.2932250163276377]
	TIME [epoch: 2.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19108530258343973		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.19108530258343973 | validation: 0.15944322106631847]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20817969239165596		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.20817969239165596 | validation: 0.2188172872155998]
	TIME [epoch: 2.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11995559469026286		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.11995559469026286 | validation: 0.19122061693230907]
	TIME [epoch: 2.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10518307142137896		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.10518307142137896 | validation: 0.1731023190376896]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1100215996515113		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1100215996515113 | validation: 0.23101032121860243]
	TIME [epoch: 2.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11470611862515202		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.11470611862515202 | validation: 0.16405146639588689]
	TIME [epoch: 2.72 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12141429583566034		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.12141429583566034 | validation: 0.24863395339285177]
	TIME [epoch: 2.72 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13486094135572496		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.13486094135572496 | validation: 0.17144595575122604]
	TIME [epoch: 2.71 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13169739998475577		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.13169739998475577 | validation: 0.2480723811030135]
	TIME [epoch: 2.71 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14082060446356334		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.14082060446356334 | validation: 0.15504509209519154]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13444777718633497		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.13444777718633497 | validation: 0.23004424403171947]
	TIME [epoch: 2.74 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441505711162173		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.1441505711162173 | validation: 0.16277409390847164]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14585326710858892		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.14585326710858892 | validation: 0.20456776941919302]
	TIME [epoch: 2.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1176031039352504		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.1176031039352504 | validation: 0.1971106214192564]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304312663528183		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11304312663528183 | validation: 0.1498659488630891]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1242910844530751		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.1242910844530751 | validation: 0.23614552005349268]
	TIME [epoch: 2.72 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13854674605578868		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.13854674605578868 | validation: 0.14897657850103438]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14944225349241336		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.14944225349241336 | validation: 0.2158272191044552]
	TIME [epoch: 2.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11401785736319087		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.11401785736319087 | validation: 0.17606059295064497]
	TIME [epoch: 2.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11295199103373936		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.11295199103373936 | validation: 0.2072624769905147]
	TIME [epoch: 2.71 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12833800829112987		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.12833800829112987 | validation: 0.18359616074393112]
	TIME [epoch: 2.71 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144698891447462		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.1144698891447462 | validation: 0.18823182350940892]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10510580897325779		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.10510580897325779 | validation: 0.19191477862828177]
	TIME [epoch: 2.72 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021653007509105		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.1021653007509105 | validation: 0.1699740858952043]
	TIME [epoch: 2.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10122489652822779		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10122489652822779 | validation: 0.19814593148480286]
	TIME [epoch: 2.72 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10899482930023609		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.10899482930023609 | validation: 0.14162453494924726]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14164193061565786		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.14164193061565786 | validation: 0.29014263973352433]
	TIME [epoch: 2.72 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2011726448365705		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.2011726448365705 | validation: 0.14576104281625094]
	TIME [epoch: 2.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160255836443988		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.160255836443988 | validation: 0.1916708185383819]
	TIME [epoch: 2.72 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103290419778894		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.103290419778894 | validation: 0.1834378019015129]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1011325272656567		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.1011325272656567 | validation: 0.15958168856274849]
	TIME [epoch: 2.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11201135272266498		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.11201135272266498 | validation: 0.21345499912591903]
	TIME [epoch: 2.72 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12172445589463021		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.12172445589463021 | validation: 0.17176424964262102]
	TIME [epoch: 2.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11218113580301242		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.11218113580301242 | validation: 0.1808509930797645]
	TIME [epoch: 2.71 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688690472537996		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.10688690472537996 | validation: 0.18996394239187986]
	TIME [epoch: 2.71 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10756419930153008		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10756419930153008 | validation: 0.14838300220704123]
	TIME [epoch: 2.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11217571563361667		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.11217571563361667 | validation: 0.21604118487683951]
	TIME [epoch: 2.71 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12368040185781247		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.12368040185781247 | validation: 0.13511885085125686]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14274498439295166		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.14274498439295166 | validation: 0.23288868952443212]
	TIME [epoch: 2.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13938599501522753		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.13938599501522753 | validation: 0.14817177386521319]
	TIME [epoch: 2.74 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11550954458369951		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.11550954458369951 | validation: 0.21016649702580834]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10830230460403302		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.10830230460403302 | validation: 0.13900892710689725]
	TIME [epoch: 2.74 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10670491927182503		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.10670491927182503 | validation: 0.19879825195796658]
	TIME [epoch: 2.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10970265458904414		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.10970265458904414 | validation: 0.15120197337223287]
	TIME [epoch: 2.73 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131276362390442		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.1131276362390442 | validation: 0.21089246258200914]
	TIME [epoch: 2.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12223167167573602		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.12223167167573602 | validation: 0.1374460618723617]
	TIME [epoch: 2.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126633577194019		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.1126633577194019 | validation: 0.1993582823935617]
	TIME [epoch: 2.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10615919120780795		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.10615919120780795 | validation: 0.15801070699155273]
	TIME [epoch: 2.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11882329935385368		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.11882329935385368 | validation: 0.2418461613829572]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563463756979022		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.13563463756979022 | validation: 0.13133172180968308]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12514680399038183		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.12514680399038183 | validation: 0.20030170142510945]
	TIME [epoch: 2.71 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10099960793547533		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.10099960793547533 | validation: 0.1552207934232936]
	TIME [epoch: 2.71 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09499101021908263		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.09499101021908263 | validation: 0.1618751905778648]
	TIME [epoch: 2.71 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0938385476329057		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.0938385476329057 | validation: 0.17030204119666834]
	TIME [epoch: 2.71 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09903001787588543		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.09903001787588543 | validation: 0.16775723867044345]
	TIME [epoch: 2.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022600756121836		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.1022600756121836 | validation: 0.1719763076390456]
	TIME [epoch: 2.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042827974629856		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.1042827974629856 | validation: 0.1613178873586744]
	TIME [epoch: 2.71 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10569353946399364		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.10569353946399364 | validation: 0.20494456854945886]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712560840406668		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.11712560840406668 | validation: 0.12974264441472894]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13182710414508172		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.13182710414508172 | validation: 0.21254998634678596]
	TIME [epoch: 2.71 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12902477973496881		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.12902477973496881 | validation: 0.12706172981382577]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12902949288249027		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.12902949288249027 | validation: 0.17483666250063573]
	TIME [epoch: 2.72 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750428505838904		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.09750428505838904 | validation: 0.1434228274738347]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935157599835424		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.0935157599835424 | validation: 0.1528661100287022]
	TIME [epoch: 2.72 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08987483392584525		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08987483392584525 | validation: 0.16948801596826402]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615432589567253		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.09615432589567253 | validation: 0.1630646597385222]
	TIME [epoch: 2.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10936723173707406		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.10936723173707406 | validation: 0.189985744250726]
	TIME [epoch: 2.71 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466733893869954		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.11466733893869954 | validation: 0.12823925521763907]
	TIME [epoch: 2.71 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10467372816605792		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.10467372816605792 | validation: 0.19765915413292806]
	TIME [epoch: 2.71 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11453401938244273		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.11453401938244273 | validation: 0.12955068554289104]
	TIME [epoch: 2.71 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444970102265435		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.1444970102265435 | validation: 0.22139670545077905]
	TIME [epoch: 2.71 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13721352800759234		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.13721352800759234 | validation: 0.1304930089522965]
	TIME [epoch: 2.71 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09316462788626698		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09316462788626698 | validation: 0.14757309405446492]
	TIME [epoch: 2.71 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181709235452014		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.09181709235452014 | validation: 0.18322628518652323]
	TIME [epoch: 2.71 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10605531427699451		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.10605531427699451 | validation: 0.1435215169302319]
	TIME [epoch: 2.72 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10190889022333853		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.10190889022333853 | validation: 0.17755089114742462]
	TIME [epoch: 2.71 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09831590349627009		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.09831590349627009 | validation: 0.1329625605630562]
	TIME [epoch: 2.71 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917335821410648		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.0917335821410648 | validation: 0.1799363080873785]
	TIME [epoch: 2.71 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09885776980445238		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09885776980445238 | validation: 0.11652818624523059]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101763771007718		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.11101763771007718 | validation: 0.17664932513781406]
	TIME [epoch: 2.74 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346803271851161		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.10346803271851161 | validation: 0.1259311367659212]
	TIME [epoch: 2.74 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10464315388510384		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10464315388510384 | validation: 0.1741423910228715]
	TIME [epoch: 2.74 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09814938879949309		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.09814938879949309 | validation: 0.12953267224846146]
	TIME [epoch: 2.73 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09853682286263907		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.09853682286263907 | validation: 0.15667225707582735]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850674194465568		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.0850674194465568 | validation: 0.15463048915432187]
	TIME [epoch: 2.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587806925547117		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.08587806925547117 | validation: 0.12642385608537254]
	TIME [epoch: 2.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09295187260259832		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09295187260259832 | validation: 0.20577214814659187]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418747917100683		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.11418747917100683 | validation: 0.1423798181869491]
	TIME [epoch: 2.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17029887325603965		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.17029887325603965 | validation: 0.22193410161331173]
	TIME [epoch: 2.74 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1417018439434704		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.1417018439434704 | validation: 0.15783471703514557]
	TIME [epoch: 2.73 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720420859988512		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.08720420859988512 | validation: 0.14111364134319468]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11940014312605224		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.11940014312605224 | validation: 0.1946230105315924]
	TIME [epoch: 2.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11433303968429223		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.11433303968429223 | validation: 0.13119693213152628]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.090773150728411		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.090773150728411 | validation: 0.15918488949027387]
	TIME [epoch: 2.73 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09519463419703003		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.09519463419703003 | validation: 0.13993836535587786]
	TIME [epoch: 2.73 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883662907557996		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.0883662907557996 | validation: 0.151187191789457]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08700661758458023		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.08700661758458023 | validation: 0.13486595143539692]
	TIME [epoch: 2.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08581436638382649		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.08581436638382649 | validation: 0.16157559990854092]
	TIME [epoch: 2.71 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09130181976369386		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.09130181976369386 | validation: 0.1139906827791609]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473784567387629		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.09473784567387629 | validation: 0.17309895911187106]
	TIME [epoch: 2.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750498405730046		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09750498405730046 | validation: 0.12216704110101949]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888472149707064		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09888472149707064 | validation: 0.174098776197413]
	TIME [epoch: 2.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09420362026949942		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.09420362026949942 | validation: 0.12056407862012729]
	TIME [epoch: 2.72 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09628938148868152		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.09628938148868152 | validation: 0.16464913306453482]
	TIME [epoch: 2.73 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0923788583794261		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0923788583794261 | validation: 0.13522609575221556]
	TIME [epoch: 2.72 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919024501202989		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.0919024501202989 | validation: 0.16676914974627505]
	TIME [epoch: 2.72 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10347416543500028		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.10347416543500028 | validation: 0.1382764533324617]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0974007081326782		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.0974007081326782 | validation: 0.15509389497477125]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09023665271968892		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.09023665271968892 | validation: 0.12301639659778565]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08896776491143582		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.08896776491143582 | validation: 0.16765792330189244]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09482811907119448		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.09482811907119448 | validation: 0.11147796074151245]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10164788142611254		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.10164788142611254 | validation: 0.16980837115751035]
	TIME [epoch: 2.72 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954524125081101		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.0954524125081101 | validation: 0.1157407066174304]
	TIME [epoch: 2.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08731012827985231		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.08731012827985231 | validation: 0.15395051350504774]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846595252631436		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0846595252631436 | validation: 0.11374168441321225]
	TIME [epoch: 2.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08502792119359671		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.08502792119359671 | validation: 0.13931576852752095]
	TIME [epoch: 2.73 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08127704617788375		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.08127704617788375 | validation: 0.1253369792421887]
	TIME [epoch: 2.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08153618580265536		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.08153618580265536 | validation: 0.14897262784370563]
	TIME [epoch: 2.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08673832237588658		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.08673832237588658 | validation: 0.12498249934294879]
	TIME [epoch: 2.72 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603745065593074		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.09603745065593074 | validation: 0.16688266042885874]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10839093430393876		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.10839093430393876 | validation: 0.13952503760584656]
	TIME [epoch: 2.72 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09399966506056803		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.09399966506056803 | validation: 0.13585484693286984]
	TIME [epoch: 2.72 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698050093156193		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.07698050093156193 | validation: 0.13375887266806488]
	TIME [epoch: 2.72 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07796047698540026		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.07796047698540026 | validation: 0.13837096158810827]
	TIME [epoch: 2.72 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08333768673119182		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.08333768673119182 | validation: 0.1300850813794215]
	TIME [epoch: 2.72 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08585847857069615		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.08585847857069615 | validation: 0.19024818590262715]
	TIME [epoch: 2.72 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903848430004804		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.11903848430004804 | validation: 0.11760942210283756]
	TIME [epoch: 2.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1625258111955125		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.1625258111955125 | validation: 0.1572860003840314]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08925234530561822		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.08925234530561822 | validation: 0.13950231676496194]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930750664197068		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.07930750664197068 | validation: 0.11707981053835322]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0790705191297963		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.0790705191297963 | validation: 0.15214536751362762]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436924607675035		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.08436924607675035 | validation: 0.11644646954100282]
	TIME [epoch: 2.72 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603926519126125		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.08603926519126125 | validation: 0.16295556692946034]
	TIME [epoch: 2.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09696467015053191		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.09696467015053191 | validation: 0.11833982455425132]
	TIME [epoch: 2.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08837146656986686		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.08837146656986686 | validation: 0.16107713453354636]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08988366869181512		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.08988366869181512 | validation: 0.1080272822830513]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308943823987927		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.09308943823987927 | validation: 0.14623418650320832]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09417822111699192		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.09417822111699192 | validation: 0.11009894295087491]
	TIME [epoch: 2.72 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840860150192135		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0840860150192135 | validation: 0.13352256096407852]
	TIME [epoch: 2.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07772660868918886		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07772660868918886 | validation: 0.13293081336053272]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780351546041417		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.0780351546041417 | validation: 0.10761967335899421]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0820592326908138		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.0820592326908138 | validation: 0.16722090153162777]
	TIME [epoch: 2.73 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985751002942681		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.0985751002942681 | validation: 0.10448499428220817]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11511397003789604		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.11511397003789604 | validation: 0.18260679520814913]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09992511052778756		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.09992511052778756 | validation: 0.11651469649399149]
	TIME [epoch: 2.73 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07541144502680908		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.07541144502680908 | validation: 0.11209424456726463]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07767895373892612		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07767895373892612 | validation: 0.13854965912967185]
	TIME [epoch: 2.73 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08003597319186602		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.08003597319186602 | validation: 0.1307931366803042]
	TIME [epoch: 2.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07898714737707756		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07898714737707756 | validation: 0.12644115169482112]
	TIME [epoch: 2.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07887403357650125		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.07887403357650125 | validation: 0.11696517099832984]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07440758661157858		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.07440758661157858 | validation: 0.12799762081697846]
	TIME [epoch: 2.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07511966353908825		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.07511966353908825 | validation: 0.10183277865210832]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798381456420793		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0798381456420793 | validation: 0.15974566006384247]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09351179414702554		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.09351179414702554 | validation: 0.09331036140412267]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10415058637489182		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.10415058637489182 | validation: 0.14360769364630963]
	TIME [epoch: 2.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08822903112633981		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.08822903112633981 | validation: 0.10228018206480075]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08225320837607132		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.08225320837607132 | validation: 0.13090226012653436]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07907079168375462		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07907079168375462 | validation: 0.1149063955167974]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087666692304286		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.09087666692304286 | validation: 0.14407474456790606]
	TIME [epoch: 2.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816637796500505		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.08816637796500505 | validation: 0.11663026250635876]
	TIME [epoch: 2.73 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0785047946152586		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0785047946152586 | validation: 0.11987334368314922]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741543738361576		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.0741543738361576 | validation: 0.12248047200590913]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308150104153875		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.07308150104153875 | validation: 0.11043141787775612]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07761775974787861		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.07761775974787861 | validation: 0.14860927009109406]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175572770773997		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.09175572770773997 | validation: 0.10832559510229026]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034985900327296		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.08034985900327296 | validation: 0.1328553880770779]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0823559096997116		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0823559096997116 | validation: 0.0962924989101043]
	TIME [epoch: 2.72 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08874651084613909		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.08874651084613909 | validation: 0.1573991668491056]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09060900573560687		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.09060900573560687 | validation: 0.0899141732941327]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09505890471753653		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.09505890471753653 | validation: 0.13330034865323953]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527518162567008		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.07527518162567008 | validation: 0.10199574679546762]
	TIME [epoch: 2.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142334398867208		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.07142334398867208 | validation: 0.12736018009223823]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299701746390169		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.07299701746390169 | validation: 0.10617657596689185]
	TIME [epoch: 2.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07405792382298204		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.07405792382298204 | validation: 0.13618618576452204]
	TIME [epoch: 2.72 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08184173225278574		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08184173225278574 | validation: 0.1059341073493863]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09501685149086102		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.09501685149086102 | validation: 0.16558625182720876]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10264035455390143		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.10264035455390143 | validation: 0.1012745552028534]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07509035827593585		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.07509035827593585 | validation: 0.11846828260965499]
	TIME [epoch: 2.72 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07547904333600006		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.07547904333600006 | validation: 0.1331069732274503]
	TIME [epoch: 2.73 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0853201067293256		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.0853201067293256 | validation: 0.11333288686819672]
	TIME [epoch: 2.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770656034683293		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.07770656034683293 | validation: 0.13192041798419218]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07334680839186501		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.07334680839186501 | validation: 0.0981227031712921]
	TIME [epoch: 2.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435518638685702		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.07435518638685702 | validation: 0.1333228625888023]
	TIME [epoch: 2.72 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575480303643081		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.07575480303643081 | validation: 0.08978674200388376]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213197077203468		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.08213197077203468 | validation: 0.1227107637084405]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387308308742062		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.07387308308742062 | validation: 0.09962466152916159]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07246536057077684		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.07246536057077684 | validation: 0.11763499584615666]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0730777003253351		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.0730777003253351 | validation: 0.1004298486350796]
	TIME [epoch: 2.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665101182640383		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.07665101182640383 | validation: 0.15244677216849015]
	TIME [epoch: 2.72 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510431685977974		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.09510431685977974 | validation: 0.10484509377689194]
	TIME [epoch: 2.73 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07535508391256508		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.07535508391256508 | validation: 0.12094394535580988]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709836793675867		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.0709836793675867 | validation: 0.09232122070712045]
	TIME [epoch: 2.73 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478322449816922		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.07478322449816922 | validation: 0.1395085805149737]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08431546405988602		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.08431546405988602 | validation: 0.09344205068206272]
	TIME [epoch: 2.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09311187969401095		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.09311187969401095 | validation: 0.12118785938095838]
	TIME [epoch: 2.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06932487648604937		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.06932487648604937 | validation: 0.11174035683322905]
	TIME [epoch: 2.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06953383978488085		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.06953383978488085 | validation: 0.1061546662911777]
	TIME [epoch: 2.73 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07257216168740721		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.07257216168740721 | validation: 0.11019795778563896]
	TIME [epoch: 2.72 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350482383867349		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.07350482383867349 | validation: 0.11968429366327898]
	TIME [epoch: 2.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339189553615702		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.07339189553615702 | validation: 0.09271311587541407]
	TIME [epoch: 2.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08108590011172057		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08108590011172057 | validation: 0.14719682179018606]
	TIME [epoch: 2.73 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918029722806083		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.0918029722806083 | validation: 0.0876918130302703]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08875214219402532		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08875214219402532 | validation: 0.11543758345033542]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439652813434139		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.07439652813434139 | validation: 0.10261155411856432]
	TIME [epoch: 2.72 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06966217239855499		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.06966217239855499 | validation: 0.09043205185333605]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06658343004662692		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.06658343004662692 | validation: 0.13102742897647565]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348028947493504		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.07348028947493504 | validation: 0.09143562189370219]
	TIME [epoch: 2.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07685371362952875		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.07685371362952875 | validation: 0.12108569859180013]
	TIME [epoch: 2.72 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527338325975717		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.07527338325975717 | validation: 0.09468645889816626]
	TIME [epoch: 2.72 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255404932741506		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.07255404932741506 | validation: 0.12214554141049874]
	TIME [epoch: 2.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917732399872366		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.06917732399872366 | validation: 0.09047879488402125]
	TIME [epoch: 2.72 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07160489977578764		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.07160489977578764 | validation: 0.12820948447951466]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07921520210231588		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.07921520210231588 | validation: 0.0955784469417981]
	TIME [epoch: 2.72 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014543299005485		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.08014543299005485 | validation: 0.12870318785577567]
	TIME [epoch: 2.74 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976809592533705		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.07976809592533705 | validation: 0.10105409001675643]
	TIME [epoch: 2.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799770687106975		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.06799770687106975 | validation: 0.10363339291713225]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701272974005315		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.06701272974005315 | validation: 0.08858071511698362]
	TIME [epoch: 2.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837447010097027		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.06837447010097027 | validation: 0.12240326831065618]
	TIME [epoch: 2.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0781114873629297		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.0781114873629297 | validation: 0.08658680803779094]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_822.pth
	Model improved!!!
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906074490024981		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.07906074490024981 | validation: 0.11957992201211223]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447302000748286		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.07447302000748286 | validation: 0.10385800614862672]
	TIME [epoch: 2.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791567065742017		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.06791567065742017 | validation: 0.10059931279462174]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469557266153		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.06469557266153 | validation: 0.11710398920849947]
	TIME [epoch: 2.72 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068003442211594		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.068003442211594 | validation: 0.08167984588920431]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777138906570133		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.0777138906570133 | validation: 0.14098913612194172]
	TIME [epoch: 2.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08260556345007541		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.08260556345007541 | validation: 0.0872987500141757]
	TIME [epoch: 2.72 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07761860769594091		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.07761860769594091 | validation: 0.1097642450798905]
	TIME [epoch: 2.72 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07011932124890237		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.07011932124890237 | validation: 0.10497570543304446]
	TIME [epoch: 2.72 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073444881101282		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.07073444881101282 | validation: 0.09371173140964734]
	TIME [epoch: 2.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979073883333524		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.06979073883333524 | validation: 0.10331105243412386]
	TIME [epoch: 2.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06256411541054527		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.06256411541054527 | validation: 0.09734524958021638]
	TIME [epoch: 2.72 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462216461952294		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.06462216461952294 | validation: 0.10425129045900841]
	TIME [epoch: 2.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06829470120859064		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.06829470120859064 | validation: 0.09707185560731765]
	TIME [epoch: 2.72 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967125232829295		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.06967125232829295 | validation: 0.1175114172141618]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07828557631918227		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.07828557631918227 | validation: 0.09497461786220868]
	TIME [epoch: 2.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209187764633898		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.07209187764633898 | validation: 0.11807481052162988]
	TIME [epoch: 2.72 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263031447236325		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.07263031447236325 | validation: 0.0784365373188949]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515440541207294		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07515440541207294 | validation: 0.11546662054105689]
	TIME [epoch: 2.73 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07446939701192157		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07446939701192157 | validation: 0.08954517043443644]
	TIME [epoch: 2.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06885057348150761		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.06885057348150761 | validation: 0.09718109462566567]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06380591696921145		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.06380591696921145 | validation: 0.09591832079558782]
	TIME [epoch: 2.74 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475128355931889		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.06475128355931889 | validation: 0.09332823661299017]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209385988055135		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.06209385988055135 | validation: 0.09130904706422575]
	TIME [epoch: 2.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359159772235795		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.06359159772235795 | validation: 0.10038458149071056]
	TIME [epoch: 2.71 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458045654536979		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.06458045654536979 | validation: 0.08365250977125406]
	TIME [epoch: 2.72 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024216659729418		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.07024216659729418 | validation: 0.11775569322609702]
	TIME [epoch: 2.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07857096933057442		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07857096933057442 | validation: 0.08157732877213655]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09017501669794731		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.09017501669794731 | validation: 0.14871944383756097]
	TIME [epoch: 2.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623237236328012		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.09623237236328012 | validation: 0.09893801659657964]
	TIME [epoch: 2.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065595970323763		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.065595970323763 | validation: 0.08374788901815358]
	TIME [epoch: 2.72 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331170502443458		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.06331170502443458 | validation: 0.10238159602017752]
	TIME [epoch: 2.72 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06381687690232686		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.06381687690232686 | validation: 0.09931301707075435]
	TIME [epoch: 2.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060805870601821896		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.060805870601821896 | validation: 0.08160403812456402]
	TIME [epoch: 2.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455120722452681		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.06455120722452681 | validation: 0.10659768531317593]
	TIME [epoch: 2.72 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06683965508192578		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06683965508192578 | validation: 0.08557035017499304]
	TIME [epoch: 2.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06894405649840381		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.06894405649840381 | validation: 0.11018971116706106]
	TIME [epoch: 2.72 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528596406994416		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.06528596406994416 | validation: 0.07499142266515389]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412650141412166		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.07412650141412166 | validation: 0.11978456727462619]
	TIME [epoch: 2.73 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069358605123646		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07069358605123646 | validation: 0.0820252558179313]
	TIME [epoch: 2.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838900209095679		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.06838900209095679 | validation: 0.1081151082182999]
	TIME [epoch: 2.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728206357698097		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.06728206357698097 | validation: 0.07997623660736759]
	TIME [epoch: 2.74 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495388573791647		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.06495388573791647 | validation: 0.09951119642838521]
	TIME [epoch: 2.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281893099274295		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.06281893099274295 | validation: 0.08333392001102809]
	TIME [epoch: 2.74 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06523697760710516		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.06523697760710516 | validation: 0.11129424690256305]
	TIME [epoch: 2.74 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028855173882209		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.07028855173882209 | validation: 0.0824470814442365]
	TIME [epoch: 2.74 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0736305658927402		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.0736305658927402 | validation: 0.1167401205396025]
	TIME [epoch: 2.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048307461839853		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.07048307461839853 | validation: 0.08463672155224658]
	TIME [epoch: 2.74 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307380329395776		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.06307380329395776 | validation: 0.09555967277097961]
	TIME [epoch: 2.74 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06267783034237429		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.06267783034237429 | validation: 0.09402039437357602]
	TIME [epoch: 2.74 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06007918221370646		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.06007918221370646 | validation: 0.09049227174574005]
	TIME [epoch: 2.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441038349268773		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.06441038349268773 | validation: 0.09873404360399622]
	TIME [epoch: 2.74 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856463093973615		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.06856463093973615 | validation: 0.09311145377179811]
	TIME [epoch: 2.74 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06801754864338866		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.06801754864338866 | validation: 0.08523203908646847]
	TIME [epoch: 2.74 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06574107461610612		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.06574107461610612 | validation: 0.10217026938455645]
	TIME [epoch: 2.74 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06509950335035392		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.06509950335035392 | validation: 0.08163871008665688]
	TIME [epoch: 2.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06380979599295104		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.06380979599295104 | validation: 0.0928676998743021]
	TIME [epoch: 2.73 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0621766485168221		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.0621766485168221 | validation: 0.0841349761784646]
	TIME [epoch: 2.74 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0624550154409428		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.0624550154409428 | validation: 0.10368485196367545]
	TIME [epoch: 2.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454928907890291		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.06454928907890291 | validation: 0.06910196838200335]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311076074226013		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.07311076074226013 | validation: 0.12217003294563972]
	TIME [epoch: 2.73 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07355983102404436		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.07355983102404436 | validation: 0.0799886228486435]
	TIME [epoch: 2.73 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720164540432393		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.07720164540432393 | validation: 0.11317749153460246]
	TIME [epoch: 2.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725390262956538		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.0725390262956538 | validation: 0.08505757323214755]
	TIME [epoch: 2.73 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061514009867093745		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.061514009867093745 | validation: 0.08645564038262897]
	TIME [epoch: 2.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975908171097327		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.05975908171097327 | validation: 0.08745447866564132]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05978163423617381		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.05978163423617381 | validation: 0.08065938574812481]
	TIME [epoch: 2.73 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06081744377576356		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.06081744377576356 | validation: 0.09073657549361501]
	TIME [epoch: 2.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05907765626376056		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.05907765626376056 | validation: 0.11179630898640887]
	TIME [epoch: 2.73 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575590556367616		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06575590556367616 | validation: 0.07478833534456544]
	TIME [epoch: 2.73 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998218486663041		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.06998218486663041 | validation: 0.1090471116023803]
	TIME [epoch: 2.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557423635843345		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.07557423635843345 | validation: 0.09659504169171174]
	TIME [epoch: 2.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227188387528746		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.06227188387528746 | validation: 0.07838717146221588]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06092594970989176		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.06092594970989176 | validation: 0.09975808958865112]
	TIME [epoch: 2.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0587386422243752		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.0587386422243752 | validation: 0.07725875188382907]
	TIME [epoch: 2.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06246321275854723		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.06246321275854723 | validation: 0.09446833069876533]
	TIME [epoch: 2.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281838324237789		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.06281838324237789 | validation: 0.07956271675282237]
	TIME [epoch: 2.73 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06610604236695135		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.06610604236695135 | validation: 0.11495350377558951]
	TIME [epoch: 2.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655465222146437		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.0655465222146437 | validation: 0.07327360826339009]
	TIME [epoch: 2.7 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06563282257597836		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.06563282257597836 | validation: 0.10267470669294959]
	TIME [epoch: 2.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06487931956932518		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.06487931956932518 | validation: 0.07763846215219695]
	TIME [epoch: 2.71 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06163657341260998		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.06163657341260998 | validation: 0.0787800750384862]
	TIME [epoch: 2.71 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05909185862398898		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.05909185862398898 | validation: 0.08486862311872806]
	TIME [epoch: 2.71 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917691089345894		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.05917691089345894 | validation: 0.0771255334446156]
	TIME [epoch: 2.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061185628693797345		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.061185628693797345 | validation: 0.10047268988753572]
	TIME [epoch: 2.71 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041411572292921		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.06041411572292921 | validation: 0.07188744905856727]
	TIME [epoch: 2.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06179061142378837		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.06179061142378837 | validation: 0.0974841536664039]
	TIME [epoch: 2.7 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0663706640817348		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.0663706640817348 | validation: 0.08167845475838657]
	TIME [epoch: 2.71 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366817275678761		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.07366817275678761 | validation: 0.10737441828756498]
	TIME [epoch: 2.71 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119653873569949		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.07119653873569949 | validation: 0.08465585666222329]
	TIME [epoch: 2.71 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06008207793483493		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.06008207793483493 | validation: 0.08758197485465935]
	TIME [epoch: 2.71 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06704585137205113		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06704585137205113 | validation: 0.09300338710528981]
	TIME [epoch: 2.71 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06180612495511767		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.06180612495511767 | validation: 0.08462800309713803]
	TIME [epoch: 2.7 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714896501669323		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.05714896501669323 | validation: 0.08653259893793942]
	TIME [epoch: 2.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059072286146624194		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.059072286146624194 | validation: 0.08609408008409938]
	TIME [epoch: 2.7 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131540234739784		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.06131540234739784 | validation: 0.08692017778549786]
	TIME [epoch: 2.7 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746776192413415		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.05746776192413415 | validation: 0.08478492774841591]
	TIME [epoch: 2.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05593944363202656		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.05593944363202656 | validation: 0.07700097896614713]
	TIME [epoch: 2.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060211994790654685		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.060211994790654685 | validation: 0.08984390012276912]
	TIME [epoch: 2.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05880476331161069		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.05880476331161069 | validation: 0.06951553092927885]
	TIME [epoch: 2.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06284893145549227		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06284893145549227 | validation: 0.11811773254798844]
	TIME [epoch: 2.71 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0734146593543813		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.0734146593543813 | validation: 0.06811314814432232]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412746244011814		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.07412746244011814 | validation: 0.09451644905107691]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06254106206721333		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.06254106206721333 | validation: 0.09229019562250573]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810461473881941		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.05810461473881941 | validation: 0.0729741166085265]
	TIME [epoch: 2.71 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056964102576757124		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.056964102576757124 | validation: 0.08670484968289036]
	TIME [epoch: 2.7 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058815660436498		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.058815660436498 | validation: 0.08458634948142625]
	TIME [epoch: 2.71 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797449728320421		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.05797449728320421 | validation: 0.08436190545157826]
	TIME [epoch: 2.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05888385686883091		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.05888385686883091 | validation: 0.08110620267906916]
	TIME [epoch: 2.71 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056092344329900054		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.056092344329900054 | validation: 0.0934101263598328]
	TIME [epoch: 2.71 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928232947187337		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.05928232947187337 | validation: 0.07271085047244137]
	TIME [epoch: 2.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261649899882507		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.06261649899882507 | validation: 0.09428690354539063]
	TIME [epoch: 2.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05826025074008071		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.05826025074008071 | validation: 0.07734930071825731]
	TIME [epoch: 2.71 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058011172272168406		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.058011172272168406 | validation: 0.0754015219942783]
	TIME [epoch: 2.71 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05518298257886908		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.05518298257886908 | validation: 0.07702503506686478]
	TIME [epoch: 2.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05496718912433351		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.05496718912433351 | validation: 0.08588593248062597]
	TIME [epoch: 2.71 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772728392960598		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.05772728392960598 | validation: 0.07357854617166279]
	TIME [epoch: 2.71 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057357931604863285		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.057357931604863285 | validation: 0.10275129192589208]
	TIME [epoch: 2.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475182300792366		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.06475182300792366 | validation: 0.07958122180852276]
	TIME [epoch: 2.71 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06943817656032993		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.06943817656032993 | validation: 0.11197651868597586]
	TIME [epoch: 2.71 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994521184106546		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.06994521184106546 | validation: 0.06995706003642062]
	TIME [epoch: 2.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793404034013281		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.05793404034013281 | validation: 0.08892947105434679]
	TIME [epoch: 2.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967077171495205		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.05967077171495205 | validation: 0.07445356181837345]
	TIME [epoch: 2.71 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0613863445702746		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.0613863445702746 | validation: 0.08073997000712617]
	TIME [epoch: 2.71 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05889460333226519		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05889460333226519 | validation: 0.08593501403685794]
	TIME [epoch: 2.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05874030601990067		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.05874030601990067 | validation: 0.07911603047270259]
	TIME [epoch: 2.71 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05569014070678801		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.05569014070678801 | validation: 0.08847976639087987]
	TIME [epoch: 2.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0576972181630479		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.0576972181630479 | validation: 0.07042085480119015]
	TIME [epoch: 2.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892676338479973		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.05892676338479973 | validation: 0.0844528460426589]
	TIME [epoch: 2.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056642637594480336		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.056642637594480336 | validation: 0.07428643922321872]
	TIME [epoch: 2.71 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05537222405141197		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.05537222405141197 | validation: 0.07957138123288347]
	TIME [epoch: 2.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05760482317703282		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.05760482317703282 | validation: 0.08986921108110266]
	TIME [epoch: 2.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060297284592577806		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.060297284592577806 | validation: 0.061666082855283216]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07395424466462297		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.07395424466462297 | validation: 0.09542733649171445]
	TIME [epoch: 2.72 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078028743568689		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.06078028743568689 | validation: 0.07949546401956142]
	TIME [epoch: 2.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053720057359724474		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.053720057359724474 | validation: 0.07344518523878883]
	TIME [epoch: 2.72 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05367760406697352		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.05367760406697352 | validation: 0.08222954108494034]
	TIME [epoch: 2.72 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05697682718649244		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.05697682718649244 | validation: 0.08204477544566742]
	TIME [epoch: 2.72 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538216174552465		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0538216174552465 | validation: 0.06692364850588434]
	TIME [epoch: 2.72 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950751953466167		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.05950751953466167 | validation: 0.08755519052737291]
	TIME [epoch: 2.72 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027842278321801		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.06027842278321801 | validation: 0.06954778326983796]
	TIME [epoch: 2.71 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05756640229821667		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.05756640229821667 | validation: 0.08550381592140094]
	TIME [epoch: 2.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055812452509311744		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.055812452509311744 | validation: 0.07084632899475611]
	TIME [epoch: 2.72 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05351889095844742		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.05351889095844742 | validation: 0.07829610103885659]
	TIME [epoch: 2.72 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053722106150650185		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.053722106150650185 | validation: 0.07170846615699115]
	TIME [epoch: 2.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05584893662443362		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.05584893662443362 | validation: 0.07733973501694015]
	TIME [epoch: 2.72 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05491826305480316		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.05491826305480316 | validation: 0.07675696839632727]
	TIME [epoch: 2.72 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057690994395290116		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.057690994395290116 | validation: 0.09707834508633431]
	TIME [epoch: 2.72 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512227545068809		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.06512227545068809 | validation: 0.07126939801910052]
	TIME [epoch: 2.72 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05592287278307575		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.05592287278307575 | validation: 0.07636528730923534]
	TIME [epoch: 2.72 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053514015756318		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.053514015756318 | validation: 0.06847317553033848]
	TIME [epoch: 2.72 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05590439797451712		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.05590439797451712 | validation: 0.08776022620820388]
	TIME [epoch: 2.72 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702836332569025		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.05702836332569025 | validation: 0.059111368392104985]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06238548747781808		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.06238548747781808 | validation: 0.09315480561516894]
	TIME [epoch: 2.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062449273798524205		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.062449273798524205 | validation: 0.07267640634206857]
	TIME [epoch: 2.72 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05555565311220617		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05555565311220617 | validation: 0.07606240952766892]
	TIME [epoch: 2.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05556339111509025		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.05556339111509025 | validation: 0.07726306703303731]
	TIME [epoch: 2.72 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05494483921038091		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.05494483921038091 | validation: 0.07918926345164468]
	TIME [epoch: 2.72 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055542321699627875		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.055542321699627875 | validation: 0.07467119115904695]
	TIME [epoch: 2.72 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055611557425367115		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.055611557425367115 | validation: 0.07190989507047789]
	TIME [epoch: 2.72 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05203453099696295		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.05203453099696295 | validation: 0.07782979226027656]
	TIME [epoch: 2.72 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05322278838133622		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.05322278838133622 | validation: 0.07320784948403089]
	TIME [epoch: 2.72 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05277189759771535		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.05277189759771535 | validation: 0.0909976187273277]
	TIME [epoch: 2.72 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05841199916018325		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.05841199916018325 | validation: 0.06324265770435337]
	TIME [epoch: 2.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057312888672271155		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.057312888672271155 | validation: 0.0801529766706636]
	TIME [epoch: 2.72 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055521854678655266		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.055521854678655266 | validation: 0.07039606696395795]
	TIME [epoch: 2.72 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627169036859652		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.05627169036859652 | validation: 0.08408038248816203]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055885940155565396		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.055885940155565396 | validation: 0.06430309423091275]
	TIME [epoch: 2.72 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649818369378417		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.05649818369378417 | validation: 0.08355249205407034]
	TIME [epoch: 2.72 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058411763189433384		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.058411763189433384 | validation: 0.06276440456465128]
	TIME [epoch: 2.72 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05278144483001592		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.05278144483001592 | validation: 0.07359900259093978]
	TIME [epoch: 2.72 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292300958933316		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.05292300958933316 | validation: 0.06834865723294507]
	TIME [epoch: 2.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052219081032777136		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.052219081032777136 | validation: 0.07455424719591573]
	TIME [epoch: 2.72 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05365332599166523		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.05365332599166523 | validation: 0.07827113876450127]
	TIME [epoch: 2.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054104550667802585		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.054104550667802585 | validation: 0.06864099188887823]
	TIME [epoch: 2.72 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05623979226534335		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.05623979226534335 | validation: 0.09375729616581858]
	TIME [epoch: 2.72 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06485379134476277		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.06485379134476277 | validation: 0.07494367175614704]
	TIME [epoch: 2.72 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654322956444933		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05654322956444933 | validation: 0.07541665715847484]
	TIME [epoch: 2.73 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053177688719147555		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.053177688719147555 | validation: 0.07239344325246154]
	TIME [epoch: 179 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077686006065862		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.05077686006065862 | validation: 0.06602004785555501]
	TIME [epoch: 5.83 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05312707944283784		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.05312707944283784 | validation: 0.07411398206434812]
	TIME [epoch: 5.81 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05147255275419388		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.05147255275419388 | validation: 0.06042028716123965]
	TIME [epoch: 5.81 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301780689990164		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.05301780689990164 | validation: 0.08205062676519016]
	TIME [epoch: 5.81 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053564784120239635		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.053564784120239635 | validation: 0.06789657413599125]
	TIME [epoch: 5.82 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05631517536312607		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.05631517536312607 | validation: 0.07121425048474302]
	TIME [epoch: 5.82 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05537883637814886		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.05537883637814886 | validation: 0.08249205884541265]
	TIME [epoch: 5.83 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05411275175308854		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.05411275175308854 | validation: 0.06640396700507914]
	TIME [epoch: 5.82 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202148027826017		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.05202148027826017 | validation: 0.07673374073664478]
	TIME [epoch: 5.81 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05105222733074449		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.05105222733074449 | validation: 0.07696465066040498]
	TIME [epoch: 5.82 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0517309159446332		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.0517309159446332 | validation: 0.06620533306016317]
	TIME [epoch: 5.81 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05006973769380163		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.05006973769380163 | validation: 0.08013762617732567]
	TIME [epoch: 5.81 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053747377248513147		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.053747377248513147 | validation: 0.05418396791625349]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1014.pth
	Model improved!!!
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668690637626942		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.05668690637626942 | validation: 0.08815303890202295]
	TIME [epoch: 5.82 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059772521066280185		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.059772521066280185 | validation: 0.0650053283943555]
	TIME [epoch: 5.82 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05943050182108119		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.05943050182108119 | validation: 0.06725143461048665]
	TIME [epoch: 5.83 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05268884096273605		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.05268884096273605 | validation: 0.07302905973682154]
	TIME [epoch: 5.82 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05223080493344225		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.05223080493344225 | validation: 0.06992117407679475]
	TIME [epoch: 5.82 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079921018014197		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.05079921018014197 | validation: 0.07274699552119153]
	TIME [epoch: 5.82 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05044175546576518		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.05044175546576518 | validation: 0.06528576740356588]
	TIME [epoch: 5.83 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248282167074425		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.05248282167074425 | validation: 0.0762337444854943]
	TIME [epoch: 5.82 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05271533761100909		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.05271533761100909 | validation: 0.06613292225328093]
	TIME [epoch: 5.83 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396049272969641		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.05396049272969641 | validation: 0.06710892291186679]
	TIME [epoch: 5.82 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05389606511023588		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.05389606511023588 | validation: 0.07438577498154579]
	TIME [epoch: 5.83 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05602214776059032		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05602214776059032 | validation: 0.061033162390793864]
	TIME [epoch: 5.82 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05546947535337859		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.05546947535337859 | validation: 0.08122065438500516]
	TIME [epoch: 5.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05706971791461074		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.05706971791461074 | validation: 0.0645517111289664]
	TIME [epoch: 5.82 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05296789785470134		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.05296789785470134 | validation: 0.06622066662817772]
	TIME [epoch: 5.82 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05361843495386435		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.05361843495386435 | validation: 0.08937540298885846]
	TIME [epoch: 5.82 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05907978598947055		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.05907978598947055 | validation: 0.0631905305011486]
	TIME [epoch: 5.82 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053405806533753174		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.053405806533753174 | validation: 0.0710344917163993]
	TIME [epoch: 5.83 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05042972930969393		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.05042972930969393 | validation: 0.06618223868533789]
	TIME [epoch: 5.82 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109740998524969		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.05109740998524969 | validation: 0.07012743453209948]
	TIME [epoch: 5.82 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142407408138091		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.05142407408138091 | validation: 0.0688403156791109]
	TIME [epoch: 5.82 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030996185285739		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.05030996185285739 | validation: 0.0662550330345351]
	TIME [epoch: 5.82 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04925787626391018		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.04925787626391018 | validation: 0.06432531103026393]
	TIME [epoch: 5.83 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05095443058575999		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.05095443058575999 | validation: 0.0734085676167631]
	TIME [epoch: 5.83 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05124004667566366		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.05124004667566366 | validation: 0.06843713428769503]
	TIME [epoch: 5.82 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05081760765253463		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.05081760765253463 | validation: 0.06700623589165995]
	TIME [epoch: 5.82 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0492659987937415		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.0492659987937415 | validation: 0.06814958105570935]
	TIME [epoch: 5.83 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049667227089895016		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.049667227089895016 | validation: 0.07292098857317202]
	TIME [epoch: 5.83 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050399503364421484		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.050399503364421484 | validation: 0.06662646969594069]
	TIME [epoch: 5.83 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0527342340293214		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.0527342340293214 | validation: 0.07886495445329027]
	TIME [epoch: 5.82 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05688620364724623		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.05688620364724623 | validation: 0.05987273075140438]
	TIME [epoch: 5.82 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05647646656617349		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.05647646656617349 | validation: 0.08162187144155492]
	TIME [epoch: 5.81 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301352008172813		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.05301352008172813 | validation: 0.06136416203288504]
	TIME [epoch: 5.83 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049198543984448694		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.049198543984448694 | validation: 0.06240296821361166]
	TIME [epoch: 5.82 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050129160221131314		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.050129160221131314 | validation: 0.06738907320539375]
	TIME [epoch: 5.82 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05032471284348184		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05032471284348184 | validation: 0.0615678391587617]
	TIME [epoch: 5.82 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010621666975853		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.05010621666975853 | validation: 0.06794477072267033]
	TIME [epoch: 5.82 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049008499502563974		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.049008499502563974 | validation: 0.06706977130177028]
	TIME [epoch: 5.83 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051540175395919076		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.051540175395919076 | validation: 0.062462991964938465]
	TIME [epoch: 5.83 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05305814447657776		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.05305814447657776 | validation: 0.0740644377874956]
	TIME [epoch: 5.82 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394229261734496		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.05394229261734496 | validation: 0.057974560196657726]
	TIME [epoch: 5.82 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109051157063378		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.05109051157063378 | validation: 0.07642965788566342]
	TIME [epoch: 5.81 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05120488731157148		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.05120488731157148 | validation: 0.06613215914787836]
	TIME [epoch: 5.83 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109818342943532		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.05109818342943532 | validation: 0.0705057353836823]
	TIME [epoch: 5.81 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04883983540692733		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.04883983540692733 | validation: 0.07430463327503532]
	TIME [epoch: 5.82 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05175302770142514		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.05175302770142514 | validation: 0.06956442288971247]
	TIME [epoch: 5.82 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791203698057396		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.04791203698057396 | validation: 0.05835748086412277]
	TIME [epoch: 5.82 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05086626483368735		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.05086626483368735 | validation: 0.07511339745736423]
	TIME [epoch: 5.82 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05266271669419839		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.05266271669419839 | validation: 0.0621417478019837]
	TIME [epoch: 5.83 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050738404152404985		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.050738404152404985 | validation: 0.07041061258267499]
	TIME [epoch: 5.82 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0485906033686436		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.0485906033686436 | validation: 0.05989900092477721]
	TIME [epoch: 5.82 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107444736304265		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.05107444736304265 | validation: 0.0748026944294508]
	TIME [epoch: 5.82 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05379130406572971		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.05379130406572971 | validation: 0.0654732333133943]
	TIME [epoch: 5.82 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052054748710574084		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.052054748710574084 | validation: 0.07415450201117324]
	TIME [epoch: 5.82 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052381468074348715		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.052381468074348715 | validation: 0.05856223293444951]
	TIME [epoch: 5.82 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487062706845863		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.0487062706845863 | validation: 0.06687811170654906]
	TIME [epoch: 5.81 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486488675875569		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0486488675875569 | validation: 0.05899421387273079]
	TIME [epoch: 5.81 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04824794159228999		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.04824794159228999 | validation: 0.07437474153405343]
	TIME [epoch: 5.82 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988973358459269		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.04988973358459269 | validation: 0.0661560113989177]
	TIME [epoch: 5.82 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05279752562114572		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05279752562114572 | validation: 0.07714529626137859]
	TIME [epoch: 5.81 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565013379130079		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.05565013379130079 | validation: 0.06098438128759373]
	TIME [epoch: 5.82 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04947969724997089		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.04947969724997089 | validation: 0.07456082112376095]
	TIME [epoch: 5.82 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051280605643282746		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.051280605643282746 | validation: 0.0728563920316608]
	TIME [epoch: 5.81 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05533838786820692		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.05533838786820692 | validation: 0.06647188421719624]
	TIME [epoch: 5.82 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04812076761683853		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.04812076761683853 | validation: 0.0646695235012744]
	TIME [epoch: 5.82 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941762356019293		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.04941762356019293 | validation: 0.06799799567939663]
	TIME [epoch: 5.82 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0485818486355205		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.0485818486355205 | validation: 0.058324756669609484]
	TIME [epoch: 5.82 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945265724992517		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.04945265724992517 | validation: 0.07967142294013603]
	TIME [epoch: 5.82 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05021753051236173		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05021753051236173 | validation: 0.05744189603219566]
	TIME [epoch: 5.82 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931883987505808		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.04931883987505808 | validation: 0.07481462647824855]
	TIME [epoch: 5.82 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049205404701874615		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.049205404701874615 | validation: 0.05742228448413322]
	TIME [epoch: 5.82 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481127557024484		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.0481127557024484 | validation: 0.07214539541538563]
	TIME [epoch: 5.82 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05276226393832122		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.05276226393832122 | validation: 0.062228419146468206]
	TIME [epoch: 5.82 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05137766205490392		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05137766205490392 | validation: 0.0743220916548296]
	TIME [epoch: 5.82 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04929076881116495		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.04929076881116495 | validation: 0.0605982636129835]
	TIME [epoch: 5.82 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495023672772286		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0495023672772286 | validation: 0.07585249359310398]
	TIME [epoch: 5.82 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050636426360775894		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.050636426360775894 | validation: 0.06751592928293579]
	TIME [epoch: 5.82 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497376739590973		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.0497376739590973 | validation: 0.06624295358932718]
	TIME [epoch: 5.81 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04766930241253389		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.04766930241253389 | validation: 0.0747410028133749]
	TIME [epoch: 5.82 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051091886667995745		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.051091886667995745 | validation: 0.05867462172841105]
	TIME [epoch: 5.81 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049945456953154804		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.049945456953154804 | validation: 0.06466307026256711]
	TIME [epoch: 5.81 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049406587621771816		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.049406587621771816 | validation: 0.060604727191762124]
	TIME [epoch: 5.82 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763269240278303		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.04763269240278303 | validation: 0.061625110518655435]
	TIME [epoch: 5.82 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814514309385727		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.04814514309385727 | validation: 0.0637727457859806]
	TIME [epoch: 5.83 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795686642480151		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.04795686642480151 | validation: 0.06975486749845665]
	TIME [epoch: 5.82 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047190142207978174		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.047190142207978174 | validation: 0.05661289461709136]
	TIME [epoch: 5.82 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05160951828998288		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.05160951828998288 | validation: 0.07713785837537031]
	TIME [epoch: 5.82 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05738599995912269		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.05738599995912269 | validation: 0.05423704964744397]
	TIME [epoch: 5.81 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218931547988289		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.05218931547988289 | validation: 0.06661426172818231]
	TIME [epoch: 5.83 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04870385598911647		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.04870385598911647 | validation: 0.0779123051680385]
	TIME [epoch: 5.82 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858156216418497		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.04858156216418497 | validation: 0.053524630438364695]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1105.pth
	Model improved!!!
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04811171724991624		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.04811171724991624 | validation: 0.06093123659042177]
	TIME [epoch: 5.82 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04864676291864686		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.04864676291864686 | validation: 0.060714103042986245]
	TIME [epoch: 5.81 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04675201310130133		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.04675201310130133 | validation: 0.06251034155457995]
	TIME [epoch: 5.82 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952490876696329		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.04952490876696329 | validation: 0.066378541508172]
	TIME [epoch: 5.82 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04665802749294124		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.04665802749294124 | validation: 0.05894977973850934]
	TIME [epoch: 5.82 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709135129126851		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.04709135129126851 | validation: 0.06424802352565621]
	TIME [epoch: 5.82 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788079279932068		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.04788079279932068 | validation: 0.061478800495435394]
	TIME [epoch: 5.82 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048587700469522106		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.048587700469522106 | validation: 0.061380163288635076]
	TIME [epoch: 5.82 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626059599566853		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.04626059599566853 | validation: 0.06438253235601354]
	TIME [epoch: 5.82 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941808481392453		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.04941808481392453 | validation: 0.07532538567015973]
	TIME [epoch: 5.82 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063950290033304		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.05063950290033304 | validation: 0.05592859561013413]
	TIME [epoch: 5.82 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049716061151575536		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.049716061151575536 | validation: 0.0740999756614764]
	TIME [epoch: 5.82 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051613419524721545		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.051613419524721545 | validation: 0.061985145623129116]
	TIME [epoch: 5.82 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483705010132703		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.0483705010132703 | validation: 0.0631698257721123]
	TIME [epoch: 5.82 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048008745775270416		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.048008745775270416 | validation: 0.06688765758529663]
	TIME [epoch: 5.83 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049885304228143836		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.049885304228143836 | validation: 0.055628946642589674]
	TIME [epoch: 5.81 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051361034360978816		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.051361034360978816 | validation: 0.07674812954818651]
	TIME [epoch: 5.82 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04875355868713651		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.04875355868713651 | validation: 0.0667182986848283]
	TIME [epoch: 5.82 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717351302847117		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.04717351302847117 | validation: 0.05998678204290003]
	TIME [epoch: 5.82 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655463509930834		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.04655463509930834 | validation: 0.060347133959947674]
	TIME [epoch: 5.82 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743775463433842		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.04743775463433842 | validation: 0.06015095007760647]
	TIME [epoch: 5.82 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0471218320032934		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.0471218320032934 | validation: 0.06372996415868647]
	TIME [epoch: 5.82 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0462689021397465		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0462689021397465 | validation: 0.06717581642470241]
	TIME [epoch: 5.82 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04523469829653739		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.04523469829653739 | validation: 0.0575809058161843]
	TIME [epoch: 5.82 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662947816394063		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.04662947816394063 | validation: 0.06320950630606946]
	TIME [epoch: 5.82 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826564400138118		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.04826564400138118 | validation: 0.06720598172590055]
	TIME [epoch: 5.82 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048335956431088754		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.048335956431088754 | validation: 0.05821483097065486]
	TIME [epoch: 5.82 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590153434288874		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.04590153434288874 | validation: 0.06315265204140105]
	TIME [epoch: 5.82 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047511163172272095		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.047511163172272095 | validation: 0.06330029203249567]
	TIME [epoch: 5.82 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047404370707726094		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.047404370707726094 | validation: 0.06412200462883416]
	TIME [epoch: 5.82 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045786942330053676		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.045786942330053676 | validation: 0.06010690693383879]
	TIME [epoch: 5.82 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04663236183248056		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.04663236183248056 | validation: 0.06013927089734618]
	TIME [epoch: 5.82 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046481505171278525		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.046481505171278525 | validation: 0.0631330474234547]
	TIME [epoch: 5.82 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04759408703411189		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.04759408703411189 | validation: 0.06939939754427445]
	TIME [epoch: 5.82 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049342938010311296		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.049342938010311296 | validation: 0.05263137614762362]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1140.pth
	Model improved!!!
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04855043291662205		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.04855043291662205 | validation: 0.07637105279522438]
	TIME [epoch: 5.82 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048044154163061334		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.048044154163061334 | validation: 0.05580108422552396]
	TIME [epoch: 5.82 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04625354345765446		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.04625354345765446 | validation: 0.05901745446803454]
	TIME [epoch: 5.82 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046672401920487445		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.046672401920487445 | validation: 0.06494407325332306]
	TIME [epoch: 5.83 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04789856766572025		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.04789856766572025 | validation: 0.0567651810249668]
	TIME [epoch: 5.82 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04700888893957299		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.04700888893957299 | validation: 0.06319478163340346]
	TIME [epoch: 5.82 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046841748355597145		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.046841748355597145 | validation: 0.05710601139323749]
	TIME [epoch: 5.82 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562058820930269		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04562058820930269 | validation: 0.05824228266901822]
	TIME [epoch: 5.82 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044973817065564636		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.044973817065564636 | validation: 0.06769401635276562]
	TIME [epoch: 5.82 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049146797848110546		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.049146797848110546 | validation: 0.05170092537383674]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1150.pth
	Model improved!!!
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046997676056480295		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.046997676056480295 | validation: 0.061102370260937115]
	TIME [epoch: 5.82 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048692824756661264		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.048692824756661264 | validation: 0.08427860082343175]
	TIME [epoch: 5.81 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153026128308973		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.05153026128308973 | validation: 0.05901187294861372]
	TIME [epoch: 5.83 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044202712945104514		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.044202712945104514 | validation: 0.06241625696145103]
	TIME [epoch: 5.83 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542940999319661		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.04542940999319661 | validation: 0.06682003874177009]
	TIME [epoch: 5.82 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04387387239418231		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04387387239418231 | validation: 0.06033272634404643]
	TIME [epoch: 5.82 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047236369785388685		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.047236369785388685 | validation: 0.055546732994048456]
	TIME [epoch: 5.82 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547942341450571		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.04547942341450571 | validation: 0.06420929999304315]
	TIME [epoch: 5.82 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04711264338354969		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.04711264338354969 | validation: 0.05441082676009928]
	TIME [epoch: 5.83 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04637162915213065		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.04637162915213065 | validation: 0.05629968968315235]
	TIME [epoch: 5.82 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04679706434735589		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.04679706434735589 | validation: 0.06644212611725718]
	TIME [epoch: 5.82 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757420983558218		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04757420983558218 | validation: 0.05931520834588364]
	TIME [epoch: 5.83 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04868593371596938		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.04868593371596938 | validation: 0.07379281836645123]
	TIME [epoch: 5.82 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04852654390873129		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.04852654390873129 | validation: 0.06293125136987175]
	TIME [epoch: 5.82 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04455444278063341		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.04455444278063341 | validation: 0.05974541101046146]
	TIME [epoch: 5.82 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04505594531008817		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.04505594531008817 | validation: 0.05649819171170113]
	TIME [epoch: 5.82 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558558554980708		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.04558558554980708 | validation: 0.05608774147926298]
	TIME [epoch: 5.81 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045163368111925		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.045163368111925 | validation: 0.05670112646977252]
	TIME [epoch: 5.81 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04597831742128005		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.04597831742128005 | validation: 0.05313643577663074]
	TIME [epoch: 5.81 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044324930551761045		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.044324930551761045 | validation: 0.05899506988162169]
	TIME [epoch: 5.82 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04664514996630021		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.04664514996630021 | validation: 0.061531130059408745]
	TIME [epoch: 5.81 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045373625501123664		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.045373625501123664 | validation: 0.06260693585394049]
	TIME [epoch: 5.81 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04457745425156547		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.04457745425156547 | validation: 0.06941655657590964]
	TIME [epoch: 5.81 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588339161085282		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04588339161085282 | validation: 0.06142775997123058]
	TIME [epoch: 5.81 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682122750412246		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.04682122750412246 | validation: 0.0598113743720629]
	TIME [epoch: 5.82 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044198100730460814		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.044198100730460814 | validation: 0.06028882290370551]
	TIME [epoch: 5.82 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04431330763313496		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.04431330763313496 | validation: 0.059604304597610805]
	TIME [epoch: 5.81 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542193095847358		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.04542193095847358 | validation: 0.06544581518359553]
	TIME [epoch: 5.82 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045688608810789004		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.045688608810789004 | validation: 0.0632085255623199]
	TIME [epoch: 5.81 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827123674564319		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.04827123674564319 | validation: 0.0634502232617871]
	TIME [epoch: 5.82 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063566965558849		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.05063566965558849 | validation: 0.061809202388331576]
	TIME [epoch: 5.81 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559631658116272		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.04559631658116272 | validation: 0.054662050704121806]
	TIME [epoch: 5.82 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04885575861794981		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.04885575861794981 | validation: 0.0680579243576855]
	TIME [epoch: 5.81 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757625077828969		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.04757625077828969 | validation: 0.05567677430990155]
	TIME [epoch: 5.81 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411721063804944		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.04411721063804944 | validation: 0.05332190408871734]
	TIME [epoch: 5.81 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04680085703818792		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.04680085703818792 | validation: 0.06638722867105504]
	TIME [epoch: 5.81 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043872040861312536		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.043872040861312536 | validation: 0.06547155573203264]
	TIME [epoch: 5.81 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442165790817929		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.0442165790817929 | validation: 0.05677006079663192]
	TIME [epoch: 5.81 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04548670518342464		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.04548670518342464 | validation: 0.061865894403676014]
	TIME [epoch: 5.82 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628173367060667		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.04628173367060667 | validation: 0.06183649827268575]
	TIME [epoch: 5.82 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045245676915652344		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.045245676915652344 | validation: 0.056975071347131694]
	TIME [epoch: 5.82 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473347411347109		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.04473347411347109 | validation: 0.05913795634219008]
	TIME [epoch: 5.82 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047125850907679574		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.047125850907679574 | validation: 0.06314550368404297]
	TIME [epoch: 5.81 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044818900512498525		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.044818900512498525 | validation: 0.06282002230923102]
	TIME [epoch: 5.81 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044358261815364196		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.044358261815364196 | validation: 0.06374219793498574]
	TIME [epoch: 5.82 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04487640387248274		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.04487640387248274 | validation: 0.05340087323293328]
	TIME [epoch: 5.81 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04357626760475552		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.04357626760475552 | validation: 0.06330524060942298]
	TIME [epoch: 5.81 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464745311818177		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.0464745311818177 | validation: 0.06182765215233439]
	TIME [epoch: 5.81 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04428552154915859		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.04428552154915859 | validation: 0.056084115019235485]
	TIME [epoch: 5.82 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046938398607585474		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.046938398607585474 | validation: 0.07072264665030338]
	TIME [epoch: 5.82 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666459126985748		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.04666459126985748 | validation: 0.05435364602703531]
	TIME [epoch: 5.82 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569846416719767		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.04569846416719767 | validation: 0.05939002127511248]
	TIME [epoch: 5.82 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043694184023820325		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.043694184023820325 | validation: 0.05596598808499839]
	TIME [epoch: 5.82 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04438072217383393		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.04438072217383393 | validation: 0.0626916338747538]
	TIME [epoch: 5.81 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04690018321514294		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.04690018321514294 | validation: 0.05339553606473818]
	TIME [epoch: 5.82 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04586893816546361		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.04586893816546361 | validation: 0.059688953593900695]
	TIME [epoch: 5.82 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044048238294785616		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.044048238294785616 | validation: 0.05954781400156171]
	TIME [epoch: 5.81 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04495563036892392		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.04495563036892392 | validation: 0.057610024272302544]
	TIME [epoch: 5.82 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043693682731705784		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.043693682731705784 | validation: 0.06439010601319352]
	TIME [epoch: 5.82 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04444163305502374		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.04444163305502374 | validation: 0.06130729088490501]
	TIME [epoch: 5.81 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545359262596753		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.04545359262596753 | validation: 0.0510290061652661]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1211.pth
	Model improved!!!
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04491714702982015		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.04491714702982015 | validation: 0.06206915812231655]
	TIME [epoch: 5.82 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045749701595364185		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.045749701595364185 | validation: 0.0596812579598866]
	TIME [epoch: 5.81 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791732324756228		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.04791732324756228 | validation: 0.06100021453013384]
	TIME [epoch: 5.82 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04596771754068441		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.04596771754068441 | validation: 0.05741334954304414]
	TIME [epoch: 5.81 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04582221599305568		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.04582221599305568 | validation: 0.0587794262756684]
	TIME [epoch: 5.82 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04586739486314029		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.04586739486314029 | validation: 0.060259097280150914]
	TIME [epoch: 5.82 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04486097103231386		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.04486097103231386 | validation: 0.05324196818671609]
	TIME [epoch: 5.82 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556632213239402		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.04556632213239402 | validation: 0.06121583769926912]
	TIME [epoch: 5.81 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236823958258364		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.04236823958258364 | validation: 0.055691456223982354]
	TIME [epoch: 5.82 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477429672301966		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.04477429672301966 | validation: 0.06047861018955578]
	TIME [epoch: 5.82 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044335548793271715		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.044335548793271715 | validation: 0.05037194507465115]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1222.pth
	Model improved!!!
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411484300752828		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.04411484300752828 | validation: 0.0618027030111554]
	TIME [epoch: 5.82 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044620950930770606		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.044620950930770606 | validation: 0.052304274773653237]
	TIME [epoch: 5.82 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043605905630106044		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.043605905630106044 | validation: 0.05891519027414547]
	TIME [epoch: 5.82 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04419989477458076		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.04419989477458076 | validation: 0.05695923265874817]
	TIME [epoch: 5.82 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461760807728537		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.04461760807728537 | validation: 0.05579570434487997]
	TIME [epoch: 5.82 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04498473934659862		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.04498473934659862 | validation: 0.05965981017250408]
	TIME [epoch: 5.82 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044965958003374065		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.044965958003374065 | validation: 0.06492743940436109]
	TIME [epoch: 5.82 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043154904408536664		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.043154904408536664 | validation: 0.0560809942033673]
	TIME [epoch: 5.81 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043951193978613634		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.043951193978613634 | validation: 0.0573215559220709]
	TIME [epoch: 5.82 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04367276991950541		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.04367276991950541 | validation: 0.056072007373431404]
	TIME [epoch: 5.82 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043427254400777236		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.043427254400777236 | validation: 0.058334606410946226]
	TIME [epoch: 5.82 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04275326955818327		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.04275326955818327 | validation: 0.06644833066961935]
	TIME [epoch: 5.81 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04546345552843141		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.04546345552843141 | validation: 0.054789693975977743]
	TIME [epoch: 5.82 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579952693945284		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.04579952693945284 | validation: 0.059981632781621864]
	TIME [epoch: 5.83 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04196007937227266		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.04196007937227266 | validation: 0.06461446324150753]
	TIME [epoch: 5.82 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044280593429327324		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.044280593429327324 | validation: 0.05271076316819942]
	TIME [epoch: 5.81 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0459458943974947		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.0459458943974947 | validation: 0.06120480232311337]
	TIME [epoch: 5.82 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426914558035106		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.04426914558035106 | validation: 0.061170114759172105]
	TIME [epoch: 5.82 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04448148645708586		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.04448148645708586 | validation: 0.059803305499792216]
	TIME [epoch: 5.82 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04331613858240218		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.04331613858240218 | validation: 0.05457250078294397]
	TIME [epoch: 5.82 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043864551186213914		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.043864551186213914 | validation: 0.058731117154687776]
	TIME [epoch: 5.82 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043897517004197834		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.043897517004197834 | validation: 0.05595743219101684]
	TIME [epoch: 5.82 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04351208541542116		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.04351208541542116 | validation: 0.05318395463970772]
	TIME [epoch: 5.82 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324055234385165		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.04324055234385165 | validation: 0.056320218137766144]
	TIME [epoch: 5.82 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04153961455479865		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.04153961455479865 | validation: 0.06002904179690265]
	TIME [epoch: 5.82 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046502562752646436		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.046502562752646436 | validation: 0.06416246374957761]
	TIME [epoch: 5.82 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04464097128922381		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.04464097128922381 | validation: 0.05950851408517264]
	TIME [epoch: 5.82 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0427634536369817		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.0427634536369817 | validation: 0.05301248441085189]
	TIME [epoch: 5.81 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04415498893972457		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.04415498893972457 | validation: 0.05975900868558407]
	TIME [epoch: 5.83 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04283736216871179		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.04283736216871179 | validation: 0.05439444010944772]
	TIME [epoch: 5.82 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044137616760715405		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.044137616760715405 | validation: 0.06136549448617077]
	TIME [epoch: 5.82 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0438894983673759		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.0438894983673759 | validation: 0.05764145011208345]
	TIME [epoch: 5.81 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043891368774411584		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.043891368774411584 | validation: 0.051038032196951216]
	TIME [epoch: 5.82 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044016531007214185		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.044016531007214185 | validation: 0.06109199325652698]
	TIME [epoch: 5.82 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04205420336124976		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.04205420336124976 | validation: 0.05215505965410212]
	TIME [epoch: 5.82 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041962982184196064		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.041962982184196064 | validation: 0.05829683401296742]
	TIME [epoch: 5.81 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264644203690965		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.04264644203690965 | validation: 0.05163846083399037]
	TIME [epoch: 5.82 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042697912516425164		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.042697912516425164 | validation: 0.05752943052352804]
	TIME [epoch: 5.81 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04327029807500393		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.04327029807500393 | validation: 0.054155988286895634]
	TIME [epoch: 5.81 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044080826514733204		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.044080826514733204 | validation: 0.0593067216852086]
	TIME [epoch: 5.82 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04240269327481777		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.04240269327481777 | validation: 0.05326631427824759]
	TIME [epoch: 5.82 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04390286164773558		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.04390286164773558 | validation: 0.060513575330552606]
	TIME [epoch: 5.82 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303435947202677		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.04303435947202677 | validation: 0.055667791261519634]
	TIME [epoch: 5.82 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04222987436960137		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.04222987436960137 | validation: 0.05407300555865016]
	TIME [epoch: 5.82 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04310918848485858		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.04310918848485858 | validation: 0.06253450240007533]
	TIME [epoch: 5.82 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041748506261240194		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.041748506261240194 | validation: 0.05607127675499719]
	TIME [epoch: 5.82 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043381548299929594		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.043381548299929594 | validation: 0.05482804814679261]
	TIME [epoch: 5.81 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375316477997154		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.04375316477997154 | validation: 0.05900737372464723]
	TIME [epoch: 5.82 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262685762950319		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.04262685762950319 | validation: 0.05678278948039089]
	TIME [epoch: 5.81 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043340020121193014		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.043340020121193014 | validation: 0.05740311802505804]
	TIME [epoch: 5.82 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04457348201257031		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.04457348201257031 | validation: 0.05968587651638522]
	TIME [epoch: 5.82 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321291660005575		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.04321291660005575 | validation: 0.05174990170733521]
	TIME [epoch: 5.82 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04038618714573871		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.04038618714573871 | validation: 0.05582553401928634]
	TIME [epoch: 5.81 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043282862136791944		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.043282862136791944 | validation: 0.05690537378762364]
	TIME [epoch: 5.81 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04234051267792128		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.04234051267792128 | validation: 0.050416893779611886]
	TIME [epoch: 5.82 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127608794519176		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.04127608794519176 | validation: 0.05265501235410594]
	TIME [epoch: 5.82 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409722654772679		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.04409722654772679 | validation: 0.06936844892986951]
	TIME [epoch: 5.81 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045879076182185265		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.045879076182185265 | validation: 0.052175315493036424]
	TIME [epoch: 5.83 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042132211128557064		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.042132211128557064 | validation: 0.05339552330592286]
	TIME [epoch: 5.82 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179537662696935		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.04179537662696935 | validation: 0.0565145239432484]
	TIME [epoch: 5.82 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04417720677651908		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.04417720677651908 | validation: 0.05258706837146304]
	TIME [epoch: 5.81 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262538475212097		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.04262538475212097 | validation: 0.05818871899333117]
	TIME [epoch: 5.81 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041819869087938104		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.041819869087938104 | validation: 0.05625841596602004]
	TIME [epoch: 5.81 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0431050872645497		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.0431050872645497 | validation: 0.05685639730531407]
	TIME [epoch: 5.81 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278817625160898		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.04278817625160898 | validation: 0.05490163449055211]
	TIME [epoch: 5.82 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111589513295792		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.04111589513295792 | validation: 0.05332514643716503]
	TIME [epoch: 5.81 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163486569115987		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.04163486569115987 | validation: 0.06358308868137184]
	TIME [epoch: 5.82 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04208020085211429		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.04208020085211429 | validation: 0.061106354893185624]
	TIME [epoch: 5.82 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04445009057773651		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.04445009057773651 | validation: 0.05500044904089061]
	TIME [epoch: 5.81 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045150090224710476		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.045150090224710476 | validation: 0.058608980311095384]
	TIME [epoch: 5.82 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178698219424381		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.04178698219424381 | validation: 0.05647543093465704]
	TIME [epoch: 5.82 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163780436675348		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.04163780436675348 | validation: 0.05566786714539598]
	TIME [epoch: 5.81 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041579963089099795		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.041579963089099795 | validation: 0.058608745013903576]
	TIME [epoch: 5.82 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375702862170691		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04375702862170691 | validation: 0.05405304945063934]
	TIME [epoch: 5.81 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04331294396025915		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.04331294396025915 | validation: 0.06589140796883594]
	TIME [epoch: 5.82 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236600818694083		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.04236600818694083 | validation: 0.05439345227619441]
	TIME [epoch: 5.82 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042615296051217735		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.042615296051217735 | validation: 0.05429069997422791]
	TIME [epoch: 5.82 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04144229862353011		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.04144229862353011 | validation: 0.05626261186693372]
	TIME [epoch: 5.81 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042066732228876774		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.042066732228876774 | validation: 0.054950164966195474]
	TIME [epoch: 5.81 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043651400022382304		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.043651400022382304 | validation: 0.06389989283869126]
	TIME [epoch: 5.82 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04336185648012483		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.04336185648012483 | validation: 0.05606721408943777]
	TIME [epoch: 5.81 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422782110766002		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.0422782110766002 | validation: 0.05284163103663336]
	TIME [epoch: 5.83 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04156422827760407		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.04156422827760407 | validation: 0.057699584010321804]
	TIME [epoch: 5.82 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231567342410322		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.04231567342410322 | validation: 0.05088035636163042]
	TIME [epoch: 5.83 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04079365482585407		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.04079365482585407 | validation: 0.05280799563402976]
	TIME [epoch: 5.82 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042445115445488735		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.042445115445488735 | validation: 0.06449050641083198]
	TIME [epoch: 5.82 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04391348530189787		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.04391348530189787 | validation: 0.05638612181708509]
	TIME [epoch: 5.82 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0410926800924533		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.0410926800924533 | validation: 0.051819347680198316]
	TIME [epoch: 5.82 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439957408589534		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.04439957408589534 | validation: 0.05215505303373899]
	TIME [epoch: 5.82 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190524988155622		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.04190524988155622 | validation: 0.05406321113561685]
	TIME [epoch: 5.82 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169981393756184		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.04169981393756184 | validation: 0.05180556497347875]
	TIME [epoch: 5.83 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186265933767579		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.04186265933767579 | validation: 0.05573226488341433]
	TIME [epoch: 5.82 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233573847478495		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.04233573847478495 | validation: 0.06029993810208683]
	TIME [epoch: 5.82 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042923772467076654		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.042923772467076654 | validation: 0.05454116414050261]
	TIME [epoch: 5.81 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04040120771459458		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.04040120771459458 | validation: 0.05384076640501147]
	TIME [epoch: 5.83 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041376382354089		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.04041376382354089 | validation: 0.060377585183427375]
	TIME [epoch: 5.82 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041785586141386756		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.041785586141386756 | validation: 0.05732497365250694]
	TIME [epoch: 5.82 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041901587421228695		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.041901587421228695 | validation: 0.05537541101111352]
	TIME [epoch: 5.81 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04334169650903429		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.04334169650903429 | validation: 0.05438622600364129]
	TIME [epoch: 5.82 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163703592710279		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.04163703592710279 | validation: 0.05542576507808183]
	TIME [epoch: 5.81 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04167331675366576		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.04167331675366576 | validation: 0.05461722235051069]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_1_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_1_v_mmd1_1323.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4619.208 seconds.
