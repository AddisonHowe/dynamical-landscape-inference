Args:
Namespace(name='model_phi1_4a_distortion_v1_4_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2817800947

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.377206829504438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.377206829504438 | validation: 6.966060029519042]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.288368041240608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.288368041240608 | validation: 6.854923441729466]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.238276613858986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.238276613858986 | validation: 6.629758291135841]
	TIME [epoch: 0.662 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.035960288788135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.035960288788135 | validation: 5.732697599084124]
	TIME [epoch: 0.657 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.443792815189422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.443792815189422 | validation: 4.5665364843793]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.42382106743829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.42382106743829 | validation: 4.115478978661847]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.280139270034427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.280139270034427 | validation: 4.1856250193077305]
	TIME [epoch: 0.658 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.34028146561259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.34028146561259 | validation: 7.143422936686043]
	TIME [epoch: 0.657 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.261495777123043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.261495777123043 | validation: 7.004483573866364]
	TIME [epoch: 0.655 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.920749971555672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.920749971555672 | validation: 6.607436646269779]
	TIME [epoch: 0.654 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.875687095374569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.875687095374569 | validation: 6.371070041600952]
	TIME [epoch: 0.655 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.015243233025942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.015243233025942 | validation: 6.27062060085151]
	TIME [epoch: 0.654 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.753028837420449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.753028837420449 | validation: 6.137281074104468]
	TIME [epoch: 0.654 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.060095242977091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.060095242977091 | validation: 6.041884599593055]
	TIME [epoch: 0.656 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.978002425307259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.978002425307259 | validation: 6.064333845666721]
	TIME [epoch: 0.654 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.497627725956465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.497627725956465 | validation: 5.992701667948653]
	TIME [epoch: 0.653 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8667681090469195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8667681090469195 | validation: 6.104814186447076]
	TIME [epoch: 0.652 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.075180560668869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075180560668869 | validation: 5.973305276681732]
	TIME [epoch: 0.655 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.13362796153889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.13362796153889 | validation: 5.931342844727593]
	TIME [epoch: 0.654 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.820078433997004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.820078433997004 | validation: 5.944227438393904]
	TIME [epoch: 0.654 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8829173012537495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8829173012537495 | validation: 5.9345226339699195]
	TIME [epoch: 0.652 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.059594694099558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.059594694099558 | validation: 5.926814003154982]
	TIME [epoch: 0.652 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8522816589877493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8522816589877493 | validation: 5.8520914056793965]
	TIME [epoch: 0.654 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.800784131217272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.800784131217272 | validation: 5.907653139644948]
	TIME [epoch: 0.651 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8349667799663925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8349667799663925 | validation: 5.8968105745374455]
	TIME [epoch: 0.652 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.071905856493441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.071905856493441 | validation: 5.924045468155623]
	TIME [epoch: 0.655 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.90464993752059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.90464993752059 | validation: 5.838896301455457]
	TIME [epoch: 0.653 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.752041716910948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.752041716910948 | validation: 5.727828962088785]
	TIME [epoch: 0.652 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7162861870856694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7162861870856694 | validation: 5.82152627636046]
	TIME [epoch: 0.659 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6994668430894477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6994668430894477 | validation: 5.899190886680746]
	TIME [epoch: 0.653 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.029547349161585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.029547349161585 | validation: 5.8668003871459025]
	TIME [epoch: 0.653 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9473169977257077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9473169977257077 | validation: 5.744926405687644]
	TIME [epoch: 0.652 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.679179056317894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.679179056317894 | validation: 5.662139968911385]
	TIME [epoch: 0.654 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6161752544619725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6161752544619725 | validation: 5.732166185920132]
	TIME [epoch: 0.654 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6656791831102042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6656791831102042 | validation: 5.82671434985775]
	TIME [epoch: 0.656 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9824999090407216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9824999090407216 | validation: 5.703876480145746]
	TIME [epoch: 0.654 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6921075703359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6921075703359 | validation: 5.590253066189924]
	TIME [epoch: 0.655 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.541054548256196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.541054548256196 | validation: 5.7079957955886105]
	TIME [epoch: 0.657 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.795046358948302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.795046358948302 | validation: 5.961876703744828]
	TIME [epoch: 0.655 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.090605441190421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.090605441190421 | validation: 5.662747795150373]
	TIME [epoch: 0.656 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6683728995744116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6683728995744116 | validation: 5.631046445078825]
	TIME [epoch: 0.655 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6154691182346745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6154691182346745 | validation: 5.652919567055441]
	TIME [epoch: 0.656 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8733660804028887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8733660804028887 | validation: 5.596093148253129]
	TIME [epoch: 0.655 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.557471678508895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.557471678508895 | validation: 5.500471580534787]
	TIME [epoch: 0.661 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5210520366900155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5210520366900155 | validation: 5.572049101952889]
	TIME [epoch: 0.655 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5260069189729606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5260069189729606 | validation: 5.514561348396626]
	TIME [epoch: 0.654 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6612972007927396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6612972007927396 | validation: 5.575668348338149]
	TIME [epoch: 0.656 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.589198913599803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.589198913599803 | validation: 5.438802220205591]
	TIME [epoch: 0.656 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.523474407170728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.523474407170728 | validation: 5.446547994965918]
	TIME [epoch: 0.655 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.433221115311192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.433221115311192 | validation: 5.347955876354962]
	TIME [epoch: 0.653 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4501072929240078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4501072929240078 | validation: 5.507410141289464]
	TIME [epoch: 0.655 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.591978913304909		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.591978913304909 | validation: 5.489232007885592]
	TIME [epoch: 0.658 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5745660693513766		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.5745660693513766 | validation: 5.342171133877798]
	TIME [epoch: 0.657 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3911148499311357		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.3911148499311357 | validation: 5.222713993540608]
	TIME [epoch: 0.656 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2983075430263624		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.2983075430263624 | validation: 5.333754415888757]
	TIME [epoch: 0.656 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3641382649911327		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.3641382649911327 | validation: 5.475035650403502]
	TIME [epoch: 0.655 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5775406948626993		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.5775406948626993 | validation: 5.347928676170951]
	TIME [epoch: 0.656 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.451425769116952		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.451425769116952 | validation: 5.149062903095348]
	TIME [epoch: 0.654 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2421259428850977		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.2421259428850977 | validation: 5.22662997444035]
	TIME [epoch: 0.656 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2948443979088116		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.2948443979088116 | validation: 5.330262744510233]
	TIME [epoch: 0.656 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4412604374831406		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.4412604374831406 | validation: 5.305595668438517]
	TIME [epoch: 0.654 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4678825972202394		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.4678825972202394 | validation: 5.113424355401564]
	TIME [epoch: 0.654 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.220965612108065		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.220965612108065 | validation: 5.090382307809602]
	TIME [epoch: 0.655 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.174173271945797		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.174173271945797 | validation: 5.087472120790853]
	TIME [epoch: 0.655 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2359695406999824		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.2359695406999824 | validation: 5.28902401748053]
	TIME [epoch: 0.654 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.509646869313217		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.509646869313217 | validation: 5.0608489127115845]
	TIME [epoch: 0.655 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.195253938990642		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.195253938990642 | validation: 5.012763977481235]
	TIME [epoch: 0.655 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1279501266608976		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.1279501266608976 | validation: 5.053150594719052]
	TIME [epoch: 0.655 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2414626062680214		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.2414626062680214 | validation: 5.207896249889207]
	TIME [epoch: 0.657 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4178710064245057		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.4178710064245057 | validation: 4.9405165964341045]
	TIME [epoch: 0.653 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.099379362593489		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.099379362593489 | validation: 4.915500935854184]
	TIME [epoch: 0.656 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0628424363852575		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.0628424363852575 | validation: 4.957215496651376]
	TIME [epoch: 0.655 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.142132427308643		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.142132427308643 | validation: 5.115806598860385]
	TIME [epoch: 0.656 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.361979373057388		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.361979373057388 | validation: 4.885736342580262]
	TIME [epoch: 0.659 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.059973651581645		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.059973651581645 | validation: 4.867963915134805]
	TIME [epoch: 0.656 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.048764206115828		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.048764206115828 | validation: 4.942678710801004]
	TIME [epoch: 0.654 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1425735188468287		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.1425735188468287 | validation: 5.028163306334347]
	TIME [epoch: 0.656 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2895356130481987		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.2895356130481987 | validation: 4.793213086301098]
	TIME [epoch: 0.655 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0023559598564997		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.0023559598564997 | validation: 4.7716314759687295]
	TIME [epoch: 0.654 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.995853960726728		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.995853960726728 | validation: 4.8640683684395025]
	TIME [epoch: 0.654 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.068297529183212		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.068297529183212 | validation: 4.929478915465937]
	TIME [epoch: 0.655 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2154348890242783		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.2154348890242783 | validation: 4.761552253240632]
	TIME [epoch: 0.657 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.99762969350257		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.99762969350257 | validation: 4.735528649441692]
	TIME [epoch: 0.656 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9932204265761033		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.9932204265761033 | validation: 4.845493855173006]
	TIME [epoch: 0.653 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0676835645879703		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.0676835645879703 | validation: 4.82120514688129]
	TIME [epoch: 0.657 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1171815228066744		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.1171815228066744 | validation: 4.69405797909309]
	TIME [epoch: 0.656 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.957594974295394		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.957594974295394 | validation: 4.7183447193519275]
	TIME [epoch: 0.656 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9722128393181646		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.9722128393181646 | validation: 4.762582158482391]
	TIME [epoch: 0.659 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0144688858542326		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.0144688858542326 | validation: 4.754462736146418]
	TIME [epoch: 0.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0636826467626936		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.0636826467626936 | validation: 4.64929469195007]
	TIME [epoch: 0.657 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9400555194830065		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.9400555194830065 | validation: 4.664800123417531]
	TIME [epoch: 0.654 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9579592584757224		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.9579592584757224 | validation: 4.692493943337131]
	TIME [epoch: 0.657 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9666796639681223		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.9666796639681223 | validation: 4.695985819447616]
	TIME [epoch: 0.656 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.019038728715317		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.019038728715317 | validation: 4.60579743400726]
	TIME [epoch: 0.655 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9255055907063094		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.9255055907063094 | validation: 4.621418897736658]
	TIME [epoch: 0.655 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9469038308626807		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.9469038308626807 | validation: 4.64074519549036]
	TIME [epoch: 0.656 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.948052035972628		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.948052035972628 | validation: 4.609263135666943]
	TIME [epoch: 0.659 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.967528840634791		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.967528840634791 | validation: 4.555494608381376]
	TIME [epoch: 0.655 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8990206291272655		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.8990206291272655 | validation: 4.60606135002855]
	TIME [epoch: 0.655 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9326921260253647		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.9326921260253647 | validation: 4.6006976865979405]
	TIME [epoch: 0.654 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9223706097086994		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.9223706097086994 | validation: 4.564039211744273]
	TIME [epoch: 0.654 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9262332789105687		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.9262332789105687 | validation: 4.5213295044796755]
	TIME [epoch: 0.654 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8894454816185453		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.8894454816185453 | validation: 4.5414798324601025]
	TIME [epoch: 0.653 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.924258906451156		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.924258906451156 | validation: 4.559290053348739]
	TIME [epoch: 0.654 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9009932594445864		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.9009932594445864 | validation: 4.521160088018287]
	TIME [epoch: 0.653 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.903737492207371		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.903737492207371 | validation: 4.474321367291584]
	TIME [epoch: 0.656 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8629085129738656		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.8629085129738656 | validation: 4.469306306982322]
	TIME [epoch: 0.653 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8863141938982166		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.8863141938982166 | validation: 4.511575225460869]
	TIME [epoch: 0.653 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.88845824667839		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.88845824667839 | validation: 4.492270882544342]
	TIME [epoch: 0.655 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8745839557444732		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.8745839557444732 | validation: 4.429197978734593]
	TIME [epoch: 0.656 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8482735339435528		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.8482735339435528 | validation: 4.443200168299926]
	TIME [epoch: 0.654 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.855579626981507		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.855579626981507 | validation: 4.48468089316158]
	TIME [epoch: 0.653 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.867483533974272		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.867483533974272 | validation: 4.4664058360473815]
	TIME [epoch: 0.653 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8852090767323695		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.8852090767323695 | validation: 4.4261769102425745]
	TIME [epoch: 0.655 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.848100563780738		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.848100563780738 | validation: 4.423923131577747]
	TIME [epoch: 0.654 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.875412268090757		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.875412268090757 | validation: 4.424697034016844]
	TIME [epoch: 0.653 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.839325248305304		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.839325248305304 | validation: 4.414032241912305]
	TIME [epoch: 0.653 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.860738925169674		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.860738925169674 | validation: 4.421379694531136]
	TIME [epoch: 0.653 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.846312066588549		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.846312066588549 | validation: 4.386120127673855]
	TIME [epoch: 0.657 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8504959287434355		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.8504959287434355 | validation: 4.362225323215195]
	TIME [epoch: 0.654 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8293643588696717		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.8293643588696717 | validation: 4.362409525331289]
	TIME [epoch: 0.654 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.835274166383407		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.835274166383407 | validation: 4.371627215701835]
	TIME [epoch: 0.653 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8188087245211877		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.8188087245211877 | validation: 4.360116299194031]
	TIME [epoch: 0.654 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8415348071783		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.8415348071783 | validation: 4.370351394639104]
	TIME [epoch: 0.653 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.823955958455254		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.823955958455254 | validation: 4.36699326891044]
	TIME [epoch: 0.653 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8233693553306956		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.8233693553306956 | validation: 4.35239208220205]
	TIME [epoch: 0.651 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8119520820755075		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.8119520820755075 | validation: 4.328356326522878]
	TIME [epoch: 0.651 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8235551794554774		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.8235551794554774 | validation: 4.3502146659220715]
	TIME [epoch: 0.651 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8291600788145588		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.8291600788145588 | validation: 4.346892605020163]
	TIME [epoch: 0.652 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8386702750300272		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.8386702750300272 | validation: 4.283055075183921]
	TIME [epoch: 0.652 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7931291317337226		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.7931291317337226 | validation: 4.263103018305073]
	TIME [epoch: 0.651 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.786967012796834		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.786967012796834 | validation: 4.295465474840234]
	TIME [epoch: 0.653 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.784212638650668		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.784212638650668 | validation: 4.271901344907502]
	TIME [epoch: 0.652 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.809924016941519		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.809924016941519 | validation: 4.376770854465933]
	TIME [epoch: 0.652 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.835508128383541		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.835508128383541 | validation: 4.301370488279269]
	TIME [epoch: 0.651 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8367187243166856		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.8367187243166856 | validation: 4.292239186599033]
	TIME [epoch: 0.651 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7983381168797576		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.7983381168797576 | validation: 4.273272770294051]
	TIME [epoch: 0.651 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7977949404057507		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.7977949404057507 | validation: 4.324470382301956]
	TIME [epoch: 0.652 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8194739381429463		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.8194739381429463 | validation: 4.269475692854352]
	TIME [epoch: 0.652 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8196990520377363		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.8196990520377363 | validation: 4.268207237614809]
	TIME [epoch: 0.651 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7909946409705118		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.7909946409705118 | validation: 4.227526275821747]
	TIME [epoch: 0.651 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7719848998714656		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 2.7719848998714656 | validation: 4.229948005542169]
	TIME [epoch: 0.651 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.760587641057082		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 2.760587641057082 | validation: 4.209809635620046]
	TIME [epoch: 0.652 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7574255570930437		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 2.7574255570930437 | validation: 4.241280744574836]
	TIME [epoch: 0.652 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7694893369783564		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 2.7694893369783564 | validation: 4.244366829836237]
	TIME [epoch: 0.655 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.806349380691699		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 2.806349380691699 | validation: 4.342801736322012]
	TIME [epoch: 0.654 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.856882984614904		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 2.856882984614904 | validation: 4.284958858609001]
	TIME [epoch: 0.657 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8678130423613766		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 2.8678130423613766 | validation: 4.226582282824248]
	TIME [epoch: 0.654 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7733337819676795		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 2.7733337819676795 | validation: 4.194727826215014]
	TIME [epoch: 0.656 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.753500524427177		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 2.753500524427177 | validation: 4.207171230929561]
	TIME [epoch: 0.654 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.755993229779438		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 2.755993229779438 | validation: 4.194836654900072]
	TIME [epoch: 0.656 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7420523728676223		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 2.7420523728676223 | validation: 4.187217768576987]
	TIME [epoch: 0.652 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7454357976685695		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 2.7454357976685695 | validation: 4.171039315162879]
	TIME [epoch: 0.656 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.740139620133618		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 2.740139620133618 | validation: 4.16556753541307]
	TIME [epoch: 0.655 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7430764363690403		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 2.7430764363690403 | validation: 4.176391546735113]
	TIME [epoch: 0.656 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7530956925340524		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 2.7530956925340524 | validation: 4.30249960934431]
	TIME [epoch: 0.653 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8284675254703604		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 2.8284675254703604 | validation: 4.374396044890285]
	TIME [epoch: 0.655 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0012733863179917		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 3.0012733863179917 | validation: 4.204558775617104]
	TIME [epoch: 0.654 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7627168659622265		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 2.7627168659622265 | validation: 4.1577139212144685]
	TIME [epoch: 0.656 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7425137052047748		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 2.7425137052047748 | validation: 4.166938536611873]
	TIME [epoch: 0.654 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.757386816646396		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 2.757386816646396 | validation: 4.219220101794261]
	TIME [epoch: 0.656 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7764471764946332		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 2.7764471764946332 | validation: 4.2253203068535985]
	TIME [epoch: 0.654 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8259213147626503		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 2.8259213147626503 | validation: 4.216491343900711]
	TIME [epoch: 0.655 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.781888717307797		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 2.781888717307797 | validation: 4.1644598865031615]
	TIME [epoch: 0.657 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7555272701154525		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 2.7555272701154525 | validation: 4.154623867944752]
	TIME [epoch: 0.651 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7500658295939946		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 2.7500658295939946 | validation: 4.149292826560104]
	TIME [epoch: 0.65 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7493965862456013		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 2.7493965862456013 | validation: 4.170737068668487]
	TIME [epoch: 0.651 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.740333990225336		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 2.740333990225336 | validation: 4.1661679685850626]
	TIME [epoch: 0.653 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7671763956834914		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 2.7671763956834914 | validation: 4.252734117202239]
	TIME [epoch: 0.651 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.813656391374449		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 2.813656391374449 | validation: 4.206269550503717]
	TIME [epoch: 0.651 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8386206104163114		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 2.8386206104163114 | validation: 4.145293818678353]
	TIME [epoch: 0.651 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.751252164742657		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 2.751252164742657 | validation: 4.1125153429223]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7343434361212418		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 2.7343434361212418 | validation: 4.113004711014265]
	TIME [epoch: 0.658 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7249149981839005		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 2.7249149981839005 | validation: 4.128648931367826]
	TIME [epoch: 0.655 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7321980254946356		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 2.7321980254946356 | validation: 4.137516797299198]
	TIME [epoch: 0.657 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7534790483269354		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 2.7534790483269354 | validation: 4.211859527404144]
	TIME [epoch: 0.656 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7683495843327126		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 2.7683495843327126 | validation: 4.155712891717471]
	TIME [epoch: 0.658 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.791234167960289		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 2.791234167960289 | validation: 4.179665694934886]
	TIME [epoch: 0.657 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7681869825747696		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 2.7681869825747696 | validation: 4.116767164277073]
	TIME [epoch: 0.657 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7677428962501365		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 2.7677428962501365 | validation: 4.13176945665768]
	TIME [epoch: 0.654 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.734672604254462		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 2.734672604254462 | validation: 4.093179933181521]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7291307983808806		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 2.7291307983808806 | validation: 4.105351510925468]
	TIME [epoch: 0.654 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7246045830240737		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 2.7246045830240737 | validation: 4.105168156936895]
	TIME [epoch: 0.653 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.722693861616451		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 2.722693861616451 | validation: 4.145750900424925]
	TIME [epoch: 0.651 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7583607012785922		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 2.7583607012785922 | validation: 4.169473341707116]
	TIME [epoch: 0.654 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8096425406583756		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 2.8096425406583756 | validation: 4.180926672981871]
	TIME [epoch: 0.654 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7598166503255177		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 2.7598166503255177 | validation: 4.105523366224358]
	TIME [epoch: 0.651 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.732841698316068		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 2.732841698316068 | validation: 4.106539525389391]
	TIME [epoch: 0.653 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7180846874423934		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 2.7180846874423934 | validation: 4.062230225198978]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.716125528787287		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 2.716125528787287 | validation: 4.097241751471695]
	TIME [epoch: 0.657 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.718620518901344		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 2.718620518901344 | validation: 4.0692973415775215]
	TIME [epoch: 0.658 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7196587262074683		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 2.7196587262074683 | validation: 4.131975715023213]
	TIME [epoch: 0.656 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.738129585065349		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 2.738129585065349 | validation: 4.103484078348506]
	TIME [epoch: 0.657 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7620357527573574		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 2.7620357527573574 | validation: 4.129747389372506]
	TIME [epoch: 0.655 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.725337508838111		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 2.725337508838111 | validation: 4.054532240917979]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7060138961093365		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 2.7060138961093365 | validation: 4.048674245250157]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6878189706234594		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 2.6878189706234594 | validation: 3.984728982126447]
	TIME [epoch: 0.663 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6683128728132215		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 2.6683128728132215 | validation: 3.925171562799968]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.629500332919807		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 2.629500332919807 | validation: 3.8976967445879733]
	TIME [epoch: 0.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.643260573492143		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 2.643260573492143 | validation: 4.641432881786666]
	TIME [epoch: 0.657 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0562838940795154		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 3.0562838940795154 | validation: 3.9593873671473454]
	TIME [epoch: 179 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6827499733154245		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 2.6827499733154245 | validation: 3.5835553252308374]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4947871542888276		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 2.4947871542888276 | validation: 3.4362449373966273]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.295522461041656		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 2.295522461041656 | validation: 3.459435506934554]
	TIME [epoch: 1.28 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2871060517710298		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 2.2871060517710298 | validation: 3.483412496765647]
	TIME [epoch: 1.28 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2842818681028736		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 2.2842818681028736 | validation: 3.434003235327523]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.249568672170601		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 2.249568672170601 | validation: 3.396615923015915]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.225053902987084		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 2.225053902987084 | validation: 3.3686893262184343]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2169041062633776		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 2.2169041062633776 | validation: 3.3804271840765994]
	TIME [epoch: 1.28 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.227886434975292		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 2.227886434975292 | validation: 3.321113790004602]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1923245123569415		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 2.1923245123569415 | validation: 3.2780815649304245]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.180709363839966		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 2.180709363839966 | validation: 3.277185464818668]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1635229575231096		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 2.1635229575231096 | validation: 3.229847570343307]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.141002692980312		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 2.141002692980312 | validation: 3.049703084483923]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0713899236278603		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 2.0713899236278603 | validation: 1.759856189986862]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4612619634914505		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.4612619634914505 | validation: 1.3562896702929637]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3970496509007715		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.3970496509007715 | validation: 1.2376200303491387]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2000914869395007		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2000914869395007 | validation: 1.1996306918427746]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1147545748323888		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.1147545748323888 | validation: 1.111130486837119]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9908469588487876		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.9908469588487876 | validation: 0.9547948604814325]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0396482332973755		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.0396482332973755 | validation: 0.9321077209977905]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9437171762331851		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9437171762331851 | validation: 1.108150434626237]
	TIME [epoch: 1.28 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447883502038587		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9447883502038587 | validation: 0.9449326994493806]
	TIME [epoch: 1.28 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9709427069279634		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.9709427069279634 | validation: 0.9800779102533481]
	TIME [epoch: 1.28 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9423675226151165		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9423675226151165 | validation: 1.0543284747531119]
	TIME [epoch: 1.28 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.929721461132616		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.929721461132616 | validation: 0.842770492615642]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613953952029161		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.8613953952029161 | validation: 0.9634548885933398]
	TIME [epoch: 1.28 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714595378917958		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8714595378917958 | validation: 0.8506010283876355]
	TIME [epoch: 1.28 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649363687685039		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.8649363687685039 | validation: 0.8786802079090013]
	TIME [epoch: 1.28 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999814641767853		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7999814641767853 | validation: 0.9323658273974257]
	TIME [epoch: 1.28 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136661392043556		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.8136661392043556 | validation: 0.798726572154019]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188352623270773		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8188352623270773 | validation: 0.8695211398055044]
	TIME [epoch: 1.28 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778209212197196		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.778209212197196 | validation: 0.8378055624892525]
	TIME [epoch: 1.28 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568076677269667		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7568076677269667 | validation: 0.7832752207226469]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643307902265863		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7643307902265863 | validation: 0.9087736443766776]
	TIME [epoch: 1.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694299268803755		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7694299268803755 | validation: 0.7130801873638621]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549730519935467		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7549730519935467 | validation: 0.733959888388942]
	TIME [epoch: 1.28 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179918244572372		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7179918244572372 | validation: 0.859186000207572]
	TIME [epoch: 1.29 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274759824398989		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7274759824398989 | validation: 0.7651764351140962]
	TIME [epoch: 1.28 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7002391127854198		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7002391127854198 | validation: 0.772447606815248]
	TIME [epoch: 1.28 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734872836699459		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6734872836699459 | validation: 0.8118181539829831]
	TIME [epoch: 1.28 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022756687108422		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7022756687108422 | validation: 0.7978702745404438]
	TIME [epoch: 1.28 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687441592392832		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8687441592392832 | validation: 0.8114575449160161]
	TIME [epoch: 1.28 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433125300691257		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8433125300691257 | validation: 0.809995331016806]
	TIME [epoch: 1.28 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882849154545011		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6882849154545011 | validation: 0.7173900159219088]
	TIME [epoch: 1.28 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288937215155008		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6288937215155008 | validation: 0.6879461762806603]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656417189465705		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6656417189465705 | validation: 0.7409308948605962]
	TIME [epoch: 1.28 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625744103498693		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.625744103498693 | validation: 0.6852458263877068]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102147924553166		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6102147924553166 | validation: 0.8087102470084233]
	TIME [epoch: 1.28 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9160769965388978		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.9160769965388978 | validation: 0.8130342477881961]
	TIME [epoch: 1.28 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9344612261404924		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.9344612261404924 | validation: 0.7349715836435847]
	TIME [epoch: 1.28 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681566991030343		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7681566991030343 | validation: 0.8142113548712567]
	TIME [epoch: 1.28 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556975728922189		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6556975728922189 | validation: 0.7024638499390469]
	TIME [epoch: 1.28 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850783944822953		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6850783944822953 | validation: 0.7683244569112864]
	TIME [epoch: 1.28 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633203621362321		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6633203621362321 | validation: 0.6883130534965989]
	TIME [epoch: 1.28 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313322685956309		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6313322685956309 | validation: 0.6234083445239621]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6000210391070717		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6000210391070717 | validation: 0.6289682413920861]
	TIME [epoch: 1.28 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558552440080455		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.558552440080455 | validation: 0.6249027813377723]
	TIME [epoch: 1.28 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5484415605441348		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.5484415605441348 | validation: 0.5764535593511294]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431172281729617		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5431172281729617 | validation: 0.575073217821898]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5257746735702602		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5257746735702602 | validation: 0.5653549500704589]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401638030760607		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5401638030760607 | validation: 0.630237121977118]
	TIME [epoch: 1.29 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375713135494765		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5375713135494765 | validation: 0.6618497342780028]
	TIME [epoch: 1.28 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6024434250887909		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6024434250887909 | validation: 0.7009303615430715]
	TIME [epoch: 1.28 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011274883903018		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.7011274883903018 | validation: 0.6803071199235637]
	TIME [epoch: 1.28 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5600521861903905		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.5600521861903905 | validation: 0.5470363356854245]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49208429383599284		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.49208429383599284 | validation: 0.6108437255872409]
	TIME [epoch: 1.29 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5126918528563682		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5126918528563682 | validation: 0.527143388793356]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5073660802060026		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.5073660802060026 | validation: 0.5403817363303535]
	TIME [epoch: 1.29 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4818117271363074		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.4818117271363074 | validation: 0.5392809486401718]
	TIME [epoch: 1.28 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47177394529873384		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.47177394529873384 | validation: 0.5006599847966325]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4693411626023436		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.4693411626023436 | validation: 0.5438793286764929]
	TIME [epoch: 1.28 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4703985944884263		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.4703985944884263 | validation: 0.5412856686906994]
	TIME [epoch: 1.28 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49935924955822786		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.49935924955822786 | validation: 0.6331826913996469]
	TIME [epoch: 1.28 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217905598022486		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5217905598022486 | validation: 0.6041186723479671]
	TIME [epoch: 1.28 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5040982841255263		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5040982841255263 | validation: 0.5599124489610346]
	TIME [epoch: 1.28 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4805128358024007		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.4805128358024007 | validation: 0.5262926233228912]
	TIME [epoch: 1.28 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4273126946326239		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.4273126946326239 | validation: 0.483145434700454]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40630797971700033		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.40630797971700033 | validation: 1.1315694429155256]
	TIME [epoch: 1.29 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399668244549137		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.1399668244549137 | validation: 1.0732929641983753]
	TIME [epoch: 1.28 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0953280817973272		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0953280817973272 | validation: 0.692718873740121]
	TIME [epoch: 1.28 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.716733317974378		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.716733317974378 | validation: 0.4883341156589245]
	TIME [epoch: 1.28 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4747760196252129		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.4747760196252129 | validation: 0.484565951380932]
	TIME [epoch: 1.28 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5102311232597961		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.5102311232597961 | validation: 0.4780008830339886]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44521062425367597		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.44521062425367597 | validation: 0.4792875039889194]
	TIME [epoch: 1.28 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38311260118868606		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.38311260118868606 | validation: 0.4994846553668715]
	TIME [epoch: 1.29 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38078675741931395		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.38078675741931395 | validation: 0.4943978705853538]
	TIME [epoch: 1.28 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3673003317339467		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3673003317339467 | validation: 0.46741496594818194]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3761138864334228		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.3761138864334228 | validation: 0.5040323720708765]
	TIME [epoch: 1.29 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36920112878055283		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.36920112878055283 | validation: 0.45364100603119284]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3709321624776787		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.3709321624776787 | validation: 0.8172970288173304]
	TIME [epoch: 1.28 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7716057435275333		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7716057435275333 | validation: 0.6157943469123993]
	TIME [epoch: 1.28 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038813500964179		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5038813500964179 | validation: 0.4591335032302549]
	TIME [epoch: 1.28 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351831543046204		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.4351831543046204 | validation: 0.46284568236649587]
	TIME [epoch: 1.28 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4237423344961857		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.4237423344961857 | validation: 0.4606548649848421]
	TIME [epoch: 1.28 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.369184409680416		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.369184409680416 | validation: 0.45428521577853015]
	TIME [epoch: 1.28 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351670957774741		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.3351670957774741 | validation: 0.4480284524428306]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33360211681141055		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.33360211681141055 | validation: 0.41328187967076674]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32900729544701407		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.32900729544701407 | validation: 0.42548922982274934]
	TIME [epoch: 1.28 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33060805032979496		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.33060805032979496 | validation: 0.41918610809427825]
	TIME [epoch: 1.29 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31978814886387164		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.31978814886387164 | validation: 0.4727196821753068]
	TIME [epoch: 1.28 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473689534723448		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3473689534723448 | validation: 0.5496749458556089]
	TIME [epoch: 1.28 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4229059235293006		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.4229059235293006 | validation: 0.6502904449453197]
	TIME [epoch: 1.28 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.496603850731636		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.496603850731636 | validation: 0.4470242903408183]
	TIME [epoch: 1.28 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34437242512807303		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.34437242512807303 | validation: 0.4954100690493526]
	TIME [epoch: 1.28 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35695833955715855		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.35695833955715855 | validation: 0.4269895970292698]
	TIME [epoch: 1.28 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2977262951083906		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.2977262951083906 | validation: 0.38093706084854745]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3240229910456954		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.3240229910456954 | validation: 0.4088403067234328]
	TIME [epoch: 1.28 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3089840382920167		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.3089840382920167 | validation: 0.39971449317680635]
	TIME [epoch: 1.28 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29824709898631385		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.29824709898631385 | validation: 0.6891920352800676]
	TIME [epoch: 1.28 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501734001396257		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6501734001396257 | validation: 0.4800111625055295]
	TIME [epoch: 1.28 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35566644251478174		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.35566644251478174 | validation: 0.38107307430981263]
	TIME [epoch: 1.28 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39826822791953076		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.39826822791953076 | validation: 0.4307536083885715]
	TIME [epoch: 1.28 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36994691942111646		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.36994691942111646 | validation: 0.4112190295240968]
	TIME [epoch: 1.28 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28931492352678667		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.28931492352678667 | validation: 0.4297380216246935]
	TIME [epoch: 1.28 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840113721171886		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2840113721171886 | validation: 0.3765415759800906]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780577723407244		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.2780577723407244 | validation: 0.4016701004259501]
	TIME [epoch: 1.28 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28546535564286624		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.28546535564286624 | validation: 0.42175737903616306]
	TIME [epoch: 1.28 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29379304460277567		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.29379304460277567 | validation: 0.49413526201245156]
	TIME [epoch: 1.28 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3533869540965152		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.3533869540965152 | validation: 0.48730109290072776]
	TIME [epoch: 1.28 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34806201907800116		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.34806201907800116 | validation: 0.43867435880583483]
	TIME [epoch: 1.28 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30854412438489137		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.30854412438489137 | validation: 0.3992825786653088]
	TIME [epoch: 1.28 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26148281363069237		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.26148281363069237 | validation: 0.41456014792882884]
	TIME [epoch: 1.28 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25150738196314903		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.25150738196314903 | validation: 0.3636911168527165]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24218612395197575		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.24218612395197575 | validation: 0.3785006114331239]
	TIME [epoch: 1.28 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24544593166128817		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.24544593166128817 | validation: 0.35548752563561303]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23359496214113043		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.23359496214113043 | validation: 0.42612361213123773]
	TIME [epoch: 1.28 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24930965513049885		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.24930965513049885 | validation: 0.3449616624080001]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568806591696629		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.2568806591696629 | validation: 0.4453846996024243]
	TIME [epoch: 1.28 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30678700739967274		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.30678700739967274 | validation: 0.4864493308586689]
	TIME [epoch: 1.28 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41950274971792917		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.41950274971792917 | validation: 0.556349822961541]
	TIME [epoch: 1.29 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36168212308808784		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.36168212308808784 | validation: 0.36550696710309827]
	TIME [epoch: 1.28 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23384668691129068		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.23384668691129068 | validation: 0.34282210534000984]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22128221353138053		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.22128221353138053 | validation: 0.35271512734511234]
	TIME [epoch: 1.28 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23279352824880675		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.23279352824880675 | validation: 0.3599683559255695]
	TIME [epoch: 1.28 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2310587948977023		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.2310587948977023 | validation: 0.38813010308859397]
	TIME [epoch: 1.28 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23746703792591717		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.23746703792591717 | validation: 0.3505331102428032]
	TIME [epoch: 1.28 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26272994227821583		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.26272994227821583 | validation: 0.4335749940078542]
	TIME [epoch: 1.28 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671872395028944		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.2671872395028944 | validation: 0.3842019688783099]
	TIME [epoch: 1.28 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25867468462596166		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.25867468462596166 | validation: 0.3933795467520029]
	TIME [epoch: 1.28 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24111544012638447		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.24111544012638447 | validation: 0.3216156381627924]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2414878585091071		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2414878585091071 | validation: 0.35084877781656437]
	TIME [epoch: 1.28 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2454108470153194		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.2454108470153194 | validation: 0.34903051766450627]
	TIME [epoch: 1.28 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2307401616571294		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.2307401616571294 | validation: 0.31481009503799345]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2212850166152984		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2212850166152984 | validation: 0.281978915276051]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20613187212107728		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.20613187212107728 | validation: 0.49046557325370693]
	TIME [epoch: 1.28 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34040482749564943		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.34040482749564943 | validation: 0.317539124861552]
	TIME [epoch: 1.28 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21765514694790294		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.21765514694790294 | validation: 0.35175150799725224]
	TIME [epoch: 1.28 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2491405428391119		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.2491405428391119 | validation: 0.36594639835113646]
	TIME [epoch: 1.28 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2696039229974047		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2696039229974047 | validation: 0.42803286275496844]
	TIME [epoch: 1.29 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2654719439316536		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2654719439316536 | validation: 0.3858164213740888]
	TIME [epoch: 1.28 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24565050166142205		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.24565050166142205 | validation: 0.49782589187107673]
	TIME [epoch: 1.28 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36155086405886777		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.36155086405886777 | validation: 0.3345483434652516]
	TIME [epoch: 1.28 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19673646319852883		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.19673646319852883 | validation: 0.2826508384538859]
	TIME [epoch: 1.28 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23890494960589478		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.23890494960589478 | validation: 0.29736002245921583]
	TIME [epoch: 1.28 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2087949889229765		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.2087949889229765 | validation: 0.31243910570261874]
	TIME [epoch: 1.28 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19599724486827808		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.19599724486827808 | validation: 0.3235553909515454]
	TIME [epoch: 1.28 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1990071378255268		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1990071378255268 | validation: 0.2890182980913523]
	TIME [epoch: 1.28 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20329149250458806		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.20329149250458806 | validation: 0.3200077506209974]
	TIME [epoch: 1.28 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2132911290842209		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.2132911290842209 | validation: 0.3098675673618184]
	TIME [epoch: 1.29 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21180956403686238		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.21180956403686238 | validation: 0.3594122001053342]
	TIME [epoch: 1.28 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22161637399849507		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.22161637399849507 | validation: 0.3567843400391821]
	TIME [epoch: 1.28 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24048455831281615		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.24048455831281615 | validation: 0.35316766122692944]
	TIME [epoch: 1.28 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21805260445669084		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.21805260445669084 | validation: 0.29697149831377473]
	TIME [epoch: 1.29 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19139879662962814		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.19139879662962814 | validation: 0.3114531508051999]
	TIME [epoch: 1.28 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1822339767700156		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.1822339767700156 | validation: 0.2676617227461916]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1827513035412714		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.1827513035412714 | validation: 0.32208936849833525]
	TIME [epoch: 1.28 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1830220086584989		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.1830220086584989 | validation: 0.26338620419755404]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191899815463919		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.191899815463919 | validation: 0.7030027203828834]
	TIME [epoch: 1.28 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703421523569312		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.6703421523569312 | validation: 0.6180427268415862]
	TIME [epoch: 1.28 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45197451516852716		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.45197451516852716 | validation: 0.38129041541018077]
	TIME [epoch: 1.28 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23625674071499858		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.23625674071499858 | validation: 0.35994415581170697]
	TIME [epoch: 1.28 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19338995754946622		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.19338995754946622 | validation: 0.37461679508418855]
	TIME [epoch: 1.28 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18951006837320705		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.18951006837320705 | validation: 0.34374935444959714]
	TIME [epoch: 1.28 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677062780018663		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.1677062780018663 | validation: 0.35948155748909455]
	TIME [epoch: 1.28 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18572717994536084		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.18572717994536084 | validation: 0.34734551446205786]
	TIME [epoch: 1.28 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17659051297049516		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.17659051297049516 | validation: 0.3213420250194248]
	TIME [epoch: 1.28 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16377335513010047		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.16377335513010047 | validation: 0.2948060418652228]
	TIME [epoch: 1.28 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17289609585914675		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.17289609585914675 | validation: 0.32181914016785984]
	TIME [epoch: 1.28 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17065624891331202		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.17065624891331202 | validation: 0.31764523889876384]
	TIME [epoch: 1.28 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1769717201556862		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.1769717201556862 | validation: 0.34990511323360995]
	TIME [epoch: 1.28 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20019542223763487		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.20019542223763487 | validation: 0.3616225033313707]
	TIME [epoch: 1.28 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376564204416613		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.2376564204416613 | validation: 0.37579034958275576]
	TIME [epoch: 1.28 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22295135598857502		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.22295135598857502 | validation: 0.3335138355175283]
	TIME [epoch: 1.28 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18649886755711215		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.18649886755711215 | validation: 0.29148142878993266]
	TIME [epoch: 1.28 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840027074046011		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1840027074046011 | validation: 0.29778631223447316]
	TIME [epoch: 1.28 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1900201404733684		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1900201404733684 | validation: 0.31225544065986605]
	TIME [epoch: 1.29 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18008615974262676		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.18008615974262676 | validation: 0.30165279089289365]
	TIME [epoch: 1.28 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17604328091562635		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.17604328091562635 | validation: 0.267402991688284]
	TIME [epoch: 1.28 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18287988349422146		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.18287988349422146 | validation: 0.3110115061302719]
	TIME [epoch: 1.28 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1814296986119892		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.1814296986119892 | validation: 0.38695310099899133]
	TIME [epoch: 1.28 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2361774410016968		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2361774410016968 | validation: 0.3228013520422802]
	TIME [epoch: 1.28 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16613621916293211		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.16613621916293211 | validation: 0.2500352856507559]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17357261358977666		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.17357261358977666 | validation: 0.42806785659438334]
	TIME [epoch: 1.28 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673466087960092		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.2673466087960092 | validation: 0.3910116481403062]
	TIME [epoch: 1.29 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22202921483582422		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.22202921483582422 | validation: 0.2777790410203449]
	TIME [epoch: 1.28 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844456076601456		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.1844456076601456 | validation: 0.24775173297781966]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750169007267281		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.1750169007267281 | validation: 0.29263822593830824]
	TIME [epoch: 1.28 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1718803645583876		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.1718803645583876 | validation: 0.38147857391439594]
	TIME [epoch: 1.28 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211988641352094		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.4211988641352094 | validation: 0.34013644457384573]
	TIME [epoch: 1.28 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37070127187244467		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.37070127187244467 | validation: 0.2821947062216736]
	TIME [epoch: 1.28 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1873202837904141		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.1873202837904141 | validation: 0.3229233695703111]
	TIME [epoch: 1.29 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16867349799742556		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.16867349799742556 | validation: 0.3496874243261977]
	TIME [epoch: 1.28 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17866050306249975		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.17866050306249975 | validation: 0.304399636893174]
	TIME [epoch: 1.29 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594143979572062		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.1594143979572062 | validation: 0.2867968627033594]
	TIME [epoch: 1.28 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157052141888861		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.157052141888861 | validation: 0.2709284983274318]
	TIME [epoch: 1.28 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1620971477779729		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.1620971477779729 | validation: 0.29619409705987804]
	TIME [epoch: 1.28 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16044009600316966		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.16044009600316966 | validation: 0.28375708835689206]
	TIME [epoch: 1.29 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17410030469788482		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.17410030469788482 | validation: 0.3294590915608342]
	TIME [epoch: 1.28 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19279840684609345		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.19279840684609345 | validation: 0.3100724335574383]
	TIME [epoch: 1.28 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978404061101616		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.1978404061101616 | validation: 0.3201094233267512]
	TIME [epoch: 1.28 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.180634688730969		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.180634688730969 | validation: 0.2949039762197085]
	TIME [epoch: 1.28 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16186067094350431		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.16186067094350431 | validation: 0.2611947966012144]
	TIME [epoch: 1.28 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15006318762606816		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.15006318762606816 | validation: 0.2532441652677438]
	TIME [epoch: 1.28 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14099357724861736		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.14099357724861736 | validation: 0.2571850307469357]
	TIME [epoch: 1.28 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400080398035433		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.1400080398035433 | validation: 0.24752827359715926]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14066325881000033		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.14066325881000033 | validation: 0.277387130942571]
	TIME [epoch: 1.28 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14728648351791218		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.14728648351791218 | validation: 0.24399323329877454]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15817495284260866		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.15817495284260866 | validation: 0.3142053467986342]
	TIME [epoch: 1.28 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19356395106939736		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.19356395106939736 | validation: 0.3290441112267704]
	TIME [epoch: 1.28 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24516077285787377		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.24516077285787377 | validation: 0.32790316402142405]
	TIME [epoch: 1.28 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19671540284658412		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.19671540284658412 | validation: 0.24245561869694596]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513218263631042		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.1513218263631042 | validation: 0.25243718427543427]
	TIME [epoch: 1.28 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13906424194444725		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.13906424194444725 | validation: 0.24805793534018014]
	TIME [epoch: 1.28 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13894882373745865		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.13894882373745865 | validation: 0.24828783393758813]
	TIME [epoch: 1.28 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1456496985038848		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1456496985038848 | validation: 0.24689225981549795]
	TIME [epoch: 1.28 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17626305067137082		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.17626305067137082 | validation: 0.2617527743285022]
	TIME [epoch: 1.28 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14683389705339625		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.14683389705339625 | validation: 0.24087184141490328]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13583290446198457		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.13583290446198457 | validation: 0.23952472182524753]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13646170204326225		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.13646170204326225 | validation: 0.25263344670015603]
	TIME [epoch: 1.28 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13874398105433636		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.13874398105433636 | validation: 0.22432756908142398]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14873168638910544		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.14873168638910544 | validation: 0.2666398435768276]
	TIME [epoch: 1.29 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16343595930541197		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.16343595930541197 | validation: 0.24472382978910803]
	TIME [epoch: 1.28 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17999646674296813		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.17999646674296813 | validation: 0.3181109750524832]
	TIME [epoch: 1.28 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20199401727197724		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.20199401727197724 | validation: 0.35026360084546193]
	TIME [epoch: 1.29 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20744647492951349		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.20744647492951349 | validation: 0.2869174064271864]
	TIME [epoch: 1.29 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15962938061587703		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.15962938061587703 | validation: 0.22863416680635895]
	TIME [epoch: 1.29 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288139207601138		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.13288139207601138 | validation: 0.240516987438106]
	TIME [epoch: 1.28 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309868698655294		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.1309868698655294 | validation: 0.23600888194436545]
	TIME [epoch: 1.29 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289520082079636		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1289520082079636 | validation: 0.2253461423490286]
	TIME [epoch: 1.29 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1314891319746344		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.1314891319746344 | validation: 0.21884961735266792]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305416944534082		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.1305416944534082 | validation: 0.22429001724398234]
	TIME [epoch: 1.29 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13608131375940433		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.13608131375940433 | validation: 0.2745021450995135]
	TIME [epoch: 1.28 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605682834479096		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.1605682834479096 | validation: 0.2564717723122367]
	TIME [epoch: 1.28 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1950277799388985		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1950277799388985 | validation: 0.2818536771701363]
	TIME [epoch: 1.28 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18613973563776648		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.18613973563776648 | validation: 0.246030004450797]
	TIME [epoch: 1.28 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14353235608827425		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.14353235608827425 | validation: 0.20729008063156337]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408596235928396		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.12408596235928396 | validation: 0.22270393494368979]
	TIME [epoch: 1.29 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308193666800768		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.1308193666800768 | validation: 0.22144319777755164]
	TIME [epoch: 1.29 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12566654979831152		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.12566654979831152 | validation: 0.2247779039144714]
	TIME [epoch: 1.28 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12644381162656582		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.12644381162656582 | validation: 0.23744838465780893]
	TIME [epoch: 1.29 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1303725420484202		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.1303725420484202 | validation: 0.23293796894396737]
	TIME [epoch: 1.28 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485223534354731		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.1485223534354731 | validation: 0.2198052664342182]
	TIME [epoch: 1.28 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12692517342988235		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.12692517342988235 | validation: 0.2554994317799215]
	TIME [epoch: 1.29 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12266288803707821		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.12266288803707821 | validation: 0.211856070303257]
	TIME [epoch: 1.29 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230514055768044		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12230514055768044 | validation: 0.22067834004302203]
	TIME [epoch: 1.28 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12838063490781598		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.12838063490781598 | validation: 0.22714787853121435]
	TIME [epoch: 1.28 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14097796840543328		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.14097796840543328 | validation: 0.31497051227582135]
	TIME [epoch: 1.28 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1956837605995019		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.1956837605995019 | validation: 0.3100762538639929]
	TIME [epoch: 1.28 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2202594586586111		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.2202594586586111 | validation: 0.24936973882445776]
	TIME [epoch: 1.28 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13899384037334728		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.13899384037334728 | validation: 0.204631108048699]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11643649434180549		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.11643649434180549 | validation: 0.2507847674996641]
	TIME [epoch: 1.29 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13038764216974927		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.13038764216974927 | validation: 0.22206637136905286]
	TIME [epoch: 1.28 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12376624624820627		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.12376624624820627 | validation: 0.2103105586936349]
	TIME [epoch: 1.28 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11718851469647171		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.11718851469647171 | validation: 0.22300113617842238]
	TIME [epoch: 1.28 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11791283824197481		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.11791283824197481 | validation: 0.21700729829004659]
	TIME [epoch: 1.28 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11555193805237296		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.11555193805237296 | validation: 0.23001191558734943]
	TIME [epoch: 1.28 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12123581267694437		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.12123581267694437 | validation: 0.20355176384711893]
	TIME [epoch: 1.29 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12516336149438004		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.12516336149438004 | validation: 0.26269596647054455]
	TIME [epoch: 1.29 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15142342876674042		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.15142342876674042 | validation: 0.2614889020112738]
	TIME [epoch: 1.28 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16648250454884916		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.16648250454884916 | validation: 0.24537957416079093]
	TIME [epoch: 1.28 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15251543842960227		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.15251543842960227 | validation: 0.20729755220732904]
	TIME [epoch: 1.28 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12395232858291476		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.12395232858291476 | validation: 0.21024929265013703]
	TIME [epoch: 1.28 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123864980882174		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1123864980882174 | validation: 0.18549719954809568]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10987260402052006		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.10987260402052006 | validation: 0.22313568413162838]
	TIME [epoch: 1.28 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929992776766717		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.10929992776766717 | validation: 0.20517669665407612]
	TIME [epoch: 1.28 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254588982537236		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.1254588982537236 | validation: 0.20926120340643561]
	TIME [epoch: 1.28 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11277722913844043		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.11277722913844043 | validation: 0.2085722658415691]
	TIME [epoch: 1.28 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11429912303888888		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.11429912303888888 | validation: 0.2110409216054112]
	TIME [epoch: 1.28 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12237337574993841		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.12237337574993841 | validation: 0.2437547098085695]
	TIME [epoch: 1.28 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13248492484347976		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.13248492484347976 | validation: 0.2039965671785391]
	TIME [epoch: 1.29 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11122085099483198		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.11122085099483198 | validation: 0.18677842173769987]
	TIME [epoch: 1.28 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1186209790652643		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.1186209790652643 | validation: 0.25146619428869704]
	TIME [epoch: 1.29 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14737879456513456		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.14737879456513456 | validation: 0.25259438332088135]
	TIME [epoch: 1.28 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18379198018898749		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.18379198018898749 | validation: 0.24865604407442216]
	TIME [epoch: 1.28 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15563741887413446		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.15563741887413446 | validation: 0.19980122950674423]
	TIME [epoch: 1.28 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10849837849850742		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.10849837849850742 | validation: 0.18296947973870778]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10232917450100455		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.10232917450100455 | validation: 0.20301369798885083]
	TIME [epoch: 1.28 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10674672580604899		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.10674672580604899 | validation: 0.19414314726594017]
	TIME [epoch: 1.28 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120065023024304		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.11120065023024304 | validation: 0.2146584258461112]
	TIME [epoch: 1.28 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12022046950095647		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.12022046950095647 | validation: 0.21323388903706397]
	TIME [epoch: 1.28 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16102410608496842		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.16102410608496842 | validation: 0.19668482107943228]
	TIME [epoch: 1.28 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11333164396447122		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.11333164396447122 | validation: 0.21858792440753583]
	TIME [epoch: 1.28 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12078875848549603		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.12078875848549603 | validation: 0.19486788446867404]
	TIME [epoch: 1.28 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11325940319431851		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.11325940319431851 | validation: 0.1770674130119407]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11117667663301575		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.11117667663301575 | validation: 0.22933309372425137]
	TIME [epoch: 1.28 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12345881069918337		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.12345881069918337 | validation: 0.2004033256997242]
	TIME [epoch: 1.29 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436184069303298		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.11436184069303298 | validation: 0.2018930330383606]
	TIME [epoch: 1.28 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12116423807160867		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.12116423807160867 | validation: 0.2125693111110869]
	TIME [epoch: 1.28 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1209978950869809		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1209978950869809 | validation: 0.2319664256584929]
	TIME [epoch: 1.28 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13079634862412937		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.13079634862412937 | validation: 0.20454613987510198]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12508478429506517		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.12508478429506517 | validation: 0.1998395169536402]
	TIME [epoch: 2.54 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11057657618138411		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.11057657618138411 | validation: 0.19170243315153124]
	TIME [epoch: 2.53 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10868808578763459		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.10868808578763459 | validation: 0.19190669991070802]
	TIME [epoch: 2.53 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1066824266808132		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.1066824266808132 | validation: 0.16784160284474553]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113490685004337		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.10113490685004337 | validation: 0.20175441961039745]
	TIME [epoch: 2.53 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10699154762739424		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.10699154762739424 | validation: 0.18920595054086828]
	TIME [epoch: 2.54 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10429956579226392		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.10429956579226392 | validation: 0.19629392377745594]
	TIME [epoch: 2.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11388131541561088		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.11388131541561088 | validation: 0.2152087491110576]
	TIME [epoch: 2.54 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288091291481811		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1288091291481811 | validation: 0.22787216923623843]
	TIME [epoch: 2.53 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13791859008363808		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.13791859008363808 | validation: 0.1921253481376209]
	TIME [epoch: 2.53 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12721664918674666		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.12721664918674666 | validation: 0.18634912077809485]
	TIME [epoch: 2.54 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045406694915727		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.11045406694915727 | validation: 0.17673273208327558]
	TIME [epoch: 2.54 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09694799153997234		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09694799153997234 | validation: 0.16415947083324536]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09521586441224755		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.09521586441224755 | validation: 0.1723659414968716]
	TIME [epoch: 2.53 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448909274781006		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.09448909274781006 | validation: 0.18226341296958606]
	TIME [epoch: 2.53 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402570456214938		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.09402570456214938 | validation: 0.1789024890443411]
	TIME [epoch: 2.53 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10195772050878837		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10195772050878837 | validation: 0.1560991768674248]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1033306387308247		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.1033306387308247 | validation: 0.18282177725353477]
	TIME [epoch: 2.54 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11666031886250319		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.11666031886250319 | validation: 0.18950836730607712]
	TIME [epoch: 2.53 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13685326947820808		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.13685326947820808 | validation: 0.22414008115201095]
	TIME [epoch: 2.53 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14507115821482536		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.14507115821482536 | validation: 0.20230949796729508]
	TIME [epoch: 2.54 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12092087354480609		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12092087354480609 | validation: 0.24061509458342623]
	TIME [epoch: 2.53 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12656572809098496		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.12656572809098496 | validation: 0.1717475421580417]
	TIME [epoch: 2.53 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067696943537406		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.10067696943537406 | validation: 0.1599890773833535]
	TIME [epoch: 2.53 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09529581976558132		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09529581976558132 | validation: 0.17810828952622101]
	TIME [epoch: 2.53 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925990510878005		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.0925990510878005 | validation: 0.1790793290149513]
	TIME [epoch: 2.53 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657094667508194		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.09657094667508194 | validation: 0.1594277975511625]
	TIME [epoch: 2.54 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09358617786503567		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.09358617786503567 | validation: 0.17667395286640275]
	TIME [epoch: 2.54 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09694244699736056		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.09694244699736056 | validation: 0.18257506285627173]
	TIME [epoch: 2.53 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09759177520225235		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.09759177520225235 | validation: 0.17128642739231334]
	TIME [epoch: 2.54 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10998031766721308		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.10998031766721308 | validation: 0.21044311919498015]
	TIME [epoch: 2.53 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130148556037232		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.130148556037232 | validation: 0.18787511898249012]
	TIME [epoch: 2.53 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12252725045195785		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.12252725045195785 | validation: 0.18673647831122456]
	TIME [epoch: 2.53 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072196610561547		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1072196610561547 | validation: 0.17393624149334963]
	TIME [epoch: 2.54 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603612080230713		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.09603612080230713 | validation: 0.16007222090413975]
	TIME [epoch: 2.53 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09086652544320527		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.09086652544320527 | validation: 0.1859792384701726]
	TIME [epoch: 2.53 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09867891477778416		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.09867891477778416 | validation: 0.1693640470902274]
	TIME [epoch: 2.53 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09789131493627273		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.09789131493627273 | validation: 0.16547898585206175]
	TIME [epoch: 2.55 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09409786057122975		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.09409786057122975 | validation: 0.18530272319177807]
	TIME [epoch: 2.53 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741128830565213		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.09741128830565213 | validation: 0.16880825617332051]
	TIME [epoch: 2.53 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09534449802848274		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.09534449802848274 | validation: 0.17846505629831816]
	TIME [epoch: 2.53 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585401402804959		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.10585401402804959 | validation: 0.22191632704871617]
	TIME [epoch: 2.53 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13273421844974695		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.13273421844974695 | validation: 0.19719973698005974]
	TIME [epoch: 2.53 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12517579148250493		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.12517579148250493 | validation: 0.18192460414522316]
	TIME [epoch: 2.54 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487117669653513		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.10487117669653513 | validation: 0.18389073627712865]
	TIME [epoch: 2.53 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10811107022471406		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.10811107022471406 | validation: 0.1691097559846858]
	TIME [epoch: 2.53 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913346296635364		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.0913346296635364 | validation: 0.16014418430968505]
	TIME [epoch: 2.53 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09163204274278215		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.09163204274278215 | validation: 0.17313930821607504]
	TIME [epoch: 2.54 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08858888848034348		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.08858888848034348 | validation: 0.15021225526182214]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724122707903022		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.08724122707903022 | validation: 0.15068196007656443]
	TIME [epoch: 2.54 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0923358228035693		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0923358228035693 | validation: 0.17660466217231663]
	TIME [epoch: 2.53 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09789019369809421		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.09789019369809421 | validation: 0.16516235180545225]
	TIME [epoch: 2.53 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944504486440242		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.10944504486440242 | validation: 0.18314950973586397]
	TIME [epoch: 2.53 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732678173699821		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.10732678173699821 | validation: 0.194798542831051]
	TIME [epoch: 2.53 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10980696929787169		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.10980696929787169 | validation: 0.17322485711898028]
	TIME [epoch: 2.53 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298038001849362		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.10298038001849362 | validation: 0.158637889499331]
	TIME [epoch: 2.53 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10666686358881736		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.10666686358881736 | validation: 0.16473954238911245]
	TIME [epoch: 2.53 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0957696471583612		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.0957696471583612 | validation: 0.15414230098877962]
	TIME [epoch: 2.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175832494156609		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.09175832494156609 | validation: 0.17369623563773468]
	TIME [epoch: 2.54 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10012486594689289		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.10012486594689289 | validation: 0.17929862096940036]
	TIME [epoch: 2.53 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912450597714852		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.08912450597714852 | validation: 0.16586281421493387]
	TIME [epoch: 2.53 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08967101388840773		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.08967101388840773 | validation: 0.1679765936691523]
	TIME [epoch: 2.53 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09183766902724763		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.09183766902724763 | validation: 0.16517680523409944]
	TIME [epoch: 2.53 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09812640411357187		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.09812640411357187 | validation: 0.16326595263200216]
	TIME [epoch: 2.53 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10444985196561218		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.10444985196561218 | validation: 0.17764596181529277]
	TIME [epoch: 2.54 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10576169643941519		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.10576169643941519 | validation: 0.16098585125444775]
	TIME [epoch: 2.54 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489451468835918		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.10489451468835918 | validation: 0.1630789440002796]
	TIME [epoch: 2.53 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968471872780006		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.09968471872780006 | validation: 0.156534877296153]
	TIME [epoch: 2.53 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08773588261446635		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.08773588261446635 | validation: 0.1504893008405342]
	TIME [epoch: 2.54 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603189260159422		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.08603189260159422 | validation: 0.1492199380466133]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186734719372318		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.08186734719372318 | validation: 0.1391709717345484]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414282968070687		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.08414282968070687 | validation: 0.15223975360324277]
	TIME [epoch: 2.54 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10013035657357193		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.10013035657357193 | validation: 0.17889012760559853]
	TIME [epoch: 2.54 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396914415881227		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10396914415881227 | validation: 0.15745079711187807]
	TIME [epoch: 2.53 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09098377211121907		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.09098377211121907 | validation: 0.14614068210866185]
	TIME [epoch: 2.54 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08672146375955755		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.08672146375955755 | validation: 0.15704176226910213]
	TIME [epoch: 2.54 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08268216447597716		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.08268216447597716 | validation: 0.13928671121991681]
	TIME [epoch: 2.54 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841026607909427		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.0841026607909427 | validation: 0.1464736254074773]
	TIME [epoch: 2.53 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08389440041597218		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.08389440041597218 | validation: 0.1950524389801765]
	TIME [epoch: 2.54 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178321381211014		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.11178321381211014 | validation: 0.1647751817456838]
	TIME [epoch: 2.54 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09241658052773255		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.09241658052773255 | validation: 0.17143294528032121]
	TIME [epoch: 2.53 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.105280480962407		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.105280480962407 | validation: 0.23076667718529206]
	TIME [epoch: 2.54 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445055244669779		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1445055244669779 | validation: 0.19311393486307746]
	TIME [epoch: 2.54 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136535426948292		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.11136535426948292 | validation: 0.15734617771409062]
	TIME [epoch: 2.54 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08496606044459595		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.08496606044459595 | validation: 0.14452779820963665]
	TIME [epoch: 2.53 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08176736263926458		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.08176736263926458 | validation: 0.14730424732974787]
	TIME [epoch: 2.53 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08343054388933346		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.08343054388933346 | validation: 0.14846268426638862]
	TIME [epoch: 2.53 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08982446887526849		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.08982446887526849 | validation: 0.15750490576207835]
	TIME [epoch: 2.54 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835626154901659		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.08835626154901659 | validation: 0.15502245205134363]
	TIME [epoch: 2.53 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462341538906333		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.08462341538906333 | validation: 0.13486345951994513]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369910875370973		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.08369910875370973 | validation: 0.15008178414596293]
	TIME [epoch: 2.53 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08470163791975586		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.08470163791975586 | validation: 0.14401717969931438]
	TIME [epoch: 2.53 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504072504641112		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.08504072504641112 | validation: 0.14593616076106875]
	TIME [epoch: 2.53 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826402686466991		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.08826402686466991 | validation: 0.15669120422991645]
	TIME [epoch: 2.53 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09466804681530042		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.09466804681530042 | validation: 0.16031805778903152]
	TIME [epoch: 2.53 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10236246215136921		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.10236246215136921 | validation: 0.14647173160049107]
	TIME [epoch: 2.53 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10119886868549344		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.10119886868549344 | validation: 0.1492276687615727]
	TIME [epoch: 2.53 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229011494083747		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.09229011494083747 | validation: 0.13606423805428183]
	TIME [epoch: 2.53 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08277990865764229		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.08277990865764229 | validation: 0.1785294925405405]
	TIME [epoch: 2.53 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10552687300285808		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.10552687300285808 | validation: 0.15711012347476463]
	TIME [epoch: 2.55 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08417356878855159		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.08417356878855159 | validation: 0.1319051385823682]
	TIME [epoch: 2.55 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08335820202866363		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.08335820202866363 | validation: 0.14527317949825647]
	TIME [epoch: 2.54 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780698077809936		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.0780698077809936 | validation: 0.13905206132633105]
	TIME [epoch: 2.54 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07882824781924531		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07882824781924531 | validation: 0.13473709336933212]
	TIME [epoch: 2.54 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08223745201887038		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08223745201887038 | validation: 0.14635624114053]
	TIME [epoch: 2.53 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08569388964616925		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.08569388964616925 | validation: 0.1323375469225968]
	TIME [epoch: 2.53 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046241667789357		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.08046241667789357 | validation: 0.1514279597247673]
	TIME [epoch: 2.53 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816862428575845		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.0816862428575845 | validation: 0.12006783420964287]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049288840606093		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.08049288840606093 | validation: 0.14373627601403344]
	TIME [epoch: 2.53 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08140587385111968		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.08140587385111968 | validation: 0.1360413082821348]
	TIME [epoch: 2.53 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831779837793362		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.0831779837793362 | validation: 0.15934609431235136]
	TIME [epoch: 2.54 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09988491306263765		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.09988491306263765 | validation: 0.19373448340481]
	TIME [epoch: 2.52 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12621358803715046		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.12621358803715046 | validation: 0.17703771020745102]
	TIME [epoch: 2.53 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10855170053328518		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.10855170053328518 | validation: 0.13294426095609532]
	TIME [epoch: 2.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08183468852751992		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08183468852751992 | validation: 0.1349223522623782]
	TIME [epoch: 2.53 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07920549669913184		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07920549669913184 | validation: 0.1367184343727365]
	TIME [epoch: 2.53 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08292158585504104		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.08292158585504104 | validation: 0.13389767638408945]
	TIME [epoch: 2.54 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835123595802351		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.0835123595802351 | validation: 0.1395929389742653]
	TIME [epoch: 2.53 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07780994208633318		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.07780994208633318 | validation: 0.1291440415668954]
	TIME [epoch: 2.53 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.079992354072298		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.079992354072298 | validation: 0.13194983024373613]
	TIME [epoch: 2.53 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853877618755029		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.07853877618755029 | validation: 0.1335158798174748]
	TIME [epoch: 2.53 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07583620162671224		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.07583620162671224 | validation: 0.1243649216627436]
	TIME [epoch: 2.54 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867151931706984		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07867151931706984 | validation: 0.14418750834460542]
	TIME [epoch: 2.53 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537005611203008		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.07537005611203008 | validation: 0.13348775117562708]
	TIME [epoch: 2.53 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950488275401654		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07950488275401654 | validation: 0.14753078790697544]
	TIME [epoch: 2.54 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882170457159915		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.0882170457159915 | validation: 0.14032635949774722]
	TIME [epoch: 2.53 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09215686850035955		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.09215686850035955 | validation: 0.16251469904376736]
	TIME [epoch: 2.54 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09870844507420203		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.09870844507420203 | validation: 0.15846616693933332]
	TIME [epoch: 2.54 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131231980697208		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.10131231980697208 | validation: 0.15006332376381115]
	TIME [epoch: 2.54 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955492912033255		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.08955492912033255 | validation: 0.13261474260723863]
	TIME [epoch: 2.53 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07664548958690827		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07664548958690827 | validation: 0.13104250637785006]
	TIME [epoch: 2.54 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07469299815535521		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07469299815535521 | validation: 0.13258410337089216]
	TIME [epoch: 2.54 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07881372032447086		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07881372032447086 | validation: 0.13434664543322938]
	TIME [epoch: 2.53 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531010463482864		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.07531010463482864 | validation: 0.1301474236475289]
	TIME [epoch: 2.54 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07910441594322105		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.07910441594322105 | validation: 0.13575408296826544]
	TIME [epoch: 2.54 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903695926962552		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.07903695926962552 | validation: 0.13246915448369992]
	TIME [epoch: 2.54 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08145458448160252		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.08145458448160252 | validation: 0.12591784518625132]
	TIME [epoch: 2.54 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07614522032139812		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.07614522032139812 | validation: 0.12573051192228876]
	TIME [epoch: 2.53 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350377006794853		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.07350377006794853 | validation: 0.13273638417239672]
	TIME [epoch: 2.54 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776306593497727		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.0776306593497727 | validation: 0.14316838557859116]
	TIME [epoch: 2.54 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306212347382001		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08306212347382001 | validation: 0.16637659326026363]
	TIME [epoch: 2.54 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10161972949101843		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.10161972949101843 | validation: 0.16014809907435346]
	TIME [epoch: 2.54 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489094764702238		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.10489094764702238 | validation: 0.13189852508175118]
	TIME [epoch: 2.54 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07740549926805229		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.07740549926805229 | validation: 0.1258527906413803]
	TIME [epoch: 2.53 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07957830648371793		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.07957830648371793 | validation: 0.12880928531325933]
	TIME [epoch: 2.54 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08007375593443605		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.08007375593443605 | validation: 0.13670600049460796]
	TIME [epoch: 2.54 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07377702746727197		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.07377702746727197 | validation: 0.11583369305400196]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719392571506537		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.0719392571506537 | validation: 0.13494887286272977]
	TIME [epoch: 2.54 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08011994007731592		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.08011994007731592 | validation: 0.12911945715966908]
	TIME [epoch: 2.53 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532462453340559		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.07532462453340559 | validation: 0.12380106911619522]
	TIME [epoch: 2.53 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07635755736502935		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.07635755736502935 | validation: 0.13866478255728637]
	TIME [epoch: 2.53 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132217816563263		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.08132217816563263 | validation: 0.14641426481275585]
	TIME [epoch: 2.53 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09146619064543518		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.09146619064543518 | validation: 0.14472631203688202]
	TIME [epoch: 2.52 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264842003833001		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09264842003833001 | validation: 0.13001358470183116]
	TIME [epoch: 2.52 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764915609594124		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.07764915609594124 | validation: 0.11926274971458178]
	TIME [epoch: 2.53 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742394279000408		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.0742394279000408 | validation: 0.12527469953404644]
	TIME [epoch: 2.53 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07654862890623326		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.07654862890623326 | validation: 0.13194494357578385]
	TIME [epoch: 2.54 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07081118732932894		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.07081118732932894 | validation: 0.12171577097935733]
	TIME [epoch: 2.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07191106276154574		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.07191106276154574 | validation: 0.11984241675317042]
	TIME [epoch: 2.53 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071489065845536		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.07071489065845536 | validation: 0.12717326377050547]
	TIME [epoch: 2.53 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409224172615478		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.07409224172615478 | validation: 0.12174702771525824]
	TIME [epoch: 2.53 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354932989327446		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.07354932989327446 | validation: 0.12033465075182546]
	TIME [epoch: 2.52 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07281977815988006		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.07281977815988006 | validation: 0.13683131610139604]
	TIME [epoch: 2.53 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942067518669547		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.07942067518669547 | validation: 0.12470479485758346]
	TIME [epoch: 2.54 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08058000166008496		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.08058000166008496 | validation: 0.1469848555001819]
	TIME [epoch: 2.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205568795699275		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.09205568795699275 | validation: 0.16144931526610673]
	TIME [epoch: 2.53 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10483010147828498		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.10483010147828498 | validation: 0.13566585601714795]
	TIME [epoch: 2.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08238316105308575		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08238316105308575 | validation: 0.1274200921242623]
	TIME [epoch: 2.53 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07304545251953773		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.07304545251953773 | validation: 0.11636922210523717]
	TIME [epoch: 2.52 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07690913077611225		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.07690913077611225 | validation: 0.12759206038891185]
	TIME [epoch: 2.53 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07377865579931213		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.07377865579931213 | validation: 0.1259415556752499]
	TIME [epoch: 2.53 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07680522149013642		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07680522149013642 | validation: 0.1284066047366347]
	TIME [epoch: 2.53 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376505948520203		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07376505948520203 | validation: 0.12228381577956382]
	TIME [epoch: 2.53 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07218101235296247		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.07218101235296247 | validation: 0.12321504014307308]
	TIME [epoch: 2.53 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07006207817156396		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.07006207817156396 | validation: 0.13060295773860198]
	TIME [epoch: 2.54 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929963762290291		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06929963762290291 | validation: 0.12326182952913244]
	TIME [epoch: 2.52 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207613597492973		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.07207613597492973 | validation: 0.12685831011338328]
	TIME [epoch: 2.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248313074003974		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.07248313074003974 | validation: 0.1276182467012466]
	TIME [epoch: 2.52 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07467130843625551		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.07467130843625551 | validation: 0.12494280382600796]
	TIME [epoch: 2.52 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07989596368213556		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.07989596368213556 | validation: 0.13173848069907954]
	TIME [epoch: 2.52 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08304964731052325		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08304964731052325 | validation: 0.13659608476422497]
	TIME [epoch: 2.53 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08340286820768342		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.08340286820768342 | validation: 0.12023900457048438]
	TIME [epoch: 2.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07851132116797127		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.07851132116797127 | validation: 0.11930356658096948]
	TIME [epoch: 2.53 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070151588107695		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.070151588107695 | validation: 0.12121407663040046]
	TIME [epoch: 2.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07040041996052111		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07040041996052111 | validation: 0.11351388606054731]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06707859185557148		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.06707859185557148 | validation: 0.13553734866707307]
	TIME [epoch: 2.54 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565352976515058		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.07565352976515058 | validation: 0.13530681324626476]
	TIME [epoch: 2.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07210166337442755		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07210166337442755 | validation: 0.135960508544464]
	TIME [epoch: 2.55 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175273819140278		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.07175273819140278 | validation: 0.13593246074458873]
	TIME [epoch: 2.54 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07676787105548051		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.07676787105548051 | validation: 0.1253183196938313]
	TIME [epoch: 2.54 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960882032066705		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06960882032066705 | validation: 0.12501990660400963]
	TIME [epoch: 2.54 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093612827754628		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.07093612827754628 | validation: 0.12865034543072112]
	TIME [epoch: 2.55 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264709085561613		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.07264709085561613 | validation: 0.1243049836830405]
	TIME [epoch: 2.54 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080473605380626		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.080473605380626 | validation: 0.1317156598329355]
	TIME [epoch: 2.54 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329755694499691		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08329755694499691 | validation: 0.12160986435582766]
	TIME [epoch: 2.53 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07342694677257719		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.07342694677257719 | validation: 0.121247246938838]
	TIME [epoch: 2.53 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07562441332633077		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.07562441332633077 | validation: 0.13353179018914907]
	TIME [epoch: 2.53 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350409243396229		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.07350409243396229 | validation: 0.11252657662303088]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298075747626893		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.07298075747626893 | validation: 0.12682306457267645]
	TIME [epoch: 2.54 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889789866537503		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.06889789866537503 | validation: 0.12257043842174027]
	TIME [epoch: 2.54 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155991895874107		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.07155991895874107 | validation: 0.12717266013131379]
	TIME [epoch: 2.54 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06970298379566373		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.06970298379566373 | validation: 0.12145321112221713]
	TIME [epoch: 2.53 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033939743628807		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.07033939743628807 | validation: 0.12169932818316398]
	TIME [epoch: 2.53 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07167111756109008		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.07167111756109008 | validation: 0.1138110441009709]
	TIME [epoch: 2.54 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06928671858624667		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.06928671858624667 | validation: 0.1252784073552052]
	TIME [epoch: 2.54 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06599523652945477		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06599523652945477 | validation: 0.11326862558148315]
	TIME [epoch: 2.54 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725097668601779		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0725097668601779 | validation: 0.11615715814471211]
	TIME [epoch: 2.55 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07038821090827066		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.07038821090827066 | validation: 0.11548680577051473]
	TIME [epoch: 2.54 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07428968315748562		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.07428968315748562 | validation: 0.13563284343648527]
	TIME [epoch: 2.53 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08000947369828965		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.08000947369828965 | validation: 0.12608386577785516]
	TIME [epoch: 2.54 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708784625381772		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07708784625381772 | validation: 0.12370400458649278]
	TIME [epoch: 2.54 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07444470165918755		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.07444470165918755 | validation: 0.11083632843045677]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06858424631588804		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.06858424631588804 | validation: 0.1159502050168227]
	TIME [epoch: 2.55 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715246244893527		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06715246244893527 | validation: 0.11729614207051631]
	TIME [epoch: 2.54 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443724361737076		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06443724361737076 | validation: 0.1163067694191188]
	TIME [epoch: 2.54 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06673203200106784		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.06673203200106784 | validation: 0.11888535324453123]
	TIME [epoch: 2.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06500475659157649		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.06500475659157649 | validation: 0.12126279200933712]
	TIME [epoch: 2.54 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564536368580993		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.06564536368580993 | validation: 0.11960611597857747]
	TIME [epoch: 2.53 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546644787937173		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.06546644787937173 | validation: 0.11494521362783781]
	TIME [epoch: 2.54 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06764861087621851		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06764861087621851 | validation: 0.12176323748995779]
	TIME [epoch: 2.54 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140471596037623		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.07140471596037623 | validation: 0.12685359851726827]
	TIME [epoch: 2.54 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08078862314327069		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.08078862314327069 | validation: 0.1269728241763884]
	TIME [epoch: 2.54 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07162960203686962		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.07162960203686962 | validation: 0.11896159980274286]
	TIME [epoch: 2.54 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06486872472255932		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06486872472255932 | validation: 0.11625710514545749]
	TIME [epoch: 2.54 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415528699277187		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.06415528699277187 | validation: 0.1072635653206282]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659076324689712		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.0659076324689712 | validation: 0.11526121058663563]
	TIME [epoch: 2.55 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741145891598561		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.06741145891598561 | validation: 0.1181867299839313]
	TIME [epoch: 2.54 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07396717484402159		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.07396717484402159 | validation: 0.14035281588960422]
	TIME [epoch: 2.55 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08552468581971467		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08552468581971467 | validation: 0.11166142840417666]
	TIME [epoch: 2.54 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07999776572668667		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.07999776572668667 | validation: 0.11799066699330861]
	TIME [epoch: 2.54 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06780245491200196		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.06780245491200196 | validation: 0.11814677020242191]
	TIME [epoch: 2.54 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06647679848038855		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.06647679848038855 | validation: 0.11053602373202907]
	TIME [epoch: 2.54 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06840099572244421		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.06840099572244421 | validation: 0.11383869159720367]
	TIME [epoch: 2.54 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358085220839829		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.06358085220839829 | validation: 0.11152984565974196]
	TIME [epoch: 2.54 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464670832306106		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.06464670832306106 | validation: 0.12127745178852117]
	TIME [epoch: 2.54 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802856462075414		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.06802856462075414 | validation: 0.13272032180572688]
	TIME [epoch: 2.54 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759766341091242		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.06759766341091242 | validation: 0.11373175195186087]
	TIME [epoch: 2.54 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688476713342181		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.0688476713342181 | validation: 0.1226796773429356]
	TIME [epoch: 2.54 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690577971688126		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06690577971688126 | validation: 0.12461898163271652]
	TIME [epoch: 2.54 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846328799957842		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.06846328799957842 | validation: 0.10894379105912028]
	TIME [epoch: 2.55 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06803816776607434		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.06803816776607434 | validation: 0.11814807399117032]
	TIME [epoch: 2.54 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364674482691665		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07364674482691665 | validation: 0.1287070457176862]
	TIME [epoch: 2.54 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07585703916404807		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.07585703916404807 | validation: 0.1144721152813272]
	TIME [epoch: 2.55 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06592776542590909		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.06592776542590909 | validation: 0.12020104789426676]
	TIME [epoch: 2.55 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668019790585883		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.0668019790585883 | validation: 0.11678504492316406]
	TIME [epoch: 2.54 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06660419437517023		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06660419437517023 | validation: 0.11216585508815978]
	TIME [epoch: 2.54 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646809910477954		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.0646809910477954 | validation: 0.12163883901429962]
	TIME [epoch: 2.54 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772849838304104		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.06772849838304104 | validation: 0.11385151663710263]
	TIME [epoch: 2.55 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06313971522515952		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.06313971522515952 | validation: 0.11402278832567388]
	TIME [epoch: 2.54 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06849538133376783		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.06849538133376783 | validation: 0.12782078230565616]
	TIME [epoch: 2.54 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638849762021012		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.0638849762021012 | validation: 0.1152292404349986]
	TIME [epoch: 2.54 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320462198732857		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.06320462198732857 | validation: 0.10789154406837138]
	TIME [epoch: 2.54 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06305060746722788		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.06305060746722788 | validation: 0.11713908058179756]
	TIME [epoch: 2.54 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340999985729692		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.06340999985729692 | validation: 0.12229347512625631]
	TIME [epoch: 2.54 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841457371585065		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.06841457371585065 | validation: 0.1236956562622874]
	TIME [epoch: 2.54 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07573174578495338		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.07573174578495338 | validation: 0.11746365235124584]
	TIME [epoch: 2.54 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387015380868889		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.07387015380868889 | validation: 0.11002401845342367]
	TIME [epoch: 2.54 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668152665816336		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.0668152665816336 | validation: 0.11121133533324097]
	TIME [epoch: 2.54 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496081028586927		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.06496081028586927 | validation: 0.10948687418052974]
	TIME [epoch: 2.54 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264511080355079		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.06264511080355079 | validation: 0.10747692009726224]
	TIME [epoch: 2.54 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453825653586726		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.06453825653586726 | validation: 0.11326102463819239]
	TIME [epoch: 2.55 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06480872826447842		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.06480872826447842 | validation: 0.11876275976368739]
	TIME [epoch: 2.54 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549537690737975		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.06549537690737975 | validation: 0.11342895687902375]
	TIME [epoch: 2.54 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06597755302783152		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.06597755302783152 | validation: 0.1172966781643873]
	TIME [epoch: 2.54 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445834105839462		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.06445834105839462 | validation: 0.1019004063033105]
	TIME [epoch: 2.55 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_766.pth
	Model improved!!!
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298205763378648		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.06298205763378648 | validation: 0.11728666386469389]
	TIME [epoch: 2.54 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337535909912738		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.06337535909912738 | validation: 0.10277099764790916]
	TIME [epoch: 2.54 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443540010246707		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.06443540010246707 | validation: 0.1272205983037704]
	TIME [epoch: 2.54 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652114987553325		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0652114987553325 | validation: 0.11132409341811074]
	TIME [epoch: 2.54 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307625835885493		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.06307625835885493 | validation: 0.11330529700916321]
	TIME [epoch: 2.55 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653208237684251		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.0653208237684251 | validation: 0.11752279705012403]
	TIME [epoch: 2.54 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06683475552713235		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06683475552713235 | validation: 0.10966555436505221]
	TIME [epoch: 2.54 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023851025763571		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.07023851025763571 | validation: 0.11774258476505112]
	TIME [epoch: 2.55 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553528238012843		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.06553528238012843 | validation: 0.10877085304385053]
	TIME [epoch: 2.55 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06208254095625266		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.06208254095625266 | validation: 0.11385711124512983]
	TIME [epoch: 2.54 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061818221154025306		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.061818221154025306 | validation: 0.11495610057064948]
	TIME [epoch: 2.54 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823293888689243		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.06823293888689243 | validation: 0.11663830583783628]
	TIME [epoch: 2.54 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113152771368124		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.07113152771368124 | validation: 0.1115061257361601]
	TIME [epoch: 2.54 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06574502392217971		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06574502392217971 | validation: 0.11375064383730843]
	TIME [epoch: 2.54 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641721162076254		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.0641721162076254 | validation: 0.11225568621327464]
	TIME [epoch: 2.54 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05979752974495549		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05979752974495549 | validation: 0.11385893840786919]
	TIME [epoch: 2.54 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06817586257667504		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.06817586257667504 | validation: 0.12369571149819505]
	TIME [epoch: 2.54 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874249945792893		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06874249945792893 | validation: 0.11062994228148659]
	TIME [epoch: 2.54 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06354523638933218		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.06354523638933218 | validation: 0.1147940927856665]
	TIME [epoch: 2.55 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343255894586036		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06343255894586036 | validation: 0.1090728014964661]
	TIME [epoch: 2.54 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392204821458196		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.06392204821458196 | validation: 0.11109969684088708]
	TIME [epoch: 2.54 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06149028677530066		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.06149028677530066 | validation: 0.11607035738871452]
	TIME [epoch: 2.54 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059438925044027924		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.059438925044027924 | validation: 0.10766917299020028]
	TIME [epoch: 2.54 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05974433512563916		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.05974433512563916 | validation: 0.1080127958862045]
	TIME [epoch: 2.54 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061074822744513355		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.061074822744513355 | validation: 0.10600176312909203]
	TIME [epoch: 2.54 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060409137680256735		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.060409137680256735 | validation: 0.10645826693212485]
	TIME [epoch: 2.55 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06369503810960586		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.06369503810960586 | validation: 0.10540049975892181]
	TIME [epoch: 2.55 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06459810912305688		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.06459810912305688 | validation: 0.12277920284796028]
	TIME [epoch: 2.54 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07111579880520547		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.07111579880520547 | validation: 0.11002071392999815]
	TIME [epoch: 2.55 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07104838636780668		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.07104838636780668 | validation: 0.10980028906372617]
	TIME [epoch: 2.54 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06198309283517911		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.06198309283517911 | validation: 0.11231228433659131]
	TIME [epoch: 2.55 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061117177678661164		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.061117177678661164 | validation: 0.10340354302404686]
	TIME [epoch: 2.54 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157035188316771		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.06157035188316771 | validation: 0.10314036416004733]
	TIME [epoch: 2.55 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044516727756154		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.06044516727756154 | validation: 0.11437493099655649]
	TIME [epoch: 2.54 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823548218511846		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.06823548218511846 | validation: 0.11797294987825749]
	TIME [epoch: 2.55 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876210077271486		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.06876210077271486 | validation: 0.11772199556617638]
	TIME [epoch: 2.56 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06155593444348689		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.06155593444348689 | validation: 0.11938186224507714]
	TIME [epoch: 2.54 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131741729534037		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.06131741729534037 | validation: 0.11022192857800589]
	TIME [epoch: 2.54 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399224170124067		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.06399224170124067 | validation: 0.11956080213458348]
	TIME [epoch: 2.54 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060715680924953115		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.060715680924953115 | validation: 0.10932544584243949]
	TIME [epoch: 2.54 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061042389080275934		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.061042389080275934 | validation: 0.10313504251000367]
	TIME [epoch: 2.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05989484670187952		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.05989484670187952 | validation: 0.10960735288095812]
	TIME [epoch: 2.54 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05830916103183311		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.05830916103183311 | validation: 0.10315747080389377]
	TIME [epoch: 2.54 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058165464795604305		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.058165464795604305 | validation: 0.10966698518008028]
	TIME [epoch: 2.54 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06176077902990464		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.06176077902990464 | validation: 0.1190970803053413]
	TIME [epoch: 2.54 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06326328525385246		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.06326328525385246 | validation: 0.11952321965324482]
	TIME [epoch: 2.54 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06377987687414388		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.06377987687414388 | validation: 0.1036227095588773]
	TIME [epoch: 2.54 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433427620887291		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.06433427620887291 | validation: 0.11699836336696688]
	TIME [epoch: 2.55 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060745618028027026		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.060745618028027026 | validation: 0.10585182619174349]
	TIME [epoch: 2.54 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965069239398355		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05965069239398355 | validation: 0.10752836573415961]
	TIME [epoch: 2.55 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883346227328524		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05883346227328524 | validation: 0.10557551290818812]
	TIME [epoch: 2.55 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0604892588663067		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.0604892588663067 | validation: 0.10959300208028573]
	TIME [epoch: 2.54 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05985970801176858		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05985970801176858 | validation: 0.1063199137720665]
	TIME [epoch: 2.54 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06135623002607766		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.06135623002607766 | validation: 0.10081877126015788]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930372039399515		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.05930372039399515 | validation: 0.11478185704676479]
	TIME [epoch: 2.54 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705093683017943		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.06705093683017943 | validation: 0.12103336742518944]
	TIME [epoch: 2.55 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728059666334839		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.06728059666334839 | validation: 0.10641639762153371]
	TIME [epoch: 2.55 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06050764093343447		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.06050764093343447 | validation: 0.10822491677666186]
	TIME [epoch: 2.54 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060028856353910205		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.060028856353910205 | validation: 0.10174267845806279]
	TIME [epoch: 2.54 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059942413563990535		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.059942413563990535 | validation: 0.11111290076554746]
	TIME [epoch: 2.54 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056937835763682706		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.056937835763682706 | validation: 0.09829960585998421]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061428227405908004		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.061428227405908004 | validation: 0.12015537255542408]
	TIME [epoch: 2.54 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06572443613804616		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.06572443613804616 | validation: 0.10321440875606065]
	TIME [epoch: 2.54 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06146550619829379		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.06146550619829379 | validation: 0.11178453943227147]
	TIME [epoch: 2.55 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06681203248289176		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06681203248289176 | validation: 0.10805609016181382]
	TIME [epoch: 2.54 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061323981261657695		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.061323981261657695 | validation: 0.10402617766574669]
	TIME [epoch: 2.54 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05837890715262208		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.05837890715262208 | validation: 0.10355815616163865]
	TIME [epoch: 2.54 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05944763315300694		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.05944763315300694 | validation: 0.11330260459729141]
	TIME [epoch: 2.54 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058734505809067486		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.058734505809067486 | validation: 0.1119069620651279]
	TIME [epoch: 2.54 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05623224540077887		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.05623224540077887 | validation: 0.10419262297831447]
	TIME [epoch: 2.54 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058538232732716765		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.058538232732716765 | validation: 0.10975875561750358]
	TIME [epoch: 2.54 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058963775640495014		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.058963775640495014 | validation: 0.10210642153272431]
	TIME [epoch: 2.55 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371886250874488		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.06371886250874488 | validation: 0.11411902947781148]
	TIME [epoch: 2.54 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138662359720506		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.06138662359720506 | validation: 0.10423823149251332]
	TIME [epoch: 2.54 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05775966676788057		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.05775966676788057 | validation: 0.10664487068936186]
	TIME [epoch: 2.54 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05720981001089632		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.05720981001089632 | validation: 0.10601671017344448]
	TIME [epoch: 2.54 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901326000083236		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.05901326000083236 | validation: 0.10837565881991827]
	TIME [epoch: 2.55 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091147595393869		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.06091147595393869 | validation: 0.10638929002129781]
	TIME [epoch: 2.54 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062318804758820776		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.062318804758820776 | validation: 0.10985943254301521]
	TIME [epoch: 2.54 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929536416520999		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.05929536416520999 | validation: 0.11133724048035219]
	TIME [epoch: 2.54 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06308912974101823		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.06308912974101823 | validation: 0.10193052810327866]
	TIME [epoch: 2.54 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848675765443825		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.05848675765443825 | validation: 0.10069424884279754]
	TIME [epoch: 2.54 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006748364106467		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.06006748364106467 | validation: 0.11043524177611984]
	TIME [epoch: 2.54 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05781778389645616		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.05781778389645616 | validation: 0.11217951593009544]
	TIME [epoch: 2.54 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06029550065997819		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.06029550065997819 | validation: 0.0981939544515166]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05862477829771077		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.05862477829771077 | validation: 0.11145732966544886]
	TIME [epoch: 2.54 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06488655602359834		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.06488655602359834 | validation: 0.10969008314417308]
	TIME [epoch: 2.55 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588347182509055		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.0588347182509055 | validation: 0.1070347655517606]
	TIME [epoch: 2.55 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684664841770564		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.05684664841770564 | validation: 0.10977683450047884]
	TIME [epoch: 2.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059842380220625435		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.059842380220625435 | validation: 0.10938033127626959]
	TIME [epoch: 2.55 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056673563174289426		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.056673563174289426 | validation: 0.11147618318718165]
	TIME [epoch: 2.54 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058757486965653846		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.058757486965653846 | validation: 0.1000582938228921]
	TIME [epoch: 2.55 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05632481122238617		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.05632481122238617 | validation: 0.09615313986557296]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05787172676643916		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.05787172676643916 | validation: 0.10690390488452646]
	TIME [epoch: 2.54 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05872656552455949		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.05872656552455949 | validation: 0.10894380907921795]
	TIME [epoch: 2.54 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062417414951745354		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.062417414951745354 | validation: 0.1424926611170064]
	TIME [epoch: 2.53 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09468859361681002		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.09468859361681002 | validation: 0.1341982673970689]
	TIME [epoch: 2.54 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08572553238959643		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.08572553238959643 | validation: 0.11863620945675349]
	TIME [epoch: 2.54 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439998376942833		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.06439998376942833 | validation: 0.1209868268271924]
	TIME [epoch: 2.55 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644152329522087		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.0644152329522087 | validation: 0.11823540039682037]
	TIME [epoch: 2.55 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06127563997895358		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.06127563997895358 | validation: 0.11493320646887902]
	TIME [epoch: 2.54 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05944510412418433		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.05944510412418433 | validation: 0.1132276065549184]
	TIME [epoch: 2.54 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05883429160409828		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.05883429160409828 | validation: 0.10990608743874608]
	TIME [epoch: 2.54 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899789202543027		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.05899789202543027 | validation: 0.11094914901640243]
	TIME [epoch: 2.54 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711921501176529		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.05711921501176529 | validation: 0.11043956310918375]
	TIME [epoch: 2.54 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058969175418459086		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.058969175418459086 | validation: 0.10715417701676931]
	TIME [epoch: 2.54 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057275350514620016		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.057275350514620016 | validation: 0.10653949736896746]
	TIME [epoch: 2.54 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741919587379224		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.05741919587379224 | validation: 0.10967403846669771]
	TIME [epoch: 2.54 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05690466273247564		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.05690466273247564 | validation: 0.10279563474525398]
	TIME [epoch: 2.54 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05688356603039177		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.05688356603039177 | validation: 0.10454613241967184]
	TIME [epoch: 2.53 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05489692212702279		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.05489692212702279 | validation: 0.10751674304610702]
	TIME [epoch: 2.54 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055970849279640425		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.055970849279640425 | validation: 0.10081143373474313]
	TIME [epoch: 2.54 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754154358608899		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.05754154358608899 | validation: 0.10017934763899819]
	TIME [epoch: 2.53 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05638759981546469		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.05638759981546469 | validation: 0.1021770958994265]
	TIME [epoch: 2.53 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05510084486346546		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.05510084486346546 | validation: 0.09829307439391412]
	TIME [epoch: 2.53 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05784444429363868		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.05784444429363868 | validation: 0.09798751569615538]
	TIME [epoch: 2.54 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0566348396540692		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.0566348396540692 | validation: 0.10178365271350037]
	TIME [epoch: 2.54 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540797160838642		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.05540797160838642 | validation: 0.10823035443880435]
	TIME [epoch: 2.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055707963281293904		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.055707963281293904 | validation: 0.10466533705293864]
	TIME [epoch: 2.54 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056776014411696885		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.056776014411696885 | validation: 0.10898696748734067]
	TIME [epoch: 2.54 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05493565324124815		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.05493565324124815 | validation: 0.09884355777808639]
	TIME [epoch: 2.54 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059275295687466		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.059275295687466 | validation: 0.10289286260748108]
	TIME [epoch: 2.54 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06025025484317464		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.06025025484317464 | validation: 0.10750765420611713]
	TIME [epoch: 2.54 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05580109265840519		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.05580109265840519 | validation: 0.10078435667309543]
	TIME [epoch: 2.54 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011977091675363		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.06011977091675363 | validation: 0.11257063475070339]
	TIME [epoch: 2.54 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05742341677533174		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.05742341677533174 | validation: 0.09595482698373616]
	TIME [epoch: 2.54 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05612568172259073		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.05612568172259073 | validation: 0.10698545749774775]
	TIME [epoch: 2.55 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747564835215753		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.05747564835215753 | validation: 0.09940754535031618]
	TIME [epoch: 2.53 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05470615230392744		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.05470615230392744 | validation: 0.09247788567277332]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658424601100942		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.05658424601100942 | validation: 0.09686832675266521]
	TIME [epoch: 2.53 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05445586281627316		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.05445586281627316 | validation: 0.10419402087964824]
	TIME [epoch: 2.53 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557150265784422		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.05557150265784422 | validation: 0.09797514043800355]
	TIME [epoch: 2.53 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504097402254951		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.05504097402254951 | validation: 0.10486191035820669]
	TIME [epoch: 2.53 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056374358877804856		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.056374358877804856 | validation: 0.1024166278102518]
	TIME [epoch: 2.53 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05890199211299871		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.05890199211299871 | validation: 0.11044440336228299]
	TIME [epoch: 2.53 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06082164624537482		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.06082164624537482 | validation: 0.10906167405008263]
	TIME [epoch: 2.53 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06021714866081902		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.06021714866081902 | validation: 0.09674212151744868]
	TIME [epoch: 2.53 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700660340733691		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.05700660340733691 | validation: 0.10109588943773817]
	TIME [epoch: 2.53 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452671091879834		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.05452671091879834 | validation: 0.10123684590355493]
	TIME [epoch: 2.53 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055067978691032866		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.055067978691032866 | validation: 0.1098232110773722]
	TIME [epoch: 2.53 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901671339949372		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.05901671339949372 | validation: 0.1106006724023338]
	TIME [epoch: 2.53 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558869765232133		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.05558869765232133 | validation: 0.09615780844852775]
	TIME [epoch: 2.53 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05882088773280599		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.05882088773280599 | validation: 0.10546628830735427]
	TIME [epoch: 2.53 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05542511760927695		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05542511760927695 | validation: 0.09957228965009263]
	TIME [epoch: 2.54 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054044324757041955		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.054044324757041955 | validation: 0.10658834848642816]
	TIME [epoch: 2.53 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641608086758572		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.05641608086758572 | validation: 0.10420678202961997]
	TIME [epoch: 2.54 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05433326304452663		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.05433326304452663 | validation: 0.09761698121662764]
	TIME [epoch: 2.52 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05577346223756706		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.05577346223756706 | validation: 0.10415690679467025]
	TIME [epoch: 2.53 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055451580761941795		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.055451580761941795 | validation: 0.10595765197389773]
	TIME [epoch: 2.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05835441446147261		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.05835441446147261 | validation: 0.09730030430417796]
	TIME [epoch: 2.54 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06093759461272657		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.06093759461272657 | validation: 0.10046512625690124]
	TIME [epoch: 2.52 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05738288515473253		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.05738288515473253 | validation: 0.10643969195057604]
	TIME [epoch: 2.53 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05771651548642118		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.05771651548642118 | validation: 0.09220319907169874]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0556060021755739		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.0556060021755739 | validation: 0.1015413083349154]
	TIME [epoch: 2.54 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055830191909651515		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.055830191909651515 | validation: 0.09951798642717258]
	TIME [epoch: 2.54 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05599509809480566		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.05599509809480566 | validation: 0.10009720115346453]
	TIME [epoch: 2.54 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570048661702862		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.05570048661702862 | validation: 0.09779125574068845]
	TIME [epoch: 2.54 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05416332842158223		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.05416332842158223 | validation: 0.10812834258771234]
	TIME [epoch: 2.54 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055461481039484485		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.055461481039484485 | validation: 0.09975033430417829]
	TIME [epoch: 2.54 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684071762202354		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.05684071762202354 | validation: 0.09959170713953591]
	TIME [epoch: 2.53 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054427135713911376		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.054427135713911376 | validation: 0.09826697454362344]
	TIME [epoch: 2.54 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513811513331784		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.05513811513331784 | validation: 0.10167395890852193]
	TIME [epoch: 2.53 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05341953193829152		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.05341953193829152 | validation: 0.10301816482673146]
	TIME [epoch: 2.54 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369520660442175		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.05369520660442175 | validation: 0.1046433261355503]
	TIME [epoch: 2.52 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054152052871196295		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.054152052871196295 | validation: 0.10093869327787496]
	TIME [epoch: 2.53 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05409835313184097		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.05409835313184097 | validation: 0.10132534754392704]
	TIME [epoch: 2.53 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051224613826395815		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.051224613826395815 | validation: 0.09820483366637688]
	TIME [epoch: 2.52 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05286122323743797		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.05286122323743797 | validation: 0.09952050359120301]
	TIME [epoch: 2.54 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615396520949046		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.05615396520949046 | validation: 0.10506337318891219]
	TIME [epoch: 2.53 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05333350996730664		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.05333350996730664 | validation: 0.09641254352012998]
	TIME [epoch: 2.53 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289019791667208		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.05289019791667208 | validation: 0.10964452845220048]
	TIME [epoch: 2.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05795908441852264		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.05795908441852264 | validation: 0.09954928276954504]
	TIME [epoch: 2.52 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061144434864812906		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.061144434864812906 | validation: 0.1016923907201241]
	TIME [epoch: 2.53 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056551756379378836		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.056551756379378836 | validation: 0.10479102749733174]
	TIME [epoch: 2.52 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255234179692622		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.05255234179692622 | validation: 0.1027905180359594]
	TIME [epoch: 2.53 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054300059197637404		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.054300059197637404 | validation: 0.09967227469910028]
	TIME [epoch: 2.52 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425383991027614		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.05425383991027614 | validation: 0.10753864210667008]
	TIME [epoch: 2.54 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05281314033287144		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.05281314033287144 | validation: 0.09324959710165044]
	TIME [epoch: 2.52 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055046712326789614		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.055046712326789614 | validation: 0.10904460291786551]
	TIME [epoch: 2.53 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05852087768259085		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.05852087768259085 | validation: 0.10460171699673536]
	TIME [epoch: 2.52 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05604784138176443		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05604784138176443 | validation: 0.09393184816817]
	TIME [epoch: 2.53 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053347770025638085		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.053347770025638085 | validation: 0.11067233659762621]
	TIME [epoch: 2.53 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654339280180398		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.05654339280180398 | validation: 0.09878088313692161]
	TIME [epoch: 2.53 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05466173186850398		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.05466173186850398 | validation: 0.10114432905073918]
	TIME [epoch: 2.52 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054702296474042		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.054702296474042 | validation: 0.10576377045359849]
	TIME [epoch: 2.53 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05344683275921682		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.05344683275921682 | validation: 0.10426030465659163]
	TIME [epoch: 2.52 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05298793929758733		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.05298793929758733 | validation: 0.10085556869640354]
	TIME [epoch: 2.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053239495026854086		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.053239495026854086 | validation: 0.10050682033784329]
	TIME [epoch: 2.53 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05413630924798677		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.05413630924798677 | validation: 0.09382109487415925]
	TIME [epoch: 2.52 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05152258710348139		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.05152258710348139 | validation: 0.10241032328360822]
	TIME [epoch: 2.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053966154161051064		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.053966154161051064 | validation: 0.11405304206062908]
	TIME [epoch: 2.52 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05476785671010687		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05476785671010687 | validation: 0.0941470436805694]
	TIME [epoch: 2.53 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05582753596799619		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.05582753596799619 | validation: 0.11301043587769986]
	TIME [epoch: 2.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054002135866251424		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.054002135866251424 | validation: 0.10903899352135]
	TIME [epoch: 2.52 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054808291488671904		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.054808291488671904 | validation: 0.10871010937177955]
	TIME [epoch: 2.52 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05586564776783714		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.05586564776783714 | validation: 0.10643803863701168]
	TIME [epoch: 2.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055372557046006535		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.055372557046006535 | validation: 0.10290036711449953]
	TIME [epoch: 2.53 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05380655774575706		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.05380655774575706 | validation: 0.10215417793859967]
	TIME [epoch: 2.52 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05438695258486451		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.05438695258486451 | validation: 0.09973911179492512]
	TIME [epoch: 2.52 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392210736786703		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.05392210736786703 | validation: 0.10545083392445959]
	TIME [epoch: 2.52 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248294864189818		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.05248294864189818 | validation: 0.1057902347827151]
	TIME [epoch: 2.53 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326690046819795		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.05326690046819795 | validation: 0.10069984853370695]
	TIME [epoch: 2.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06097262114360124		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.06097262114360124 | validation: 0.10312557099315725]
	TIME [epoch: 2.52 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05542018929280463		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.05542018929280463 | validation: 0.10526859537251024]
	TIME [epoch: 2.52 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053313305211645075		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.053313305211645075 | validation: 0.09649292330230133]
	TIME [epoch: 2.53 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520735454480801		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.0520735454480801 | validation: 0.10741780441651692]
	TIME [epoch: 2.52 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317147562644579		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.05317147562644579 | validation: 0.10193593120405942]
	TIME [epoch: 2.53 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053976129439182376		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.053976129439182376 | validation: 0.09822410791207327]
	TIME [epoch: 2.52 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052701375070384415		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.052701375070384415 | validation: 0.1072727638376593]
	TIME [epoch: 2.53 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053775548578575666		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.053775548578575666 | validation: 0.10902929718952847]
	TIME [epoch: 2.53 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057629990676527014		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.057629990676527014 | validation: 0.10187228264463509]
	TIME [epoch: 2.53 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567989722357128		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05567989722357128 | validation: 0.10975745774072776]
	TIME [epoch: 2.53 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356884581365896		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.05356884581365896 | validation: 0.1046391959576555]
	TIME [epoch: 2.53 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404253623062287		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.05404253623062287 | validation: 0.10314565942320371]
	TIME [epoch: 2.53 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05502592168316836		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.05502592168316836 | validation: 0.09772927880461776]
	TIME [epoch: 2.53 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053355087655155835		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.053355087655155835 | validation: 0.10109469681628501]
	TIME [epoch: 2.53 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05391699324926944		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.05391699324926944 | validation: 0.09591610205924705]
	TIME [epoch: 2.52 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052955505652483165		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.052955505652483165 | validation: 0.1059159300719279]
	TIME [epoch: 2.52 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115962313817754		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.05115962313817754 | validation: 0.10364309438586404]
	TIME [epoch: 2.52 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051797316904691554		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.051797316904691554 | validation: 0.10468870662908372]
	TIME [epoch: 2.53 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05226740942291654		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.05226740942291654 | validation: 0.10605347791744918]
	TIME [epoch: 2.52 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05165344860710899		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.05165344860710899 | validation: 0.10917558241576537]
	TIME [epoch: 2.52 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531856970131539		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.05531856970131539 | validation: 0.10577773550554892]
	TIME [epoch: 2.52 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06618083981293835		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06618083981293835 | validation: 0.10765748890521139]
	TIME [epoch: 2.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057175970427344305		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.057175970427344305 | validation: 0.10569274043862752]
	TIME [epoch: 2.53 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05720627175115153		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.05720627175115153 | validation: 0.1034109645696889]
	TIME [epoch: 2.53 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358706198316718		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.05358706198316718 | validation: 0.09916801002839462]
	TIME [epoch: 2.52 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153021827276469		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.05153021827276469 | validation: 0.1012070808434459]
	TIME [epoch: 2.53 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05339659628883215		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.05339659628883215 | validation: 0.1017800096364454]
	TIME [epoch: 2.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111513226268958		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.05111513226268958 | validation: 0.10289430939561571]
	TIME [epoch: 2.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05279359744145713		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.05279359744145713 | validation: 0.09532010505504251]
	TIME [epoch: 2.53 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053727135150196566		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.053727135150196566 | validation: 0.10183214822945069]
	TIME [epoch: 2.52 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244442693502951		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.05244442693502951 | validation: 0.09993212889652209]
	TIME [epoch: 2.53 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05362450024681802		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05362450024681802 | validation: 0.09316012819775027]
	TIME [epoch: 2.53 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241189524673311		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.05241189524673311 | validation: 0.11249682353181929]
	TIME [epoch: 188 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052676998575225904		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.052676998575225904 | validation: 0.10166876484132868]
	TIME [epoch: 5.46 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05604123573867909		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.05604123573867909 | validation: 0.0988323939047708]
	TIME [epoch: 5.43 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054073329487632216		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.054073329487632216 | validation: 0.0996032755503789]
	TIME [epoch: 5.44 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998789000678608		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.04998789000678608 | validation: 0.09773700820554351]
	TIME [epoch: 5.44 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053108722510015555		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.053108722510015555 | validation: 0.09718699904864525]
	TIME [epoch: 5.44 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05211425690143196		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.05211425690143196 | validation: 0.10309410447848999]
	TIME [epoch: 5.44 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143458639290774		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.05143458639290774 | validation: 0.10599205170250886]
	TIME [epoch: 5.44 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05119747070729458		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.05119747070729458 | validation: 0.10360163089748427]
	TIME [epoch: 5.46 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05179639612082628		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.05179639612082628 | validation: 0.11108484623602007]
	TIME [epoch: 5.42 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186230424823343		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.05186230424823343 | validation: 0.09940915054178225]
	TIME [epoch: 5.46 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513028143886394		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.0513028143886394 | validation: 0.10029960966917804]
	TIME [epoch: 5.44 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143585759042742		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.05143585759042742 | validation: 0.10848910050215158]
	TIME [epoch: 5.46 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05307537734324875		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.05307537734324875 | validation: 0.09907651627715236]
	TIME [epoch: 5.44 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05178047894639152		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.05178047894639152 | validation: 0.10179200194621546]
	TIME [epoch: 5.44 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04991251605985696		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.04991251605985696 | validation: 0.09557434150545502]
	TIME [epoch: 5.44 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311073139118422		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.05311073139118422 | validation: 0.1061385092802738]
	TIME [epoch: 5.45 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054219063986462326		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.054219063986462326 | validation: 0.10639862663833398]
	TIME [epoch: 5.44 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270374768306701		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.05270374768306701 | validation: 0.10172422468878276]
	TIME [epoch: 5.43 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050729183487306664		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.050729183487306664 | validation: 0.09990211391465467]
	TIME [epoch: 5.44 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_4_v_mmd1_20250421_122741/states/model_phi1_4a_distortion_v1_4_v_mmd1_1020.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2694.701 seconds.
