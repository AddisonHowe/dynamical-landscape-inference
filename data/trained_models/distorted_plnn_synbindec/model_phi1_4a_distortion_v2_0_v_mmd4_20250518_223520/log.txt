Args:
Namespace(name='model_phi1_4a_distortion_v2_0_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_0/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.025831710547208786, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1597283556

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.991777704737964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.991777704737964 | validation: 7.478206366505891]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.873482218290782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.873482218290782 | validation: 7.583847583868163]
	TIME [epoch: 0.767 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.40514566459037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.40514566459037 | validation: 7.26499237550631]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.236598316654313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.236598316654313 | validation: 7.201113255109459]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.001961523624134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.001961523624134 | validation: 7.096101529144541]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.706558412053759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.706558412053759 | validation: 7.0438297201987]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.5713369477771835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5713369477771835 | validation: 6.849454721844797]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.393978670624968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.393978670624968 | validation: 6.762530095497744]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.242084546485858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.242084546485858 | validation: 6.772841732006618]
	TIME [epoch: 0.71 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.215237260840379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.215237260840379 | validation: 6.985425850463328]
	TIME [epoch: 0.712 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.791720238652317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.791720238652317 | validation: 6.603320221882111]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.920964893789504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.920964893789504 | validation: 6.62291915843478]
	TIME [epoch: 0.711 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.086057083482532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.086057083482532 | validation: 6.8327682103813885]
	TIME [epoch: 0.71 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.287010488390702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.287010488390702 | validation: 6.636384278406533]
	TIME [epoch: 0.71 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.83265556217633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.83265556217633 | validation: 6.433627683019137]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.880175533500952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.880175533500952 | validation: 6.516052414889163]
	TIME [epoch: 0.712 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6542228979718026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6542228979718026 | validation: 6.545850868278028]
	TIME [epoch: 0.721 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7224649307396716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7224649307396716 | validation: 6.537470397850015]
	TIME [epoch: 0.712 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.890439193062611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.890439193062611 | validation: 6.4143895186510695]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.686427374961717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686427374961717 | validation: 6.437990079498029]
	TIME [epoch: 0.711 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.589742403534339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.589742403534339 | validation: 6.4205011778867025]
	TIME [epoch: 0.709 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.572074165563553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.572074165563553 | validation: 6.327557392622173]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.456034605067813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.456034605067813 | validation: 6.299547139395902]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.45872814226749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.45872814226749 | validation: 6.329886760316547]
	TIME [epoch: 0.711 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4836984275582505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4836984275582505 | validation: 6.301479305322838]
	TIME [epoch: 0.714 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.437379603183199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437379603183199 | validation: 6.249769596079069]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.328547740160032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328547740160032 | validation: 6.226215119124783]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3350521535176965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3350521535176965 | validation: 6.2346403051491315]
	TIME [epoch: 0.712 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.35383811023514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35383811023514 | validation: 6.2198050012482655]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342493562964017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342493562964017 | validation: 6.164037260038174]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.231785688928171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.231785688928171 | validation: 6.135058300567358]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.218229990275374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.218229990275374 | validation: 6.1505421992396405]
	TIME [epoch: 0.711 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.221617245125855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.221617245125855 | validation: 6.12623093899668]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.22893143358951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.22893143358951 | validation: 6.104430339564072]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.148725391560088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.148725391560088 | validation: 6.046049746675354]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1495003009015905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1495003009015905 | validation: 6.137578758817385]
	TIME [epoch: 0.713 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.344053442711284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.344053442711284 | validation: 6.074739708802191]
	TIME [epoch: 0.71 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.333657816954043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.333657816954043 | validation: 6.085716544291687]
	TIME [epoch: 0.71 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.129812323855673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.129812323855673 | validation: 6.015754215353279]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.07791808014399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.07791808014399 | validation: 5.983209988663901]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.069501614917055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.069501614917055 | validation: 5.96390767629428]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.995750125735434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.995750125735434 | validation: 5.955357931737403]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9661004824025383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9661004824025383 | validation: 5.905966955399183]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9475879309104234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9475879309104234 | validation: 5.947773281505277]
	TIME [epoch: 0.709 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9794442358795368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9794442358795368 | validation: 5.896886787967237]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9738798007007565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9738798007007565 | validation: 5.89780782174001]
	TIME [epoch: 0.71 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9429270004464763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9429270004464763 | validation: 5.862444151379677]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.907370389441479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.907370389441479 | validation: 5.843330916885108]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9687040771040714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9687040771040714 | validation: 5.885766003858572]
	TIME [epoch: 0.709 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.111820924712656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.111820924712656 | validation: 5.772967135313085]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8286355639759244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8286355639759244 | validation: 5.7773052781030625]
	TIME [epoch: 0.711 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7862753476017668		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.7862753476017668 | validation: 5.773887108764709]
	TIME [epoch: 0.711 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7796693862882296		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.7796693862882296 | validation: 5.744400158130009]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7816655331245275		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.7816655331245275 | validation: 5.738637519527036]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.800840767128501		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.800840767128501 | validation: 5.745934403368576]
	TIME [epoch: 0.712 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.825654264673434		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.825654264673434 | validation: 5.731133872280336]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.85342934444834		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.85342934444834 | validation: 5.732201658395886]
	TIME [epoch: 0.716 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8258497088089167		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.8258497088089167 | validation: 5.671153406916019]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.780634240948748		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.780634240948748 | validation: 5.701475190620447]
	TIME [epoch: 0.709 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.813742151207733		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.813742151207733 | validation: 5.638148070584962]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7781893885386717		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.7781893885386717 | validation: 5.642022158697451]
	TIME [epoch: 0.712 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7535595744171633		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.7535595744171633 | validation: 5.606107003771396]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.687251696978163		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.687251696978163 | validation: 5.578281890298875]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6800258500918055		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.6800258500918055 | validation: 5.5698349471738915]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.663546966030367		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.663546966030367 | validation: 5.544511817037005]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.635379811086581		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.635379811086581 | validation: 5.530601455737845]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.623834939142854		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.623834939142854 | validation: 5.501227228913539]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6264031705598354		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.6264031705598354 | validation: 5.49911444280788]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6260150454770055		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.6260150454770055 | validation: 5.473089066513186]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6479348578787354		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.6479348578787354 | validation: 5.479914114777671]
	TIME [epoch: 0.713 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6375784319307547		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.6375784319307547 | validation: 5.428332115969778]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6181879930335787		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.6181879930335787 | validation: 5.424749271589878]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5735216839995325		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.5735216839995325 | validation: 5.372344611909531]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.54543506040758		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.54543506040758 | validation: 5.358672400474394]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5331109737668176		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.5331109737668176 | validation: 5.342236328617917]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.516668760236271		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.516668760236271 | validation: 5.321930526640516]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5120498888178693		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.5120498888178693 | validation: 5.303125323437365]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.499268545678605		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.499268545678605 | validation: 5.276293366736816]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4865408805379126		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.4865408805379126 | validation: 5.267586416098027]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4718481566967583		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.4718481566967583 | validation: 5.20877914028998]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.470336809148325		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.470336809148325 | validation: 5.205872587281937]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4581725222997886		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.4581725222997886 | validation: 5.149761629223644]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.443743589729422		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.443743589729422 | validation: 5.137242208106045]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4198802359119442		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.4198802359119442 | validation: 5.079924473206193]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.404683733187994		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.404683733187994 | validation: 5.062105210870872]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.384938711339796		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.384938711339796 | validation: 4.999315332263412]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3611136291663826		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.3611136291663826 | validation: 4.960456858215745]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.355258522365201		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 3.355258522365201 | validation: 4.929708924396005]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3225866843399663		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.3225866843399663 | validation: 4.857209026168293]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3050424786790393		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.3050424786790393 | validation: 4.79438036475893]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2786107027412448		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 3.2786107027412448 | validation: 4.732565306003844]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2537778994232585		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 3.2537778994232585 | validation: 4.668646515709496]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2150044714583625		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 3.2150044714583625 | validation: 4.549126062505265]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1783885181075777		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.1783885181075777 | validation: 4.331620328365022]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0617658030786754		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 3.0617658030786754 | validation: 3.6816879738707406]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.815880324204926		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.815880324204926 | validation: 3.4963747055352368]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.604417962404133		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.604417962404133 | validation: 3.401280641285009]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5013199190881097		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.5013199190881097 | validation: 3.4792615775958633]
	TIME [epoch: 0.711 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.390079728344196		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 3.390079728344196 | validation: 3.2435056758001437]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4054968580785876		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.4054968580785876 | validation: 3.2898101460832723]
	TIME [epoch: 0.712 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3998759512052827		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.3998759512052827 | validation: 2.9535626423165016]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.300044030545258		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.300044030545258 | validation: 2.860344901485301]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.277899941664826		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.277899941664826 | validation: 2.8993121626125347]
	TIME [epoch: 0.712 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.219438944951911		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.219438944951911 | validation: 2.8183443229236076]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.169555587864028		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.169555587864028 | validation: 2.592552631249225]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1513967490395376		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.1513967490395376 | validation: 2.6559072807151605]
	TIME [epoch: 0.709 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.130580211545282		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.130580211545282 | validation: 2.4909983493090233]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1197457096823875		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.1197457096823875 | validation: 2.563839636457647]
	TIME [epoch: 0.711 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.094900641640136		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.094900641640136 | validation: 2.3238208877839037]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.114056051936557		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.114056051936557 | validation: 2.615139375453207]
	TIME [epoch: 0.71 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1141133599804744		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.1141133599804744 | validation: 2.3161613791537095]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1124221402165877		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.1124221402165877 | validation: 2.4459384726591527]
	TIME [epoch: 0.712 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.063833376295761		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.063833376295761 | validation: 2.2875343967804582]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.036279067197818		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.036279067197818 | validation: 2.2413787001838497]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0226404592931857		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.0226404592931857 | validation: 2.2398173571059377]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0046407921860494		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.0046407921860494 | validation: 2.1425365803050016]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9875574407620407		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.9875574407620407 | validation: 2.542898277437213]
	TIME [epoch: 0.71 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.139438738315181		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.139438738315181 | validation: 2.755977777618872]
	TIME [epoch: 0.706 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6360088834134		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.6360088834134 | validation: 2.1941005267009643]
	TIME [epoch: 0.707 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.00263046954505		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.00263046954505 | validation: 2.5139389129516956]
	TIME [epoch: 0.708 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1304832605245196		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.1304832605245196 | validation: 2.1955694233153458]
	TIME [epoch: 0.706 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0001721629753413		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.0001721629753413 | validation: 2.0910553213379752]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9855187797767115		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.9855187797767115 | validation: 2.086874488163063]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9497515875314861		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.9497515875314861 | validation: 2.1180961919494017]
	TIME [epoch: 0.711 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9378153405180516		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.9378153405180516 | validation: 1.9474183179480153]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9132273837479303		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.9132273837479303 | validation: 2.073439361021783]
	TIME [epoch: 0.712 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9182514828444903		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.9182514828444903 | validation: 2.0763841445656626]
	TIME [epoch: 0.707 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0866075067305134		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.0866075067305134 | validation: 2.4356823878431864]
	TIME [epoch: 0.707 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1249736653991698		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.1249736653991698 | validation: 1.9535403887371865]
	TIME [epoch: 0.705 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.877994015078412		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.877994015078412 | validation: 1.8277912600369222]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8261513818792876		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.8261513818792876 | validation: 1.8403347823510452]
	TIME [epoch: 0.711 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.806219815107679		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.806219815107679 | validation: 1.8648377178604827]
	TIME [epoch: 0.708 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.890391703510669		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.890391703510669 | validation: 2.3175241708726806]
	TIME [epoch: 0.706 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.065668506219661		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.065668506219661 | validation: 1.7375592321459274]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8067288758018822		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.8067288758018822 | validation: 1.637686736929689]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.686493689129742		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.686493689129742 | validation: 1.5220702976195097]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6153415229937826		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.6153415229937826 | validation: 1.4728088822238325]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5662819598076316		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.5662819598076316 | validation: 1.5109759553073565]
	TIME [epoch: 0.71 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.680438844131044		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.680438844131044 | validation: 2.643844119983813]
	TIME [epoch: 0.711 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3146198372158113		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.3146198372158113 | validation: 1.5415482917502843]
	TIME [epoch: 0.709 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5976948307307492		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.5976948307307492 | validation: 1.3186117029688516]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5785568672077614		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5785568672077614 | validation: 1.8649646890247895]
	TIME [epoch: 0.711 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.885097851886581		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.885097851886581 | validation: 1.4033668252911438]
	TIME [epoch: 0.71 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5575369071368244		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.5575369071368244 | validation: 1.3421404976275801]
	TIME [epoch: 0.709 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.413500525565318		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.413500525565318 | validation: 1.1388378014790426]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3909493681728324		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3909493681728324 | validation: 1.284294142895804]
	TIME [epoch: 0.71 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3624543965589515		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.3624543965589515 | validation: 1.319325174401843]
	TIME [epoch: 0.708 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4588066104255923		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.4588066104255923 | validation: 1.5590394417363507]
	TIME [epoch: 0.707 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5624208052974746		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.5624208052974746 | validation: 1.3532278135635893]
	TIME [epoch: 0.71 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4620443508796654		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.4620443508796654 | validation: 1.1950156217086096]
	TIME [epoch: 0.709 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3225983302004543		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.3225983302004543 | validation: 1.0980401178650125]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2583805429873212		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.2583805429873212 | validation: 1.2210003398402354]
	TIME [epoch: 0.708 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2610413277200054		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.2610413277200054 | validation: 1.228824503490631]
	TIME [epoch: 0.708 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.32835557161707		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.32835557161707 | validation: 1.2840206129056715]
	TIME [epoch: 0.708 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.363462718757163		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.363462718757163 | validation: 1.2579860424249731]
	TIME [epoch: 0.706 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3360639653770963		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3360639653770963 | validation: 1.0986517841071717]
	TIME [epoch: 0.707 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2078867202724137		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.2078867202724137 | validation: 1.0733745362268068]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1716023879966775		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.1716023879966775 | validation: 1.165304460684024]
	TIME [epoch: 0.71 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1867101801522628		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.1867101801522628 | validation: 1.1298486929818061]
	TIME [epoch: 0.709 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2058171587139368		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2058171587139368 | validation: 1.1757987482001766]
	TIME [epoch: 0.71 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2069054485916502		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.2069054485916502 | validation: 1.086807447118734]
	TIME [epoch: 0.709 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1766146176339984		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1766146176339984 | validation: 1.0184863453781912]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1488557570062587		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1488557570062587 | validation: 1.1802471074030891]
	TIME [epoch: 0.708 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1842288171536537		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1842288171536537 | validation: 0.8990345270129514]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216603836838033		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1216603836838033 | validation: 1.0433565151187414]
	TIME [epoch: 0.713 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067713135862489		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.067713135862489 | validation: 0.9347155271207195]
	TIME [epoch: 0.707 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.052743487499348		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.052743487499348 | validation: 1.0481388768294575]
	TIME [epoch: 0.707 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1107078988815027		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1107078988815027 | validation: 1.3286175652362773]
	TIME [epoch: 0.707 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2239121518218423		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2239121518218423 | validation: 0.912709896557379]
	TIME [epoch: 0.707 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0768191973946457		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0768191973946457 | validation: 1.0475377507299597]
	TIME [epoch: 0.708 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0253813758470318		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.0253813758470318 | validation: 0.9158037592014516]
	TIME [epoch: 0.71 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0105986229485053		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.0105986229485053 | validation: 0.9802957839097961]
	TIME [epoch: 0.707 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0561350176648003		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0561350176648003 | validation: 1.1936072187142504]
	TIME [epoch: 0.708 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1730108258044993		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.1730108258044993 | validation: 0.954299401749477]
	TIME [epoch: 0.706 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1547004709866109		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.1547004709866109 | validation: 1.0517665898638993]
	TIME [epoch: 0.706 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014017835897379		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.014017835897379 | validation: 0.7508755793348283]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335708212148177		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9335708212148177 | validation: 0.8320060885702593]
	TIME [epoch: 0.708 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799981222884901		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8799981222884901 | validation: 0.7763004923849883]
	TIME [epoch: 0.706 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862679747307028		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.862679747307028 | validation: 0.8607990642005066]
	TIME [epoch: 0.707 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8979678198670521		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8979678198670521 | validation: 1.1960148013066203]
	TIME [epoch: 0.706 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1220388020370415		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1220388020370415 | validation: 1.1158118177969745]
	TIME [epoch: 0.707 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1768257801281123		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1768257801281123 | validation: 1.081096401282798]
	TIME [epoch: 0.708 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170290849015486		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.0170290849015486 | validation: 0.8060219534338922]
	TIME [epoch: 0.709 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9071945702397932		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9071945702397932 | validation: 0.8641273649250133]
	TIME [epoch: 0.706 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585496487378552		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.9585496487378552 | validation: 1.133565999258965]
	TIME [epoch: 0.707 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.094655108088986		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.094655108088986 | validation: 0.91287348745613]
	TIME [epoch: 0.707 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132447589967607		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.132447589967607 | validation: 0.9372894047978083]
	TIME [epoch: 0.711 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9214744745230451		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.9214744745230451 | validation: 0.8042036022938981]
	TIME [epoch: 0.706 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603661517281888		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8603661517281888 | validation: 0.8525387921863825]
	TIME [epoch: 0.706 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075461751168282		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.9075461751168282 | validation: 1.1104618136028084]
	TIME [epoch: 0.707 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0026625209516191		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.0026625209516191 | validation: 0.8353998605985633]
	TIME [epoch: 0.709 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9275127181085691		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.9275127181085691 | validation: 0.924503513225093]
	TIME [epoch: 0.708 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982803803809682		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8982803803809682 | validation: 0.8999189441635335]
	TIME [epoch: 0.709 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847931836082944		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.8847931836082944 | validation: 0.845154108745942]
	TIME [epoch: 0.709 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642165207131794		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.9642165207131794 | validation: 1.0352909101054146]
	TIME [epoch: 0.719 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766395085886012		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9766395085886012 | validation: 0.8163259099638385]
	TIME [epoch: 0.71 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9018341928205158		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.9018341928205158 | validation: 0.8275572166561844]
	TIME [epoch: 0.708 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536874262262463		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.8536874262262463 | validation: 0.8715070975233972]
	TIME [epoch: 0.709 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721240133570887		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8721240133570887 | validation: 0.8624411623491203]
	TIME [epoch: 0.709 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122106024654875		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9122106024654875 | validation: 0.9835573012218565]
	TIME [epoch: 0.708 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014764670203715		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.9014764670203715 | validation: 0.7384674251981758]
	TIME [epoch: 175 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943896225348602		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7943896225348602 | validation: 0.8006461909970608]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768701321468154		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7768701321468154 | validation: 0.7626746398189918]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165697927109369		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8165697927109369 | validation: 0.9355981075374802]
	TIME [epoch: 1.38 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9192778169204843		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9192778169204843 | validation: 0.9291118920020998]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9464643355915178		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9464643355915178 | validation: 0.7967917324592628]
	TIME [epoch: 1.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9563550606023039		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.9563550606023039 | validation: 0.9578490289574351]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919143400124463		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8919143400124463 | validation: 0.56986102392749]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793239339591965		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.793239339591965 | validation: 0.7059198837155037]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212332660174597		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7212332660174597 | validation: 0.678620621063579]
	TIME [epoch: 1.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144053336424228		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7144053336424228 | validation: 0.6667859159394118]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237876870316403		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7237876870316403 | validation: 0.6727990384706015]
	TIME [epoch: 1.38 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319060490137477		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7319060490137477 | validation: 0.7967204888173911]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600053943215099		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7600053943215099 | validation: 0.6389227665891374]
	TIME [epoch: 1.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533435221594401		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8533435221594401 | validation: 1.0204095084798865]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342440596573508		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9342440596573508 | validation: 0.5441928406773836]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757608673458947		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.757608673458947 | validation: 0.7949880160149051]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365494746529201		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7365494746529201 | validation: 1.0472197275822828]
	TIME [epoch: 1.38 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9434573772434915		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.9434573772434915 | validation: 0.8894632711344563]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012660195350076		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.012660195350076 | validation: 0.9046445039635114]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721081664175978		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.8721081664175978 | validation: 0.7271574233947942]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085885598624634		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7085885598624634 | validation: 0.598117456824656]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145703572495898		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7145703572495898 | validation: 0.8294511432527591]
	TIME [epoch: 1.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7685126591795168		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7685126591795168 | validation: 0.8259590677033931]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8695837101207475		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.8695837101207475 | validation: 0.8260781019605201]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81471054001544		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.81471054001544 | validation: 0.8556325804251103]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056773850160107		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.8056773850160107 | validation: 0.6729557339778545]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574383651977933		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7574383651977933 | validation: 0.8787723064343805]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840815111725407		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7840815111725407 | validation: 0.6599630362819773]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235865662406525		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7235865662406525 | validation: 0.7493164123053294]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097435931801035		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7097435931801035 | validation: 0.7203837873774804]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280790073216619		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7280790073216619 | validation: 0.748215412546368]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001974113339938		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8001974113339938 | validation: 0.8968683935151424]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8343371812550728		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8343371812550728 | validation: 0.6550068771060471]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393715635069812		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.8393715635069812 | validation: 0.7424676894314971]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887126160645178		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6887126160645178 | validation: 0.5550193228970655]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597080012601609		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6597080012601609 | validation: 0.6741679136878211]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448695001101925		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6448695001101925 | validation: 0.6933752189659845]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6688872086870151		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6688872086870151 | validation: 0.7478872035527748]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620519535296549		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7620519535296549 | validation: 0.9349225603954482]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513745507551067		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.8513745507551067 | validation: 0.6320918432489719]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481690934213291		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7481690934213291 | validation: 0.6985103938763348]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762151257979062		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6762151257979062 | validation: 0.6972746984798105]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609516027554466		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6609516027554466 | validation: 0.6370853069305458]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149403336278204		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.7149403336278204 | validation: 0.9454291387472082]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340517748049632		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8340517748049632 | validation: 0.6730818655354653]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658940700623091		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8658940700623091 | validation: 0.6419794963932713]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218703773433071		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6218703773433071 | validation: 0.5327305558940173]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062282110777966		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6062282110777966 | validation: 0.5751234361296832]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969842447000859		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.5969842447000859 | validation: 0.6161579781941193]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5952594007714211		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.5952594007714211 | validation: 0.5547411706854474]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713740830087932		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6713740830087932 | validation: 1.069534903958178]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896613055013641		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.896613055013641 | validation: 0.5755286254787807]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617132777156985		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6617132777156985 | validation: 0.6178012822124729]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412537206747074		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7412537206747074 | validation: 0.9335556876753179]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382809954303688		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8382809954303688 | validation: 0.6286537065962631]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937313989100664		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6937313989100664 | validation: 0.5840113039808728]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364119224124797		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6364119224124797 | validation: 0.7533809491124336]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019562582036772		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7019562582036772 | validation: 0.6497969000107643]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7113712912556841		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7113712912556841 | validation: 0.6756892178824888]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6670863720360262		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6670863720360262 | validation: 0.6808466818493448]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497329551606909		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6497329551606909 | validation: 0.5973061450811809]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027110732001657		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.7027110732001657 | validation: 0.7186011294276522]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662993478937608		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.662993478937608 | validation: 0.6039529647866868]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556341116966943		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6556341116966943 | validation: 0.658502213615583]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637797803175556		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.637797803175556 | validation: 0.6396766493228508]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167666001960064		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6167666001960064 | validation: 0.601790253968976]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372561117225759		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6372561117225759 | validation: 0.7377083763739337]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6632507658847362		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6632507658847362 | validation: 0.5257361955839065]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346751711216909		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6346751711216909 | validation: 0.6967351186607699]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6200958953231072		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6200958953231072 | validation: 0.5133364553286025]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862012598856443		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5862012598856443 | validation: 0.66342725601376]
	TIME [epoch: 1.39 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867226150542669		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5867226150542669 | validation: 0.49149136311475633]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5657301510217407		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5657301510217407 | validation: 0.6781138392013801]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899030093905818		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5899030093905818 | validation: 0.5072845307450589]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5907914797540101		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5907914797540101 | validation: 0.6851335026330363]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226164262166269		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6226164262166269 | validation: 0.6890583021918663]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976051538854151		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6976051538854151 | validation: 0.733203659598323]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0247556269957867		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.0247556269957867 | validation: 0.6359918260584627]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043839351477562		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6043839351477562 | validation: 0.4949823320678709]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887552196624065		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5887552196624065 | validation: 0.5781103733070815]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598835978673355		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.6598835978673355 | validation: 0.8274288109897152]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093003784191848		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7093003784191848 | validation: 0.5238177456413716]
	TIME [epoch: 1.38 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637526059610257		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.637526059610257 | validation: 0.5194175228771215]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341135910171356		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5341135910171356 | validation: 0.5419310277244809]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536351748988429		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.536351748988429 | validation: 0.5750246498523038]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5839489886129144		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5839489886129144 | validation: 0.6478981088419304]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5993373409324234		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5993373409324234 | validation: 0.5438381799219557]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5856974424191324		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5856974424191324 | validation: 0.5824605112003381]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6126651842470815		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6126651842470815 | validation: 0.6288568062876643]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947040223209737		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5947040223209737 | validation: 0.5069602846430462]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633005207718015		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6633005207718015 | validation: 0.6451895437613997]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5676312811245423		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5676312811245423 | validation: 0.42657062506718085]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49701815354686285		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.49701815354686285 | validation: 0.5228987767929838]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4886791670456886		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.4886791670456886 | validation: 0.5180703415360338]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261780461845167		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5261780461845167 | validation: 0.6299410047257791]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506156096675446		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6506156096675446 | validation: 0.6989702414513376]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379828188187506		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.7379828188187506 | validation: 0.48830178788171175]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47737607254249353		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.47737607254249353 | validation: 0.41886170775757936]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44836364078423613		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.44836364078423613 | validation: 0.5275242196013477]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4712064612772333		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.4712064612772333 | validation: 0.33862720496288934]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204757653603774		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5204757653603774 | validation: 0.6617608389174924]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.599346692036843		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.599346692036843 | validation: 0.45202311406412243]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5302009377916022		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5302009377916022 | validation: 0.6550068708698764]
	TIME [epoch: 1.38 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630023501318403		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6630023501318403 | validation: 0.7635408325513401]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880239363376933		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7880239363376933 | validation: 0.4567236100288133]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45779117047327317		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.45779117047327317 | validation: 0.48103976094823775]
	TIME [epoch: 1.38 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005887037979155		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5005887037979155 | validation: 0.6959695549322715]
	TIME [epoch: 1.38 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6910923953303214		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6910923953303214 | validation: 0.5259467557993259]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5059641585099643		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5059641585099643 | validation: 0.48917904637449117]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.463636029071522		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.463636029071522 | validation: 0.4809652193695113]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489344610240657		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.489344610240657 | validation: 0.6555222376624013]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5754588137099821		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.5754588137099821 | validation: 0.41275070893698357]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531882144147094		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5531882144147094 | validation: 0.5387563026737666]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46988854277370606		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.46988854277370606 | validation: 0.40803779168571075]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42904846372326305		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.42904846372326305 | validation: 0.4515364937206197]
	TIME [epoch: 1.39 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.437651524905983		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.437651524905983 | validation: 0.5474441826082478]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5223055856192772		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.5223055856192772 | validation: 0.6552582540404238]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536450289241749		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8536450289241749 | validation: 0.5281015561981176]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4981472631775181		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.4981472631775181 | validation: 0.32418463491143545]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4662478666446455		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.4662478666446455 | validation: 0.48205169905953754]
	TIME [epoch: 1.38 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4236703583224575		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.4236703583224575 | validation: 0.406682973035766]
	TIME [epoch: 1.38 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43337808728902555		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.43337808728902555 | validation: 0.5572113939001445]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4921659366402055		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.4921659366402055 | validation: 0.5104569383923461]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5628187907538311		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.5628187907538311 | validation: 0.6203727467682433]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731800344165592		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8731800344165592 | validation: 0.4559572705490098]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45883780739638313		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.45883780739638313 | validation: 0.4808467492260011]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526874641201171		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.526874641201171 | validation: 0.5287934577873886]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600742634192109		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.600742634192109 | validation: 0.5986361681088475]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4962374459335397		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.4962374459335397 | validation: 0.35812978911077753]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380669094152267		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.380669094152267 | validation: 0.38175156682977673]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36580086376132287		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.36580086376132287 | validation: 0.3580589155579429]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850518044810916		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.3850518044810916 | validation: 0.4968889392946225]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46711337859566004		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.46711337859566004 | validation: 0.643590638683763]
	TIME [epoch: 1.39 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188761893485942		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.7188761893485942 | validation: 0.4226241784936779]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41910243463652946		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.41910243463652946 | validation: 0.4889797747339227]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42161234590138874		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.42161234590138874 | validation: 0.36102243359611075]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48864877400867		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.48864877400867 | validation: 0.5489448888193396]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.496886156574358		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.496886156574358 | validation: 0.3838515133251981]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4313012374765489		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.4313012374765489 | validation: 0.48886161947334356]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025039100837696		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6025039100837696 | validation: 0.6064263432301917]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399724612881557		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5399724612881557 | validation: 0.43774076705941267]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46848185957079963		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.46848185957079963 | validation: 0.39918117425533844]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40613542278177917		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.40613542278177917 | validation: 0.4386591825335904]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4223926268891491		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.4223926268891491 | validation: 0.4544827615663394]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493610177267016		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.4493610177267016 | validation: 0.4878686497418738]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4985523436489922		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.4985523436489922 | validation: 0.4453669626569803]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42333102225207087		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.42333102225207087 | validation: 0.38803601103821905]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45010755710047806		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.45010755710047806 | validation: 0.4849747572724976]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469136569703512		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.4469136569703512 | validation: 0.39771166939939556]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46207416747511604		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.46207416747511604 | validation: 0.41271167474643405]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39750689059183947		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.39750689059183947 | validation: 0.3979676027752135]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39587457651835		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.39587457651835 | validation: 0.4199325636372308]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4309285257404885		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.4309285257404885 | validation: 0.4747771987654297]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301104346817249		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5301104346817249 | validation: 0.40047616594786767]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.400808132875615		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.400808132875615 | validation: 0.42635796099040574]
	TIME [epoch: 1.39 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.385660595067814		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.385660595067814 | validation: 0.3409709941434085]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41402753259471836		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.41402753259471836 | validation: 0.49092290106844305]
	TIME [epoch: 1.39 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493905825833352		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.4493905825833352 | validation: 0.27062399697222317]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38879288160046216		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.38879288160046216 | validation: 0.3589911167290749]
	TIME [epoch: 1.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328498603561137		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.328498603561137 | validation: 0.3157331422673642]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33791240843269604		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.33791240843269604 | validation: 0.45210094261436656]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47285427471347946		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.47285427471347946 | validation: 0.6136442806587554]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5830134254480127		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5830134254480127 | validation: 0.4317844928875939]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5448295742174412		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5448295742174412 | validation: 0.3802321365966543]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3301892795287428		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.3301892795287428 | validation: 0.28698725106566075]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323183017231721		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.323183017231721 | validation: 0.3242379650096348]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31713866666024926		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.31713866666024926 | validation: 0.24742366038710079]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33961628506287495		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.33961628506287495 | validation: 0.43785119282538815]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39390755686448886		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.39390755686448886 | validation: 0.36654860914741133]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4478768597134563		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.4478768597134563 | validation: 0.4493336978368144]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4526312996012793		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4526312996012793 | validation: 0.571269907615522]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114473770989136		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5114473770989136 | validation: 0.44009008606796174]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132529341201805		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5132529341201805 | validation: 0.3780648716685282]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35317096111436774		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.35317096111436774 | validation: 0.32333998859186386]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32454344877761626		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.32454344877761626 | validation: 0.3531445289302534]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33696132560624104		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.33696132560624104 | validation: 0.37097006579794695]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42210081398111704		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.42210081398111704 | validation: 0.4351385272025097]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42417201658498194		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.42417201658498194 | validation: 0.39357164260125505]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43434211573208886		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.43434211573208886 | validation: 0.3287498864433962]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32692760876763266		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.32692760876763266 | validation: 0.33304942772958646]
	TIME [epoch: 1.39 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31933012382631765		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.31933012382631765 | validation: 0.29998591551880366]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471819176815643		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.3471819176815643 | validation: 0.423136520611069]
	TIME [epoch: 1.39 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42217053173244373		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.42217053173244373 | validation: 0.3419080613349624]
	TIME [epoch: 1.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3893850926250727		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.3893850926250727 | validation: 0.3931605042939399]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3775691514718473		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.3775691514718473 | validation: 0.3704631322500038]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589628832381171		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.3589628832381171 | validation: 0.3295372161298742]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43332698413510434		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.43332698413510434 | validation: 0.45053766256797856]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3837427209733653		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.3837427209733653 | validation: 0.2786948529624013]
	TIME [epoch: 1.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31782286681020033		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.31782286681020033 | validation: 0.3320608908274848]
	TIME [epoch: 1.39 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978662861973126		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.2978662861973126 | validation: 0.27938422179528993]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3219987595649036		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.3219987595649036 | validation: 0.4146269561835236]
	TIME [epoch: 1.39 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907155573376636		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3907155573376636 | validation: 0.4532557934610454]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49899481317354516		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.49899481317354516 | validation: 0.3074710370316076]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37638502205614677		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.37638502205614677 | validation: 0.42615633567752165]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3408371361828627		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.3408371361828627 | validation: 0.20842080292836931]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29510543083846985		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.29510543083846985 | validation: 0.30847730973470044]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878856679243356		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2878856679243356 | validation: 0.2251282395530522]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187853186071831		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.3187853186071831 | validation: 0.3724694950530598]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3562946311214275		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3562946311214275 | validation: 0.4539455391424662]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4315118178928532		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4315118178928532 | validation: 0.39027759912577215]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4811197972874605		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.4811197972874605 | validation: 0.33618229199381294]
	TIME [epoch: 1.39 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2955010355636735		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2955010355636735 | validation: 0.26348712699459625]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743378127741819		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2743378127741819 | validation: 0.31172284580397314]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803503975815489		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.2803503975815489 | validation: 0.24709084916696586]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3016974827531404		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3016974827531404 | validation: 0.38875182478601855]
	TIME [epoch: 1.38 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34982725563830913		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.34982725563830913 | validation: 0.39920137714755904]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4280906416513064		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.4280906416513064 | validation: 0.3175790982946405]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40516830040969487		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.40516830040969487 | validation: 0.3552671375135824]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32111375460879094		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.32111375460879094 | validation: 0.2913074811413944]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3168862280139988		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.3168862280139988 | validation: 0.36812678865547527]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37258995554420565		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.37258995554420565 | validation: 0.3060637982622673]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3612454382155638		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3612454382155638 | validation: 0.33340292481508493]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3398069233986382		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3398069233986382 | validation: 0.3405681844312472]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036268219370235		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3036268219370235 | validation: 0.2841530848141116]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3363476351596296		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.3363476351596296 | validation: 0.42871160530171004]
	TIME [epoch: 1.39 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3523819882482124		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3523819882482124 | validation: 0.28795285775213086]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30430175118759856		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.30430175118759856 | validation: 0.297228935564306]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30149805718309103		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.30149805718309103 | validation: 0.3461743382411928]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33105680853669467		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.33105680853669467 | validation: 0.2665040455814131]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3525582298602012		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.3525582298602012 | validation: 0.3566735646334359]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29803294038489664		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.29803294038489664 | validation: 0.22634724596106254]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807852928495177		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.2807852928495177 | validation: 0.29186978356983184]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30094537549424966		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.30094537549424966 | validation: 0.27886080721257356]
	TIME [epoch: 1.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35284234210197923		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.35284234210197923 | validation: 0.358586075944902]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3831481700646539		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3831481700646539 | validation: 0.428314304255029]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.359098325233179		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.359098325233179 | validation: 0.22994984175654382]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2735847861266508		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.2735847861266508 | validation: 0.29412220404474193]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605788962061254		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.2605788962061254 | validation: 0.25975254509055795]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26890157494708733		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.26890157494708733 | validation: 0.2631772347780363]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228338827287402		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.3228338827287402 | validation: 0.36909045938724144]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36221898665208274		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.36221898665208274 | validation: 0.2536247538836548]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880480081772877		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.2880480081772877 | validation: 0.2870935681401672]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2819272461357791		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.2819272461357791 | validation: 0.28454793655498956]
	TIME [epoch: 1.38 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925072291464769		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.2925072291464769 | validation: 0.31603253746124776]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163268359893098		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.3163268359893098 | validation: 0.32260064763918916]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31595522355416683		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.31595522355416683 | validation: 0.3107293580074277]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.330979280721965		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.330979280721965 | validation: 0.3013571446057586]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29137457660089416		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.29137457660089416 | validation: 0.2692073387487365]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29785385875132536		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.29785385875132536 | validation: 0.32862185299818725]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2885139049918022		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.2885139049918022 | validation: 0.2601780735570882]
	TIME [epoch: 1.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789105616471799		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2789105616471799 | validation: 0.29261760615723303]
	TIME [epoch: 1.39 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785204215065206		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.2785204215065206 | validation: 0.31579114421088833]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30451173011574556		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.30451173011574556 | validation: 0.23167805083444107]
	TIME [epoch: 1.39 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3254929867418455		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.3254929867418455 | validation: 0.3262266840893845]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893892023268303		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.2893892023268303 | validation: 0.1807737916319685]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983912156079445		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.2983912156079445 | validation: 0.25394999410603164]
	TIME [epoch: 1.39 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22587086541198179		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.22587086541198179 | validation: 0.175649156162942]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20666104465471052		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.20666104465471052 | validation: 0.24592167732791115]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21921896614858438		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.21921896614858438 | validation: 0.19432935631175957]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815101260009344		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2815101260009344 | validation: 0.30933893017322694]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116286138739492		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3116286138739492 | validation: 0.4304359956631735]
	TIME [epoch: 1.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40347632331035954		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.40347632331035954 | validation: 0.3733761732429379]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872115642427362		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.3872115642427362 | validation: 0.2995425911312007]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24975491714136477		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.24975491714136477 | validation: 0.18595258913831736]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21109007859003637		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.21109007859003637 | validation: 0.247223106217252]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21254942985005698		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.21254942985005698 | validation: 0.1590269110400283]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21787369309646396		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.21787369309646396 | validation: 0.2612400013516293]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22622859895035125		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.22622859895035125 | validation: 0.16684640630637199]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21676731047524697		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.21676731047524697 | validation: 0.35361757603782706]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3019664068683405		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.3019664068683405 | validation: 0.45573365514803854]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47855653503173545		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.47855653503173545 | validation: 0.25929488060654937]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3075514992929786		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.3075514992929786 | validation: 0.28705304023140077]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24640573176191286		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.24640573176191286 | validation: 0.24617704585396716]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803184636478644		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.2803184636478644 | validation: 0.3007485400072117]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34450455098678867		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.34450455098678867 | validation: 0.2789637616134604]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.291119386098917		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.291119386098917 | validation: 0.24964968095753495]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2511788027626845		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.2511788027626845 | validation: 0.22062540487794854]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535787468368226		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.2535787468368226 | validation: 0.24761144108444283]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2949611780970549		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.2949611780970549 | validation: 0.27209017574022776]
	TIME [epoch: 1.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2554135475506854		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.2554135475506854 | validation: 0.2645591539622365]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529449322610947		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2529449322610947 | validation: 0.229812161145902]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2434189865300418		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.2434189865300418 | validation: 0.2650982826334932]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26610785315267615		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.26610785315267615 | validation: 0.22185051475963924]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612635854613607		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.2612635854613607 | validation: 0.27801083028535384]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24603129482664768		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.24603129482664768 | validation: 0.17267417279955316]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26920735304534615		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.26920735304534615 | validation: 0.2282511884621945]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.208511481257347		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.208511481257347 | validation: 0.1635954973616999]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.197042325496908		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.197042325496908 | validation: 0.23070930781824597]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22357964466482286		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.22357964466482286 | validation: 0.24376763567992754]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30203440280679217		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.30203440280679217 | validation: 0.2951860380468087]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3345167761876013		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.3345167761876013 | validation: 0.36027399113902503]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036720352739989		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.3036720352739989 | validation: 0.21186217119530495]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23368034222873835		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.23368034222873835 | validation: 0.23428307590472086]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21635390618755332		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.21635390618755332 | validation: 0.22688256971224174]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2337856466731592		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2337856466731592 | validation: 0.17863282709679473]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28456997120766525		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.28456997120766525 | validation: 0.26863055364681293]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24760736906914563		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.24760736906914563 | validation: 0.2658717747528027]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27208331887505804		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.27208331887505804 | validation: 0.24641129027458836]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27846181464875525		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.27846181464875525 | validation: 0.2442389326440497]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564287126490281		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.2564287126490281 | validation: 0.24588654224546236]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2466680325621359		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2466680325621359 | validation: 0.20952014364593588]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1993240578211477		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.1993240578211477 | validation: 0.18808396285669787]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1979963803297306		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1979963803297306 | validation: 0.24375513501517984]
	TIME [epoch: 1.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22780501438925116		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.22780501438925116 | validation: 0.24711921268583215]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778290080748427		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.2778290080748427 | validation: 0.247904601402597]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514989550164922		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.2514989550164922 | validation: 0.2627320951929564]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522325076344399		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2522325076344399 | validation: 0.17257639285906623]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22909941577752813		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.22909941577752813 | validation: 0.2565554312239769]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22504905360114857		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.22504905360114857 | validation: 0.142056882804552]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23453734934646356		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.23453734934646356 | validation: 0.20573970415263593]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19438871074747918		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.19438871074747918 | validation: 0.13231715079416081]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18731969551508787		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.18731969551508787 | validation: 0.19669976635578146]
	TIME [epoch: 2.74 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19228399786256886		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.19228399786256886 | validation: 0.217130242445304]
	TIME [epoch: 2.74 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857762980473049		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.2857762980473049 | validation: 0.3094782608467863]
	TIME [epoch: 2.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35510963356039077		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.35510963356039077 | validation: 0.25974573465010037]
	TIME [epoch: 2.75 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23325745499082812		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.23325745499082812 | validation: 0.17755475598420403]
	TIME [epoch: 2.75 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18724886352584155		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.18724886352584155 | validation: 0.18778352153163594]
	TIME [epoch: 2.75 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18855709721224323		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.18855709721224323 | validation: 0.20058764539894264]
	TIME [epoch: 2.75 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21019664045728512		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.21019664045728512 | validation: 0.20744529448772908]
	TIME [epoch: 2.75 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25101403309552234		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.25101403309552234 | validation: 0.26665830010563474]
	TIME [epoch: 2.74 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25824140052166		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.25824140052166 | validation: 0.1825149573326722]
	TIME [epoch: 2.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22948169498193846		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.22948169498193846 | validation: 0.20425992827750453]
	TIME [epoch: 2.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20347035460106191		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.20347035460106191 | validation: 0.16828711702603574]
	TIME [epoch: 2.73 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22687229716970392		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.22687229716970392 | validation: 0.220979603460256]
	TIME [epoch: 2.74 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21452411182245595		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.21452411182245595 | validation: 0.19079434874352683]
	TIME [epoch: 2.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20476072900866688		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.20476072900866688 | validation: 0.21869157960909957]
	TIME [epoch: 2.73 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336396306227841		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.2336396306227841 | validation: 0.20585872168227742]
	TIME [epoch: 2.73 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24333974048625384		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.24333974048625384 | validation: 0.21255366036805656]
	TIME [epoch: 2.73 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24055427500710536		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.24055427500710536 | validation: 0.25462797858838027]
	TIME [epoch: 2.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21694984539555473		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.21694984539555473 | validation: 0.14539029161152447]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18349364356959783		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.18349364356959783 | validation: 0.19602971259490232]
	TIME [epoch: 2.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18193225373714036		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.18193225373714036 | validation: 0.12537474543596935]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687059983285237		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1687059983285237 | validation: 0.1890046696299592]
	TIME [epoch: 2.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17511225826213483		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.17511225826213483 | validation: 0.19697595169664459]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25726665384852526		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.25726665384852526 | validation: 0.3180389920053599]
	TIME [epoch: 2.73 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32305468184419817		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.32305468184419817 | validation: 0.24025381681164848]
	TIME [epoch: 2.76 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23183033346694445		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.23183033346694445 | validation: 0.1833155643849]
	TIME [epoch: 2.73 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734694517062316		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.1734694517062316 | validation: 0.144087772977734]
	TIME [epoch: 2.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19912098117041288		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.19912098117041288 | validation: 0.21620644977222772]
	TIME [epoch: 2.73 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21621216509664287		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.21621216509664287 | validation: 0.18556406975368517]
	TIME [epoch: 2.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22205569110566145		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.22205569110566145 | validation: 0.22791408845028635]
	TIME [epoch: 2.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2452717589325604		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.2452717589325604 | validation: 0.23358321290435466]
	TIME [epoch: 2.73 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21991377843734256		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.21991377843734256 | validation: 0.18632154219314243]
	TIME [epoch: 2.73 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.190738075170537		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.190738075170537 | validation: 0.14923668244277485]
	TIME [epoch: 2.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2119134981810808		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.2119134981810808 | validation: 0.21408053240576708]
	TIME [epoch: 2.73 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18068379447997493		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.18068379447997493 | validation: 0.15448517589706892]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19489596769102313		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.19489596769102313 | validation: 0.22341699356572287]
	TIME [epoch: 2.73 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18817795523898595		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.18817795523898595 | validation: 0.1289612836812788]
	TIME [epoch: 2.73 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712242723020836		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.1712242723020836 | validation: 0.17612665204583758]
	TIME [epoch: 2.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16905815975373997		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16905815975373997 | validation: 0.1331898519873551]
	TIME [epoch: 2.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18541742950378165		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.18541742950378165 | validation: 0.20849864906588522]
	TIME [epoch: 2.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21697247286159976		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.21697247286159976 | validation: 0.3064007687576892]
	TIME [epoch: 2.73 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3103195983769539		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.3103195983769539 | validation: 0.26246283110517016]
	TIME [epoch: 2.74 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27108855064893467		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.27108855064893467 | validation: 0.16842488834052854]
	TIME [epoch: 2.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18197639556745165		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.18197639556745165 | validation: 0.17041755438771866]
	TIME [epoch: 2.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15908231453366037		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.15908231453366037 | validation: 0.13619677702384755]
	TIME [epoch: 2.74 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16150524478352857		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.16150524478352857 | validation: 0.17780766071881093]
	TIME [epoch: 2.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16823239960254505		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.16823239960254505 | validation: 0.15254225354984385]
	TIME [epoch: 2.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20654142317528318		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.20654142317528318 | validation: 0.21701718018369442]
	TIME [epoch: 2.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2127701185748957		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.2127701185748957 | validation: 0.18606958625361866]
	TIME [epoch: 2.73 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23317118764860115		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.23317118764860115 | validation: 0.18599314509486858]
	TIME [epoch: 2.74 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.192831148217716		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.192831148217716 | validation: 0.2315833781894701]
	TIME [epoch: 2.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19883445480087342		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.19883445480087342 | validation: 0.15503908514609874]
	TIME [epoch: 2.73 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669379547642175		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.1669379547642175 | validation: 0.16871102722792475]
	TIME [epoch: 2.74 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16577610584201996		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.16577610584201996 | validation: 0.1723146759123014]
	TIME [epoch: 2.74 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17196770579686357		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.17196770579686357 | validation: 0.15112927737683243]
	TIME [epoch: 2.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1969645503585236		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1969645503585236 | validation: 0.20378676106523824]
	TIME [epoch: 2.73 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20735736908048227		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.20735736908048227 | validation: 0.1609033466861332]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2191931189474171		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.2191931189474171 | validation: 0.19571421495091024]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19356252485356337		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.19356252485356337 | validation: 0.16986747895942172]
	TIME [epoch: 2.73 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17473667886477245		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.17473667886477245 | validation: 0.1581275973756388]
	TIME [epoch: 2.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17055071184924855		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.17055071184924855 | validation: 0.16874887294669383]
	TIME [epoch: 2.74 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15501191750016094		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.15501191750016094 | validation: 0.15232025402003774]
	TIME [epoch: 2.73 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588775164766381		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.1588775164766381 | validation: 0.18537181771628564]
	TIME [epoch: 2.73 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20217986642593921		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.20217986642593921 | validation: 0.2218173452421799]
	TIME [epoch: 2.73 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23752898021707275		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.23752898021707275 | validation: 0.16438907071587666]
	TIME [epoch: 2.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25319692464501414		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.25319692464501414 | validation: 0.1614498447281542]
	TIME [epoch: 2.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144174000576708		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.13144174000576708 | validation: 0.13730788745141217]
	TIME [epoch: 2.73 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12156610234368027		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.12156610234368027 | validation: 0.10912770491358198]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462276708146757		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.1462276708146757 | validation: 0.2026917210150026]
	TIME [epoch: 2.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921057988676647		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.1921057988676647 | validation: 0.1427363349237771]
	TIME [epoch: 2.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20908395769518506		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.20908395769518506 | validation: 0.20537689561279784]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16463191321743936		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.16463191321743936 | validation: 0.14066512439390927]
	TIME [epoch: 2.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15084138379248963		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.15084138379248963 | validation: 0.18644419261313588]
	TIME [epoch: 2.74 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19454397202844448		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.19454397202844448 | validation: 0.2956277438103381]
	TIME [epoch: 2.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28998160082862084		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.28998160082862084 | validation: 0.21777946244768487]
	TIME [epoch: 2.74 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2303872300772324		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.2303872300772324 | validation: 0.12154621028290907]
	TIME [epoch: 2.73 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15985928805065033		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.15985928805065033 | validation: 0.16117463259588194]
	TIME [epoch: 2.73 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13770288678204046		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.13770288678204046 | validation: 0.12142175010999634]
	TIME [epoch: 2.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13882299085457284		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.13882299085457284 | validation: 0.1561931914884198]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15522277993675704		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.15522277993675704 | validation: 0.15362310534221452]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18731949485443927		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.18731949485443927 | validation: 0.20026882160296894]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21553530283673772		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.21553530283673772 | validation: 0.20901818006514625]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23048799402532955		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.23048799402532955 | validation: 0.1768271250524229]
	TIME [epoch: 2.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1766922232867669		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1766922232867669 | validation: 0.14889511693461574]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15148372964397971		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.15148372964397971 | validation: 0.14700611627092025]
	TIME [epoch: 2.73 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13269171002711277		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.13269171002711277 | validation: 0.11647818846397624]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14556789462297984		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.14556789462297984 | validation: 0.1867482185780085]
	TIME [epoch: 2.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16842481566227782		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.16842481566227782 | validation: 0.12231911949086723]
	TIME [epoch: 2.73 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955580235932506		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.1955580235932506 | validation: 0.15375962043599178]
	TIME [epoch: 2.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14043232171856831		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.14043232171856831 | validation: 0.15373702232406827]
	TIME [epoch: 2.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14680933786759817		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.14680933786759817 | validation: 0.16991918184346522]
	TIME [epoch: 2.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178513509779581		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.178513509779581 | validation: 0.22675903355776994]
	TIME [epoch: 2.73 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25379513556590555		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.25379513556590555 | validation: 0.18800613643409633]
	TIME [epoch: 2.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18231508980946584		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.18231508980946584 | validation: 0.1147794779512295]
	TIME [epoch: 2.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473484459699532		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1473484459699532 | validation: 0.1458341567436562]
	TIME [epoch: 2.73 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297271334416613		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.1297271334416613 | validation: 0.11416051282824863]
	TIME [epoch: 2.73 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255865624457276		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.1255865624457276 | validation: 0.14355487297196898]
	TIME [epoch: 2.73 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13110692225306125		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.13110692225306125 | validation: 0.12617381802536154]
	TIME [epoch: 2.73 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16373399361756455		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.16373399361756455 | validation: 0.20321186071750705]
	TIME [epoch: 2.74 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18049320016883927		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.18049320016883927 | validation: 0.15587904802061248]
	TIME [epoch: 2.74 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17994572982982468		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.17994572982982468 | validation: 0.17871287027656257]
	TIME [epoch: 2.74 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18193073016234973		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.18193073016234973 | validation: 0.22115260658082223]
	TIME [epoch: 2.74 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18562078603440554		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.18562078603440554 | validation: 0.16550829762648486]
	TIME [epoch: 2.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15257245117697715		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.15257245117697715 | validation: 0.12391080337174804]
	TIME [epoch: 2.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474683331086035		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.1474683331086035 | validation: 0.15620944173111623]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16767279880323563		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.16767279880323563 | validation: 0.1196992113711425]
	TIME [epoch: 2.73 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18264763847465104		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.18264763847465104 | validation: 0.17094636610068947]
	TIME [epoch: 2.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14926410062915824		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.14926410062915824 | validation: 0.13407358566785982]
	TIME [epoch: 2.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540086766789011		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.1540086766789011 | validation: 0.170718182258293]
	TIME [epoch: 2.72 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17402003020215362		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.17402003020215362 | validation: 0.1623292824921696]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17824252108414723		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.17824252108414723 | validation: 0.1717989263108688]
	TIME [epoch: 2.73 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659844245406961		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1659844245406961 | validation: 0.1438878482932955]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15932936107193515		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.15932936107193515 | validation: 0.15044839447319497]
	TIME [epoch: 2.72 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147688421803819		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.147688421803819 | validation: 0.12752826002071563]
	TIME [epoch: 2.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15217071710050253		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.15217071710050253 | validation: 0.166238578449734]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15185548639559843		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.15185548639559843 | validation: 0.1106363947598008]
	TIME [epoch: 2.73 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15505860616271677		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.15505860616271677 | validation: 0.15946889810706857]
	TIME [epoch: 2.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15134565257829383		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.15134565257829383 | validation: 0.11678465091189937]
	TIME [epoch: 2.74 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17275270436572968		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.17275270436572968 | validation: 0.15304354504222617]
	TIME [epoch: 2.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313259859855847		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1313259859855847 | validation: 0.11045415216550153]
	TIME [epoch: 2.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440496612539659		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.1440496612539659 | validation: 0.14151538978511122]
	TIME [epoch: 2.72 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14083138580816593		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.14083138580816593 | validation: 0.15700740258898305]
	TIME [epoch: 2.73 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1852390129661238		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.1852390129661238 | validation: 0.19636947449248282]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2116626037857379		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.2116626037857379 | validation: 0.1893611455393545]
	TIME [epoch: 2.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17650816699147348		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.17650816699147348 | validation: 0.12002816229702518]
	TIME [epoch: 2.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11167988119316867		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.11167988119316867 | validation: 0.12012344971924518]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11115638738516168		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.11115638738516168 | validation: 0.1344412235077639]
	TIME [epoch: 2.74 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12436404393446761		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.12436404393446761 | validation: 0.14081088916422174]
	TIME [epoch: 2.73 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13548793384643298		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.13548793384643298 | validation: 0.14796976645778395]
	TIME [epoch: 2.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17876319555597014		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.17876319555597014 | validation: 0.17970780004577713]
	TIME [epoch: 2.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1744692928798703		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.1744692928798703 | validation: 0.11776604348735908]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608852230984438		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.1608852230984438 | validation: 0.1651858249412095]
	TIME [epoch: 2.73 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14110408726372395		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.14110408726372395 | validation: 0.09939315952059159]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14059485749616651		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.14059485749616651 | validation: 0.13365106965454082]
	TIME [epoch: 2.73 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12260439306734112		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.12260439306734112 | validation: 0.12344858074775172]
	TIME [epoch: 2.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15925325945447064		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.15925325945447064 | validation: 0.16352177529131628]
	TIME [epoch: 2.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16346190454724038		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.16346190454724038 | validation: 0.13009096529755015]
	TIME [epoch: 2.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397670711371054		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.1397670711371054 | validation: 0.13526045826602498]
	TIME [epoch: 2.73 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12255265752681599		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.12255265752681599 | validation: 0.12469997694292512]
	TIME [epoch: 2.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264980253519956		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.1264980253519956 | validation: 0.1458390669777248]
	TIME [epoch: 2.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.138451995802601		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.138451995802601 | validation: 0.1293772208543769]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652151313771601		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.1652151313771601 | validation: 0.1477550613859117]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378185303329008		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.1378185303329008 | validation: 0.11484225180748771]
	TIME [epoch: 2.72 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397326804025045		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.12397326804025045 | validation: 0.12886820349149772]
	TIME [epoch: 2.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13000603167177346		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.13000603167177346 | validation: 0.10801286720070565]
	TIME [epoch: 2.72 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14189108878282966		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.14189108878282966 | validation: 0.1751458933323661]
	TIME [epoch: 2.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700024914854415		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.1700024914854415 | validation: 0.12106085272750855]
	TIME [epoch: 2.72 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14095529261333312		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.14095529261333312 | validation: 0.12903160854570686]
	TIME [epoch: 2.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11507111440229775		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.11507111440229775 | validation: 0.11164171671580672]
	TIME [epoch: 2.72 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944098493576411		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.10944098493576411 | validation: 0.12309774463342474]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12159591698574414		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.12159591698574414 | validation: 0.1480761848269531]
	TIME [epoch: 2.72 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13983863901094093		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.13983863901094093 | validation: 0.15114719052557285]
	TIME [epoch: 2.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536877739100562		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.1536877739100562 | validation: 0.13350983117996615]
	TIME [epoch: 2.72 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17025308553829446		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.17025308553829446 | validation: 0.15825427957304652]
	TIME [epoch: 2.74 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615416368133332		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.1615416368133332 | validation: 0.1076072856929137]
	TIME [epoch: 2.72 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14888214554820461		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.14888214554820461 | validation: 0.13377603049996467]
	TIME [epoch: 2.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12712739173539872		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.12712739173539872 | validation: 0.10625057203998817]
	TIME [epoch: 2.73 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13951291049722606		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.13951291049722606 | validation: 0.15174214186164148]
	TIME [epoch: 2.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12222811831018594		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.12222811831018594 | validation: 0.10518691591325764]
	TIME [epoch: 2.73 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10488815627118381		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.10488815627118381 | validation: 0.11672058371847074]
	TIME [epoch: 2.74 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10287100607296402		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.10287100607296402 | validation: 0.11084893893884014]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035976209282093		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.1035976209282093 | validation: 0.12010792208170157]
	TIME [epoch: 2.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12252164325661608		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.12252164325661608 | validation: 0.16179514766082898]
	TIME [epoch: 2.72 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740695320197149		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1740695320197149 | validation: 0.16275062209002072]
	TIME [epoch: 2.74 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20916910985124265		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.20916910985124265 | validation: 0.1617814133601292]
	TIME [epoch: 2.72 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16081129141843678		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.16081129141843678 | validation: 0.10302581562251585]
	TIME [epoch: 2.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10546171351981709		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.10546171351981709 | validation: 0.12026242358914659]
	TIME [epoch: 2.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1062661135894731		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1062661135894731 | validation: 0.08658231125285834]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11644047290028565		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.11644047290028565 | validation: 0.14718496808196008]
	TIME [epoch: 2.73 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13630904978714967		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.13630904978714967 | validation: 0.10592620651545848]
	TIME [epoch: 2.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16041053889493392		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.16041053889493392 | validation: 0.13665464693719706]
	TIME [epoch: 2.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13687562194998404		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.13687562194998404 | validation: 0.14941405599118524]
	TIME [epoch: 2.73 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14539151692936017		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.14539151692936017 | validation: 0.12314523722572959]
	TIME [epoch: 2.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1160821896705383		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.1160821896705383 | validation: 0.0972963012342597]
	TIME [epoch: 2.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13454665340949717		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.13454665340949717 | validation: 0.13769295032862522]
	TIME [epoch: 2.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144879982076113		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.13144879982076113 | validation: 0.09222260391593753]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100782959384479		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.12100782959384479 | validation: 0.11938452029723173]
	TIME [epoch: 2.73 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.114557056998211		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.114557056998211 | validation: 0.09711509822433312]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12893282242665344		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.12893282242665344 | validation: 0.1406660840601382]
	TIME [epoch: 2.74 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12872752557458164		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.12872752557458164 | validation: 0.11372777789231812]
	TIME [epoch: 2.74 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15624953419910775		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.15624953419910775 | validation: 0.13757638255597468]
	TIME [epoch: 2.73 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217073358198953		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.1217073358198953 | validation: 0.11818055930344636]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10999141945030334		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.10999141945030334 | validation: 0.10605012963407923]
	TIME [epoch: 2.74 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10661138357753373		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.10661138357753373 | validation: 0.10769446276859163]
	TIME [epoch: 2.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10358136230606474		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.10358136230606474 | validation: 0.12073536481710774]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1109481272005457		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.1109481272005457 | validation: 0.12448545233270553]
	TIME [epoch: 2.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518418173017009		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.1518418173017009 | validation: 0.1526794866133796]
	TIME [epoch: 2.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15157806167803103		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.15157806167803103 | validation: 0.09193992403178575]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11713910754560652		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.11713910754560652 | validation: 0.11911575677459546]
	TIME [epoch: 2.74 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441545039952878		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.10441545039952878 | validation: 0.09533794725913587]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10663693753681561		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.10663693753681561 | validation: 0.12216876474949903]
	TIME [epoch: 2.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12216595108486285		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.12216595108486285 | validation: 0.11614463666955706]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17471577865515708		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.17471577865515708 | validation: 0.13128547426034956]
	TIME [epoch: 2.73 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11794773786609682		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.11794773786609682 | validation: 0.12390617067955763]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1053351434343993		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.1053351434343993 | validation: 0.10209721963620666]
	TIME [epoch: 2.74 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817879267790931		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.10817879267790931 | validation: 0.1236313485041784]
	TIME [epoch: 2.73 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11343959747719064		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.11343959747719064 | validation: 0.09411487300606718]
	TIME [epoch: 2.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09269647435469096		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.09269647435469096 | validation: 0.09706477121525621]
	TIME [epoch: 2.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858831311528312		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.0858831311528312 | validation: 0.10738455170629738]
	TIME [epoch: 2.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09020836609248903		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.09020836609248903 | validation: 0.08429936209968927]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09041983785675087		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.09041983785675087 | validation: 0.10550265678720305]
	TIME [epoch: 2.75 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654153982355598		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09654153982355598 | validation: 0.11403898161715649]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10775784860177905		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.10775784860177905 | validation: 0.15516163925843882]
	TIME [epoch: 2.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2146081890850832		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2146081890850832 | validation: 0.19509867315316476]
	TIME [epoch: 2.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2139899966341907		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.2139899966341907 | validation: 0.09732892226266096]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187369692690354		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.10187369692690354 | validation: 0.09531934977827485]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09674536288035673		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.09674536288035673 | validation: 0.14579035693413805]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14305753232263174		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.14305753232263174 | validation: 0.1012745971291354]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731367065387917		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.1731367065387917 | validation: 0.11446888380306966]
	TIME [epoch: 2.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192377276011691		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.10192377276011691 | validation: 0.12366307515816494]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10694834996226897		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.10694834996226897 | validation: 0.07832625280098318]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11349821264708357		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.11349821264708357 | validation: 0.11235393526330967]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10569105034752457		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.10569105034752457 | validation: 0.10259655440237887]
	TIME [epoch: 2.73 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12735561771043533		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.12735561771043533 | validation: 0.13638947934126258]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1282587717917992		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.1282587717917992 | validation: 0.0971566957459146]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11224777856204404		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.11224777856204404 | validation: 0.10186672262978665]
	TIME [epoch: 2.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005333741132868		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.1005333741132868 | validation: 0.09310415006939951]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09625085299247597		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.09625085299247597 | validation: 0.10731105216875428]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838945586650572		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.09838945586650572 | validation: 0.10443725840174528]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1223421341123046		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.1223421341123046 | validation: 0.12454203876072052]
	TIME [epoch: 2.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12026515322792129		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.12026515322792129 | validation: 0.08831964429686817]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11161627523863688		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.11161627523863688 | validation: 0.11599023005841197]
	TIME [epoch: 2.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400222016114828		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.10400222016114828 | validation: 0.08641108658092649]
	TIME [epoch: 2.74 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11883538014352256		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.11883538014352256 | validation: 0.13139726330683568]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476516251840344		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.10476516251840344 | validation: 0.09333322851596117]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002003409241016		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.1002003409241016 | validation: 0.1127694786265836]
	TIME [epoch: 2.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09794816518456685		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.09794816518456685 | validation: 0.09366003813979033]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14082482701123947		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.14082482701123947 | validation: 0.12192630576642434]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10641811234078448		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.10641811234078448 | validation: 0.09665869789984051]
	TIME [epoch: 2.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09907398678870831		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.09907398678870831 | validation: 0.11662538272226863]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373167794110792		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.10373167794110792 | validation: 0.12046046441163619]
	TIME [epoch: 2.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11957193461691537		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.11957193461691537 | validation: 0.10357113050529275]
	TIME [epoch: 2.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08937621441649317		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.08937621441649317 | validation: 0.09287308859517322]
	TIME [epoch: 2.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08948956891505423		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.08948956891505423 | validation: 0.11050200327529579]
	TIME [epoch: 2.74 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09342380577647004		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.09342380577647004 | validation: 0.07943660631130167]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115819416544205		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.115819416544205 | validation: 0.15403347558906638]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15433908337337562		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.15433908337337562 | validation: 0.11621976622141852]
	TIME [epoch: 2.73 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14388027650099072		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.14388027650099072 | validation: 0.11278881630709092]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10090350242078738		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.10090350242078738 | validation: 0.11732991178110726]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10444039747575508		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.10444039747575508 | validation: 0.07664448587391366]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08846306914746105		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.08846306914746105 | validation: 0.10424393769884331]
	TIME [epoch: 2.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09992621272467371		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.09992621272467371 | validation: 0.1038160863848213]
	TIME [epoch: 2.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10738290703745854		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.10738290703745854 | validation: 0.12089689696781086]
	TIME [epoch: 2.73 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1081747095747342		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.1081747095747342 | validation: 0.08571286995167292]
	TIME [epoch: 2.74 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10048283618556861		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.10048283618556861 | validation: 0.11086323469064295]
	TIME [epoch: 2.74 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10535236903120096		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.10535236903120096 | validation: 0.08271028238887605]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11756555691204415		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.11756555691204415 | validation: 0.11778146750933921]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696995513643756		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.10696995513643756 | validation: 0.08797863591926812]
	TIME [epoch: 2.74 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09210786053249939		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.09210786053249939 | validation: 0.09684694358137827]
	TIME [epoch: 2.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08363262662320926		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.08363262662320926 | validation: 0.09452077399026801]
	TIME [epoch: 2.74 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08727233745994585		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.08727233745994585 | validation: 0.08818384878393448]
	TIME [epoch: 2.74 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08895561125048161		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.08895561125048161 | validation: 0.0927593576093293]
	TIME [epoch: 2.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152821431833596		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.1152821431833596 | validation: 0.13183000223440783]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13695924352171754		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.13695924352171754 | validation: 0.08330537064331942]
	TIME [epoch: 2.74 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021723661139777		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.1021723661139777 | validation: 0.10270506533969309]
	TIME [epoch: 2.74 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115199741055059		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.09115199741055059 | validation: 0.08733677489793032]
	TIME [epoch: 2.74 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09250926026205437		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.09250926026205437 | validation: 0.09469812675728427]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858823869177861		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0858823869177861 | validation: 0.09070425405772375]
	TIME [epoch: 2.74 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812708137051591		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07812708137051591 | validation: 0.08109212988328107]
	TIME [epoch: 2.74 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07797338390198197		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.07797338390198197 | validation: 0.07973447233338388]
	TIME [epoch: 2.74 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114543869766554		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.08114543869766554 | validation: 0.08831143223459371]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660204655791745		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.08660204655791745 | validation: 0.09790158258917919]
	TIME [epoch: 2.73 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035849651594906		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.1035849651594906 | validation: 0.14765031529660638]
	TIME [epoch: 2.73 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15363231299816021		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.15363231299816021 | validation: 0.10051129174417306]
	TIME [epoch: 2.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15640914928814692		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.15640914928814692 | validation: 0.10563446690739525]
	TIME [epoch: 2.73 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224684994323556		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.09224684994323556 | validation: 0.08346590592377963]
	TIME [epoch: 2.75 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132054122658641		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.08132054122658641 | validation: 0.0884182704632249]
	TIME [epoch: 2.74 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07873851586026363		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.07873851586026363 | validation: 0.07893754431879244]
	TIME [epoch: 2.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07528963280420993		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.07528963280420993 | validation: 0.07473126339946591]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760274635909001		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.07760274635909001 | validation: 0.09463087193651952]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129301183560182		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.09129301183560182 | validation: 0.09140432822604234]
	TIME [epoch: 2.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348790494135461		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.1348790494135461 | validation: 0.13758252117946088]
	TIME [epoch: 2.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13191133881290257		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.13191133881290257 | validation: 0.09364299742607674]
	TIME [epoch: 2.73 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402349824783869		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.09402349824783869 | validation: 0.09183530113920516]
	TIME [epoch: 2.73 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08303084684110261		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.08303084684110261 | validation: 0.09282219754846904]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08553592844501391		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08553592844501391 | validation: 0.07548951213764134]
	TIME [epoch: 2.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596169466990905		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.09596169466990905 | validation: 0.09532415649992765]
	TIME [epoch: 2.73 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197075403523357		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.09197075403523357 | validation: 0.0720590531715111]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0946085501663553		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.0946085501663553 | validation: 0.13664539465570877]
	TIME [epoch: 2.74 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14531681987739822		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.14531681987739822 | validation: 0.09863849983921996]
	TIME [epoch: 2.74 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09819722569138702		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.09819722569138702 | validation: 0.09326345239127397]
	TIME [epoch: 2.74 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744274003238164		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.08744274003238164 | validation: 0.06696819358156243]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10051095638831882		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.10051095638831882 | validation: 0.10341145792292097]
	TIME [epoch: 2.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09175037794128756		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.09175037794128756 | validation: 0.084354242697128]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09347279885043047		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.09347279885043047 | validation: 0.09114294894482017]
	TIME [epoch: 2.73 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08782394708970685		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.08782394708970685 | validation: 0.07586278545444153]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07798239049284472		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.07798239049284472 | validation: 0.08293958907046167]
	TIME [epoch: 2.73 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0730275952151772		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.0730275952151772 | validation: 0.07952567808486076]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140414552184339		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.07140414552184339 | validation: 0.06940359642783152]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822778545762429		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.0822778545762429 | validation: 0.11113872656787588]
	TIME [epoch: 2.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10713713348692822		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.10713713348692822 | validation: 0.10760857537138431]
	TIME [epoch: 2.74 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501898429902284		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.1501898429902284 | validation: 0.1132624725130893]
	TIME [epoch: 2.74 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594995261838412		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.10594995261838412 | validation: 0.09874884639424462]
	TIME [epoch: 2.73 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09026030074137928		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.09026030074137928 | validation: 0.07239054268730781]
	TIME [epoch: 2.74 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07920149991907152		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.07920149991907152 | validation: 0.09633313297216083]
	TIME [epoch: 2.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08595343179472568		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.08595343179472568 | validation: 0.08562309366850492]
	TIME [epoch: 2.74 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958841746636752		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.0958841746636752 | validation: 0.10598160758425296]
	TIME [epoch: 2.73 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085396759889165		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.10085396759889165 | validation: 0.08190136171463827]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10844403471037069		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.10844403471037069 | validation: 0.10623634314136009]
	TIME [epoch: 2.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08847317052414702		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.08847317052414702 | validation: 0.06962969179706079]
	TIME [epoch: 2.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014798783132591		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.08014798783132591 | validation: 0.07447347201428609]
	TIME [epoch: 2.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07549978661600235		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.07549978661600235 | validation: 0.08374052935063528]
	TIME [epoch: 2.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738920921759706		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.0738920921759706 | validation: 0.06924555372364291]
	TIME [epoch: 2.72 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083353228427514		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08083353228427514 | validation: 0.10834721187677801]
	TIME [epoch: 2.72 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590907141830397		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.09590907141830397 | validation: 0.07397859123816346]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11072126498850204		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.11072126498850204 | validation: 0.11850631952756377]
	TIME [epoch: 2.73 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11584839844534785		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.11584839844534785 | validation: 0.09745603287683753]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10563485615159937		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.10563485615159937 | validation: 0.09132872854243243]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08062870571191028		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.08062870571191028 | validation: 0.06642101310057481]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904454736175798		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.07904454736175798 | validation: 0.08047341398933164]
	TIME [epoch: 2.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028655819250267		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.07028655819250267 | validation: 0.0786906601138304]
	TIME [epoch: 2.74 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07151709168184874		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.07151709168184874 | validation: 0.07515301427598173]
	TIME [epoch: 2.74 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417782722326934		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.07417782722326934 | validation: 0.09044010556951865]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0793383795547085		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.0793383795547085 | validation: 0.07295098672293833]
	TIME [epoch: 2.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08309698935973436		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.08309698935973436 | validation: 0.09090759836670756]
	TIME [epoch: 2.73 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282801018120655		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.09282801018120655 | validation: 0.0841801995737746]
	TIME [epoch: 2.73 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487570916373525		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.10487570916373525 | validation: 0.10528627818496746]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464189781611466		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.09464189781611466 | validation: 0.06955282337250525]
	TIME [epoch: 2.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950941768197198		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.07950941768197198 | validation: 0.08751319908075088]
	TIME [epoch: 2.74 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07476231113407643		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.07476231113407643 | validation: 0.06829044533269414]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07066896518075498		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.07066896518075498 | validation: 0.09013529197278897]
	TIME [epoch: 2.73 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085129794988546		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.07085129794988546 | validation: 0.05777837219084234]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08028697953412062		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.08028697953412062 | validation: 0.09953152935488987]
	TIME [epoch: 2.73 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908897353863683		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.09908897353863683 | validation: 0.0881001588870495]
	TIME [epoch: 2.73 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10797361584143915		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.10797361584143915 | validation: 0.112007127056146]
	TIME [epoch: 2.73 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10219033203778903		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.10219033203778903 | validation: 0.07796648003524773]
	TIME [epoch: 2.73 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636767999717819		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.08636767999717819 | validation: 0.07701367508022301]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07371068883288531		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.07371068883288531 | validation: 0.07490625444556138]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06782210038167483		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06782210038167483 | validation: 0.07704592014404338]
	TIME [epoch: 2.72 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945787670305732		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.06945787670305732 | validation: 0.07801934288362422]
	TIME [epoch: 2.73 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07329198727088419		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.07329198727088419 | validation: 0.07208889118540722]
	TIME [epoch: 2.72 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08918954531062244		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.08918954531062244 | validation: 0.11453497107927567]
	TIME [epoch: 2.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11240224612552915		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.11240224612552915 | validation: 0.08326981508064298]
	TIME [epoch: 2.73 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669137707241126		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.10669137707241126 | validation: 0.08304314375762316]
	TIME [epoch: 2.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117841705070051		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.07117841705070051 | validation: 0.08760048716124377]
	TIME [epoch: 2.73 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07607328104676324		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.07607328104676324 | validation: 0.06315634748507995]
	TIME [epoch: 2.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08572768736396899		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.08572768736396899 | validation: 0.09121368242926288]
	TIME [epoch: 2.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08531955628172744		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.08531955628172744 | validation: 0.07445770987734222]
	TIME [epoch: 2.73 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08441776993611257		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.08441776993611257 | validation: 0.094301886701051]
	TIME [epoch: 2.73 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08131403282836441		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.08131403282836441 | validation: 0.07213991508201108]
	TIME [epoch: 2.73 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764863319609812		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.0764863319609812 | validation: 0.0828359129994033]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874314816604205		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.07874314816604205 | validation: 0.06468430983141853]
	TIME [epoch: 2.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08035922614848176		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.08035922614848176 | validation: 0.08498837178797804]
	TIME [epoch: 2.73 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995806865704645		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.07995806865704645 | validation: 0.07173259232677914]
	TIME [epoch: 2.73 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.083632547083464		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.083632547083464 | validation: 0.09890965893597913]
	TIME [epoch: 2.73 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741847267400527		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.09741847267400527 | validation: 0.07674606733294252]
	TIME [epoch: 2.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11467297466666526		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.11467297466666526 | validation: 0.0843178456779235]
	TIME [epoch: 2.73 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07229325962758093		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07229325962758093 | validation: 0.08228427059613941]
	TIME [epoch: 2.73 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06969500909216253		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.06969500909216253 | validation: 0.061863743130511496]
	TIME [epoch: 2.74 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320733766977458		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.07320733766977458 | validation: 0.08082925958641822]
	TIME [epoch: 2.74 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07428036022313003		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.07428036022313003 | validation: 0.07158222098910953]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431013747829691		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.07431013747829691 | validation: 0.08932094716730087]
	TIME [epoch: 2.73 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555856964573184		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.07555856964573184 | validation: 0.06708660220119204]
	TIME [epoch: 2.73 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192253392993804		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.08192253392993804 | validation: 0.09053770152001284]
	TIME [epoch: 2.73 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08167355466675445		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.08167355466675445 | validation: 0.0746961184188372]
	TIME [epoch: 2.73 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09593405128752293		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.09593405128752293 | validation: 0.08640363281365358]
	TIME [epoch: 2.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08146357941542487		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.08146357941542487 | validation: 0.06187543102703274]
	TIME [epoch: 2.73 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07002759244791051		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.07002759244791051 | validation: 0.07460747776882286]
	TIME [epoch: 2.73 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833273862833555		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.06833273862833555 | validation: 0.06680831182016894]
	TIME [epoch: 2.73 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689985116525434		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.0689985116525434 | validation: 0.07948793785454009]
	TIME [epoch: 2.73 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06881565265674443		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.06881565265674443 | validation: 0.06289531157103855]
	TIME [epoch: 2.73 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07025300619624508		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.07025300619624508 | validation: 0.0741872360607316]
	TIME [epoch: 2.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08206813825525786		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.08206813825525786 | validation: 0.08449361382869892]
	TIME [epoch: 2.73 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07276683635551802		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07276683635551802 | validation: 0.07220833161047994]
	TIME [epoch: 2.73 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761817571312764		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.0761817571312764 | validation: 0.10357104844334629]
	TIME [epoch: 2.73 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09397686949258668		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.09397686949258668 | validation: 0.09730463497517798]
	TIME [epoch: 2.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722867410970252		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.10722867410970252 | validation: 0.08701998269392683]
	TIME [epoch: 2.73 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08392095634233442		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.08392095634233442 | validation: 0.06659051788731593]
	TIME [epoch: 2.73 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07390944302142854		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.07390944302142854 | validation: 0.07579518050322569]
	TIME [epoch: 2.73 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06654049768943168		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.06654049768943168 | validation: 0.07862273774807538]
	TIME [epoch: 2.73 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003625033780635		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.07003625033780635 | validation: 0.07249519572677653]
	TIME [epoch: 2.73 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771636029825365		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0771636029825365 | validation: 0.09494073987881124]
	TIME [epoch: 2.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09171782086418166		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.09171782086418166 | validation: 0.07664649765717595]
	TIME [epoch: 2.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08350246505378561		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.08350246505378561 | validation: 0.08478805514740777]
	TIME [epoch: 2.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07644362710807344		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.07644362710807344 | validation: 0.06866492588339165]
	TIME [epoch: 2.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08088972804604504		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.08088972804604504 | validation: 0.08664472569364691]
	TIME [epoch: 2.73 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477152403808472		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.07477152403808472 | validation: 0.06372959953026791]
	TIME [epoch: 2.74 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06716492128909213		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.06716492128909213 | validation: 0.06197976076459717]
	TIME [epoch: 2.73 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06586771447647814		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.06586771447647814 | validation: 0.07937844814424622]
	TIME [epoch: 2.73 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653276782550323		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.06653276782550323 | validation: 0.06856886986597992]
	TIME [epoch: 2.73 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965913173088757		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.06965913173088757 | validation: 0.08164404095624175]
	TIME [epoch: 2.73 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471905603553192		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.07471905603553192 | validation: 0.06623255733702997]
	TIME [epoch: 2.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08278150281965001		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.08278150281965001 | validation: 0.0995769817376203]
	TIME [epoch: 2.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08766713892770414		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.08766713892770414 | validation: 0.08083168398049972]
	TIME [epoch: 2.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012816844495398		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.08012816844495398 | validation: 0.08621577791458174]
	TIME [epoch: 2.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0731438231658945		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.0731438231658945 | validation: 0.06881478243578114]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548883423661651		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.06548883423661651 | validation: 0.0558382287169604]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_889.pth
	Model improved!!!
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.063729465447537		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.063729465447537 | validation: 0.07037551870629159]
	TIME [epoch: 2.73 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346494031696578		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.06346494031696578 | validation: 0.06578886540997302]
	TIME [epoch: 2.73 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06955276393376418		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06955276393376418 | validation: 0.08560832649144091]
	TIME [epoch: 2.73 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07443240620951504		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.07443240620951504 | validation: 0.06856601882156478]
	TIME [epoch: 2.73 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.087642722027661		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.087642722027661 | validation: 0.0917804887790949]
	TIME [epoch: 2.73 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08776385339035933		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.08776385339035933 | validation: 0.06955173122651076]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08156202211381845		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.08156202211381845 | validation: 0.08101030363873074]
	TIME [epoch: 2.73 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317368204644231		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.07317368204644231 | validation: 0.0705916416994245]
	TIME [epoch: 2.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06406858769132111		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.06406858769132111 | validation: 0.06666076359457114]
	TIME [epoch: 2.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06520651631008956		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.06520651631008956 | validation: 0.07344635701977906]
	TIME [epoch: 2.73 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10973060769243878		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.10973060769243878 | validation: 0.09181456859585085]
	TIME [epoch: 2.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569817128346878		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.07569817128346878 | validation: 0.07065805610527402]
	TIME [epoch: 2.74 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905826343534978		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.06905826343534978 | validation: 0.06961266068029551]
	TIME [epoch: 2.73 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0680681053062825		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.0680681053062825 | validation: 0.06437949760659985]
	TIME [epoch: 2.73 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456114428881964		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.06456114428881964 | validation: 0.06340949660461258]
	TIME [epoch: 2.73 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347241735723894		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.06347241735723894 | validation: 0.0683156980237141]
	TIME [epoch: 2.73 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355764952305502		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06355764952305502 | validation: 0.06644499009510915]
	TIME [epoch: 2.73 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06274560118342451		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.06274560118342451 | validation: 0.05611036215265419]
	TIME [epoch: 2.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307357124074872		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.06307357124074872 | validation: 0.06766989488213399]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236771806254446		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.06236771806254446 | validation: 0.06444864011644424]
	TIME [epoch: 2.73 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06699671329906115		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.06699671329906115 | validation: 0.10022939185808083]
	TIME [epoch: 2.73 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0965789645804899		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0965789645804899 | validation: 0.0881665349030475]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12356647619114872		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.12356647619114872 | validation: 0.07643310071662043]
	TIME [epoch: 2.73 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800034612246762		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.06800034612246762 | validation: 0.06188521535453611]
	TIME [epoch: 2.73 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06260621752106733		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06260621752106733 | validation: 0.06654936262685782]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800667515994745		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.06800667515994745 | validation: 0.08895153625522026]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07356365218201649		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.07356365218201649 | validation: 0.0644397043434574]
	TIME [epoch: 2.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737147485840231		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.0737147485840231 | validation: 0.07407133485385187]
	TIME [epoch: 2.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06757190070822387		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.06757190070822387 | validation: 0.06731999533850176]
	TIME [epoch: 2.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426998289891916		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.06426998289891916 | validation: 0.07716061391772487]
	TIME [epoch: 2.73 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06350274928377603		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06350274928377603 | validation: 0.061486215357366605]
	TIME [epoch: 2.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06024150810382146		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.06024150810382146 | validation: 0.05311964212657086]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060981435452662776		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.060981435452662776 | validation: 0.06554309867031814]
	TIME [epoch: 2.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06468745178441981		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06468745178441981 | validation: 0.06122019084422264]
	TIME [epoch: 2.73 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827271174841809		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.06827271174841809 | validation: 0.08711682110274177]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07308604151727771		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.07308604151727771 | validation: 0.06368745603967611]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07703482935088825		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.07703482935088825 | validation: 0.1005088911244918]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996796818627567		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.08996796818627567 | validation: 0.06931735297223815]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545013708886159		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.07545013708886159 | validation: 0.07214984518845448]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540167864841148		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.06540167864841148 | validation: 0.061161977305463945]
	TIME [epoch: 2.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06377116038512533		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.06377116038512533 | validation: 0.06633839387274433]
	TIME [epoch: 2.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343051207449453		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.06343051207449453 | validation: 0.05977663131153386]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06104708791963551		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.06104708791963551 | validation: 0.07180982179667218]
	TIME [epoch: 2.73 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625239310505849		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0625239310505849 | validation: 0.07164505751067489]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06326899522457048		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.06326899522457048 | validation: 0.056776633413708456]
	TIME [epoch: 2.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06432914223274626		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.06432914223274626 | validation: 0.08635350071637238]
	TIME [epoch: 2.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08190404815426987		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.08190404815426987 | validation: 0.06717746137632193]
	TIME [epoch: 2.73 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905664920380765		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.08905664920380765 | validation: 0.07312540766285938]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07362475747855177		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.07362475747855177 | validation: 0.05516238279454801]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259504124357093		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06259504124357093 | validation: 0.06563850020719895]
	TIME [epoch: 2.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06012873848649		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.06012873848649 | validation: 0.056388468721557276]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484769728783739		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.06484769728783739 | validation: 0.06870913360924617]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06552060531916026		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.06552060531916026 | validation: 0.05693389658243201]
	TIME [epoch: 2.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06172254084588562		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.06172254084588562 | validation: 0.06066572383873631]
	TIME [epoch: 2.72 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06165074566283837		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.06165074566283837 | validation: 0.05819616207840281]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06077062128497989		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.06077062128497989 | validation: 0.06109300666580736]
	TIME [epoch: 2.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05871409419368346		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.05871409419368346 | validation: 0.06350859388164085]
	TIME [epoch: 2.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823905977382072		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05823905977382072 | validation: 0.05582248982686764]
	TIME [epoch: 2.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062123327102308994		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.062123327102308994 | validation: 0.09271413566370619]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08294525773134907		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.08294525773134907 | validation: 0.08297249263051017]
	TIME [epoch: 2.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11325397612018158		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.11325397612018158 | validation: 0.0726991398385643]
	TIME [epoch: 2.73 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06951844626515306		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.06951844626515306 | validation: 0.0651401449451442]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382555435169864		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.06382555435169864 | validation: 0.053797495263197805]
	TIME [epoch: 2.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521471843173263		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.06521471843173263 | validation: 0.07353598959140767]
	TIME [epoch: 2.73 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06675863242595016		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.06675863242595016 | validation: 0.05650680350728609]
	TIME [epoch: 2.73 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314559108662218		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.06314559108662218 | validation: 0.05783189176079811]
	TIME [epoch: 2.73 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059736258246314566		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.059736258246314566 | validation: 0.05263059721815332]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_956.pth
	Model improved!!!
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142688530412675		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.06142688530412675 | validation: 0.06345910463329661]
	TIME [epoch: 2.73 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05821387760649583		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05821387760649583 | validation: 0.06469452225307078]
	TIME [epoch: 2.73 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778936360828316		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.05778936360828316 | validation: 0.06549723808719811]
	TIME [epoch: 2.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059142451932869634		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.059142451932869634 | validation: 0.05954994397183978]
	TIME [epoch: 2.73 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831166701057482		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.05831166701057482 | validation: 0.06265283210118847]
	TIME [epoch: 2.74 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057184414025283924		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.057184414025283924 | validation: 0.05967107876826207]
	TIME [epoch: 2.73 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881074318535263		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.05881074318535263 | validation: 0.05951035238015806]
	TIME [epoch: 2.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06064768066911404		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.06064768066911404 | validation: 0.05752219800337571]
	TIME [epoch: 2.73 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07318721790491078		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.07318721790491078 | validation: 0.09609309922902515]
	TIME [epoch: 2.74 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10971870814381436		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.10971870814381436 | validation: 0.06646797594324698]
	TIME [epoch: 2.73 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878569020125492		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.08878569020125492 | validation: 0.0583015466447125]
	TIME [epoch: 2.73 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057592516723100236		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.057592516723100236 | validation: 0.07428724611217646]
	TIME [epoch: 2.73 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06265781421935611		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.06265781421935611 | validation: 0.05505430717422394]
	TIME [epoch: 2.72 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06647190693667321		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.06647190693667321 | validation: 0.05864559258429927]
	TIME [epoch: 2.73 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05997399234944704		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.05997399234944704 | validation: 0.05645498214513837]
	TIME [epoch: 2.73 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05741217926529006		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.05741217926529006 | validation: 0.06223545176276647]
	TIME [epoch: 2.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0621452320414024		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.0621452320414024 | validation: 0.05839960665601184]
	TIME [epoch: 2.73 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06682064865703125		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.06682064865703125 | validation: 0.06934061779024274]
	TIME [epoch: 2.73 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285531387416007		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.06285531387416007 | validation: 0.0605749063026956]
	TIME [epoch: 2.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05814194489449835		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.05814194489449835 | validation: 0.06173141616462268]
	TIME [epoch: 2.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797598236031892		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.05797598236031892 | validation: 0.06232740886439075]
	TIME [epoch: 2.73 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584736045578409		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.0584736045578409 | validation: 0.06409299107805604]
	TIME [epoch: 2.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056962771245501184		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.056962771245501184 | validation: 0.06143297759043334]
	TIME [epoch: 2.72 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630673410266065		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.05630673410266065 | validation: 0.061538120443824285]
	TIME [epoch: 2.73 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058894125262474846		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.058894125262474846 | validation: 0.05751679484515376]
	TIME [epoch: 2.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644607058652468		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.05644607058652468 | validation: 0.06418610096755138]
	TIME [epoch: 2.73 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06112118808149923		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.06112118808149923 | validation: 0.05593141650730341]
	TIME [epoch: 2.74 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06764051439176436		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.06764051439176436 | validation: 0.08579826034691257]
	TIME [epoch: 2.73 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08201336067843958		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.08201336067843958 | validation: 0.05833342465415097]
	TIME [epoch: 2.73 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001779620580173		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.07001779620580173 | validation: 0.0594839457455459]
	TIME [epoch: 2.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05675543346939847		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.05675543346939847 | validation: 0.05945859225588793]
	TIME [epoch: 2.74 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055198600085986434		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.055198600085986434 | validation: 0.05789441700767685]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925092041319932		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.05925092041319932 | validation: 0.05845800885421191]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744901985989001		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.05744901985989001 | validation: 0.06331461568642577]
	TIME [epoch: 2.72 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059632543038244314		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.059632543038244314 | validation: 0.05574163471078336]
	TIME [epoch: 2.73 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06025719779267336		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.06025719779267336 | validation: 0.07290250105255547]
	TIME [epoch: 2.73 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058889344924194		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.07058889344924194 | validation: 0.06780441545974719]
	TIME [epoch: 2.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657288616240355		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.09657288616240355 | validation: 0.07260539367719547]
	TIME [epoch: 2.74 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649304891467608		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.0649304891467608 | validation: 0.0556659552127239]
	TIME [epoch: 2.72 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0561406445207091		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.0561406445207091 | validation: 0.05402367407648581]
	TIME [epoch: 2.73 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645883287215057		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.06645883287215057 | validation: 0.07229343389900404]
	TIME [epoch: 2.73 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676045894387852		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.0676045894387852 | validation: 0.05451417029649658]
	TIME [epoch: 2.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06234058522207322		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.06234058522207322 | validation: 0.05900363073581617]
	TIME [epoch: 2.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834652990071418		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05834652990071418 | validation: 0.05092659354780266]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057815721754750915		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.057815721754750915 | validation: 0.05435392018494575]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05625955507830115		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.05625955507830115 | validation: 0.05794026639651261]
	TIME [epoch: 5.85 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747361785186187		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.05747361785186187 | validation: 0.048909049401435915]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_1003.pth
	Model improved!!!
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055509504077494304		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.055509504077494304 | validation: 0.05722804940566883]
	TIME [epoch: 5.85 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594955687893333		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.05594955687893333 | validation: 0.04722759294031133]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_1005.pth
	Model improved!!!
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654222918019186		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.0654222918019186 | validation: 0.07166507572647156]
	TIME [epoch: 5.84 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061433218099194546		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.061433218099194546 | validation: 0.056387464452109064]
	TIME [epoch: 5.84 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633432517242545		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.0633432517242545 | validation: 0.060401656019758346]
	TIME [epoch: 5.85 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065195422990348		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.06065195422990348 | validation: 0.05113588150370352]
	TIME [epoch: 5.83 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626693994381717		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.05626693994381717 | validation: 0.05932308658662991]
	TIME [epoch: 5.84 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060827919264324154		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.060827919264324154 | validation: 0.04893383850715907]
	TIME [epoch: 5.83 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644358976979549		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.0644358976979549 | validation: 0.07368040297360022]
	TIME [epoch: 5.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701515872008745		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.06701515872008745 | validation: 0.05213169455863751]
	TIME [epoch: 5.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06423953657478185		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06423953657478185 | validation: 0.06061193167289128]
	TIME [epoch: 5.83 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05680942803106546		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.05680942803106546 | validation: 0.06605251916667004]
	TIME [epoch: 5.84 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07079525155892613		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.07079525155892613 | validation: 0.05898811894562703]
	TIME [epoch: 5.83 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06756401689173448		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.06756401689173448 | validation: 0.05746718233324032]
	TIME [epoch: 5.83 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777176940008169		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.05777176940008169 | validation: 0.061681829922481224]
	TIME [epoch: 5.84 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05637663248457028		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.05637663248457028 | validation: 0.0534581418912538]
	TIME [epoch: 5.83 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804632489828226		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.06804632489828226 | validation: 0.06980033768886836]
	TIME [epoch: 5.84 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060813280270897004		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.060813280270897004 | validation: 0.05083720466202401]
	TIME [epoch: 5.84 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057275220527824536		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.057275220527824536 | validation: 0.058664256444537634]
	TIME [epoch: 5.84 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055846817473280755		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.055846817473280755 | validation: 0.054954851595744494]
	TIME [epoch: 5.85 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05726404697550111		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.05726404697550111 | validation: 0.05104014376136783]
	TIME [epoch: 5.84 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05551139433108631		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.05551139433108631 | validation: 0.07355099849079731]
	TIME [epoch: 5.84 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435256754236164		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.06435256754236164 | validation: 0.05423833639464051]
	TIME [epoch: 5.84 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664321987823241		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.06664321987823241 | validation: 0.05953546049188612]
	TIME [epoch: 5.84 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058441203829625366		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.058441203829625366 | validation: 0.06660838944047343]
	TIME [epoch: 5.84 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554638871654656		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.0554638871654656 | validation: 0.051132108047343416]
	TIME [epoch: 5.85 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589830265543328		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.05589830265543328 | validation: 0.06486374243686518]
	TIME [epoch: 5.85 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945332523474388		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.05945332523474388 | validation: 0.04956740253882191]
	TIME [epoch: 5.84 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06280783901131687		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06280783901131687 | validation: 0.05999986466254667]
	TIME [epoch: 5.84 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06003478711513997		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.06003478711513997 | validation: 0.06120652728123859]
	TIME [epoch: 5.83 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664063557797757		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.05664063557797757 | validation: 0.051838292933482605]
	TIME [epoch: 5.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892148137874864		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.05892148137874864 | validation: 0.06263583647178023]
	TIME [epoch: 5.84 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949699509692307		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.05949699509692307 | validation: 0.05521640339033295]
	TIME [epoch: 5.84 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059220693477925364		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.059220693477925364 | validation: 0.06173244133953926]
	TIME [epoch: 5.84 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057387662335952674		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.057387662335952674 | validation: 0.054028377681096744]
	TIME [epoch: 5.84 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913089232664878		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.05913089232664878 | validation: 0.062300888725996345]
	TIME [epoch: 5.84 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630017793340819		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.05630017793340819 | validation: 0.05388589398206786]
	TIME [epoch: 5.85 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0544643938672477		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.0544643938672477 | validation: 0.058223396097797545]
	TIME [epoch: 5.84 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05679691887276744		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.05679691887276744 | validation: 0.05401916957445421]
	TIME [epoch: 5.84 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087958198205369		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.06087958198205369 | validation: 0.06447307530016352]
	TIME [epoch: 5.83 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701760037705637		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.06701760037705637 | validation: 0.05203420175007996]
	TIME [epoch: 5.85 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05574900504188123		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.05574900504188123 | validation: 0.05488322814483763]
	TIME [epoch: 5.84 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255347471853489		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.05255347471853489 | validation: 0.05742042970508593]
	TIME [epoch: 5.84 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055755430681351886		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.055755430681351886 | validation: 0.05532004216749699]
	TIME [epoch: 5.83 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053492153502835205		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.053492153502835205 | validation: 0.0580807988896968]
	TIME [epoch: 5.84 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055471804641333944		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.055471804641333944 | validation: 0.05129532909614068]
	TIME [epoch: 5.84 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541350077995078		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05541350077995078 | validation: 0.053651953408153374]
	TIME [epoch: 5.84 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782832657630144		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.05782832657630144 | validation: 0.06324158118982963]
	TIME [epoch: 5.84 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059721733144342246		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.059721733144342246 | validation: 0.059433574811418345]
	TIME [epoch: 5.84 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08073770820890493		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.08073770820890493 | validation: 0.07305155200581938]
	TIME [epoch: 5.84 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06372192444464239		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.06372192444464239 | validation: 0.05616132377930688]
	TIME [epoch: 5.84 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509770771074328		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.05509770771074328 | validation: 0.050561166426494386]
	TIME [epoch: 5.84 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05919271621469191		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.05919271621469191 | validation: 0.06582348383258951]
	TIME [epoch: 5.84 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652009280598571		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06652009280598571 | validation: 0.06051393282818194]
	TIME [epoch: 5.84 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799925721700966		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.05799925721700966 | validation: 0.0520812964816543]
	TIME [epoch: 5.84 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287359080839048		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.05287359080839048 | validation: 0.06403012701582979]
	TIME [epoch: 5.85 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05607160072774926		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.05607160072774926 | validation: 0.05657715897141929]
	TIME [epoch: 5.83 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563372780600717		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.0563372780600717 | validation: 0.057482599322944555]
	TIME [epoch: 5.84 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054724657270187375		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.054724657270187375 | validation: 0.04910546001659079]
	TIME [epoch: 5.83 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157935773999443		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.06157935773999443 | validation: 0.06167137116877835]
	TIME [epoch: 5.84 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059986456205534874		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.059986456205534874 | validation: 0.0549087230947724]
	TIME [epoch: 5.84 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05415782033616094		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.05415782033616094 | validation: 0.05418703839140269]
	TIME [epoch: 5.83 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053456731626881915		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.053456731626881915 | validation: 0.05224533960341441]
	TIME [epoch: 5.84 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05388846087335301		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.05388846087335301 | validation: 0.06026147085757377]
	TIME [epoch: 5.83 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052725008787700485		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.052725008787700485 | validation: 0.06021086864998505]
	TIME [epoch: 5.83 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05506646652962813		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.05506646652962813 | validation: 0.054425559325255606]
	TIME [epoch: 5.84 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772611911602622		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.05772611911602622 | validation: 0.08199219446243147]
	TIME [epoch: 5.84 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545817828320539		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.07545817828320539 | validation: 0.04982703216539325]
	TIME [epoch: 5.83 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353005007530521		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.06353005007530521 | validation: 0.051624751856913856]
	TIME [epoch: 5.84 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05277779946823689		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.05277779946823689 | validation: 0.05425161652781194]
	TIME [epoch: 5.84 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05528932751930803		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05528932751930803 | validation: 0.05193320967620229]
	TIME [epoch: 5.84 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055328619015429396		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.055328619015429396 | validation: 0.05382421715378082]
	TIME [epoch: 5.85 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346013706191432		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.05346013706191432 | validation: 0.05299550492574189]
	TIME [epoch: 5.85 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055094349014964974		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.055094349014964974 | validation: 0.047528090144615065]
	TIME [epoch: 5.84 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608975838746011		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.05608975838746011 | validation: 0.05407459930376079]
	TIME [epoch: 5.84 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08777198922620016		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.08777198922620016 | validation: 0.0723070503463292]
	TIME [epoch: 5.84 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06206065530169809		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06206065530169809 | validation: 0.057397820925907096]
	TIME [epoch: 5.85 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057710423513447885		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.057710423513447885 | validation: 0.055373722872617485]
	TIME [epoch: 5.84 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05401524835583604		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.05401524835583604 | validation: 0.05213261577668258]
	TIME [epoch: 5.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522436489971229		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05522436489971229 | validation: 0.05702848241417004]
	TIME [epoch: 5.84 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218757358725298		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.05218757358725298 | validation: 0.05743990543193955]
	TIME [epoch: 5.84 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05361660640830358		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.05361660640830358 | validation: 0.05489285663759439]
	TIME [epoch: 5.85 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05254192781702589		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.05254192781702589 | validation: 0.06113682688868256]
	TIME [epoch: 5.84 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05324070652518998		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.05324070652518998 | validation: 0.04978837500582897]
	TIME [epoch: 5.84 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705225363121457		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05705225363121457 | validation: 0.06038526317647624]
	TIME [epoch: 5.84 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05865900618447627		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.05865900618447627 | validation: 0.05228856090026094]
	TIME [epoch: 5.84 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054879085614987506		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.054879085614987506 | validation: 0.05035928131879968]
	TIME [epoch: 5.85 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053139707137978553		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.053139707137978553 | validation: 0.05222654148974736]
	TIME [epoch: 5.84 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522729043785295		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.0522729043785295 | validation: 0.054023476467786435]
	TIME [epoch: 5.84 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054054276567409215		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.054054276567409215 | validation: 0.0603038491386514]
	TIME [epoch: 5.84 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0572180902411966		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.0572180902411966 | validation: 0.05898335474252747]
	TIME [epoch: 5.84 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055368438042166644		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.055368438042166644 | validation: 0.05469392211877087]
	TIME [epoch: 5.85 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05370691218468933		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.05370691218468933 | validation: 0.048340037105952804]
	TIME [epoch: 5.84 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054430799362229706		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.054430799362229706 | validation: 0.05701973467316889]
	TIME [epoch: 5.84 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05342254559097575		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05342254559097575 | validation: 0.05700186540971465]
	TIME [epoch: 5.83 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052517032207914933		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.052517032207914933 | validation: 0.0558536925941252]
	TIME [epoch: 5.84 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051764418100819096		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.051764418100819096 | validation: 0.06664901467859963]
	TIME [epoch: 5.84 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05563384652719144		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.05563384652719144 | validation: 0.047715647222903106]
	TIME [epoch: 5.84 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060907946346385196		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.060907946346385196 | validation: 0.06670131968648138]
	TIME [epoch: 5.84 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065195123150942		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.06065195123150942 | validation: 0.053751627349018866]
	TIME [epoch: 5.84 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719050391756984		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.05719050391756984 | validation: 0.05594491706034044]
	TIME [epoch: 5.84 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05305534581621768		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.05305534581621768 | validation: 0.05366333603517756]
	TIME [epoch: 5.84 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053991380916723224		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.053991380916723224 | validation: 0.052842788699671905]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250518_223520/states/model_phi1_4a_distortion_v2_0_v_mmd4_1106.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3344.425 seconds.
