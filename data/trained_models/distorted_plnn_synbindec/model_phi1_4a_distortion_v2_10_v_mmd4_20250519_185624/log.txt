Args:
Namespace(name='model_phi1_4a_distortion_v2_10_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_10/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_10/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.022531174, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1780471596

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6464914865718105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6464914865718105 | validation: 4.4783781816733805]
	TIME [epoch: 177 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.250223732812895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.250223732812895 | validation: 6.6643076172254645]
	TIME [epoch: 0.701 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.965245224062886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.965245224062886 | validation: 5.634285175429294]
	TIME [epoch: 0.654 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.1643752205892905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1643752205892905 | validation: 5.4994407866222765]
	TIME [epoch: 0.657 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.363960647729255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.363960647729255 | validation: 5.454265852611144]
	TIME [epoch: 0.656 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.262841173610479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.262841173610479 | validation: 5.200234546426959]
	TIME [epoch: 0.654 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.09671880307624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.09671880307624 | validation: 4.831929171485993]
	TIME [epoch: 0.654 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.906826860311118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.906826860311118 | validation: 4.043257971975365]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.636208267221996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.636208267221996 | validation: 3.514944890164103]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.763691461562344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.763691461562344 | validation: 4.270962495018842]
	TIME [epoch: 0.654 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.388441073355583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.388441073355583 | validation: 4.3172438932615735]
	TIME [epoch: 0.656 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.278155144046753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.278155144046753 | validation: 3.80952879624542]
	TIME [epoch: 0.654 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.051851509545988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.051851509545988 | validation: 2.708100976868728]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.749887126975801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.749887126975801 | validation: 2.283309866161637]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5823190820257116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5823190820257116 | validation: 3.8628620001280733]
	TIME [epoch: 0.658 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6535836360835288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6535836360835288 | validation: 3.1636170583730965]
	TIME [epoch: 0.655 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.313857327377609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.313857327377609 | validation: 2.12579585274777]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0753468168322433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0753468168322433 | validation: 2.703688471007357]
	TIME [epoch: 0.653 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.000136354150253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.000136354150253 | validation: 2.0251950459582795]
	TIME [epoch: 0.648 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.662743026962232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.662743026962232 | validation: 2.480683592126187]
	TIME [epoch: 0.655 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2020221123956434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2020221123956434 | validation: 3.2064115095262666]
	TIME [epoch: 0.653 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.012308767591872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.012308767591872 | validation: 2.5234720234035892]
	TIME [epoch: 0.653 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.649586964345857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.649586964345857 | validation: 1.8839189532390728]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4948925998365654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4948925998365654 | validation: 1.752535903844801]
	TIME [epoch: 0.658 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3069424625673856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3069424625673856 | validation: 1.7525087030766173]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2079755699804116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2079755699804116 | validation: 1.7958242886940985]
	TIME [epoch: 0.656 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1640464486556747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1640464486556747 | validation: 2.3949860715199724]
	TIME [epoch: 0.653 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3627975537122587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3627975537122587 | validation: 1.990682209364708]
	TIME [epoch: 0.654 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.450424288677127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.450424288677127 | validation: 2.162928498231392]
	TIME [epoch: 0.654 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.204609575094218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.204609575094218 | validation: 2.0425720007184167]
	TIME [epoch: 0.652 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1198800116022167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1198800116022167 | validation: 1.6800967087355858]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0934659879838278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0934659879838278 | validation: 1.9427341173975508]
	TIME [epoch: 0.662 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.036283296030134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.036283296030134 | validation: 1.8321445727202559]
	TIME [epoch: 0.656 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.017016863523794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.017016863523794 | validation: 1.8580318639671702]
	TIME [epoch: 0.657 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0032511848431565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0032511848431565 | validation: 1.8664840050976672]
	TIME [epoch: 0.654 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.003943102909268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.003943102909268 | validation: 2.055850734385389]
	TIME [epoch: 0.654 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.02982503486653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.02982503486653 | validation: 1.770620513285325]
	TIME [epoch: 0.652 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0577096540400097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0577096540400097 | validation: 2.3104357346760027]
	TIME [epoch: 0.651 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.105480341890323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.105480341890323 | validation: 1.7392556276712101]
	TIME [epoch: 0.653 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.01025645173654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.01025645173654 | validation: 1.9120240478609816]
	TIME [epoch: 0.653 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9586950112519208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9586950112519208 | validation: 1.8970239720155904]
	TIME [epoch: 0.653 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9507947849222362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9507947849222362 | validation: 1.813229915190788]
	TIME [epoch: 0.657 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9488492461022964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9488492461022964 | validation: 1.991488935047622]
	TIME [epoch: 0.656 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9506533097008574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9506533097008574 | validation: 1.7965994908580727]
	TIME [epoch: 0.657 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.966586311670051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.966586311670051 | validation: 2.3595767647529957]
	TIME [epoch: 0.653 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.078930084183534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.078930084183534 | validation: 1.756549614336346]
	TIME [epoch: 0.654 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.028344615978055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.028344615978055 | validation: 2.0408433053750015]
	TIME [epoch: 0.654 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9423235527236296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9423235527236296 | validation: 1.903418114041866]
	TIME [epoch: 0.653 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9127142595730726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9127142595730726 | validation: 1.8896527530145333]
	TIME [epoch: 0.652 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9438258702426219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9438258702426219 | validation: 2.128714312595625]
	TIME [epoch: 0.653 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9606106482610244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9606106482610244 | validation: 1.8335717309633008]
	TIME [epoch: 0.654 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9416129453373843		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.9416129453373843 | validation: 2.0842215479792823]
	TIME [epoch: 0.655 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9081002114347114		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.9081002114347114 | validation: 1.8905525890391566]
	TIME [epoch: 0.657 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.906528972814299		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.906528972814299 | validation: 2.087967084374727]
	TIME [epoch: 0.658 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9174731628602815		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.9174731628602815 | validation: 1.811014822370531]
	TIME [epoch: 0.656 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8695551898872373		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.8695551898872373 | validation: 1.9847772948438387]
	TIME [epoch: 0.654 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8638492694048139		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.8638492694048139 | validation: 1.796217426615699]
	TIME [epoch: 0.653 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8650195428665695		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.8650195428665695 | validation: 2.022881917156377]
	TIME [epoch: 0.653 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8712878431471587		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.8712878431471587 | validation: 1.6655652786007473]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9019327014574345		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.9019327014574345 | validation: 2.3866369775159946]
	TIME [epoch: 0.657 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0620804843910228		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.0620804843910228 | validation: 1.8757786547040585]
	TIME [epoch: 0.651 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.904152774034133		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.904152774034133 | validation: 1.6560332192452365]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9070451369479045		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.9070451369479045 | validation: 2.155307612861357]
	TIME [epoch: 0.656 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9066875912111099		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.9066875912111099 | validation: 1.8556812958252795]
	TIME [epoch: 0.65 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8405091833049523		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.8405091833049523 | validation: 1.7608794135734689]
	TIME [epoch: 0.649 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8458147204499329		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.8458147204499329 | validation: 1.9296593228443868]
	TIME [epoch: 0.648 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8379070875353984		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.8379070875353984 | validation: 1.7983249027049504]
	TIME [epoch: 0.649 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8308696473119899		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.8308696473119899 | validation: 1.8613116363425644]
	TIME [epoch: 0.648 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8246943875157007		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.8246943875157007 | validation: 1.7386723638460244]
	TIME [epoch: 0.649 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8268516497563951		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.8268516497563951 | validation: 1.979388150175074]
	TIME [epoch: 0.651 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.84366240173472		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.84366240173472 | validation: 1.6439596999531987]
	TIME [epoch: 0.652 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9055070939026109		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.9055070939026109 | validation: 2.4068565787926115]
	TIME [epoch: 0.657 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0279531525233585		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.0279531525233585 | validation: 1.9892101251505871]
	TIME [epoch: 0.652 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.858926448637103		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.858926448637103 | validation: 1.5854813408060462]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9125033009203436		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.9125033009203436 | validation: 2.1187246825194452]
	TIME [epoch: 0.652 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8998693203595134		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.8998693203595134 | validation: 1.8929123632708262]
	TIME [epoch: 0.649 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8299040966810973		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.8299040966810973 | validation: 1.6549651896165791]
	TIME [epoch: 0.649 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.819132927065989		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.819132927065989 | validation: 1.8789864658107023]
	TIME [epoch: 0.651 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8031510962487263		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.8031510962487263 | validation: 1.7906882671671145]
	TIME [epoch: 0.654 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8072389486256046		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.8072389486256046 | validation: 1.8028478392229397]
	TIME [epoch: 0.653 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7939861796756862		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.7939861796756862 | validation: 1.7069838402664024]
	TIME [epoch: 0.653 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7779737717873025		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7779737717873025 | validation: 1.7735937199132312]
	TIME [epoch: 0.652 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7590113167839259		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7590113167839259 | validation: 1.6664457665714072]
	TIME [epoch: 0.65 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7515214704942035		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.7515214704942035 | validation: 1.9083021451773874]
	TIME [epoch: 0.649 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7886342792546486		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.7886342792546486 | validation: 1.575184331729557]
	TIME [epoch: 0.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9308458220492781		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.9308458220492781 | validation: 2.409173421496906]
	TIME [epoch: 0.652 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.01938014821175		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.01938014821175 | validation: 2.1419810525209804]
	TIME [epoch: 0.65 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8577175188820758		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8577175188820758 | validation: 1.5400348699331516]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8245515424056362		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.8245515424056362 | validation: 1.8365060316133777]
	TIME [epoch: 0.653 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7782344149191323		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.7782344149191323 | validation: 1.705348264742802]
	TIME [epoch: 0.653 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7261925972524512		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.7261925972524512 | validation: 1.5555083598268975]
	TIME [epoch: 0.653 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7392195547820841		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.7392195547820841 | validation: 1.8078772414191828]
	TIME [epoch: 0.651 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7350572716660708		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.7350572716660708 | validation: 1.5502741166751057]
	TIME [epoch: 0.649 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7316598872158875		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.7316598872158875 | validation: 1.8246205058974003]
	TIME [epoch: 0.649 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.724335528591525		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.724335528591525 | validation: 1.5060590447302709]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.730016504047168		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.730016504047168 | validation: 1.9972677859972863]
	TIME [epoch: 0.652 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7766057009791554		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.7766057009791554 | validation: 1.4668794940998784]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6997447926040326		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.6997447926040326 | validation: 1.800343208016383]
	TIME [epoch: 0.658 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7305945418416615		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.7305945418416615 | validation: 1.4688286418881995]
	TIME [epoch: 0.655 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6506746252184525		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6506746252184525 | validation: 1.643359110932284]
	TIME [epoch: 0.654 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6507025654847909		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6507025654847909 | validation: 1.5013364455974656]
	TIME [epoch: 0.656 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.687999683516654		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.687999683516654 | validation: 1.9864230814461807]
	TIME [epoch: 0.656 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7439399878463338		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.7439399878463338 | validation: 1.416911639663863]
	TIME [epoch: 0.653 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6708880671045188		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6708880671045188 | validation: 1.7474808811774842]
	TIME [epoch: 0.655 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6369758680613802		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.6369758680613802 | validation: 1.4468825854368683]
	TIME [epoch: 0.654 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6023420036940428		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.6023420036940428 | validation: 1.7376109704118907]
	TIME [epoch: 0.652 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6269135994618915		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.6269135994618915 | validation: 1.3898987152038382]
	TIME [epoch: 0.651 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6381206764477123		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.6381206764477123 | validation: 1.7659347607585112]
	TIME [epoch: 0.658 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6269562790892798		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.6269562790892798 | validation: 1.4634541491918582]
	TIME [epoch: 0.651 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6435689850199173		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.6435689850199173 | validation: 1.6947898251540097]
	TIME [epoch: 0.653 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5902680894923271		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.5902680894923271 | validation: 1.4032524573276772]
	TIME [epoch: 0.652 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5049506718678305		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5049506718678305 | validation: 1.4848982183305233]
	TIME [epoch: 0.656 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.515484673604809		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.515484673604809 | validation: 1.4052519915255535]
	TIME [epoch: 0.653 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5362803748835818		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.5362803748835818 | validation: 1.5665929479754241]
	TIME [epoch: 0.656 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5453780170073514		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.5453780170073514 | validation: 1.3604822534565522]
	TIME [epoch: 0.654 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6550627820187687		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.6550627820187687 | validation: 2.0451362210842894]
	TIME [epoch: 0.654 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.75475041235217		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.75475041235217 | validation: 1.5018474851122694]
	TIME [epoch: 0.653 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.492808891802565		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.492808891802565 | validation: 1.3479898816981284]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.548679267747812		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.548679267747812 | validation: 1.8347777130037244]
	TIME [epoch: 0.653 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.580011948757746		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.580011948757746 | validation: 1.4250224139513192]
	TIME [epoch: 0.652 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4677114525707913		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.4677114525707913 | validation: 1.4314209746946442]
	TIME [epoch: 0.652 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4549678837417594		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.4549678837417594 | validation: 1.4777807646807781]
	TIME [epoch: 0.658 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4770285858541319		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.4770285858541319 | validation: 1.4431884757070927]
	TIME [epoch: 0.655 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4714618710887906		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.4714618710887906 | validation: 1.490423004912122]
	TIME [epoch: 0.654 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4865513742508167		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.4865513742508167 | validation: 1.4128462330752827]
	TIME [epoch: 0.655 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4751994566407103		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4751994566407103 | validation: 1.6668584877570218]
	TIME [epoch: 0.653 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5099771305865302		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.5099771305865302 | validation: 1.3851655267065333]
	TIME [epoch: 0.652 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5446569476440675		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.5446569476440675 | validation: 1.919129586701617]
	TIME [epoch: 0.651 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6941505013904528		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.6941505013904528 | validation: 1.4196112022718637]
	TIME [epoch: 0.652 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4784139797901628		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.4784139797901628 | validation: 1.3612509544349363]
	TIME [epoch: 0.653 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4971249966349012		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.4971249966349012 | validation: 1.7596323086003516]
	TIME [epoch: 0.652 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5746795373759312		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.5746795373759312 | validation: 1.3978658328459141]
	TIME [epoch: 0.654 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4156069559663695		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.4156069559663695 | validation: 1.335648413542681]
	TIME [epoch: 0.656 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.445615665998694		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.445615665998694 | validation: 1.5741044028119173]
	TIME [epoch: 0.657 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4669518593285988		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.4669518593285988 | validation: 1.384451862917285]
	TIME [epoch: 0.655 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4207263235438121		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.4207263235438121 | validation: 1.4658815241157228]
	TIME [epoch: 0.654 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4234792840323636		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.4234792840323636 | validation: 1.3823415085690784]
	TIME [epoch: 0.653 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4162980134147025		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.4162980134147025 | validation: 1.5696648452671926]
	TIME [epoch: 0.652 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4332056359889465		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.4332056359889465 | validation: 1.3570202958446562]
	TIME [epoch: 0.656 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4241385517942495		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.4241385517942495 | validation: 1.5543055452873453]
	TIME [epoch: 0.653 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4341616012317366		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.4341616012317366 | validation: 1.366853871289182]
	TIME [epoch: 0.655 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4273093024003227		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.4273093024003227 | validation: 1.5964546577402678]
	TIME [epoch: 0.657 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.455855815828986		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.455855815828986 | validation: 1.3461419412877231]
	TIME [epoch: 0.658 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.433595739338324		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.433595739338324 | validation: 1.5362784884420941]
	TIME [epoch: 0.655 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4434327343988487		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.4434327343988487 | validation: 1.3172742006955647]
	TIME [epoch: 0.655 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3915072140805227		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.3915072140805227 | validation: 1.466671116313063]
	TIME [epoch: 0.652 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4224091073403324		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.4224091073403324 | validation: 1.6695742623219862]
	TIME [epoch: 0.651 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6730818725744063		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.6730818725744063 | validation: 1.452135466405862]
	TIME [epoch: 0.651 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4071300794724682		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.4071300794724682 | validation: 1.3648954535438378]
	TIME [epoch: 0.65 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3557236823464194		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.3557236823464194 | validation: 1.430151919680947]
	TIME [epoch: 0.652 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3748718860104825		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.3748718860104825 | validation: 1.4071097448324352]
	TIME [epoch: 0.655 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3741795313552319		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.3741795313552319 | validation: 1.4808973003631107]
	TIME [epoch: 0.658 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3944661129372853		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.3944661129372853 | validation: 1.486978847625591]
	TIME [epoch: 0.662 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4561564856472182		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.4561564856472182 | validation: 1.6646659675678543]
	TIME [epoch: 0.654 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4579952762693702		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.4579952762693702 | validation: 1.3412300263966532]
	TIME [epoch: 0.655 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3694669132042807		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3694669132042807 | validation: 1.4313055928702587]
	TIME [epoch: 0.654 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3720307306445807		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3720307306445807 | validation: 1.3551152644051827]
	TIME [epoch: 0.653 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4131589787626513		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.4131589787626513 | validation: 1.6432931019428878]
	TIME [epoch: 0.652 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4752409954907308		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.4752409954907308 | validation: 1.3538343401589295]
	TIME [epoch: 0.653 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4259213160989723		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.4259213160989723 | validation: 1.5795942526901356]
	TIME [epoch: 0.654 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4215916924318492		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.4215916924318492 | validation: 1.3764638972492778]
	TIME [epoch: 0.655 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3741279145112975		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3741279145112975 | validation: 1.416700458358157]
	TIME [epoch: 0.658 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3592197664960661		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3592197664960661 | validation: 1.3549190866211642]
	TIME [epoch: 0.66 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3756951107425324		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.3756951107425324 | validation: 1.4362392385989065]
	TIME [epoch: 0.656 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4004053976584532		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.4004053976584532 | validation: 1.5180963039090551]
	TIME [epoch: 0.655 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4430546679984741		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.4430546679984741 | validation: 1.3884398690435729]
	TIME [epoch: 0.653 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3854432161920949		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.3854432161920949 | validation: 1.4987454030330198]
	TIME [epoch: 0.655 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372552364998363		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.372552364998363 | validation: 1.3905383300640721]
	TIME [epoch: 0.652 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3927820562459892		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3927820562459892 | validation: 1.5740946008025702]
	TIME [epoch: 0.653 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923170745411904		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3923170745411904 | validation: 1.329498496939989]
	TIME [epoch: 0.653 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3535809779880674		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3535809779880674 | validation: 1.418932744964163]
	TIME [epoch: 0.655 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3530984508517596		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3530984508517596 | validation: 1.320980357350346]
	TIME [epoch: 0.656 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3432905613785788		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3432905613785788 | validation: 1.3962491577046823]
	TIME [epoch: 0.657 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341654068675992		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.341654068675992 | validation: 1.3310040976231794]
	TIME [epoch: 0.658 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3704450404334214		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3704450404334214 | validation: 1.5514467374685734]
	TIME [epoch: 0.655 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412786347336732		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.412786347336732 | validation: 1.3978104059185967]
	TIME [epoch: 0.654 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831552188150062		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3831552188150062 | validation: 1.4358105744180538]
	TIME [epoch: 0.653 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4031795467110493		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.4031795467110493 | validation: 1.4367491114034863]
	TIME [epoch: 0.652 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3777514395347477		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3777514395347477 | validation: 1.3874715675714187]
	TIME [epoch: 0.653 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3858541766365924		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3858541766365924 | validation: 1.4590847300530012]
	TIME [epoch: 0.655 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3541943351874335		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3541943351874335 | validation: 1.3697418008099511]
	TIME [epoch: 0.653 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3601644352126436		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3601644352126436 | validation: 1.446029597461642]
	TIME [epoch: 0.654 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3671073381610104		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3671073381610104 | validation: 1.3604246784057812]
	TIME [epoch: 0.659 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3509139596105166		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3509139596105166 | validation: 1.4263324211826163]
	TIME [epoch: 0.655 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3443236240126248		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.3443236240126248 | validation: 1.3225578123860453]
	TIME [epoch: 0.653 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3246534054469803		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.3246534054469803 | validation: 1.4234943865777765]
	TIME [epoch: 0.651 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3199537687735154		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.3199537687735154 | validation: 1.328478868982407]
	TIME [epoch: 0.651 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3597511948997072		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3597511948997072 | validation: 1.569619748755634]
	TIME [epoch: 0.651 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4261682625473868		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.4261682625473868 | validation: 1.290074460832124]
	TIME [epoch: 0.649 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3698325222212222		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3698325222212222 | validation: 1.329691951902921]
	TIME [epoch: 0.652 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3130487914806868		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.3130487914806868 | validation: 1.3576335355168934]
	TIME [epoch: 0.652 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.31718258723952		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.31718258723952 | validation: 1.3577064261945084]
	TIME [epoch: 0.656 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3398912723045442		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.3398912723045442 | validation: 1.5635468073552834]
	TIME [epoch: 0.654 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4373877017275556		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.4373877017275556 | validation: 1.4477140469063536]
	TIME [epoch: 0.653 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4236758303371324		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.4236758303371324 | validation: 1.424766960683127]
	TIME [epoch: 0.651 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3700980793947575		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.3700980793947575 | validation: 1.3513839621865682]
	TIME [epoch: 0.65 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3063967718462408		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.3063967718462408 | validation: 1.3496226583941215]
	TIME [epoch: 0.65 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3046221966586375		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.3046221966586375 | validation: 1.354774833249957]
	TIME [epoch: 0.658 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3079326331436527		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.3079326331436527 | validation: 1.3636749638262877]
	TIME [epoch: 0.652 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.311673691193107		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.311673691193107 | validation: 1.3813914533142055]
	TIME [epoch: 0.653 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.317551814380442		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.317551814380442 | validation: 1.3804262912208043]
	TIME [epoch: 187 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3469647854214042		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.3469647854214042 | validation: 1.433369815472161]
	TIME [epoch: 1.28 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3638435548310621		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.3638435548310621 | validation: 1.3368133741439243]
	TIME [epoch: 1.27 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3113460341885013		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.3113460341885013 | validation: 1.3897762430150236]
	TIME [epoch: 1.27 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3092012665580015		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.3092012665580015 | validation: 1.317426881752808]
	TIME [epoch: 1.27 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3097159322561698		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.3097159322561698 | validation: 1.5221519276225288]
	TIME [epoch: 1.28 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3612061692056256		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3612061692056256 | validation: 1.2845347726049365]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3738825395211105		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.3738825395211105 | validation: 1.4360491196839054]
	TIME [epoch: 1.28 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.329695741590403		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.329695741590403 | validation: 1.3151739545002408]
	TIME [epoch: 1.28 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.299275548775534		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.299275548775534 | validation: 1.309858764572685]
	TIME [epoch: 1.27 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3013775101044933		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.3013775101044933 | validation: 1.4588925521350191]
	TIME [epoch: 1.27 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3626944951603692		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.3626944951603692 | validation: 1.4092347390962772]
	TIME [epoch: 1.27 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3725288750652458		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.3725288750652458 | validation: 1.4013712923222539]
	TIME [epoch: 1.28 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.331194798457138		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.331194798457138 | validation: 1.329767476506547]
	TIME [epoch: 1.28 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2857588975160903		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.2857588975160903 | validation: 1.2743401500848437]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2896311879315066		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.2896311879315066 | validation: 1.3204019459076126]
	TIME [epoch: 1.28 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2816550141735275		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.2816550141735275 | validation: 1.2659227680368064]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2742660971061182		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2742660971061182 | validation: 1.3067736623710635]
	TIME [epoch: 1.28 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2761780957231157		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.2761780957231157 | validation: 1.2975175047831353]
	TIME [epoch: 1.27 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2934700724200061		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.2934700724200061 | validation: 1.3419488500441492]
	TIME [epoch: 1.27 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2947551486429012		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.2947551486429012 | validation: 1.3469221259562845]
	TIME [epoch: 1.27 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3202353182232254		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.3202353182232254 | validation: 1.4412184651434636]
	TIME [epoch: 1.28 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923728587157462		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.3923728587157462 | validation: 1.3971856058735972]
	TIME [epoch: 1.28 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.345136186215891		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.345136186215891 | validation: 1.292628764984808]
	TIME [epoch: 1.28 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293751050841974		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.293751050841974 | validation: 1.2809885125409441]
	TIME [epoch: 1.28 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2613792679851332		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.2613792679851332 | validation: 1.3045070786091395]
	TIME [epoch: 1.27 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2608232431129278		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.2608232431129278 | validation: 1.2554120511289597]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.280970371615394		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.280970371615394 | validation: 1.399831030241112]
	TIME [epoch: 1.28 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3188864131761482		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.3188864131761482 | validation: 1.295873864384673]
	TIME [epoch: 1.27 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3380717087229146		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.3380717087229146 | validation: 1.4273144288131592]
	TIME [epoch: 1.28 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3061640614129448		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.3061640614129448 | validation: 1.2697369175682678]
	TIME [epoch: 1.27 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2796404517239321		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.2796404517239321 | validation: 1.3050825010761047]
	TIME [epoch: 1.27 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2812397353146496		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.2812397353146496 | validation: 1.3408333266676498]
	TIME [epoch: 1.28 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2971412730218044		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.2971412730218044 | validation: 1.3883247243647288]
	TIME [epoch: 1.28 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.329622476946447		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.329622476946447 | validation: 1.3040516613535553]
	TIME [epoch: 1.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.287706568639702		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.287706568639702 | validation: 1.327242652477012]
	TIME [epoch: 1.27 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2724540527363224		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.2724540527363224 | validation: 1.2308187842119769]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2542915171908808		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.2542915171908808 | validation: 1.2847922190908787]
	TIME [epoch: 1.28 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2531880227894658		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.2531880227894658 | validation: 1.2381534157123213]
	TIME [epoch: 1.27 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.246477609641028		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.246477609641028 | validation: 1.301129858919425]
	TIME [epoch: 1.27 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2503476736396664		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.2503476736396664 | validation: 1.2154920610345152]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526574619065696		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.2526574619065696 | validation: 1.3549162578543112]
	TIME [epoch: 1.28 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293334076393749		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.293334076393749 | validation: 1.2254169185437986]
	TIME [epoch: 1.27 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3058726622632322		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.3058726622632322 | validation: 1.3715225847928332]
	TIME [epoch: 1.27 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2807140798955874		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.2807140798955874 | validation: 1.2807550445544706]
	TIME [epoch: 1.27 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2628207809008454		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.2628207809008454 | validation: 1.2847218338089332]
	TIME [epoch: 1.27 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2786476397785114		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.2786476397785114 | validation: 1.2619283444523153]
	TIME [epoch: 1.28 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.254167725895319		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.254167725895319 | validation: 1.322482284909455]
	TIME [epoch: 1.28 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2866278842678969		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.2866278842678969 | validation: 1.4491073923424445]
	TIME [epoch: 1.27 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3826415080613668		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.3826415080613668 | validation: 1.4272205828298572]
	TIME [epoch: 1.28 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3406522639257108		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.3406522639257108 | validation: 1.2252023922218442]
	TIME [epoch: 1.27 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2302361115084455		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.2302361115084455 | validation: 1.2646829088758793]
	TIME [epoch: 1.28 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2367502264946502		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.2367502264946502 | validation: 1.2852686626210659]
	TIME [epoch: 1.28 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2507373456295472		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.2507373456295472 | validation: 1.2437365224353276]
	TIME [epoch: 1.27 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365860937412414		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.2365860937412414 | validation: 1.2581250629755836]
	TIME [epoch: 1.27 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2319139232140122		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.2319139232140122 | validation: 1.2396526762239066]
	TIME [epoch: 1.27 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2277608786062222		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.2277608786062222 | validation: 1.3047747597115915]
	TIME [epoch: 1.27 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2390742055931645		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.2390742055931645 | validation: 1.2224381752612132]
	TIME [epoch: 1.28 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244189574099608		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.244189574099608 | validation: 1.3087626880831367]
	TIME [epoch: 1.27 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.247558530268385		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.247558530268385 | validation: 1.1777208148558127]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2544207507178422		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.2544207507178422 | validation: 1.3169573694366372]
	TIME [epoch: 1.27 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.261477528971518		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.261477528971518 | validation: 1.1922782066988713]
	TIME [epoch: 1.27 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2417460644188112		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.2417460644188112 | validation: 1.219287302256652]
	TIME [epoch: 1.27 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2300891726139183		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.2300891726139183 | validation: 1.272615399900836]
	TIME [epoch: 1.27 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365738338897996		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.2365738338897996 | validation: 1.2785790862791013]
	TIME [epoch: 1.26 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2497011365142006		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.2497011365142006 | validation: 1.3262258027399303]
	TIME [epoch: 1.27 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2908965811555606		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.2908965811555606 | validation: 1.4285121267472185]
	TIME [epoch: 1.27 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3309851532721482		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.3309851532721482 | validation: 1.2303839187635004]
	TIME [epoch: 1.27 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2304930339142184		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.2304930339142184 | validation: 1.2198905069236294]
	TIME [epoch: 1.27 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.203189811800669		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.203189811800669 | validation: 1.2350726882654977]
	TIME [epoch: 1.27 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207886127546961		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.207886127546961 | validation: 1.1968658343509648]
	TIME [epoch: 1.27 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2099826681333996		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.2099826681333996 | validation: 1.285122495458898]
	TIME [epoch: 1.28 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2303767263107952		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.2303767263107952 | validation: 1.1847167422469214]
	TIME [epoch: 1.28 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2287133165356225		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.2287133165356225 | validation: 1.2956504470854342]
	TIME [epoch: 1.27 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2411620281792355		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.2411620281792355 | validation: 1.1688151696095628]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2271812368997597		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.2271812368997597 | validation: 1.2755639835106252]
	TIME [epoch: 1.27 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.230236448133941		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.230236448133941 | validation: 1.1875417228402063]
	TIME [epoch: 1.27 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2103574891301154		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.2103574891301154 | validation: 1.1898302400537255]
	TIME [epoch: 1.27 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.199789449061588		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.199789449061588 | validation: 1.2328745886584578]
	TIME [epoch: 1.27 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.21305146535765		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.21305146535765 | validation: 1.2127798765925368]
	TIME [epoch: 1.27 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2452368833626752		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.2452368833626752 | validation: 1.3355662092161247]
	TIME [epoch: 1.27 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2924072242568974		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.2924072242568974 | validation: 1.2531202221629676]
	TIME [epoch: 1.27 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2782207038644426		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.2782207038644426 | validation: 1.1493478946836642]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2100396533128759		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.2100396533128759 | validation: 1.258134091144539]
	TIME [epoch: 1.28 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2185392380589182		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.2185392380589182 | validation: 1.1473784776813636]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1930531727186027		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.1930531727186027 | validation: 1.1713488189191446]
	TIME [epoch: 1.28 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816454374520393		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.1816454374520393 | validation: 1.2344742465114582]
	TIME [epoch: 1.28 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1947990972288227		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1947990972288227 | validation: 1.1625478199104076]
	TIME [epoch: 1.28 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2047150350320766		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.2047150350320766 | validation: 1.2639555576313406]
	TIME [epoch: 1.28 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2245094253862028		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.2245094253862028 | validation: 1.190201424612768]
	TIME [epoch: 1.27 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2179074272719244		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.2179074272719244 | validation: 1.2119973797260706]
	TIME [epoch: 1.28 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2039230533803922		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.2039230533803922 | validation: 1.213802953414269]
	TIME [epoch: 1.28 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1904331445753717		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.1904331445753717 | validation: 1.1504760878315066]
	TIME [epoch: 1.28 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1779269080738477		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.1779269080738477 | validation: 1.2181703016702488]
	TIME [epoch: 1.27 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911460564578606		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.1911460564578606 | validation: 1.102703265936101]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1991908753078249		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.1991908753078249 | validation: 1.2231159316182774]
	TIME [epoch: 1.28 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190184563249471		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.190184563249471 | validation: 1.207723901822166]
	TIME [epoch: 1.28 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1985762192787734		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.1985762192787734 | validation: 1.1671482472922177]
	TIME [epoch: 1.27 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1890180561812844		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.1890180561812844 | validation: 1.2275437207531752]
	TIME [epoch: 1.28 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1879808679535708		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.1879808679535708 | validation: 1.1601931654371507]
	TIME [epoch: 1.27 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2008872733334588		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.2008872733334588 | validation: 1.2485847565122183]
	TIME [epoch: 1.27 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2104493845355269		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.2104493845355269 | validation: 1.2031380745152749]
	TIME [epoch: 1.27 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194572891475138		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.194572891475138 | validation: 1.1606137012778448]
	TIME [epoch: 1.27 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1792618676504423		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1792618676504423 | validation: 1.2044362923122809]
	TIME [epoch: 1.27 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1592819417994418		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.1592819417994418 | validation: 1.068407791666057]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1737923915686008		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1737923915686008 | validation: 1.3056208081934568]
	TIME [epoch: 1.27 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2193824502455024		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.2193824502455024 | validation: 1.0414449323925088]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.185494792204254		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.185494792204254 | validation: 1.1658744523855178]
	TIME [epoch: 1.27 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1510098948378402		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.1510098948378402 | validation: 1.1357882971434763]
	TIME [epoch: 1.27 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141435981533592		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.141435981533592 | validation: 1.0921709331669813]
	TIME [epoch: 1.27 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144727929638934		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.144727929638934 | validation: 1.163078269830114]
	TIME [epoch: 1.27 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137081030851559		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.137081030851559 | validation: 1.0908728934214904]
	TIME [epoch: 1.27 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1642022360322768		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1642022360322768 | validation: 1.2533672391771487]
	TIME [epoch: 1.27 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2061975177492135		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.2061975177492135 | validation: 1.168502587105192]
	TIME [epoch: 1.27 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183607210594722		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.183607210594722 | validation: 1.1438372149328424]
	TIME [epoch: 1.27 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1581413319693523		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1581413319693523 | validation: 1.166250475043109]
	TIME [epoch: 1.27 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1473682138247927		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.1473682138247927 | validation: 1.0105747587159084]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460096422555794		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1460096422555794 | validation: 1.1764782167972059]
	TIME [epoch: 1.28 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425806577149198		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.1425806577149198 | validation: 1.029258073371997]
	TIME [epoch: 1.28 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1393559099256723		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.1393559099256723 | validation: 1.1411327307493893]
	TIME [epoch: 1.28 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127647553691		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.127647553691 | validation: 1.059808720590878]
	TIME [epoch: 1.28 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1131649963142403		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.1131649963142403 | validation: 1.0873089240777325]
	TIME [epoch: 1.28 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1300207009576684		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.1300207009576684 | validation: 1.1810970399334992]
	TIME [epoch: 1.28 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1571258275286316		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.1571258275286316 | validation: 1.0412304292355536]
	TIME [epoch: 1.28 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1386822634497429		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.1386822634497429 | validation: 1.1851961667695812]
	TIME [epoch: 1.28 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280595003616407		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1280595003616407 | validation: 1.0299679403588942]
	TIME [epoch: 1.28 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1758709447964122		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.1758709447964122 | validation: 1.2997502384891935]
	TIME [epoch: 1.28 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1910877870979064		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.1910877870979064 | validation: 1.097627263227574]
	TIME [epoch: 1.28 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1387797005183442		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.1387797005183442 | validation: 1.0441272101451589]
	TIME [epoch: 1.27 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1113232005183036		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.1113232005183036 | validation: 1.1675243336557168]
	TIME [epoch: 1.28 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1190716408113648		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.1190716408113648 | validation: 0.9510498364220548]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1099203838759575		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.1099203838759575 | validation: 1.1322959369535055]
	TIME [epoch: 1.28 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1014954739214429		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.1014954739214429 | validation: 1.0014295704532994]
	TIME [epoch: 1.28 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087002356728349		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.087002356728349 | validation: 1.084303786203289]
	TIME [epoch: 1.28 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0829799112171967		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0829799112171967 | validation: 1.0141745165502984]
	TIME [epoch: 1.28 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097961781634799		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.097961781634799 | validation: 1.157970094636872]
	TIME [epoch: 1.28 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1210998652069923		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.1210998652069923 | validation: 1.098125888334436]
	TIME [epoch: 1.28 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136939341978942		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.136939341978942 | validation: 1.0928876339661973]
	TIME [epoch: 1.27 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310388886607183		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.1310388886607183 | validation: 1.17561185988083]
	TIME [epoch: 1.28 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1043167416095994		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.1043167416095994 | validation: 0.9178250512272763]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095720180720352		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.1095720180720352 | validation: 1.2475096052637946]
	TIME [epoch: 1.28 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144373036675691		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.144373036675691 | validation: 0.9118289957167378]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1143545317629684		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.1143545317629684 | validation: 1.0652836040893374]
	TIME [epoch: 1.28 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0724055885142323		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0724055885142323 | validation: 1.0251630671143968]
	TIME [epoch: 1.28 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.058446845812155		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.058446845812155 | validation: 0.9994535732665928]
	TIME [epoch: 1.28 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0584271218518588		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0584271218518588 | validation: 1.0591947790831435]
	TIME [epoch: 1.27 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0643115706979747		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.0643115706979747 | validation: 1.0059022561132733]
	TIME [epoch: 1.27 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0660058614765076		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.0660058614765076 | validation: 1.114151246540752]
	TIME [epoch: 1.28 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0793837578762153		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.0793837578762153 | validation: 0.9780321326084831]
	TIME [epoch: 1.28 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785342478395934		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.0785342478395934 | validation: 1.0761696154909257]
	TIME [epoch: 1.28 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0871526366322548		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.0871526366322548 | validation: 0.9755986552081692]
	TIME [epoch: 1.28 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0653632750179753		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.0653632750179753 | validation: 1.0600481490043265]
	TIME [epoch: 1.27 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0557203946083162		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0557203946083162 | validation: 0.9607097535079525]
	TIME [epoch: 1.27 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0898298413539278		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0898298413539278 | validation: 1.2988187561141202]
	TIME [epoch: 1.27 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147156302370734		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.147156302370734 | validation: 0.9162493061776626]
	TIME [epoch: 1.28 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1340728333467132		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.1340728333467132 | validation: 1.1714657476744417]
	TIME [epoch: 1.27 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067987773055598		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.067987773055598 | validation: 0.9799974517842647]
	TIME [epoch: 1.27 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0368071452448233		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0368071452448233 | validation: 0.9876068741326339]
	TIME [epoch: 1.27 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0419842648412152		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.0419842648412152 | validation: 1.0119549115413762]
	TIME [epoch: 1.27 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029599024651902		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.029599024651902 | validation: 0.9455352362408249]
	TIME [epoch: 1.27 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0305520694020371		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.0305520694020371 | validation: 1.0573181322986174]
	TIME [epoch: 1.27 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0287070979833701		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.0287070979833701 | validation: 0.913851027310205]
	TIME [epoch: 1.27 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0341814528627236		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.0341814528627236 | validation: 1.152471002404479]
	TIME [epoch: 1.27 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0623496666084207		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0623496666084207 | validation: 0.817070738199599]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1136140777704646		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.1136140777704646 | validation: 1.246975197995605]
	TIME [epoch: 1.28 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0998428268463982		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0998428268463982 | validation: 0.8879507121676442]
	TIME [epoch: 1.27 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0308182093570444		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0308182093570444 | validation: 1.0390983038114787]
	TIME [epoch: 1.28 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0233384435446287		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.0233384435446287 | validation: 0.99446070498462]
	TIME [epoch: 1.27 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0320558610791692		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.0320558610791692 | validation: 0.9723532187454638]
	TIME [epoch: 1.28 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02189173021796		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.02189173021796 | validation: 0.9990378775032277]
	TIME [epoch: 1.28 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0098476422970746		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0098476422970746 | validation: 0.9124325961098805]
	TIME [epoch: 1.28 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0036884974252964		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.0036884974252964 | validation: 1.0258913163938075]
	TIME [epoch: 1.27 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0160991966937594		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.0160991966937594 | validation: 0.8801920801471271]
	TIME [epoch: 1.28 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055382898974913		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.055382898974913 | validation: 1.206383399370899]
	TIME [epoch: 1.28 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184984010665364		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.1184984010665364 | validation: 0.9340368746515143]
	TIME [epoch: 1.28 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0321509227559773		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.0321509227559773 | validation: 0.9601562978436444]
	TIME [epoch: 1.28 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001494372349883		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.001494372349883 | validation: 1.1245520290311504]
	TIME [epoch: 1.27 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224763692776353		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0224763692776353 | validation: 0.8606840341507017]
	TIME [epoch: 1.27 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099068519616024		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.099068519616024 | validation: 1.3003177004748636]
	TIME [epoch: 1.28 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0924449030887222		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0924449030887222 | validation: 0.913783550469813]
	TIME [epoch: 1.28 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9948413711524714		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9948413711524714 | validation: 0.949802787461652]
	TIME [epoch: 1.28 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9908636106494811		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9908636106494811 | validation: 1.0503407767346278]
	TIME [epoch: 1.28 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.005836913309075		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.005836913309075 | validation: 0.8534960732810601]
	TIME [epoch: 1.27 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030089031474472		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.030089031474472 | validation: 1.1862543884705965]
	TIME [epoch: 1.28 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0468019074079309		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.0468019074079309 | validation: 0.8733447498310254]
	TIME [epoch: 1.28 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0175498107774614		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.0175498107774614 | validation: 1.05638525188924]
	TIME [epoch: 1.28 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001028630032677		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.001028630032677 | validation: 0.8972442429515941]
	TIME [epoch: 1.28 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978948820725303		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.978948820725303 | validation: 1.0291077608463286]
	TIME [epoch: 1.27 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9847981422402978		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9847981422402978 | validation: 0.8466242050031441]
	TIME [epoch: 1.27 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034316486365818		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.034316486365818 | validation: 1.1951811642027224]
	TIME [epoch: 1.27 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1051240791295438		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.1051240791295438 | validation: 0.8325328175509331]
	TIME [epoch: 1.27 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0116992755642407		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.0116992755642407 | validation: 0.9618856139332568]
	TIME [epoch: 1.27 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9746557443814701		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9746557443814701 | validation: 0.9684749692502757]
	TIME [epoch: 1.27 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0006107870682235		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.0006107870682235 | validation: 1.013389058300423]
	TIME [epoch: 1.27 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0367489394254426		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.0367489394254426 | validation: 0.9813341324389306]
	TIME [epoch: 1.27 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122531985288685		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.0122531985288685 | validation: 1.00721085804708]
	TIME [epoch: 1.27 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9727683096243349		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.9727683096243349 | validation: 0.8034296732895498]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0053263177067049		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0053263177067049 | validation: 1.291342178593699]
	TIME [epoch: 1.28 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0992107699723934		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.0992107699723934 | validation: 0.8333410473795837]
	TIME [epoch: 1.28 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0396905059314208		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.0396905059314208 | validation: 1.077558119298199]
	TIME [epoch: 1.28 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9837328242534144		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9837328242534144 | validation: 0.9158144292836389]
	TIME [epoch: 1.28 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9726336431275615		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9726336431275615 | validation: 0.9404678288582448]
	TIME [epoch: 1.28 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658429107851109		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.9658429107851109 | validation: 0.9110753404186988]
	TIME [epoch: 1.28 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.965470126973238		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.965470126973238 | validation: 0.948261127709772]
	TIME [epoch: 1.28 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658733201877269		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.9658733201877269 | validation: 0.868094990234008]
	TIME [epoch: 1.28 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9876923151812992		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.9876923151812992 | validation: 1.177999275225832]
	TIME [epoch: 1.27 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0741204649965728		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.0741204649965728 | validation: 0.7730909504471287]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1029515657410847		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.1029515657410847 | validation: 1.1272045071164165]
	TIME [epoch: 1.28 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016590906865854		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.016590906865854 | validation: 0.8648862203989545]
	TIME [epoch: 1.28 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0012065346886683		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.0012065346886683 | validation: 1.0261744458227777]
	TIME [epoch: 1.29 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9879719845710816		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.9879719845710816 | validation: 0.8958777323663973]
	TIME [epoch: 1.28 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702512244297321		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9702512244297321 | validation: 0.9535522010661174]
	TIME [epoch: 1.28 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9563608497966334		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.9563608497966334 | validation: 0.9255525749981012]
	TIME [epoch: 1.28 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597531211744627		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.9597531211744627 | validation: 0.9579793642481885]
	TIME [epoch: 1.28 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9846538178020896		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.9846538178020896 | validation: 0.9991511828518207]
	TIME [epoch: 1.28 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0075942607533233		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.0075942607533233 | validation: 0.9623543797989772]
	TIME [epoch: 1.29 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9789828874545414		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.9789828874545414 | validation: 0.9155737744349861]
	TIME [epoch: 1.28 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525293926617912		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9525293926617912 | validation: 1.012529772493553]
	TIME [epoch: 1.28 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9630562987185415		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.9630562987185415 | validation: 0.8118636913494742]
	TIME [epoch: 1.28 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126006380811823		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.0126006380811823 | validation: 1.3010495686033547]
	TIME [epoch: 1.28 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0984807203135303		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.0984807203135303 | validation: 0.7909835625804931]
	TIME [epoch: 1.28 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0328630555549092		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.0328630555549092 | validation: 1.0544351672675472]
	TIME [epoch: 1.28 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944985770823052		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.9944985770823052 | validation: 0.8941888037287585]
	TIME [epoch: 1.28 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524356964953483		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9524356964953483 | validation: 0.9068430603835029]
	TIME [epoch: 1.28 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.94992956538412		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.94992956538412 | validation: 0.9378122078420109]
	TIME [epoch: 1.28 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9409487048782422		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.9409487048782422 | validation: 0.9097456479516137]
	TIME [epoch: 1.28 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451990902003852		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9451990902003852 | validation: 0.9713053229382027]
	TIME [epoch: 1.28 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462664445642582		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9462664445642582 | validation: 0.8188261526082548]
	TIME [epoch: 1.28 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.97995595126959		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.97995595126959 | validation: 1.2817321811093085]
	TIME [epoch: 1.27 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0703481074180434		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.0703481074180434 | validation: 0.7875077401256241]
	TIME [epoch: 1.28 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0452319924989069		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.0452319924989069 | validation: 1.0906038710808421]
	TIME [epoch: 1.28 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978897388977592		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.978897388977592 | validation: 0.8648487533131499]
	TIME [epoch: 1.28 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9588428178916948		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.9588428178916948 | validation: 0.9909965137331604]
	TIME [epoch: 1.28 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9756783831330558		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9756783831330558 | validation: 0.9136988865005287]
	TIME [epoch: 1.28 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964827133316928		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.964827133316928 | validation: 0.9859200391260615]
	TIME [epoch: 1.27 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9522474414809073		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9522474414809073 | validation: 0.8566273065524943]
	TIME [epoch: 1.28 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342606798843929		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9342606798843929 | validation: 0.9379970914263698]
	TIME [epoch: 1.28 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293270131494106		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.9293270131494106 | validation: 0.9044784677431482]
	TIME [epoch: 1.28 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9384613326726918		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.9384613326726918 | validation: 0.9966412243671368]
	TIME [epoch: 1.28 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959490113528122		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.959490113528122 | validation: 0.9007367216001101]
	TIME [epoch: 1.28 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9968611210485953		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.9968611210485953 | validation: 1.0379682296448638]
	TIME [epoch: 1.28 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9787841447560444		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9787841447560444 | validation: 0.8704900016387248]
	TIME [epoch: 1.28 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9482827066111564		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9482827066111564 | validation: 1.077172648772664]
	TIME [epoch: 1.28 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.975085633441889		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.975085633441889 | validation: 0.775251356823085]
	TIME [epoch: 1.28 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1243465798959762		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.1243465798959762 | validation: 1.2042940038036456]
	TIME [epoch: 1.28 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0485695763462404		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.0485695763462404 | validation: 0.8492080680578227]
	TIME [epoch: 1.28 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398845465010695		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9398845465010695 | validation: 0.9174779238983476]
	TIME [epoch: 1.28 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9375560587844675		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.9375560587844675 | validation: 0.9451438520948973]
	TIME [epoch: 1.28 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332487556684304		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9332487556684304 | validation: 0.8741881809052812]
	TIME [epoch: 1.28 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262134056307513		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.9262134056307513 | validation: 0.9745531896859196]
	TIME [epoch: 1.28 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930463545595592		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.930463545595592 | validation: 0.8144409383453889]
	TIME [epoch: 1.28 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527724333900355		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.9527724333900355 | validation: 1.1649938236542154]
	TIME [epoch: 1.28 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0040638853959014		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.0040638853959014 | validation: 0.7642260740247313]
	TIME [epoch: 1.28 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0240848999130285		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.0240848999130285 | validation: 1.122231263261092]
	TIME [epoch: 1.27 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9791501998556974		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.9791501998556974 | validation: 0.833117281855042]
	TIME [epoch: 1.27 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9369754135760434		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.9369754135760434 | validation: 0.9580634526507563]
	TIME [epoch: 1.27 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172706247844503		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.9172706247844503 | validation: 0.8605404124630427]
	TIME [epoch: 1.27 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921978481251414		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.921978481251414 | validation: 0.9137933742837353]
	TIME [epoch: 1.27 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196057269982333		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9196057269982333 | validation: 0.9065840191954868]
	TIME [epoch: 1.27 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163631088802714		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.9163631088802714 | validation: 0.921565833549779]
	TIME [epoch: 1.27 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9351477298527854		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.9351477298527854 | validation: 1.0289586896441762]
	TIME [epoch: 1.27 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9678104406538927		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.9678104406538927 | validation: 0.8891843921247496]
	TIME [epoch: 1.28 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9790693244214957		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.9790693244214957 | validation: 1.1971802531476772]
	TIME [epoch: 1.27 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0188868830764215		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.0188868830764215 | validation: 0.7212038281807559]
	TIME [epoch: 1.27 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0555452784289485		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.0555452784289485 | validation: 1.1133913703074059]
	TIME [epoch: 1.28 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784588766792426		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.9784588766792426 | validation: 0.8716456616183255]
	TIME [epoch: 1.28 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196555342462994		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.9196555342462994 | validation: 0.946276288124198]
	TIME [epoch: 1.28 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9512790905726558		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.9512790905726558 | validation: 0.9475396831873045]
	TIME [epoch: 1.28 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453197560520531		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.9453197560520531 | validation: 0.9168198250343229]
	TIME [epoch: 1.28 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262191716921271		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.9262191716921271 | validation: 0.8803662817484587]
	TIME [epoch: 1.28 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189171025319949		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.9189171025319949 | validation: 0.9667173299829588]
	TIME [epoch: 1.28 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122027242226204		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.9122027242226204 | validation: 0.8291658894412898]
	TIME [epoch: 1.28 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216911237214137		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.9216911237214137 | validation: 1.0579899699485946]
	TIME [epoch: 1.28 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957002235503938		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.957002235503938 | validation: 0.7372934846027923]
	TIME [epoch: 1.28 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0205837757125715		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.0205837757125715 | validation: 1.1948670882954289]
	TIME [epoch: 1.28 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0094291827103188		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.0094291827103188 | validation: 0.7973326021049215]
	TIME [epoch: 1.28 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9362301908780983		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.9362301908780983 | validation: 0.9252522137369372]
	TIME [epoch: 1.28 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090325717138376		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.9090325717138376 | validation: 0.9178251795508731]
	TIME [epoch: 1.29 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093657957764515		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.9093657957764515 | validation: 0.8870638632441965]
	TIME [epoch: 1.28 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114011234866181		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.9114011234866181 | validation: 0.997277080033282]
	TIME [epoch: 1.28 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.939680387279982		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.939680387279982 | validation: 0.8313860864708347]
	TIME [epoch: 1.28 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9640975873470925		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.9640975873470925 | validation: 1.0861196179261199]
	TIME [epoch: 1.28 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604145926867484		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.9604145926867484 | validation: 0.8138760774822735]
	TIME [epoch: 1.28 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256250680317136		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.9256250680317136 | validation: 1.0354563881280143]
	TIME [epoch: 1.28 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.934666781605507		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.934666781605507 | validation: 0.7703325788170867]
	TIME [epoch: 1.28 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9957995585994895		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.9957995585994895 | validation: 1.1369835681731417]
	TIME [epoch: 1.28 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.986315753811991		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.986315753811991 | validation: 0.79284678171426]
	TIME [epoch: 1.28 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9334654264914393		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.9334654264914393 | validation: 0.9738096912332028]
	TIME [epoch: 1.28 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132828585969318		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.9132828585969318 | validation: 0.8935269043669619]
	TIME [epoch: 1.28 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9073924851584366		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.9073924851584366 | validation: 0.9102348510275301]
	TIME [epoch: 1.28 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075905811845429		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.9075905811845429 | validation: 0.8687685278205953]
	TIME [epoch: 1.28 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088343101067955		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.9088343101067955 | validation: 0.922095608627858]
	TIME [epoch: 1.28 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983181071240225		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.8983181071240225 | validation: 0.8493653933035538]
	TIME [epoch: 1.28 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9063400281435807		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.9063400281435807 | validation: 0.9502966926468016]
	TIME [epoch: 1.28 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048315966005669		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.9048315966005669 | validation: 0.7686586919958364]
	TIME [epoch: 1.28 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353801264883075		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.9353801264883075 | validation: 1.2561360592867912]
	TIME [epoch: 1.28 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0422346305009196		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.0422346305009196 | validation: 0.7396121599497345]
	TIME [epoch: 1.28 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9919606271373155		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.9919606271373155 | validation: 1.054047212211053]
	TIME [epoch: 1.28 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450576827279347		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.9450576827279347 | validation: 0.8372173764728246]
	TIME [epoch: 1.28 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9267423297490024		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.9267423297490024 | validation: 0.9322277523232047]
	TIME [epoch: 1.28 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021409573124269		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.9021409573124269 | validation: 0.8896733077556576]
	TIME [epoch: 191 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940716403091841		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.8940716403091841 | validation: 0.8931206369293294]
	TIME [epoch: 2.54 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853684781979252		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.8853684781979252 | validation: 0.8885462475211359]
	TIME [epoch: 2.52 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846450144307655		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.8846450144307655 | validation: 0.8928214155496225]
	TIME [epoch: 2.51 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887574382589936		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.887574382589936 | validation: 0.8517465494485216]
	TIME [epoch: 2.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935774108750079		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.8935774108750079 | validation: 0.9401471781459345]
	TIME [epoch: 2.51 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8928983772577589		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.8928983772577589 | validation: 0.8023529530498724]
	TIME [epoch: 2.52 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9201438873894108		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.9201438873894108 | validation: 1.1565698414529233]
	TIME [epoch: 2.51 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9972337391139463		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.9972337391139463 | validation: 0.7500073331167277]
	TIME [epoch: 2.52 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0100392772965776		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.0100392772965776 | validation: 1.1284907814144323]
	TIME [epoch: 2.52 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802494789562356		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.9802494789562356 | validation: 0.8191177383296971]
	TIME [epoch: 2.52 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9972544371888489		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.9972544371888489 | validation: 0.9529468897957646]
	TIME [epoch: 2.52 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025495257742057		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.9025495257742057 | validation: 0.8606103911562688]
	TIME [epoch: 2.52 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879054281586036		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.879054281586036 | validation: 0.8930895654894695]
	TIME [epoch: 2.51 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891507620229954		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.8891507620229954 | validation: 0.8798167984069712]
	TIME [epoch: 2.52 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860476116916456		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8860476116916456 | validation: 0.8511186277388023]
	TIME [epoch: 2.51 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865838244636476		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8865838244636476 | validation: 0.959979369817157]
	TIME [epoch: 2.52 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862939983936378		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8862939983936378 | validation: 0.7405665139506916]
	TIME [epoch: 2.52 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9148137535931402		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.9148137535931402 | validation: 1.1656338928705536]
	TIME [epoch: 2.52 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936160475534493		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.9936160475534493 | validation: 0.733979446493814]
	TIME [epoch: 2.51 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9684399406322212		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.9684399406322212 | validation: 1.0026940450213042]
	TIME [epoch: 2.52 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030073273212935		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.9030073273212935 | validation: 0.8388659812679539]
	TIME [epoch: 2.52 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838419986321773		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.8838419986321773 | validation: 0.8669025002079094]
	TIME [epoch: 2.52 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703395004864042		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.8703395004864042 | validation: 0.9082403782853321]
	TIME [epoch: 2.52 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726745626149659		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.8726745626149659 | validation: 0.7983183561680196]
	TIME [epoch: 2.52 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873042794283414		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8873042794283414 | validation: 1.0712528882169878]
	TIME [epoch: 2.51 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256166487387256		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.9256166487387256 | validation: 0.7256562346651982]
	TIME [epoch: 2.51 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9895133907420902		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.9895133907420902 | validation: 1.140859226793646]
	TIME [epoch: 2.52 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9697129517582852		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.9697129517582852 | validation: 0.7765374325058293]
	TIME [epoch: 2.52 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060475127817813		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.9060475127817813 | validation: 0.9202592705518106]
	TIME [epoch: 2.52 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943852164315242		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.8943852164315242 | validation: 0.8593453023704895]
	TIME [epoch: 2.52 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899498275435517		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.8899498275435517 | validation: 0.8965809580230752]
	TIME [epoch: 2.52 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8898060952612525		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.8898060952612525 | validation: 0.8391464634144379]
	TIME [epoch: 2.52 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937683232415282		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.8937683232415282 | validation: 0.9289504900916996]
	TIME [epoch: 2.52 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870725539053231		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.8870725539053231 | validation: 0.799268057305677]
	TIME [epoch: 2.52 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940664703230086		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.8940664703230086 | validation: 1.0086381925833707]
	TIME [epoch: 2.52 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899896083870056		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.899896083870056 | validation: 0.7125974550318459]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367606500926772		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.9367606500926772 | validation: 1.1083362082675898]
	TIME [epoch: 2.51 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9484466047323492		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.9484466047323492 | validation: 0.7756009281363176]
	TIME [epoch: 2.52 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9024122186513112		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.9024122186513112 | validation: 0.9056850811387943]
	TIME [epoch: 2.52 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797376993909046		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.8797376993909046 | validation: 0.8495641127126536]
	TIME [epoch: 2.52 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714895486181365		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.8714895486181365 | validation: 0.8755007218919787]
	TIME [epoch: 2.52 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864276517415133		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.864276517415133 | validation: 0.8496574823088117]
	TIME [epoch: 2.52 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612866207237826		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8612866207237826 | validation: 0.8767817233136246]
	TIME [epoch: 2.51 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862891357780822		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.862891357780822 | validation: 0.7944384393224435]
	TIME [epoch: 2.51 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711214648951109		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.8711214648951109 | validation: 0.999803966036321]
	TIME [epoch: 2.52 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861611518362565		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.8861611518362565 | validation: 0.7224618560633007]
	TIME [epoch: 2.52 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9477775532357525		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.9477775532357525 | validation: 1.1544157827297798]
	TIME [epoch: 2.52 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9768888259344857		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.9768888259344857 | validation: 0.72565941259489]
	TIME [epoch: 2.52 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525417284717856		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.9525417284717856 | validation: 0.9388602160471802]
	TIME [epoch: 2.51 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072301168421991		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.9072301168421991 | validation: 0.8286695979485604]
	TIME [epoch: 2.52 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771547282138505		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.8771547282138505 | validation: 0.8884352274511207]
	TIME [epoch: 2.52 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86151635947661		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.86151635947661 | validation: 0.8657744813450216]
	TIME [epoch: 2.52 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856975582638184		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.856975582638184 | validation: 0.8549281073761708]
	TIME [epoch: 2.52 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509871590467831		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.8509871590467831 | validation: 0.8687595561623791]
	TIME [epoch: 2.52 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499144974614552		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.8499144974614552 | validation: 0.8362904446259851]
	TIME [epoch: 2.52 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573360168473483		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.8573360168473483 | validation: 0.9383117151186626]
	TIME [epoch: 2.52 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797328246454478		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.8797328246454478 | validation: 0.7418439813330038]
	TIME [epoch: 2.51 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420910230579054		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.9420910230579054 | validation: 1.08485302286489]
	TIME [epoch: 2.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9437782678334137		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.9437782678334137 | validation: 0.7073420002867966]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159912343852998		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.9159912343852998 | validation: 1.0816868083650346]
	TIME [epoch: 2.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366525308680043		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.9366525308680043 | validation: 0.8086267308225801]
	TIME [epoch: 2.51 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761650456747845		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.8761650456747845 | validation: 0.8413199679340897]
	TIME [epoch: 2.51 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513763905319972		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.8513763905319972 | validation: 0.8847806182586475]
	TIME [epoch: 2.51 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524010693237278		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.8524010693237278 | validation: 0.7547123802465695]
	TIME [epoch: 2.52 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705108043379011		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.8705108043379011 | validation: 1.0075874782823064]
	TIME [epoch: 2.51 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866528336012794		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.8866528336012794 | validation: 0.7043921019701618]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909424603105586		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.909424603105586 | validation: 1.0076682805215083]
	TIME [epoch: 2.52 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999999944472588		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.8999999944472588 | validation: 0.7351800929359663]
	TIME [epoch: 2.52 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795899040493694		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8795899040493694 | validation: 0.9004494101241957]
	TIME [epoch: 2.52 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584629205562507		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.8584629205562507 | validation: 0.8062074233594743]
	TIME [epoch: 2.52 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442688322094607		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.8442688322094607 | validation: 0.9019484350278666]
	TIME [epoch: 2.52 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468911825471932		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.8468911825471932 | validation: 0.7483185968461886]
	TIME [epoch: 2.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655465794017898		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.8655465794017898 | validation: 1.007381692224602]
	TIME [epoch: 2.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966907544061313		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.8966907544061313 | validation: 0.72154392379083]
	TIME [epoch: 2.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107923838866075		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.9107923838866075 | validation: 1.0309082352463041]
	TIME [epoch: 2.51 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029368743732838		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.9029368743732838 | validation: 0.7283096449347851]
	TIME [epoch: 2.52 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810008925056337		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.8810008925056337 | validation: 0.9928657872644202]
	TIME [epoch: 2.52 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894422694852467		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.894422694852467 | validation: 0.7645359141734706]
	TIME [epoch: 2.52 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980772081456792		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.8980772081456792 | validation: 0.9301972226247548]
	TIME [epoch: 2.52 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85996910043514		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.85996910043514 | validation: 0.8064203446491631]
	TIME [epoch: 2.51 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8436180113765557		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.8436180113765557 | validation: 0.8662317927437997]
	TIME [epoch: 2.52 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837220845172498		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.837220845172498 | validation: 0.8138475013026192]
	TIME [epoch: 2.52 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381151070780325		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.8381151070780325 | validation: 0.8476935377598784]
	TIME [epoch: 2.52 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474752061250933		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.8474752061250933 | validation: 0.8570298419038025]
	TIME [epoch: 2.52 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585888550252804		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.8585888550252804 | validation: 0.8883634585209808]
	TIME [epoch: 2.52 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737030753028681		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.8737030753028681 | validation: 0.8543026834921357]
	TIME [epoch: 2.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722009769513572		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.8722009769513572 | validation: 0.8379005262687209]
	TIME [epoch: 2.52 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493648250210117		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.8493648250210117 | validation: 0.8693888366102446]
	TIME [epoch: 2.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415659408083283		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8415659408083283 | validation: 0.7783279465189628]
	TIME [epoch: 2.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329965892707025		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.8329965892707025 | validation: 0.9685452445231237]
	TIME [epoch: 2.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631660834643108		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.8631660834643108 | validation: 0.6603982503733578]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671902994008421		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.9671902994008421 | validation: 1.1444441325596357]
	TIME [epoch: 2.52 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578059183619327		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.9578059183619327 | validation: 0.7652646166385081]
	TIME [epoch: 2.52 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419350957500515		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.8419350957500515 | validation: 0.8098834061108067]
	TIME [epoch: 2.52 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831193215023378		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.831193215023378 | validation: 0.918581862134251]
	TIME [epoch: 3.12 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842124301059994		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.842124301059994 | validation: 0.6922286730796481]
	TIME [epoch: 2.52 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828300850734349		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.8828300850734349 | validation: 1.0616410941108174]
	TIME [epoch: 2.51 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134300038539525		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.9134300038539525 | validation: 0.7343669554867529]
	TIME [epoch: 2.51 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688271445294015		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.8688271445294015 | validation: 0.8853808545104735]
	TIME [epoch: 2.51 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322641030426863		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.8322641030426863 | validation: 0.8337554581773032]
	TIME [epoch: 2.52 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276549823496833		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.8276549823496833 | validation: 0.797869170168696]
	TIME [epoch: 2.52 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328695758926611		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.8328695758926611 | validation: 0.8700309241290656]
	TIME [epoch: 2.52 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378335135551416		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.8378335135551416 | validation: 0.7484427164662013]
	TIME [epoch: 2.52 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681125263823367		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.8681125263823367 | validation: 0.9915407477352268]
	TIME [epoch: 2.52 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839322084617723		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.8839322084617723 | validation: 0.7150058628050546]
	TIME [epoch: 2.52 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602765366760505		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.8602765366760505 | validation: 0.9810840295937772]
	TIME [epoch: 2.52 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649403574913376		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.8649403574913376 | validation: 0.7525966298598442]
	TIME [epoch: 2.51 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8884855332862854		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.8884855332862854 | validation: 0.9409648157186304]
	TIME [epoch: 2.52 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861278891926962		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.861278891926962 | validation: 0.7587046889821432]
	TIME [epoch: 2.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312863618456696		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.8312863618456696 | validation: 0.8884530770542338]
	TIME [epoch: 2.52 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196271533995042		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.8196271533995042 | validation: 0.7766633335757993]
	TIME [epoch: 2.52 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259717522430341		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.8259717522430341 | validation: 0.9288465708019396]
	TIME [epoch: 2.52 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392039140998736		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.8392039140998736 | validation: 0.7103308008086716]
	TIME [epoch: 2.52 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678085380842662		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.8678085380842662 | validation: 0.9507226460927942]
	TIME [epoch: 2.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852566604354858		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.852566604354858 | validation: 0.7212140366573657]
	TIME [epoch: 2.52 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465717862036026		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.8465717862036026 | validation: 0.9312459056846084]
	TIME [epoch: 2.52 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416492542066994		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.8416492542066994 | validation: 0.714052013600575]
	TIME [epoch: 2.52 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476505792580612		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.8476505792580612 | validation: 1.0173506120832452]
	TIME [epoch: 2.52 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698998578199297		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.8698998578199297 | validation: 0.7049905869534778]
	TIME [epoch: 2.52 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8664339516288533		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.8664339516288533 | validation: 0.9317550994533289]
	TIME [epoch: 2.52 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340853469576461		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.8340853469576461 | validation: 0.7586119685853792]
	TIME [epoch: 2.52 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149746886314267		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.8149746886314267 | validation: 0.8533464794235125]
	TIME [epoch: 2.53 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157531001825393		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.8157531001825393 | validation: 0.794017977773608]
	TIME [epoch: 2.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8226386159871965		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.8226386159871965 | validation: 0.8441694640595782]
	TIME [epoch: 2.52 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444910822757277		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.8444910822757277 | validation: 0.7954860605557359]
	TIME [epoch: 2.51 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845727632737254		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.845727632737254 | validation: 0.8866160022295859]
	TIME [epoch: 2.53 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362269292357479		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.8362269292357479 | validation: 0.7313176976064206]
	TIME [epoch: 2.52 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260199627109495		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.8260199627109495 | validation: 0.9588740610422687]
	TIME [epoch: 2.53 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8539703663720383		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.8539703663720383 | validation: 0.652421814669288]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698844717156942		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.8698844717156942 | validation: 1.0124415105630182]
	TIME [epoch: 2.52 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756156838661546		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.8756156838661546 | validation: 0.7294530927088395]
	TIME [epoch: 2.52 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278688098558411		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.8278688098558411 | validation: 0.8844337345112747]
	TIME [epoch: 2.52 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.818346049405582		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.818346049405582 | validation: 0.76117993681182]
	TIME [epoch: 2.51 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014791719646124		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.8014791719646124 | validation: 0.8120229646172641]
	TIME [epoch: 2.52 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941889804216377		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.7941889804216377 | validation: 0.7840267286359182]
	TIME [epoch: 2.51 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999297483310954		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.7999297483310954 | validation: 0.7863066614070835]
	TIME [epoch: 2.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971682919408902		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.7971682919408902 | validation: 0.8310918243325871]
	TIME [epoch: 2.51 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088261200353614		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.8088261200353614 | validation: 0.7849216733302585]
	TIME [epoch: 2.52 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365898005031217		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.8365898005031217 | validation: 0.9533432899815056]
	TIME [epoch: 2.52 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772427868053859		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.8772427868053859 | validation: 0.7227562801356725]
	TIME [epoch: 2.51 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596948748956912		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.8596948748956912 | validation: 0.9357933300486801]
	TIME [epoch: 2.52 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831935384829373		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.831935384829373 | validation: 0.6888070326828619]
	TIME [epoch: 2.52 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074517499503147		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.9074517499503147 | validation: 0.9377367470161986]
	TIME [epoch: 2.52 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415440351581802		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.8415440351581802 | validation: 0.7203159774950575]
	TIME [epoch: 2.52 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804544970138922		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.804544970138922 | validation: 0.8627504266691823]
	TIME [epoch: 2.52 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012894172590075		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.8012894172590075 | validation: 0.7285166892078992]
	TIME [epoch: 2.51 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8053661436396052		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.8053661436396052 | validation: 0.8493718437104203]
	TIME [epoch: 2.52 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025286381603917		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.8025286381603917 | validation: 0.7179596317012775]
	TIME [epoch: 2.51 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091245246748904		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.8091245246748904 | validation: 0.9645337788143161]
	TIME [epoch: 2.52 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837557912873346		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.837557912873346 | validation: 0.6540932521306831]
	TIME [epoch: 2.51 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8539730334876563		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.8539730334876563 | validation: 0.9402920517051419]
	TIME [epoch: 2.52 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209743214353603		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.8209743214353603 | validation: 0.7028854875932368]
	TIME [epoch: 2.51 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814635104356636		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.814635104356636 | validation: 0.887561946809825]
	TIME [epoch: 2.51 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060811000865528		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.8060811000865528 | validation: 0.7073211512880038]
	TIME [epoch: 2.51 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009194150620471		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.8009194150620471 | validation: 0.8836651630850133]
	TIME [epoch: 2.51 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076187147842431		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.8076187147842431 | validation: 0.703019832901308]
	TIME [epoch: 2.52 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8115006156485638		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.8115006156485638 | validation: 0.9312885155765]
	TIME [epoch: 2.52 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168752506949295		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.8168752506949295 | validation: 0.6842270900183136]
	TIME [epoch: 2.52 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8184490894916661		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.8184490894916661 | validation: 0.9244000842034558]
	TIME [epoch: 2.52 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820996013920946		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.820996013920946 | validation: 0.6826521136474291]
	TIME [epoch: 2.51 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8333803407434976		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.8333803407434976 | validation: 0.8998351208701042]
	TIME [epoch: 2.51 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146354911890265		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.8146354911890265 | validation: 0.7265846598765376]
	TIME [epoch: 2.52 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017454719427443		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.8017454719427443 | validation: 0.8289532611390202]
	TIME [epoch: 2.52 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7879036739414533		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.7879036739414533 | validation: 0.785646545020676]
	TIME [epoch: 2.52 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861804328615096		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.7861804328615096 | validation: 0.7918913405944126]
	TIME [epoch: 2.51 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902668425501699		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.7902668425501699 | validation: 0.8133438572261911]
	TIME [epoch: 2.52 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081953431904717		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.8081953431904717 | validation: 0.788509036662645]
	TIME [epoch: 2.52 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212152580252793		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.8212152580252793 | validation: 0.8163575320565637]
	TIME [epoch: 2.51 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999363585081927		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.7999363585081927 | validation: 0.7640675392582211]
	TIME [epoch: 2.51 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996690817764894		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.7996690817764894 | validation: 0.79875609180228]
	TIME [epoch: 2.52 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782224623088699		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.7782224623088699 | validation: 0.7594590830259673]
	TIME [epoch: 2.52 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7685607571463036		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.7685607571463036 | validation: 0.8127286536206141]
	TIME [epoch: 2.52 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811246707765553		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.7811246707765553 | validation: 0.6967039265108026]
	TIME [epoch: 2.51 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844804236687637		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.7844804236687637 | validation: 1.057956057693159]
	TIME [epoch: 2.52 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8975180171035285		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.8975180171035285 | validation: 0.6268823304528164]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905255964688021		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.8905255964688021 | validation: 0.8611256740235931]
	TIME [epoch: 2.52 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873345971639917		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.7873345971639917 | validation: 0.7460274797990059]
	TIME [epoch: 2.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788485293676416		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.7788485293676416 | validation: 0.7660220591719213]
	TIME [epoch: 2.52 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823172118081806		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.7823172118081806 | validation: 0.7986845416072943]
	TIME [epoch: 2.51 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.781670482891792		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.781670482891792 | validation: 0.7536746448823023]
	TIME [epoch: 2.52 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820745568785933		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.7820745568785933 | validation: 0.8375867573237145]
	TIME [epoch: 2.52 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854793751982396		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.7854793751982396 | validation: 0.6887777034853149]
	TIME [epoch: 2.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967173400882877		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.7967173400882877 | validation: 0.8873591852629912]
	TIME [epoch: 2.51 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022562546200357		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.8022562546200357 | validation: 0.6619377955423773]
	TIME [epoch: 2.52 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064584256565188		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.8064584256565188 | validation: 0.9651961550319839]
	TIME [epoch: 2.51 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228908978583417		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.8228908978583417 | validation: 0.6594204108423448]
	TIME [epoch: 2.53 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251358801276186		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.8251358801276186 | validation: 0.8737522873208827]
	TIME [epoch: 2.51 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878985271630663		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.7878985271630663 | validation: 0.7200806417583829]
	TIME [epoch: 2.52 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692356629139033		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.7692356629139033 | validation: 0.783629741257776]
	TIME [epoch: 2.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640820495982706		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.7640820495982706 | validation: 0.795652567329487]
	TIME [epoch: 2.52 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760654509875534		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.760654509875534 | validation: 0.7421828320008201]
	TIME [epoch: 2.51 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666427221186837		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.7666427221186837 | validation: 0.7959383637729633]
	TIME [epoch: 2.52 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634061491488668		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.7634061491488668 | validation: 0.7117353967907684]
	TIME [epoch: 2.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721129002816386		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.7721129002816386 | validation: 0.9384794488988581]
	TIME [epoch: 2.51 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271768618511638		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.8271768618511638 | validation: 0.6233186112881607]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703528590199626		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.8703528590199626 | validation: 0.8654727611830458]
	TIME [epoch: 2.52 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830717466425616		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.7830717466425616 | validation: 0.6950264277868833]
	TIME [epoch: 2.52 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740535698352123		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.7740535698352123 | validation: 0.8765342004464064]
	TIME [epoch: 2.53 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052373660795803		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.8052373660795803 | validation: 0.7244815885571232]
	TIME [epoch: 2.52 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996608141081144		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.7996608141081144 | validation: 0.7903648428155492]
	TIME [epoch: 2.52 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661691266565979		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.7661691266565979 | validation: 0.7904241496618707]
	TIME [epoch: 2.52 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527806104170285		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.7527806104170285 | validation: 0.721782089745846]
	TIME [epoch: 2.52 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593551417655325		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.7593551417655325 | validation: 0.8354858611902106]
	TIME [epoch: 2.52 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766182427448966		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.766182427448966 | validation: 0.6579657298787144]
	TIME [epoch: 2.52 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915172238355098		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.7915172238355098 | validation: 0.9375550365424868]
	TIME [epoch: 2.52 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202164871881205		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.8202164871881205 | validation: 0.6343473933384154]
	TIME [epoch: 2.52 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028608716805732		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.8028608716805732 | validation: 0.8365211401322761]
	TIME [epoch: 2.52 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747920261706157		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.7747920261706157 | validation: 0.6864674375440518]
	TIME [epoch: 2.52 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534441454567177		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.7534441454567177 | validation: 0.8444855549396119]
	TIME [epoch: 2.52 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633382526402751		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.7633382526402751 | validation: 0.6904215790317102]
	TIME [epoch: 2.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765574628510469		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.765574628510469 | validation: 0.8896944539950161]
	TIME [epoch: 2.52 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777335952018056		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.777335952018056 | validation: 0.6541251970090828]
	TIME [epoch: 2.52 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790803411152203		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.7790803411152203 | validation: 0.8652702494926756]
	TIME [epoch: 2.52 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628628073505417		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.7628628073505417 | validation: 0.6977131501138502]
	TIME [epoch: 2.52 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552583029379608		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.7552583029379608 | validation: 0.8473853971690017]
	TIME [epoch: 2.52 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700243631565709		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.7700243631565709 | validation: 0.6324341311925563]
	TIME [epoch: 2.52 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793612239558002		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.793612239558002 | validation: 0.8680356703637822]
	TIME [epoch: 2.52 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793739118630445		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.7793739118630445 | validation: 0.702647889131994]
	TIME [epoch: 2.52 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556200957673264		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.7556200957673264 | validation: 0.7788427767660557]
	TIME [epoch: 2.52 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395608980247005		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.7395608980247005 | validation: 0.7471140665606556]
	TIME [epoch: 2.52 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531176435319248		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.7531176435319248 | validation: 0.7616365799560367]
	TIME [epoch: 2.53 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748330094260544		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.7748330094260544 | validation: 0.7893582596073853]
	TIME [epoch: 2.51 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664273384680906		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.7664273384680906 | validation: 0.7405357733316136]
	TIME [epoch: 2.52 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544178180114156		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.7544178180114156 | validation: 0.767508494002239]
	TIME [epoch: 2.52 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450254234154179		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.7450254234154179 | validation: 0.7226842686867114]
	TIME [epoch: 2.52 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731740377665248		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.731740377665248 | validation: 0.760295649324191]
	TIME [epoch: 2.52 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351294944880812		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.7351294944880812 | validation: 0.7136889018001993]
	TIME [epoch: 2.52 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736310227913956		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.736310227913956 | validation: 0.7940275677427195]
	TIME [epoch: 2.51 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347267514683423		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.7347267514683423 | validation: 0.6367377694907893]
	TIME [epoch: 2.52 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666288367915292		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.7666288367915292 | validation: 1.0364181619153576]
	TIME [epoch: 2.52 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8715798761120636		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.8715798761120636 | validation: 0.6201516621412811]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887413211076849		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.7887413211076849 | validation: 0.798982619099387]
	TIME [epoch: 2.51 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372109913596506		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.7372109913596506 | validation: 0.7092703695099972]
	TIME [epoch: 2.51 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418222478608419		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.7418222478608419 | validation: 0.7644782770622947]
	TIME [epoch: 2.52 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458082367227883		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.7458082367227883 | validation: 0.7469579733556686]
	TIME [epoch: 2.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459787966330529		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.7459787966330529 | validation: 0.7397857015159905]
	TIME [epoch: 2.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450424369356704		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.7450424369356704 | validation: 0.7587434815299233]
	TIME [epoch: 2.52 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414569680431896		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.7414569680431896 | validation: 0.7315701101059444]
	TIME [epoch: 2.52 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368318058035996		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.7368318058035996 | validation: 0.7462328600259798]
	TIME [epoch: 2.52 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7315013454551744		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.7315013454551744 | validation: 0.7295515678993081]
	TIME [epoch: 2.52 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280544092357506		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.7280544092357506 | validation: 0.7345966861803084]
	TIME [epoch: 2.52 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353054966671293		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.7353054966671293 | validation: 0.7749797965129561]
	TIME [epoch: 2.52 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338142297831578		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.7338142297831578 | validation: 0.6478469230266404]
	TIME [epoch: 2.52 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618965817331232		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.7618965817331232 | validation: 0.943819114911603]
	TIME [epoch: 2.52 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993755153342582		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.7993755153342582 | validation: 0.603097793925066]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802994770352253		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.802994770352253 | validation: 0.8407418588004282]
	TIME [epoch: 2.52 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749035566039235		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.749035566039235 | validation: 0.6969221736936516]
	TIME [epoch: 2.52 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235120039214123		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.7235120039214123 | validation: 0.750744651150129]
	TIME [epoch: 2.52 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173871736157201		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.7173871736157201 | validation: 0.7108942207562093]
	TIME [epoch: 2.53 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157636053607875		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.7157636053607875 | validation: 0.7274377649195546]
	TIME [epoch: 2.52 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239788449859512		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.7239788449859512 | validation: 0.7168386491882817]
	TIME [epoch: 2.52 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159154793824297		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.7159154793824297 | validation: 0.7340021962514731]
	TIME [epoch: 2.52 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718075126131946		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.718075126131946 | validation: 0.7279519780097351]
	TIME [epoch: 2.52 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417140248458134		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.7417140248458134 | validation: 0.813548774819579]
	TIME [epoch: 2.52 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757762676472838		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.757762676472838 | validation: 0.6413207895065195]
	TIME [epoch: 2.52 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616593712747688		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.7616593712747688 | validation: 0.8738036589699205]
	TIME [epoch: 2.52 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592568095245997		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.7592568095245997 | validation: 0.5918502014610407]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784649063292172		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.784649063292172 | validation: 0.8610293085309186]
	TIME [epoch: 2.51 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484845349588715		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.7484845349588715 | validation: 0.6705835238947899]
	TIME [epoch: 2.52 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7210913669252423		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.7210913669252423 | validation: 0.7548885228616202]
	TIME [epoch: 2.52 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7147215184979723		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.7147215184979723 | validation: 0.6944731315672884]
	TIME [epoch: 2.52 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7113944836202281		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.7113944836202281 | validation: 0.7758258098572584]
	TIME [epoch: 2.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136702483851528		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.7136702483851528 | validation: 0.6699284166134656]
	TIME [epoch: 2.52 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174674335066296		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.7174674335066296 | validation: 0.8172747139844643]
	TIME [epoch: 2.52 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257402020296951		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.7257402020296951 | validation: 0.6308555220068195]
	TIME [epoch: 2.52 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399514624202704		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.7399514624202704 | validation: 0.8649562151617121]
	TIME [epoch: 2.52 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456059009124197		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.7456059009124197 | validation: 0.6177173783430918]
	TIME [epoch: 2.52 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451601033005606		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.7451601033005606 | validation: 0.829300458065699]
	TIME [epoch: 2.52 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304909622956656		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.7304909622956656 | validation: 0.6590999256680798]
	TIME [epoch: 2.51 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7163369234714688		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.7163369234714688 | validation: 0.7564689598720852]
	TIME [epoch: 2.53 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996890097348532		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.6996890097348532 | validation: 0.6948487666460818]
	TIME [epoch: 2.52 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996401565504508		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.6996401565504508 | validation: 0.7285029139326433]
	TIME [epoch: 2.52 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032669818845417		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.7032669818845417 | validation: 0.6885957962570117]
	TIME [epoch: 2.51 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070914602882461		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.7070914602882461 | validation: 0.7812444260865846]
	TIME [epoch: 2.52 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312567511630244		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.7312567511630244 | validation: 0.6425985782974668]
	TIME [epoch: 2.52 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379452726535206		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.7379452726535206 | validation: 0.7960047677959172]
	TIME [epoch: 2.52 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165446186571426		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.7165446186571426 | validation: 0.629768659922309]
	TIME [epoch: 2.52 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7131839545351509		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.7131839545351509 | validation: 0.8402469649312645]
	TIME [epoch: 2.52 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333535575372385		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.7333535575372385 | validation: 0.5968287685513272]
	TIME [epoch: 2.52 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577756554100873		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.7577756554100873 | validation: 0.8221605503162581]
	TIME [epoch: 2.53 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7242092825132014		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.7242092825132014 | validation: 0.6717350574040493]
	TIME [epoch: 2.52 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700579149984784		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.700579149984784 | validation: 0.7455395771765609]
	TIME [epoch: 2.52 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936865979260602		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.6936865979260602 | validation: 0.6998063465280728]
	TIME [epoch: 2.52 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933472366479992		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.6933472366479992 | validation: 0.7216689090788467]
	TIME [epoch: 2.52 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921088560007016		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.6921088560007016 | validation: 0.7050087398284384]
	TIME [epoch: 2.51 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694570367691954		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.694570367691954 | validation: 0.7520389743220202]
	TIME [epoch: 2.52 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155076384303898		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.7155076384303898 | validation: 0.6503912894861981]
	TIME [epoch: 2.52 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213650811607428		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.7213650811607428 | validation: 0.7955227097652455]
	TIME [epoch: 2.52 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7036214024212811		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.7036214024212811 | validation: 0.6156524635449167]
	TIME [epoch: 2.52 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082604349811712		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.7082604349811712 | validation: 0.8418287885224331]
	TIME [epoch: 2.52 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350811916112258		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.7350811916112258 | validation: 0.5810261264091102]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257878429250139		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.7257878429250139 | validation: 0.8080381452219897]
	TIME [epoch: 2.52 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073998283169333		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.7073998283169333 | validation: 0.6339368970463033]
	TIME [epoch: 2.52 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887008561717329		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.6887008561717329 | validation: 0.7465575505158766]
	TIME [epoch: 2.52 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916917532413718		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.6916917532413718 | validation: 0.6660029145237201]
	TIME [epoch: 2.52 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936307733160214		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.6936307733160214 | validation: 0.7402394552042376]
	TIME [epoch: 2.52 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918035385853293		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.6918035385853293 | validation: 0.6566919449392303]
	TIME [epoch: 2.51 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6950158456561693		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.6950158456561693 | validation: 0.7712179726625181]
	TIME [epoch: 2.52 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935622592270082		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.6935622592270082 | validation: 0.6076025458798942]
	TIME [epoch: 2.52 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949195385150951		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.6949195385150951 | validation: 0.8156772686185267]
	TIME [epoch: 2.52 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712899729484092		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.712899729484092 | validation: 0.5904705471926569]
	TIME [epoch: 2.52 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286461178333806		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.7286461178333806 | validation: 0.7979943532422502]
	TIME [epoch: 2.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037325637286168		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.7037325637286168 | validation: 0.6407907453194337]
	TIME [epoch: 2.52 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810136400294304		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.6810136400294304 | validation: 0.7043595317420799]
	TIME [epoch: 2.53 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687445574404122		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.6687445574404122 | validation: 0.669303334844249]
	TIME [epoch: 2.52 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776799057128992		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.6776799057128992 | validation: 0.6879367993871486]
	TIME [epoch: 2.52 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729235122718841		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.6729235122718841 | validation: 0.6645067074404027]
	TIME [epoch: 2.52 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778610438096615		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.6778610438096615 | validation: 0.7125930074150624]
	TIME [epoch: 2.52 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931886275223288		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.6931886275223288 | validation: 0.6594856413419384]
	TIME [epoch: 2.51 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096444445013654		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.7096444445013654 | validation: 0.7413649496583705]
	TIME [epoch: 2.52 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010486277462976		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.7010486277462976 | validation: 0.6395673773721671]
	TIME [epoch: 2.52 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680968354171126		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.680968354171126 | validation: 0.7724626738903676]
	TIME [epoch: 2.52 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889127503842678		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.6889127503842678 | validation: 0.5766945486657005]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245805611971048		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.7245805611971048 | validation: 0.8658976989932455]
	TIME [epoch: 2.52 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244847016812073		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.7244847016812073 | validation: 0.6042712980638897]
	TIME [epoch: 2.51 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797281682194395		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.6797281682194395 | validation: 0.7120461945135479]
	TIME [epoch: 2.52 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677739743438739		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.6677739743438739 | validation: 0.6920828622381219]
	TIME [epoch: 2.49 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690710948335326		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.6690710948335326 | validation: 0.679128713982822]
	TIME [epoch: 2.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702830243449259		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.6702830243449259 | validation: 0.6785403768462297]
	TIME [epoch: 2.49 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692928527036495		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.6692928527036495 | validation: 0.7245841247473991]
	TIME [epoch: 2.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762871101466685		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.6762871101466685 | validation: 0.6219198239818027]
	TIME [epoch: 2.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813427609548367		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.6813427609548367 | validation: 0.7820181071142729]
	TIME [epoch: 2.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889062215593619		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.6889062215593619 | validation: 0.5763548455478976]
	TIME [epoch: 2.49 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026840864323662		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.7026840864323662 | validation: 0.8017868818523243]
	TIME [epoch: 2.53 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082517902423128		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.7082517902423128 | validation: 0.6119872055269233]
	TIME [epoch: 2.52 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701386921540864		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.6701386921540864 | validation: 0.7160832045118994]
	TIME [epoch: 2.52 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615024883569047		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.6615024883569047 | validation: 0.6158488293793012]
	TIME [epoch: 2.52 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6609301543345751		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.6609301543345751 | validation: 0.7296925131502052]
	TIME [epoch: 2.52 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666580350561678		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.666580350561678 | validation: 0.5973329497072444]
	TIME [epoch: 2.52 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723327507726289		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.6723327507726289 | validation: 0.7749452189777326]
	TIME [epoch: 2.52 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809564244512731		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.6809564244512731 | validation: 0.581426678230135]
	TIME [epoch: 2.52 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830661449943624		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.6830661449943624 | validation: 0.7737210505955114]
	TIME [epoch: 2.51 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676967662677796		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.6676967662677796 | validation: 0.6162666529724931]
	TIME [epoch: 2.52 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639487134785034		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.6639487134785034 | validation: 0.7384144047341419]
	TIME [epoch: 2.51 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635124129327735		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.6635124129327735 | validation: 0.5907270490968334]
	TIME [epoch: 2.51 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743740672896158		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.6743740672896158 | validation: 0.7529245474890355]
	TIME [epoch: 2.52 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598119485477614		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.6598119485477614 | validation: 0.6022339442823242]
	TIME [epoch: 2.52 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604821868493093		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.6604821868493093 | validation: 0.7564271148879215]
	TIME [epoch: 2.52 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606088674869034		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.6606088674869034 | validation: 0.5787301580598809]
	TIME [epoch: 2.51 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761736005029702		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.6761736005029702 | validation: 0.7376641101525601]
	TIME [epoch: 2.51 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746132098312698		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.6746132098312698 | validation: 0.6465560646867905]
	TIME [epoch: 2.52 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602553880677188		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.6602553880677188 | validation: 0.6912614737470493]
	TIME [epoch: 2.52 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518924536356454		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.6518924536356454 | validation: 0.6595873517549372]
	TIME [epoch: 2.51 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563191569988851		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.6563191569988851 | validation: 0.6388035721342583]
	TIME [epoch: 2.51 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524917697940552		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.6524917697940552 | validation: 0.7332120859746132]
	TIME [epoch: 2.51 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583370388762811		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.6583370388762811 | validation: 0.6068196930508835]
	TIME [epoch: 2.52 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66383306979142		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.66383306979142 | validation: 0.745963758128548]
	TIME [epoch: 2.53 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535458094351799		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.6535458094351799 | validation: 0.5934598050321066]
	TIME [epoch: 2.52 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588000614276167		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.6588000614276167 | validation: 0.7605471016601094]
	TIME [epoch: 2.52 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691204905971233		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.6691204905971233 | validation: 0.5863899166503995]
	TIME [epoch: 2.53 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673556482398015		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.673556482398015 | validation: 0.7459871240443144]
	TIME [epoch: 2.52 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652786973150236		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.652786973150236 | validation: 0.5890667767430028]
	TIME [epoch: 2.53 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642945373937179		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.642945373937179 | validation: 0.7189655155491602]
	TIME [epoch: 2.52 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420862815695872		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.6420862815695872 | validation: 0.574203175612299]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499916582677256		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.6499916582677256 | validation: 0.7608034007700246]
	TIME [epoch: 2.51 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590576949534187		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.6590576949534187 | validation: 0.5644246957691198]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582765552650409		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.6582765552650409 | validation: 0.7405108122895175]
	TIME [epoch: 2.52 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552589798695416		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.6552589798695416 | validation: 0.611558104366729]
	TIME [epoch: 2.51 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.637611909943144		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.637611909943144 | validation: 0.6789485121954866]
	TIME [epoch: 2.52 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361381451017013		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.6361381451017013 | validation: 0.6267338304100397]
	TIME [epoch: 2.53 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306133259255965		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.6306133259255965 | validation: 0.6660714487926289]
	TIME [epoch: 2.51 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354391451167652		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.6354391451167652 | validation: 0.6526218469798756]
	TIME [epoch: 2.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389705266198119		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.6389705266198119 | validation: 0.6551771041949009]
	TIME [epoch: 2.52 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450809162396374		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.6450809162396374 | validation: 0.6625491883577347]
	TIME [epoch: 2.52 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467836399037262		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.6467836399037262 | validation: 0.645593956307354]
	TIME [epoch: 2.52 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335757536370391		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.6335757536370391 | validation: 0.6562398340319024]
	TIME [epoch: 2.52 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627292116419822		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.627292116419822 | validation: 0.602194047996343]
	TIME [epoch: 2.52 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263749017006857		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.6263749017006857 | validation: 0.7212911650245556]
	TIME [epoch: 2.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390581505813776		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.6390581505813776 | validation: 0.5218815177536446]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_870.pth
	Model improved!!!
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6928834397651272		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.6928834397651272 | validation: 0.7984822365803107]
	TIME [epoch: 2.51 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733620532625699		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.6733620532625699 | validation: 0.5852298789259083]
	TIME [epoch: 2.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344708787435944		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.6344708787435944 | validation: 0.6890138215855621]
	TIME [epoch: 2.51 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226771669918822		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.6226771669918822 | validation: 0.5973607649751814]
	TIME [epoch: 2.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623671701078191		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.623671701078191 | validation: 0.6720341488116637]
	TIME [epoch: 2.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206545434225934		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.6206545434225934 | validation: 0.587293274043198]
	TIME [epoch: 2.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621584270343824		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.621584270343824 | validation: 0.7137984226583973]
	TIME [epoch: 2.51 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374968611541847		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.6374968611541847 | validation: 0.5624382295563286]
	TIME [epoch: 2.51 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441306909688072		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.6441306909688072 | validation: 0.7245497630582487]
	TIME [epoch: 2.51 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379903907436384		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.6379903907436384 | validation: 0.5718055607602054]
	TIME [epoch: 2.51 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270070531324636		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.6270070531324636 | validation: 0.720999773124263]
	TIME [epoch: 2.51 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335618925177403		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.6335618925177403 | validation: 0.5802541346944285]
	TIME [epoch: 2.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218633986776999		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.6218633986776999 | validation: 0.6986722494907629]
	TIME [epoch: 2.51 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.623221648299657		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.623221648299657 | validation: 0.5871674338185613]
	TIME [epoch: 2.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138569843424149		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.6138569843424149 | validation: 0.6955959594582899]
	TIME [epoch: 2.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6150163846967853		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.6150163846967853 | validation: 0.5532056886252361]
	TIME [epoch: 2.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.631329181560224		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.631329181560224 | validation: 0.7566080942471576]
	TIME [epoch: 2.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540281846984035		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.6540281846984035 | validation: 0.558519379887309]
	TIME [epoch: 2.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240220297249013		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.6240220297249013 | validation: 0.6849264826275558]
	TIME [epoch: 2.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6080468440538153		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.6080468440538153 | validation: 0.6009896775942029]
	TIME [epoch: 2.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6103159931009029		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.6103159931009029 | validation: 0.6622443272398948]
	TIME [epoch: 2.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6072349238127805		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.6072349238127805 | validation: 0.5932594398723086]
	TIME [epoch: 2.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6061908279505671		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.6061908279505671 | validation: 0.6772564588289959]
	TIME [epoch: 2.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6114487864201873		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.6114487864201873 | validation: 0.575964705031104]
	TIME [epoch: 2.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285023680949298		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.6285023680949298 | validation: 0.7235913631806992]
	TIME [epoch: 2.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335753330382466		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.6335753330382466 | validation: 0.5624353780137016]
	TIME [epoch: 2.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208708377544434		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.6208708377544434 | validation: 0.7040511662490457]
	TIME [epoch: 2.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6164301488367567		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.6164301488367567 | validation: 0.5561698058800063]
	TIME [epoch: 2.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335385615869469		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.6335385615869469 | validation: 0.7266231202840141]
	TIME [epoch: 2.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315001934059983		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.6315001934059983 | validation: 0.5885200169136641]
	TIME [epoch: 2.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6036973906244045		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.6036973906244045 | validation: 0.6477282049735971]
	TIME [epoch: 2.52 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994335752750845		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.5994335752750845 | validation: 0.5745577878131374]
	TIME [epoch: 2.52 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6032788963891609		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.6032788963891609 | validation: 0.6675413069312978]
	TIME [epoch: 2.52 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6111556030832764		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.6111556030832764 | validation: 0.565541564724939]
	TIME [epoch: 2.52 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112938781592018		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.6112938781592018 | validation: 0.6821535095235617]
	TIME [epoch: 2.52 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6013665461741569		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.6013665461741569 | validation: 0.5618200693185987]
	TIME [epoch: 2.52 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6011447470024597		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.6011447470024597 | validation: 0.6990921564619401]
	TIME [epoch: 2.51 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6135989374275785		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.6135989374275785 | validation: 0.5263811311614801]
	TIME [epoch: 2.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186136748083192		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.6186136748083192 | validation: 0.7237123551468958]
	TIME [epoch: 2.52 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616731116061832		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.616731116061832 | validation: 0.546907875137993]
	TIME [epoch: 2.51 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5997953452693499		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.5997953452693499 | validation: 0.6694608609033753]
	TIME [epoch: 2.52 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5929596700404629		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.5929596700404629 | validation: 0.5920678633228232]
	TIME [epoch: 2.52 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5961801102848296		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.5961801102848296 | validation: 0.641860280899286]
	TIME [epoch: 2.52 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5872681436746725		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.5872681436746725 | validation: 0.5730083159672775]
	TIME [epoch: 2.51 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6003150945989293		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.6003150945989293 | validation: 0.6637366757400689]
	TIME [epoch: 2.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5943079995280615		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.5943079995280615 | validation: 0.5727296145011591]
	TIME [epoch: 2.51 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001604341136096		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.6001604341136096 | validation: 0.6502964191714551]
	TIME [epoch: 2.52 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925131154640938		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.5925131154640938 | validation: 0.5857227965064363]
	TIME [epoch: 2.52 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5866818733923551		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.5866818733923551 | validation: 0.6748061899788006]
	TIME [epoch: 2.51 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5917412968443373		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.5917412968443373 | validation: 0.5340270828197314]
	TIME [epoch: 2.51 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042177594074357		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.6042177594074357 | validation: 0.7343159186856273]
	TIME [epoch: 2.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214266014297475		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.6214266014297475 | validation: 0.53500318146322]
	TIME [epoch: 2.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6020998022405742		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.6020998022405742 | validation: 0.6650936793352774]
	TIME [epoch: 2.51 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585351679328309		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.585351679328309 | validation: 0.5895847711418076]
	TIME [epoch: 2.51 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5768056892993925		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.5768056892993925 | validation: 0.6328780440206813]
	TIME [epoch: 2.52 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789921821538715		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.5789921821538715 | validation: 0.5835334623816141]
	TIME [epoch: 2.51 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5912618266579875		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.5912618266579875 | validation: 0.6491679152641112]
	TIME [epoch: 2.52 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863861218099727		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.5863861218099727 | validation: 0.5679661557316112]
	TIME [epoch: 2.52 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5803536403092048		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.5803536403092048 | validation: 0.6662487246160813]
	TIME [epoch: 2.52 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5835355461248233		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.5835355461248233 | validation: 0.537805396449471]
	TIME [epoch: 2.52 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5858376216528016		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.5858376216528016 | validation: 0.6923003786579498]
	TIME [epoch: 2.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58967504144913		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.58967504144913 | validation: 0.51902124965542]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6057959419913526		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.6057959419913526 | validation: 0.6976946914057437]
	TIME [epoch: 2.52 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895042159131053		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.5895042159131053 | validation: 0.5502358868371181]
	TIME [epoch: 2.51 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5761940607958217		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.5761940607958217 | validation: 0.6051103098894131]
	TIME [epoch: 2.51 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719680528117594		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.5719680528117594 | validation: 0.6106987795337511]
	TIME [epoch: 2.52 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5753689628961906		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.5753689628961906 | validation: 0.5990443358399891]
	TIME [epoch: 2.52 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5847116736841814		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.5847116736841814 | validation: 0.6367468300901057]
	TIME [epoch: 2.52 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5764897588997737		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.5764897588997737 | validation: 0.5742767735571367]
	TIME [epoch: 2.52 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668565059742663		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.5668565059742663 | validation: 0.5976242947206505]
	TIME [epoch: 2.52 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564062083976277		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.564062083976277 | validation: 0.640249744429427]
	TIME [epoch: 2.52 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755329922677159		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.5755329922677159 | validation: 0.5073689519070589]
	TIME [epoch: 2.51 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965127346423768		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.5965127346423768 | validation: 0.7376146675891223]
	TIME [epoch: 2.52 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6118450712735956		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.6118450712735956 | validation: 0.5264067900221634]
	TIME [epoch: 2.52 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5773724134374695		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.5773724134374695 | validation: 0.6328410663327069]
	TIME [epoch: 2.52 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564558397235451		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.564558397235451 | validation: 0.5584014106597786]
	TIME [epoch: 2.52 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568266112096773		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.568266112096773 | validation: 0.6198153089455154]
	TIME [epoch: 2.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635497521513103		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.5635497521513103 | validation: 0.5685366417615438]
	TIME [epoch: 2.51 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5709749690429531		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.5709749690429531 | validation: 0.6221203188058935]
	TIME [epoch: 2.52 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5756959386535295		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.5756959386535295 | validation: 0.5628976095050379]
	TIME [epoch: 2.52 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670508364936392		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.5670508364936392 | validation: 0.6416674334172567]
	TIME [epoch: 2.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.566962887253754		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.566962887253754 | validation: 0.5324128059113316]
	TIME [epoch: 2.52 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5639708635413397		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.5639708635413397 | validation: 0.6778483914583645]
	TIME [epoch: 2.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802285907825814		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.5802285907825814 | validation: 0.5007291430045538]
	TIME [epoch: 2.52 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974604163621732		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.5974604163621732 | validation: 0.6834854162496131]
	TIME [epoch: 2.52 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.573010641236671		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.573010641236671 | validation: 0.5435379263460128]
	TIME [epoch: 2.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5502590103626614		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.5502590103626614 | validation: 0.5953279962175938]
	TIME [epoch: 2.52 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478423651101467		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.5478423651101467 | validation: 0.5772188415865953]
	TIME [epoch: 2.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.551538588052472		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.551538588052472 | validation: 0.5893606060174225]
	TIME [epoch: 2.52 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5532722624461347		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.5532722624461347 | validation: 0.5715683669905837]
	TIME [epoch: 2.53 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574971332935414		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.5574971332935414 | validation: 0.6185918202150309]
	TIME [epoch: 2.52 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5577906651906847		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.5577906651906847 | validation: 0.5174264488750316]
	TIME [epoch: 2.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659736322172811		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.5659736322172811 | validation: 0.6705711216895516]
	TIME [epoch: 2.52 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5652069363773197		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.5652069363773197 | validation: 0.509948074099204]
	TIME [epoch: 2.53 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668847860686563		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.5668847860686563 | validation: 0.6949462997279207]
	TIME [epoch: 2.52 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.572553008941338		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.572553008941338 | validation: 0.5188065074304589]
	TIME [epoch: 2.53 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512488820062412		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.5512488820062412 | validation: 0.6127758653537345]
	TIME [epoch: 2.52 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529067580733361		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.5529067580733361 | validation: 0.5637676643265975]
	TIME [epoch: 2.53 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546119160468394		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.546119160468394 | validation: 0.6020322304252136]
	TIME [epoch: 2.52 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458130430935946		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.5458130430935946 | validation: 0.5421685573321042]
	TIME [epoch: 2.53 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5517080616982422		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.5517080616982422 | validation: 0.617020433363101]
	TIME [epoch: 2.52 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5502567122711262		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.5502567122711262 | validation: 0.5187397532183534]
	TIME [epoch: 2.53 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542766684242211		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.5542766684242211 | validation: 0.6439788668847013]
	TIME [epoch: 2.52 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530072921121747		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.5530072921121747 | validation: 0.5089513950176673]
	TIME [epoch: 2.52 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5588816750734518		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.5588816750734518 | validation: 0.6618209671236546]
	TIME [epoch: 2.52 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5556163982046963		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.5556163982046963 | validation: 0.5123520138888433]
	TIME [epoch: 2.52 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508306898865913		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.5508306898865913 | validation: 0.6186953961674946]
	TIME [epoch: 2.52 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546081092545826		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.546081092545826 | validation: 0.519033984078539]
	TIME [epoch: 2.53 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5483754604731405		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.5483754604731405 | validation: 0.6322394741140275]
	TIME [epoch: 2.52 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454726923388938		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.5454726923388938 | validation: 0.5415523274104481]
	TIME [epoch: 2.53 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544018014735694		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.544018014735694 | validation: 0.6103956409054181]
	TIME [epoch: 2.52 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410581738816016		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.5410581738816016 | validation: 0.5409483143248471]
	TIME [epoch: 2.53 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5393119117680714		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.5393119117680714 | validation: 0.6175551667705351]
	TIME [epoch: 2.53 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5380834616283676		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.5380834616283676 | validation: 0.5119157676644618]
	TIME [epoch: 2.52 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5352507996502801		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.5352507996502801 | validation: 0.6741739830161452]
	TIME [epoch: 2.52 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5567653560543062		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.5567653560543062 | validation: 0.5014995596461126]
	TIME [epoch: 2.53 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5493278230439116		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.5493278230439116 | validation: 0.634912511177216]
	TIME [epoch: 2.52 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367643028765082		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.5367643028765082 | validation: 0.5316962891235977]
	TIME [epoch: 2.52 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.531890321511028		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.531890321511028 | validation: 0.5691223907055639]
	TIME [epoch: 2.53 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5285247783561217		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.5285247783561217 | validation: 0.5707992714117797]
	TIME [epoch: 2.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5256400372992741		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.5256400372992741 | validation: 0.5691469595117756]
	TIME [epoch: 2.52 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5233158076731356		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.5233158076731356 | validation: 0.557477719720419]
	TIME [epoch: 2.53 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537457829472095		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.537457829472095 | validation: 0.5708764839305659]
	TIME [epoch: 2.52 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536612464594075		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.536612464594075 | validation: 0.5497213058108734]
	TIME [epoch: 2.53 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326907986581177		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.5326907986581177 | validation: 0.6084420410848789]
	TIME [epoch: 2.52 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280227095747441		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.5280227095747441 | validation: 0.5032692992640863]
	TIME [epoch: 2.53 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5354808736136134		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.5354808736136134 | validation: 0.6834936026001994]
	TIME [epoch: 2.52 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5576437609250465		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.5576437609250465 | validation: 0.4785797468466861]
	TIME [epoch: 2.53 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_998.pth
	Model improved!!!
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372600422635209		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.5372600422635209 | validation: 0.6068933567933859]
	TIME [epoch: 2.52 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270214731402222		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.5270214731402222 | validation: 0.5327961298783084]
	TIME [epoch: 2.53 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122357140580238		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.5122357140580238 | validation: 0.5802883087381296]
	TIME [epoch: 195 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5161307664989885		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.5161307664989885 | validation: 0.5464875508962753]
	TIME [epoch: 5.44 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5190479549201806		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.5190479549201806 | validation: 0.5662527468047948]
	TIME [epoch: 5.43 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139992588257636		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.5139992588257636 | validation: 0.5410386593981613]
	TIME [epoch: 5.43 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5131024032621775		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.5131024032621775 | validation: 0.6001608795747003]
	TIME [epoch: 5.43 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5269872248579088		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.5269872248579088 | validation: 0.4915400980648757]
	TIME [epoch: 5.42 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5355611197622768		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.5355611197622768 | validation: 0.6536959387108372]
	TIME [epoch: 5.43 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5450639537223951		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.5450639537223951 | validation: 0.5065031750736536]
	TIME [epoch: 5.42 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.527954699466651		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.527954699466651 | validation: 0.608276659602859]
	TIME [epoch: 5.42 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5155882586640923		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.5155882586640923 | validation: 0.5079838014155028]
	TIME [epoch: 5.43 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159529138933501		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.5159529138933501 | validation: 0.5843201016546499]
	TIME [epoch: 5.43 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179364825713039		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.5179364825713039 | validation: 0.5024347764705602]
	TIME [epoch: 5.42 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510683436079632		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.510683436079632 | validation: 0.6074046971303787]
	TIME [epoch: 5.43 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5107038294976853		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.5107038294976853 | validation: 0.4900952270068114]
	TIME [epoch: 5.43 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250775324332385		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.5250775324332385 | validation: 0.6383115984686681]
	TIME [epoch: 5.43 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5294046806292378		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.5294046806292378 | validation: 0.49584344755417975]
	TIME [epoch: 5.42 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521801758552601		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.521801758552601 | validation: 0.591412466680265]
	TIME [epoch: 5.42 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5093372847122455		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.5093372847122455 | validation: 0.5093627469930019]
	TIME [epoch: 5.43 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082629087811836		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.5082629087811836 | validation: 0.5988677695040041]
	TIME [epoch: 5.42 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5171394924607601		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.5171394924607601 | validation: 0.5027044368947688]
	TIME [epoch: 5.42 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165416818191841		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.5165416818191841 | validation: 0.5931259737698075]
	TIME [epoch: 5.43 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5057785147836724		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.5057785147836724 | validation: 0.5013225417696398]
	TIME [epoch: 5.42 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5017315159088714		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.5017315159088714 | validation: 0.587172447916389]
	TIME [epoch: 5.43 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5061125527252635		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.5061125527252635 | validation: 0.49977648187515256]
	TIME [epoch: 5.41 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110598887124475		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.5110598887124475 | validation: 0.6162565189802586]
	TIME [epoch: 5.42 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5171421485300233		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.5171421485300233 | validation: 0.48050503665825045]
	TIME [epoch: 5.42 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122935300801805		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.5122935300801805 | validation: 0.5981490616896439]
	TIME [epoch: 5.43 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5085836381147015		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.5085836381147015 | validation: 0.5057214100725563]
	TIME [epoch: 5.42 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111024890253734		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.5111024890253734 | validation: 0.5744645002184923]
	TIME [epoch: 5.42 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5075697116137673		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.5075697116137673 | validation: 0.4980480673884561]
	TIME [epoch: 5.42 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49883937568801456		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.49883937568801456 | validation: 0.5630120130503145]
	TIME [epoch: 5.44 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026001833115535		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.5026001833115535 | validation: 0.5007452911306408]
	TIME [epoch: 5.44 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986462698969984		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.4986462698969984 | validation: 0.5754841315044736]
	TIME [epoch: 5.44 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5006378274355007		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.5006378274355007 | validation: 0.48460104317916797]
	TIME [epoch: 5.44 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060151882655658		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.5060151882655658 | validation: 0.6111746583625646]
	TIME [epoch: 5.43 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5092297570527633		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.5092297570527633 | validation: 0.4712383607039393]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1036.pth
	Model improved!!!
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089783096427325		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.5089783096427325 | validation: 0.6108484226156231]
	TIME [epoch: 5.43 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5079855971875132		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.5079855971875132 | validation: 0.4918620883550506]
	TIME [epoch: 5.41 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020309604624553		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.5020309604624553 | validation: 0.5733364889744299]
	TIME [epoch: 5.42 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48807764861995223		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.48807764861995223 | validation: 0.5144855602375127]
	TIME [epoch: 5.41 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4878392935149532		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.4878392935149532 | validation: 0.5696214319795313]
	TIME [epoch: 5.43 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48880178743291763		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.48880178743291763 | validation: 0.5005688274304265]
	TIME [epoch: 5.42 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49402421306074423		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.49402421306074423 | validation: 0.5843998722038053]
	TIME [epoch: 5.43 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4968689288350819		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.4968689288350819 | validation: 0.498682233934061]
	TIME [epoch: 5.41 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501794177810841		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.501794177810841 | validation: 0.5979950181141825]
	TIME [epoch: 5.42 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4925350089824849		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.4925350089824849 | validation: 0.48659629580506797]
	TIME [epoch: 5.42 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.488026402820725		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.488026402820725 | validation: 0.5935285457250629]
	TIME [epoch: 5.42 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48644712779040383		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.48644712779040383 | validation: 0.467812960005344]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960468049087395		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.4960468049087395 | validation: 0.5819769139615535]
	TIME [epoch: 5.43 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4885955090626601		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.4885955090626601 | validation: 0.47760975429212604]
	TIME [epoch: 5.43 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4874098950181694		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.4874098950181694 | validation: 0.5727808740398616]
	TIME [epoch: 5.42 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48594950147385246		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.48594950147385246 | validation: 0.48395422345690753]
	TIME [epoch: 5.41 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49099189867392923		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.49099189867392923 | validation: 0.5682112166964769]
	TIME [epoch: 5.43 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4831067214794878		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.4831067214794878 | validation: 0.46813078412014636]
	TIME [epoch: 5.43 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4851645558987909		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.4851645558987909 | validation: 0.581606101999023]
	TIME [epoch: 5.42 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48700516753605066		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.48700516753605066 | validation: 0.48058095684529106]
	TIME [epoch: 5.43 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487499996481507		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.487499996481507 | validation: 0.571607036434294]
	TIME [epoch: 5.43 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48441948607623786		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.48441948607623786 | validation: 0.5012526295877738]
	TIME [epoch: 5.43 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741600075624865		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.4741600075624865 | validation: 0.546974506965881]
	TIME [epoch: 5.42 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47263687397210324		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.47263687397210324 | validation: 0.5121094340591018]
	TIME [epoch: 5.42 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48014034068505		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.48014034068505 | validation: 0.5188648875191493]
	TIME [epoch: 5.41 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47805678095792575		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.47805678095792575 | validation: 0.5376778750513619]
	TIME [epoch: 5.42 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4823851602734303		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.4823851602734303 | validation: 0.522195262506048]
	TIME [epoch: 5.42 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741521063668869		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.4741521063668869 | validation: 0.5190775251367733]
	TIME [epoch: 5.42 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.472510604864483		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.472510604864483 | validation: 0.5219919164783168]
	TIME [epoch: 5.42 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46725595816071847		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.46725595816071847 | validation: 0.5245525943194302]
	TIME [epoch: 5.43 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4655413030954189		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.4655413030954189 | validation: 0.5001795868574186]
	TIME [epoch: 5.42 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4707240028367761		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.4707240028367761 | validation: 0.5846238341009337]
	TIME [epoch: 5.43 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47906932553951864		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.47906932553951864 | validation: 0.4201693596281557]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1069.pth
	Model improved!!!
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5197775652895174		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.5197775652895174 | validation: 0.5977185347889357]
	TIME [epoch: 5.43 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48983672054533145		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.48983672054533145 | validation: 0.4871147733472224]
	TIME [epoch: 5.43 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4657082139881051		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.4657082139881051 | validation: 0.5088619941883697]
	TIME [epoch: 5.42 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4629697912287435		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.4629697912287435 | validation: 0.5222552345362625]
	TIME [epoch: 5.43 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46705135816427906		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.46705135816427906 | validation: 0.5150036137787971]
	TIME [epoch: 5.43 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592219641942183		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.4592219641942183 | validation: 0.5335195251859015]
	TIME [epoch: 5.43 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46025994045484153		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.46025994045484153 | validation: 0.4749289062627955]
	TIME [epoch: 5.42 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47351466395467584		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.47351466395467584 | validation: 0.5881809239627588]
	TIME [epoch: 5.42 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47726021912655164		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.47726021912655164 | validation: 0.438509363935391]
	TIME [epoch: 5.44 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856198090675148		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.4856198090675148 | validation: 0.5733816929198062]
	TIME [epoch: 5.43 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47363431962716707		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.47363431962716707 | validation: 0.47422587178842196]
	TIME [epoch: 5.43 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4671442050297091		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.4671442050297091 | validation: 0.5599309247645824]
	TIME [epoch: 5.43 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673347903330841		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.4673347903330841 | validation: 0.46290400204177495]
	TIME [epoch: 5.43 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46727782916266464		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.46727782916266464 | validation: 0.547180576746297]
	TIME [epoch: 5.42 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4631383995764164		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.4631383995764164 | validation: 0.4832906959994554]
	TIME [epoch: 5.43 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4623387482818888		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.4623387482818888 | validation: 0.5260918575646356]
	TIME [epoch: 5.42 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46431142806418846		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.46431142806418846 | validation: 0.5000858908747793]
	TIME [epoch: 5.43 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45993790550124913		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.45993790550124913 | validation: 0.5139652600711785]
	TIME [epoch: 5.42 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45184134590072206		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.45184134590072206 | validation: 0.49897467421317415]
	TIME [epoch: 5.42 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4527229647294812		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.4527229647294812 | validation: 0.5047496606462334]
	TIME [epoch: 5.43 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44975835072265397		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.44975835072265397 | validation: 0.5121370959056369]
	TIME [epoch: 5.42 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44863975765275216		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.44863975765275216 | validation: 0.5135973517758664]
	TIME [epoch: 5.43 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45036872324943217		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.45036872324943217 | validation: 0.5091145912205738]
	TIME [epoch: 5.43 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46098763438367407		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.46098763438367407 | validation: 0.5247231105949187]
	TIME [epoch: 5.42 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4612157172642271		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.4612157172642271 | validation: 0.5111866737442933]
	TIME [epoch: 5.42 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4540698333607203		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.4540698333607203 | validation: 0.5157132967667811]
	TIME [epoch: 5.42 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44937945722897965		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.44937945722897965 | validation: 0.5194884782617082]
	TIME [epoch: 5.42 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4447414774541209		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.4447414774541209 | validation: 0.45626271596574564]
	TIME [epoch: 5.42 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.452650525106283		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.452650525106283 | validation: 0.6072156781770977]
	TIME [epoch: 5.42 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48758761329634726		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.48758761329634726 | validation: 0.42420331113208065]
	TIME [epoch: 5.42 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4798281756200591		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.4798281756200591 | validation: 0.5331581441411748]
	TIME [epoch: 5.42 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4509241650620842		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.4509241650620842 | validation: 0.5014877964828495]
	TIME [epoch: 5.42 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44561865221543073		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.44561865221543073 | validation: 0.49079905470852947]
	TIME [epoch: 5.42 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43976132978538346		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.43976132978538346 | validation: 0.5179407088940297]
	TIME [epoch: 5.44 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44594867334776733		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.44594867334776733 | validation: 0.4490991789048897]
	TIME [epoch: 5.42 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45382326547619806		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.45382326547619806 | validation: 0.5749861525219607]
	TIME [epoch: 5.43 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46105811513733186		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.46105811513733186 | validation: 0.4464680385698816]
	TIME [epoch: 5.41 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45418830129335347		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.45418830129335347 | validation: 0.5416580084400278]
	TIME [epoch: 5.42 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44744675705232845		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.44744675705232845 | validation: 0.47327113717955865]
	TIME [epoch: 5.42 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393609709950764		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.4393609709950764 | validation: 0.5448567228228945]
	TIME [epoch: 5.42 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44589428977153533		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.44589428977153533 | validation: 0.44844326491796443]
	TIME [epoch: 5.43 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4443163339940325		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.4443163339940325 | validation: 0.5361392615541954]
	TIME [epoch: 5.43 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44348947333543565		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.44348947333543565 | validation: 0.46915475409053853]
	TIME [epoch: 5.42 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4416660832022251		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.4416660832022251 | validation: 0.5201368232386226]
	TIME [epoch: 5.42 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4424759237255597		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.4424759237255597 | validation: 0.47329598131927714]
	TIME [epoch: 5.42 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4410239706021777		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.4410239706021777 | validation: 0.5416842149708636]
	TIME [epoch: 5.43 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44772559963956715		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.44772559963956715 | validation: 0.4605830971365512]
	TIME [epoch: 5.43 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43958161679926117		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.43958161679926117 | validation: 0.5452232811235804]
	TIME [epoch: 5.43 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44627608805247276		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.44627608805247276 | validation: 0.43933500795994207]
	TIME [epoch: 5.44 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4507047307295698		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.4507047307295698 | validation: 0.5459131582279694]
	TIME [epoch: 5.43 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4476880687085003		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.4476880687085003 | validation: 0.45538852785345424]
	TIME [epoch: 5.43 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45116429168250677		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.45116429168250677 | validation: 0.5427731219397681]
	TIME [epoch: 5.42 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399997924823056		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.4399997924823056 | validation: 0.4788526921597714]
	TIME [epoch: 5.44 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4279418802602609		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.4279418802602609 | validation: 0.5043692059766909]
	TIME [epoch: 5.42 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4399644679867488		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.4399644679867488 | validation: 0.47847690189107417]
	TIME [epoch: 5.43 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4316569268791183		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.4316569268791183 | validation: 0.5086098359648645]
	TIME [epoch: 5.44 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4350928166935973		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.4350928166935973 | validation: 0.46519134190424904]
	TIME [epoch: 5.44 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42974972565830355		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.42974972565830355 | validation: 0.5364732119929605]
	TIME [epoch: 5.42 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4328798801323468		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.4328798801323468 | validation: 0.4341825099386929]
	TIME [epoch: 5.43 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4413724783167452		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.4413724783167452 | validation: 0.5499041888169692]
	TIME [epoch: 5.43 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43926049506618287		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.43926049506618287 | validation: 0.4353980523279301]
	TIME [epoch: 5.43 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44189182380035824		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.44189182380035824 | validation: 0.5381635146553052]
	TIME [epoch: 5.43 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42930509320801635		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.42930509320801635 | validation: 0.46247797118480866]
	TIME [epoch: 5.43 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4280164361282715		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.4280164361282715 | validation: 0.4973062984398269]
	TIME [epoch: 5.42 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4259524889468166		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.4259524889468166 | validation: 0.4885532050364228]
	TIME [epoch: 5.43 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42377953319341727		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.42377953319341727 | validation: 0.49625412687967146]
	TIME [epoch: 5.43 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42387906838872264		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.42387906838872264 | validation: 0.49118411483504887]
	TIME [epoch: 5.43 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42117169211393296		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.42117169211393296 | validation: 0.4887977094923036]
	TIME [epoch: 5.42 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42112193336541465		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.42112193336541465 | validation: 0.46454953835587276]
	TIME [epoch: 5.43 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4219383724754692		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.4219383724754692 | validation: 0.5287615604428132]
	TIME [epoch: 5.42 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44135827164912306		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.44135827164912306 | validation: 0.41956936664307015]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1140.pth
	Model improved!!!
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.452364035824445		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.452364035824445 | validation: 0.551721892196637]
	TIME [epoch: 5.42 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362131323866798		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.4362131323866798 | validation: 0.44745338165787507]
	TIME [epoch: 5.43 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42837953307903787		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.42837953307903787 | validation: 0.5050268139489582]
	TIME [epoch: 5.41 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4178194817518288		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.4178194817518288 | validation: 0.4804336325834035]
	TIME [epoch: 5.42 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41846525149523567		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.41846525149523567 | validation: 0.4756377091863563]
	TIME [epoch: 5.41 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4193929054011143		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.4193929054011143 | validation: 0.4973859475992213]
	TIME [epoch: 5.43 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42001603797984277		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.42001603797984277 | validation: 0.4579618244436818]
	TIME [epoch: 5.41 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42265423769495175		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.42265423769495175 | validation: 0.5359517555233018]
	TIME [epoch: 5.42 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42926740092678456		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.42926740092678456 | validation: 0.4314433505494867]
	TIME [epoch: 5.42 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4291050643657917		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.4291050643657917 | validation: 0.5214144249098316]
	TIME [epoch: 5.42 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4231974111728042		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.4231974111728042 | validation: 0.4505919609124721]
	TIME [epoch: 5.42 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41789272151212487		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.41789272151212487 | validation: 0.5029942093375203]
	TIME [epoch: 5.44 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4222657781482272		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.4222657781482272 | validation: 0.4463716060224281]
	TIME [epoch: 5.42 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211005196184244		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.4211005196184244 | validation: 0.509691278259628]
	TIME [epoch: 5.44 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4230703840524141		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.4230703840524141 | validation: 0.46007908454439267]
	TIME [epoch: 5.41 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4177068186760225		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.4177068186760225 | validation: 0.5146759671705479]
	TIME [epoch: 5.45 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4160151882162479		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.4160151882162479 | validation: 0.4363018775517322]
	TIME [epoch: 5.42 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41801785554005305		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.41801785554005305 | validation: 0.531252175567493]
	TIME [epoch: 5.43 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4275431304849638		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.4275431304849638 | validation: 0.4342681831086022]
	TIME [epoch: 5.42 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41432525204416937		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.41432525204416937 | validation: 0.515482985148199]
	TIME [epoch: 5.42 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093531179883029		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.4093531179883029 | validation: 0.4396756598617992]
	TIME [epoch: 5.42 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117922906692336		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.4117922906692336 | validation: 0.5140315419583953]
	TIME [epoch: 5.42 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4094593523333279		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.4094593523333279 | validation: 0.4388114503121132]
	TIME [epoch: 5.41 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4141797708516552		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.4141797708516552 | validation: 0.5071204915990695]
	TIME [epoch: 5.42 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4116631195684923		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.4116631195684923 | validation: 0.4430191642730133]
	TIME [epoch: 5.42 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117120823807834		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.4117120823807834 | validation: 0.5082465836353788]
	TIME [epoch: 5.42 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097942031573827		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.4097942031573827 | validation: 0.43543660986914257]
	TIME [epoch: 5.42 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4148549286567573		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.4148549286567573 | validation: 0.519192475600719]
	TIME [epoch: 5.43 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4137300111476124		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.4137300111476124 | validation: 0.4393597968629795]
	TIME [epoch: 5.42 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4065263620350774		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.4065263620350774 | validation: 0.4899638363542453]
	TIME [epoch: 5.43 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40541632911033815		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.40541632911033815 | validation: 0.4550584089727521]
	TIME [epoch: 5.44 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40306619206382077		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.40306619206382077 | validation: 0.5011928096000247]
	TIME [epoch: 5.42 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41181451455992457		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.41181451455992457 | validation: 0.44620483370775166]
	TIME [epoch: 5.42 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.410282298895094		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.410282298895094 | validation: 0.5170941761841548]
	TIME [epoch: 5.42 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4100985468880127		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.4100985468880127 | validation: 0.4296414035693543]
	TIME [epoch: 5.42 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41207611192131566		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.41207611192131566 | validation: 0.5359960830385329]
	TIME [epoch: 5.42 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42164262616148124		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.42164262616148124 | validation: 0.43298924244106535]
	TIME [epoch: 5.41 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4054752159862466		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.4054752159862466 | validation: 0.4811757611204488]
	TIME [epoch: 5.43 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3972709916458758		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.3972709916458758 | validation: 0.47554181583323507]
	TIME [epoch: 5.42 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3978800040236795		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.3978800040236795 | validation: 0.4577312459716152]
	TIME [epoch: 5.43 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39894457947424344		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.39894457947424344 | validation: 0.4832076994917771]
	TIME [epoch: 5.41 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975210181122568		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.3975210181122568 | validation: 0.43036249602919474]
	TIME [epoch: 5.42 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4051816893724531		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.4051816893724531 | validation: 0.523033586569078]
	TIME [epoch: 5.41 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.402791630049902		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.402791630049902 | validation: 0.4275254596444171]
	TIME [epoch: 5.43 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40251563249101807		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.40251563249101807 | validation: 0.5054645054293082]
	TIME [epoch: 5.42 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39828122166347557		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.39828122166347557 | validation: 0.43559072970459634]
	TIME [epoch: 5.43 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014841696936873		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.4014841696936873 | validation: 0.492959178061154]
	TIME [epoch: 5.41 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3987434381521513		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.3987434381521513 | validation: 0.4371773906264278]
	TIME [epoch: 5.42 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40324512584114436		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.40324512584114436 | validation: 0.4973048216240966]
	TIME [epoch: 5.43 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3970760452662054		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.3970760452662054 | validation: 0.4393691276919134]
	TIME [epoch: 5.42 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39521148911565973		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.39521148911565973 | validation: 0.4883214543105956]
	TIME [epoch: 5.42 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39480223768995887		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.39480223768995887 | validation: 0.4380880851721144]
	TIME [epoch: 5.43 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3915017082072022		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.3915017082072022 | validation: 0.4993836556067421]
	TIME [epoch: 5.41 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948424032262289		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.3948424032262289 | validation: 0.4192550053914045]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1194.pth
	Model improved!!!
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39550754729226056		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.39550754729226056 | validation: 0.5241454087194444]
	TIME [epoch: 5.42 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39588310555849904		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.39588310555849904 | validation: 0.42383988845388654]
	TIME [epoch: 5.42 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39878547687963717		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.39878547687963717 | validation: 0.4926864209962982]
	TIME [epoch: 5.41 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3936112390443908		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.3936112390443908 | validation: 0.4453556494815762]
	TIME [epoch: 5.44 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3909151581890393		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.3909151581890393 | validation: 0.47654774542940503]
	TIME [epoch: 5.43 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872309665962068		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.3872309665962068 | validation: 0.44461256114582987]
	TIME [epoch: 5.43 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3816925046903572		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.3816925046903572 | validation: 0.4715058118526309]
	TIME [epoch: 5.42 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38623578595327973		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.38623578595327973 | validation: 0.4625825069991649]
	TIME [epoch: 5.42 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3859140037584524		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.3859140037584524 | validation: 0.47964725015378734]
	TIME [epoch: 5.41 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3902275436407269		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.3902275436407269 | validation: 0.43291452617270815]
	TIME [epoch: 5.41 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40080286310829794		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.40080286310829794 | validation: 0.4935230862097439]
	TIME [epoch: 5.41 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39196375037343073		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.39196375037343073 | validation: 0.43967997044153223]
	TIME [epoch: 5.41 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38374892597891475		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.38374892597891475 | validation: 0.5033229050324071]
	TIME [epoch: 5.42 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38639262394790874		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.38639262394790874 | validation: 0.4164853818046834]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1208.pth
	Model improved!!!
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39473179272602543		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.39473179272602543 | validation: 0.497293781530191]
	TIME [epoch: 5.41 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830637951807549		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.3830637951807549 | validation: 0.429433883238475]
	TIME [epoch: 5.42 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825669387459681		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.3825669387459681 | validation: 0.47755250289969653]
	TIME [epoch: 5.42 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3863464227326985		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.3863464227326985 | validation: 0.4265221913001602]
	TIME [epoch: 5.42 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3816546517973501		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.3816546517973501 | validation: 0.4866773217262066]
	TIME [epoch: 5.41 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380379914547741		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.380379914547741 | validation: 0.42230882069134223]
	TIME [epoch: 5.42 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3857696290925809		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.3857696290925809 | validation: 0.48612788714866206]
	TIME [epoch: 5.42 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3834689465282084		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.3834689465282084 | validation: 0.4184161208654075]
	TIME [epoch: 5.42 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38581758300862945		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.38581758300862945 | validation: 0.4909413436432033]
	TIME [epoch: 5.42 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37871298769715367		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.37871298769715367 | validation: 0.43227526506209624]
	TIME [epoch: 5.43 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37971400663699967		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.37971400663699967 | validation: 0.47613610185965466]
	TIME [epoch: 5.42 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771369144582464		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.3771369144582464 | validation: 0.43437855206949294]
	TIME [epoch: 5.42 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3709479385355806		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.3709479385355806 | validation: 0.4626867156587567]
	TIME [epoch: 5.43 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37916281510389016		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.37916281510389016 | validation: 0.42045316276547207]
	TIME [epoch: 5.44 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3778651225267997		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.3778651225267997 | validation: 0.4936725081107081]
	TIME [epoch: 5.42 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38659589795738203		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.38659589795738203 | validation: 0.4148606298935029]
	TIME [epoch: 5.43 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1224.pth
	Model improved!!!
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37884277081991996		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.37884277081991996 | validation: 0.4886371231692696]
	TIME [epoch: 5.43 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3752133995620835		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.3752133995620835 | validation: 0.4304125351458293]
	TIME [epoch: 5.42 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3759724432239028		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.3759724432239028 | validation: 0.4624397164410473]
	TIME [epoch: 5.43 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37182499600477664		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.37182499600477664 | validation: 0.4418622947542916]
	TIME [epoch: 5.44 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796310192067969		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.3796310192067969 | validation: 0.47347855740268346]
	TIME [epoch: 5.43 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37358156736724		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.37358156736724 | validation: 0.4218783880984036]
	TIME [epoch: 5.42 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726892934531867		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.3726892934531867 | validation: 0.49118804392748566]
	TIME [epoch: 5.43 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3836029573195701		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.3836029573195701 | validation: 0.4372701214706089]
	TIME [epoch: 5.42 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.372300313388143		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.372300313388143 | validation: 0.47351526765710195]
	TIME [epoch: 5.44 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36894535688471425		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.36894535688471425 | validation: 0.4414362907200081]
	TIME [epoch: 5.42 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36901864416160024		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.36901864416160024 | validation: 0.4687668366271126]
	TIME [epoch: 5.43 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37270526204966026		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.37270526204966026 | validation: 0.43788920693155725]
	TIME [epoch: 5.43 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3742412194850685		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.3742412194850685 | validation: 0.47460174938502714]
	TIME [epoch: 5.43 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3749564113081252		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.3749564113081252 | validation: 0.40837852555880183]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37509502119356614		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.37509502119356614 | validation: 0.4902969456450018]
	TIME [epoch: 5.43 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3746102864824867		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.3746102864824867 | validation: 0.42351838366059447]
	TIME [epoch: 5.42 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37165277032875155		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.37165277032875155 | validation: 0.46747370004275574]
	TIME [epoch: 5.43 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36709344596950066		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.36709344596950066 | validation: 0.4346277199337881]
	TIME [epoch: 5.43 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37092717461653857		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.37092717461653857 | validation: 0.4728228695399834]
	TIME [epoch: 5.43 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3681682795346447		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.3681682795346447 | validation: 0.4184467095355065]
	TIME [epoch: 5.41 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37084989630027204		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.37084989630027204 | validation: 0.4839364007882786]
	TIME [epoch: 5.43 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36819666496360753		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.36819666496360753 | validation: 0.41725301184124713]
	TIME [epoch: 5.42 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.361601366534926		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.361601366534926 | validation: 0.4742364017584413]
	TIME [epoch: 5.43 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3658556154919019		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.3658556154919019 | validation: 0.4432556643871293]
	TIME [epoch: 5.42 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672462439283653		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.3672462439283653 | validation: 0.4551916120780306]
	TIME [epoch: 5.43 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36205437035502736		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.36205437035502736 | validation: 0.4494662980714987]
	TIME [epoch: 5.42 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606291984382946		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.3606291984382946 | validation: 0.46689275702774957]
	TIME [epoch: 5.43 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609286055884213		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.3609286055884213 | validation: 0.4277604581141514]
	TIME [epoch: 5.42 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3632679571056838		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.3632679571056838 | validation: 0.4865486226654307]
	TIME [epoch: 5.44 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37277390713165803		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.37277390713165803 | validation: 0.40344948787491586]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1254.pth
	Model improved!!!
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36600304240819287		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.36600304240819287 | validation: 0.47364107334088434]
	TIME [epoch: 5.44 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3661946643704836		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.3661946643704836 | validation: 0.4126315765557451]
	TIME [epoch: 5.42 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3641792933726657		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.3641792933726657 | validation: 0.4759268743744988]
	TIME [epoch: 5.42 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36312420298993076		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.36312420298993076 | validation: 0.4186785817105408]
	TIME [epoch: 5.41 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3612073918227904		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.3612073918227904 | validation: 0.4596712575080069]
	TIME [epoch: 5.42 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3645201565793117		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.3645201565793117 | validation: 0.43715358589797976]
	TIME [epoch: 5.42 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35935458949000665		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.35935458949000665 | validation: 0.4440706788484852]
	TIME [epoch: 5.42 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35470159503467685		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.35470159503467685 | validation: 0.43010225331652996]
	TIME [epoch: 5.41 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35284988424242786		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.35284988424242786 | validation: 0.4504580766819629]
	TIME [epoch: 5.42 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3513823212384607		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.3513823212384607 | validation: 0.4179451292200545]
	TIME [epoch: 5.41 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355987052159833		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.355987052159833 | validation: 0.4742206357263834]
	TIME [epoch: 5.42 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3646737409389305		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.3646737409389305 | validation: 0.3916949937147847]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1266.pth
	Model improved!!!
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3711163500865303		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.3711163500865303 | validation: 0.46870347655990285]
	TIME [epoch: 5.43 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36138077104773686		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.36138077104773686 | validation: 0.43120955104112685]
	TIME [epoch: 5.41 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35535044073465616		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.35535044073465616 | validation: 0.4362071416980328]
	TIME [epoch: 5.42 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35273630017127316		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.35273630017127316 | validation: 0.44010078087578103]
	TIME [epoch: 5.42 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34931054764566855		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.34931054764566855 | validation: 0.4229192656195775]
	TIME [epoch: 5.42 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34731520474686206		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.34731520474686206 | validation: 0.4216571756963606]
	TIME [epoch: 5.41 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504925610519753		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.3504925610519753 | validation: 0.44560282464579226]
	TIME [epoch: 5.44 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35187985964673824		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.35187985964673824 | validation: 0.4220430198837337]
	TIME [epoch: 5.41 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34854197233235373		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.34854197233235373 | validation: 0.42974562670649413]
	TIME [epoch: 5.42 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3541369258041169		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.3541369258041169 | validation: 0.49366052732454435]
	TIME [epoch: 5.41 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37172319398862186		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.37172319398862186 | validation: 0.401225605841473]
	TIME [epoch: 5.43 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3571482025339536		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.3571482025339536 | validation: 0.4834451962732338]
	TIME [epoch: 5.41 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3588316239868985		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.3588316239868985 | validation: 0.4240844424094026]
	TIME [epoch: 5.42 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459692622070038		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.3459692622070038 | validation: 0.4370574064038488]
	TIME [epoch: 5.42 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3497467384585643		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.3497467384585643 | validation: 0.4404888169594416]
	TIME [epoch: 5.42 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3503851522250924		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.3503851522250924 | validation: 0.43264612689361526]
	TIME [epoch: 5.42 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34688747978268125		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.34688747978268125 | validation: 0.44690919520996186]
	TIME [epoch: 5.43 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35136999866028035		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.35136999866028035 | validation: 0.42155282764505436]
	TIME [epoch: 5.41 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488460738793161		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.3488460738793161 | validation: 0.4600014990910358]
	TIME [epoch: 5.42 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35572909205807657		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.35572909205807657 | validation: 0.40363329499876005]
	TIME [epoch: 5.41 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3542590149106117		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.3542590149106117 | validation: 0.4713655427268303]
	TIME [epoch: 5.42 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3481040250017132		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.3481040250017132 | validation: 0.418840409846019]
	TIME [epoch: 5.42 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437369250403099		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.3437369250403099 | validation: 0.45824645559505295]
	TIME [epoch: 5.43 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35081087004750444		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.35081087004750444 | validation: 0.4247386965641935]
	TIME [epoch: 5.42 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34381318335486627		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.34381318335486627 | validation: 0.42954619012085654]
	TIME [epoch: 5.42 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34983753674630813		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.34983753674630813 | validation: 0.4106275151280853]
	TIME [epoch: 5.43 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34390600853476977		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.34390600853476977 | validation: 0.4654481961626802]
	TIME [epoch: 5.42 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530549460978432		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.3530549460978432 | validation: 0.4048317953360582]
	TIME [epoch: 5.44 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35304975346882517		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.35304975346882517 | validation: 0.4633384710310477]
	TIME [epoch: 5.42 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.344515907421872		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.344515907421872 | validation: 0.42516209907619623]
	TIME [epoch: 5.43 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34586953110069557		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.34586953110069557 | validation: 0.44746862120705955]
	TIME [epoch: 5.42 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33958492957082825		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.33958492957082825 | validation: 0.4237180460787558]
	TIME [epoch: 5.43 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3436345482934401		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.3436345482934401 | validation: 0.457385618025536]
	TIME [epoch: 5.42 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34654482016418475		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.34654482016418475 | validation: 0.4178724700914438]
	TIME [epoch: 5.42 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3423231537962097		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.3423231537962097 | validation: 0.4542908559376785]
	TIME [epoch: 5.41 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34288569077295633		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.34288569077295633 | validation: 0.40925785276815846]
	TIME [epoch: 5.42 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3430186037122941		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.3430186037122941 | validation: 0.47508869812557236]
	TIME [epoch: 5.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35343631015851734		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.35343631015851734 | validation: 0.39836605825918586]
	TIME [epoch: 5.41 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34907029861651795		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.34907029861651795 | validation: 0.4603404994034757]
	TIME [epoch: 5.42 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3393914689380029		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.3393914689380029 | validation: 0.4419757680963743]
	TIME [epoch: 5.41 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379774805118187		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.3379774805118187 | validation: 0.43372162982359913]
	TIME [epoch: 5.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33924754269789376		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.33924754269789376 | validation: 0.4399385388511881]
	TIME [epoch: 5.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354750136222856		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.3354750136222856 | validation: 0.4250463866863271]
	TIME [epoch: 5.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33975976314846873		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.33975976314846873 | validation: 0.44039094065916534]
	TIME [epoch: 5.42 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33480738612293404		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.33480738612293404 | validation: 0.40406327197373976]
	TIME [epoch: 5.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415702836190212		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.3415702836190212 | validation: 0.4634038534243497]
	TIME [epoch: 5.41 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401222057651438		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.3401222057651438 | validation: 0.39936320392405267]
	TIME [epoch: 5.41 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415972262567721		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.3415972262567721 | validation: 0.4575469894351876]
	TIME [epoch: 5.42 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354875637741043		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.3354875637741043 | validation: 0.40752831684914104]
	TIME [epoch: 5.41 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33511661580843255		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.33511661580843255 | validation: 0.42990655344466155]
	TIME [epoch: 5.42 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3332235242339418		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.3332235242339418 | validation: 0.42484715883985835]
	TIME [epoch: 5.41 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323099577875729		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.3323099577875729 | validation: 0.43575898758012477]
	TIME [epoch: 5.41 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33335604339148944		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.33335604339148944 | validation: 0.42243424114902584]
	TIME [epoch: 5.4 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.338851400852236		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.338851400852236 | validation: 0.46653411745556533]
	TIME [epoch: 5.42 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353042921411499		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.3353042921411499 | validation: 0.39383530261990773]
	TIME [epoch: 5.42 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3434202520150768		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.3434202520150768 | validation: 0.46683795495904523]
	TIME [epoch: 5.42 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33628868575016785		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.33628868575016785 | validation: 0.401461945534689]
	TIME [epoch: 5.42 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3329386681014276		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.3329386681014276 | validation: 0.4207286354442967]
	TIME [epoch: 5.42 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.329126050400724		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.329126050400724 | validation: 0.45522293385071677]
	TIME [epoch: 5.41 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33337223697799756		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.33337223697799756 | validation: 0.3895594616384423]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1326.pth
	Model improved!!!
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33960899908594966		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.33960899908594966 | validation: 0.45963262001243255]
	TIME [epoch: 5.41 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3369707854134203		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.3369707854134203 | validation: 0.4270064349406362]
	TIME [epoch: 5.41 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32920610687683466		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.32920610687683466 | validation: 0.437202025444053]
	TIME [epoch: 5.41 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3345425285198826		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.3345425285198826 | validation: 0.42251467828532424]
	TIME [epoch: 5.41 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33274210372442986		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.33274210372442986 | validation: 0.42331135527774305]
	TIME [epoch: 5.41 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280960366272383		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.3280960366272383 | validation: 0.42066342598321627]
	TIME [epoch: 5.4 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3282736565821772		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.3282736565821772 | validation: 0.43606578636358617]
	TIME [epoch: 5.42 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33255064662336753		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.33255064662336753 | validation: 0.4120891306000739]
	TIME [epoch: 5.41 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3324161645233454		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.3324161645233454 | validation: 0.42513218852149826]
	TIME [epoch: 5.42 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3246628216501183		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.3246628216501183 | validation: 0.4221028323128286]
	TIME [epoch: 5.41 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3283728522279328		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.3283728522279328 | validation: 0.4418557041454676]
	TIME [epoch: 5.42 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32820328820541345		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.32820328820541345 | validation: 0.3870212321008304]
	TIME [epoch: 5.41 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1338.pth
	Model improved!!!
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3393433056431146		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.3393433056431146 | validation: 0.46548956110941386]
	TIME [epoch: 5.42 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3314161026036554		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.3314161026036554 | validation: 0.4038082044936333]
	TIME [epoch: 5.42 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328188947617954		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.328188947617954 | validation: 0.42831634353440884]
	TIME [epoch: 5.43 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3251836321990878		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.3251836321990878 | validation: 0.43903168842094237]
	TIME [epoch: 5.42 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32759197189763956		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.32759197189763956 | validation: 0.3889087412541802]
	TIME [epoch: 5.43 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3288242894572329		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.3288242894572329 | validation: 0.46330145641505605]
	TIME [epoch: 5.42 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32847880188242706		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.32847880188242706 | validation: 0.39415538804595956]
	TIME [epoch: 5.42 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32651156039432866		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.32651156039432866 | validation: 0.4274749485953031]
	TIME [epoch: 5.42 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209376348959098		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.3209376348959098 | validation: 0.4304445798388532]
	TIME [epoch: 5.42 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32131418327589684		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.32131418327589684 | validation: 0.4098668668537937]
	TIME [epoch: 5.42 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3210831640935321		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.3210831640935321 | validation: 0.45038212644997816]
	TIME [epoch: 5.42 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280125679998595		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.3280125679998595 | validation: 0.39165883507465327]
	TIME [epoch: 5.41 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3171503558662523		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.3171503558662523 | validation: 0.4241286246469456]
	TIME [epoch: 5.41 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32145751195212086		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.32145751195212086 | validation: 0.4229973512964249]
	TIME [epoch: 5.42 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3255619883423115		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.3255619883423115 | validation: 0.41203581474796025]
	TIME [epoch: 5.43 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.324393003803155		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.324393003803155 | validation: 0.43052444255373684]
	TIME [epoch: 5.43 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32066958011507773		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.32066958011507773 | validation: 0.42777496227205236]
	TIME [epoch: 5.42 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205216812651851		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.3205216812651851 | validation: 0.42213256080387684]
	TIME [epoch: 5.42 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184152534784599		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.3184152534784599 | validation: 0.43507075877147017]
	TIME [epoch: 5.42 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190577151084115		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.3190577151084115 | validation: 0.38096148138902963]
	TIME [epoch: 5.42 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1358.pth
	Model improved!!!
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33298342219835214		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.33298342219835214 | validation: 0.46606973929032275]
	TIME [epoch: 5.38 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32664680522144907		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.32664680522144907 | validation: 0.4138691551828492]
	TIME [epoch: 5.39 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3138137293543187		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.3138137293543187 | validation: 0.3889628327947792]
	TIME [epoch: 5.39 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229653687499176		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.3229653687499176 | validation: 0.4521017297054628]
	TIME [epoch: 5.38 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3225944166137941		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.3225944166137941 | validation: 0.3979895205192818]
	TIME [epoch: 5.39 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32052362792213573		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.32052362792213573 | validation: 0.43970371926243407]
	TIME [epoch: 5.38 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195568365165606		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.3195568365165606 | validation: 0.4023687812581532]
	TIME [epoch: 5.39 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31511587725773527		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.31511587725773527 | validation: 0.41370938933871493]
	TIME [epoch: 5.43 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169495119090738		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.3169495119090738 | validation: 0.4397334375016089]
	TIME [epoch: 5.42 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31948780193691795		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.31948780193691795 | validation: 0.40733695891620947]
	TIME [epoch: 5.42 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197101783218364		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.3197101783218364 | validation: 0.4453178631593335]
	TIME [epoch: 5.42 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32058097285795073		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.32058097285795073 | validation: 0.40462565311619686]
	TIME [epoch: 5.43 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30922394027659006		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.30922394027659006 | validation: 0.4201808830746905]
	TIME [epoch: 5.42 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127183060460173		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.3127183060460173 | validation: 0.4319023089714968]
	TIME [epoch: 5.42 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3211029066775537		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.3211029066775537 | validation: 0.41793148545462117]
	TIME [epoch: 5.42 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31535608366831475		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.31535608366831475 | validation: 0.44280287802087576]
	TIME [epoch: 5.43 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32137793937131565		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.32137793937131565 | validation: 0.41241995515809526]
	TIME [epoch: 5.41 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31225216678766166		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.31225216678766166 | validation: 0.40921703289813793]
	TIME [epoch: 5.43 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3154903238108741		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.3154903238108741 | validation: 0.41608131733447573]
	TIME [epoch: 5.41 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135085137818521		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.3135085137818521 | validation: 0.39998533502125905]
	TIME [epoch: 5.43 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31206665165803804		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.31206665165803804 | validation: 0.44184099342290184]
	TIME [epoch: 5.42 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3176411519367332		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.3176411519367332 | validation: 0.4048183579978444]
	TIME [epoch: 5.42 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3141992113162263		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.3141992113162263 | validation: 0.44832002061249643]
	TIME [epoch: 5.42 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3148655938824085		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.3148655938824085 | validation: 0.3907558637627986]
	TIME [epoch: 5.43 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31710567163793		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.31710567163793 | validation: 0.4279009164767996]
	TIME [epoch: 5.43 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31324269227758494		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.31324269227758494 | validation: 0.3947155295447165]
	TIME [epoch: 5.42 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3128483109074636		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.3128483109074636 | validation: 0.4339916209247096]
	TIME [epoch: 5.42 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31727574891237437		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.31727574891237437 | validation: 0.4083480600734959]
	TIME [epoch: 5.42 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31543178344148165		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.31543178344148165 | validation: 0.421959835733012]
	TIME [epoch: 5.43 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109471751685606		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.3109471751685606 | validation: 0.4159066742159347]
	TIME [epoch: 5.42 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31222936489123804		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.31222936489123804 | validation: 0.4118930547252466]
	TIME [epoch: 5.43 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3087247674286838		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.3087247674286838 | validation: 0.42858074422516224]
	TIME [epoch: 5.41 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3120061018312169		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.3120061018312169 | validation: 0.38829276558330006]
	TIME [epoch: 5.42 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129995746003975		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.3129995746003975 | validation: 0.4556136409732788]
	TIME [epoch: 5.43 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204259642776043		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.3204259642776043 | validation: 0.40203714185941397]
	TIME [epoch: 5.41 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30646037294676715		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.30646037294676715 | validation: 0.4187421444755618]
	TIME [epoch: 5.42 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098364927397843		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.3098364927397843 | validation: 0.4285548993874292]
	TIME [epoch: 5.42 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30897973392755884		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.30897973392755884 | validation: 0.3959846394434574]
	TIME [epoch: 5.43 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3082026137950603		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.3082026137950603 | validation: 0.43248880012666]
	TIME [epoch: 5.43 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30514774690061236		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.30514774690061236 | validation: 0.39736191648924474]
	TIME [epoch: 5.43 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30956846558027595		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.30956846558027595 | validation: 0.4403649728360473]
	TIME [epoch: 5.42 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3072719496693614		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.3072719496693614 | validation: 0.4014888869597066]
	TIME [epoch: 5.42 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3079927582593202		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.3079927582593202 | validation: 0.4280800054785098]
	TIME [epoch: 5.39 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30323559612894746		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.30323559612894746 | validation: 0.39014880403297525]
	TIME [epoch: 5.39 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3088721497705751		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.3088721497705751 | validation: 0.41606610003776173]
	TIME [epoch: 5.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047262546959867		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.3047262546959867 | validation: 0.40643683182155316]
	TIME [epoch: 5.38 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307402879263834		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.307402879263834 | validation: 0.43111255610031984]
	TIME [epoch: 5.39 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30858170938399715		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.30858170938399715 | validation: 0.4056254160248307]
	TIME [epoch: 5.38 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30546894660109375		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.30546894660109375 | validation: 0.41780691616546756]
	TIME [epoch: 5.39 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047430926128331		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.3047430926128331 | validation: 0.39545009472762993]
	TIME [epoch: 5.39 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052867733325874		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.3052867733325874 | validation: 0.45034776308149954]
	TIME [epoch: 5.39 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055936616820288		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.3055936616820288 | validation: 0.3930054790575313]
	TIME [epoch: 5.39 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080929605528776		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.3080929605528776 | validation: 0.4346442567643875]
	TIME [epoch: 5.38 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056744043064666		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.3056744043064666 | validation: 0.40054225184220726]
	TIME [epoch: 5.37 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304255902639509		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.304255902639509 | validation: 0.40883595445651333]
	TIME [epoch: 5.39 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29987137405868564		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.29987137405868564 | validation: 0.43138101791408306]
	TIME [epoch: 5.39 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3032820797144072		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.3032820797144072 | validation: 0.3901873859320226]
	TIME [epoch: 5.38 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30444882046020266		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.30444882046020266 | validation: 0.42349137663272274]
	TIME [epoch: 5.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29912097314190256		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.29912097314190256 | validation: 0.4177408060070238]
	TIME [epoch: 5.38 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3011958429994902		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.3011958429994902 | validation: 0.39208213104795114]
	TIME [epoch: 5.39 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30092094805460806		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.30092094805460806 | validation: 0.41253473022401377]
	TIME [epoch: 5.39 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29986763896426133		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.29986763896426133 | validation: 0.4060401426503676]
	TIME [epoch: 5.38 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30460393387675594		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.30460393387675594 | validation: 0.41409086334324025]
	TIME [epoch: 5.38 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3016185760391645		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.3016185760391645 | validation: 0.3902841449900052]
	TIME [epoch: 5.37 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30184383182016855		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.30184383182016855 | validation: 0.44760162294737227]
	TIME [epoch: 5.38 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017935023352693		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.3017935023352693 | validation: 0.38212018702761374]
	TIME [epoch: 5.37 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30082669833740805		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.30082669833740805 | validation: 0.426011660768131]
	TIME [epoch: 5.38 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2976433094351377		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.2976433094351377 | validation: 0.3980820091991683]
	TIME [epoch: 5.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3020478917519298		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.3020478917519298 | validation: 0.41070111934077486]
	TIME [epoch: 5.38 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2972477996309923		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.2972477996309923 | validation: 0.41277623876363373]
	TIME [epoch: 5.38 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29697144158276917		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.29697144158276917 | validation: 0.3996494231671247]
	TIME [epoch: 5.39 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30088428445676835		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.30088428445676835 | validation: 0.4311566619653925]
	TIME [epoch: 5.38 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29860375988246834		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.29860375988246834 | validation: 0.38993896556048]
	TIME [epoch: 5.39 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30249828897578257		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.30249828897578257 | validation: 0.427532800839615]
	TIME [epoch: 5.38 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970967384613945		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.2970967384613945 | validation: 0.4035898895377372]
	TIME [epoch: 5.39 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29603508596583505		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.29603508596583505 | validation: 0.41037135604811925]
	TIME [epoch: 5.38 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935048472872156		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.2935048472872156 | validation: 0.4048947140486995]
	TIME [epoch: 5.38 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29657782956698214		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.29657782956698214 | validation: 0.4119207517358529]
	TIME [epoch: 5.37 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017785340980775		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.3017785340980775 | validation: 0.4173213294209219]
	TIME [epoch: 5.38 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29404865350745085		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.29404865350745085 | validation: 0.4211927105298938]
	TIME [epoch: 5.37 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29850311443221034		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.29850311443221034 | validation: 0.38686611167683943]
	TIME [epoch: 5.38 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.301613487811225		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.301613487811225 | validation: 0.4340243722802276]
	TIME [epoch: 5.37 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29936377258060337		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.29936377258060337 | validation: 0.39550396308416497]
	TIME [epoch: 5.38 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29702333275795867		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.29702333275795867 | validation: 0.41068149004252663]
	TIME [epoch: 5.37 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2958833259404957		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.2958833259404957 | validation: 0.42340890327264663]
	TIME [epoch: 5.38 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2960099227851996		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.2960099227851996 | validation: 0.39940139259100144]
	TIME [epoch: 5.37 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29100846430423327		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.29100846430423327 | validation: 0.4005259253367026]
	TIME [epoch: 5.41 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2942949330475955		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.2942949330475955 | validation: 0.4035080367796428]
	TIME [epoch: 5.41 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938774661957867		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.2938774661957867 | validation: 0.4074402361311238]
	TIME [epoch: 5.42 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2911623547423443		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.2911623547423443 | validation: 0.41516889294487314]
	TIME [epoch: 5.41 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29422646295969174		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.29422646295969174 | validation: 0.3892595575405815]
	TIME [epoch: 5.4 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938366390351104		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.2938366390351104 | validation: 0.43440643798191536]
	TIME [epoch: 5.41 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29781389686897536		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.29781389686897536 | validation: 0.38896436666672135]
	TIME [epoch: 5.41 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29739647478989517		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.29739647478989517 | validation: 0.4231060702882471]
	TIME [epoch: 5.42 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29407818989009693		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.29407818989009693 | validation: 0.3988206109475207]
	TIME [epoch: 5.42 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2941499483108171		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.2941499483108171 | validation: 0.40888238094191004]
	TIME [epoch: 5.41 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28939573530689855		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.28939573530689855 | validation: 0.40252840693855707]
	TIME [epoch: 5.41 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29588457898214016		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.29588457898214016 | validation: 0.4122789975721043]
	TIME [epoch: 5.41 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921018203385351		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.2921018203385351 | validation: 0.39386484430363833]
	TIME [epoch: 5.41 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.288741621486843		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.288741621486843 | validation: 0.4175861035825172]
	TIME [epoch: 5.42 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29092108345813755		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.29092108345813755 | validation: 0.3996086495368973]
	TIME [epoch: 5.4 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_10_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_10_v_mmd4_1459.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5103.796 seconds.
