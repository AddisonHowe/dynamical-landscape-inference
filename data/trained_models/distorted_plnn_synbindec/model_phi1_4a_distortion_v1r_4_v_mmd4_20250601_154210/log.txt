Args:
Namespace(name='model_phi1_4a_distortion_v1r_4_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.052832592, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1017158890

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.260664350713694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.260664350713694 | validation: 7.198103353208342]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.981328790011405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.981328790011405 | validation: 6.5586119896266855]
	TIME [epoch: 0.788 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.04275363919278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.04275363919278 | validation: 6.246116558074936]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.764265705911409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.764265705911409 | validation: 6.616764307263622]
	TIME [epoch: 0.697 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.4452660376024316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4452660376024316 | validation: 4.879537582080761]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.9598993406845056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9598993406845056 | validation: 6.1942817082682415]
	TIME [epoch: 0.692 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.079682591704636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.079682591704636 | validation: 5.11872837309267]
	TIME [epoch: 0.699 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.400725249632603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.400725249632603 | validation: 5.484719724713614]
	TIME [epoch: 0.8 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.649935176166796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.649935176166796 | validation: 7.1410870412973795]
	TIME [epoch: 0.698 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.755486198263895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.755486198263895 | validation: 7.015596537188301]
	TIME [epoch: 0.693 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.288554206509769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.288554206509769 | validation: 5.046444415533851]
	TIME [epoch: 0.991 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.357908820843011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.357908820843011 | validation: 4.246481658096413]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.154198882925721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.154198882925721 | validation: 2.6827795673377626]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.681503830934276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.681503830934276 | validation: 2.821488286120613]
	TIME [epoch: 0.693 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.37005058522064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.37005058522064 | validation: 4.143214132384772]
	TIME [epoch: 0.69 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.59538014685804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.59538014685804 | validation: 4.201486912323307]
	TIME [epoch: 0.692 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.724059322948896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.724059322948896 | validation: 3.5326093845286834]
	TIME [epoch: 0.694 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.583667992194794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.583667992194794 | validation: 2.466917398921201]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6629381293994534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6629381293994534 | validation: 5.331048731481413]
	TIME [epoch: 0.694 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7743983302727395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7743983302727395 | validation: 2.7712820882743725]
	TIME [epoch: 0.703 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4389853487458697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4389853487458697 | validation: 2.6229778512359525]
	TIME [epoch: 0.693 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6338532851102867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6338532851102867 | validation: 3.264365585914537]
	TIME [epoch: 0.692 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.065755520789609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065755520789609 | validation: 2.7286022958915366]
	TIME [epoch: 0.691 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.781913001289179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.781913001289179 | validation: 2.064785043579788]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4074932704790504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4074932704790504 | validation: 2.698801206356469]
	TIME [epoch: 0.695 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.69838910202921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.69838910202921 | validation: 2.2770333187257235]
	TIME [epoch: 0.692 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6066957811758544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6066957811758544 | validation: 2.365160469558643]
	TIME [epoch: 0.691 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3716749236472716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3716749236472716 | validation: 1.8100346416344353]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4631483236416427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4631483236416427 | validation: 2.2099781990066383]
	TIME [epoch: 0.697 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.248538501497183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.248538501497183 | validation: 2.215037106328342]
	TIME [epoch: 0.695 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5107576562543477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5107576562543477 | validation: 2.5124723046834774]
	TIME [epoch: 0.703 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.480984004720156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.480984004720156 | validation: 1.513681561902524]
	TIME [epoch: 0.762 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.09530561979216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.09530561979216 | validation: 2.037185579669996]
	TIME [epoch: 0.692 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0639123941752424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0639123941752424 | validation: 2.122940184863362]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5153415430275143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5153415430275143 | validation: 2.34587827314056]
	TIME [epoch: 0.691 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.276970252647489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.276970252647489 | validation: 1.2512791942228236]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8669175256296415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8669175256296415 | validation: 1.8875388072305193]
	TIME [epoch: 0.764 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8905583053630435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8905583053630435 | validation: 1.8314335410656224]
	TIME [epoch: 0.695 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.277460354566732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.277460354566732 | validation: 1.962967082070631]
	TIME [epoch: 0.692 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9042992916504125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9042992916504125 | validation: 1.242883592812405]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9013329240556305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9013329240556305 | validation: 1.7155218026930599]
	TIME [epoch: 0.697 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7289143562701994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7289143562701994 | validation: 1.5957820026846834]
	TIME [epoch: 0.695 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.039089366115994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.039089366115994 | validation: 1.7833724224717198]
	TIME [epoch: 0.693 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7403430741804837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7403430741804837 | validation: 1.2185398189897887]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8636776009125893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8636776009125893 | validation: 1.5412683480341998]
	TIME [epoch: 0.697 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5721428490339875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5721428490339875 | validation: 1.4289516117469756]
	TIME [epoch: 0.706 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9097523720144827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9097523720144827 | validation: 1.6302251226050697]
	TIME [epoch: 0.694 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6145233285656688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6145233285656688 | validation: 1.0200985280433752]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6293858036616382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6293858036616382 | validation: 1.673172850311417]
	TIME [epoch: 0.702 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6667385763333074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6667385763333074 | validation: 0.9759164351184257]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6244286369648782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6244286369648782 | validation: 1.5520147877860764]
	TIME [epoch: 0.696 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.537132107870433		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.537132107870433 | validation: 0.9350562682258711]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5449605028909497		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.5449605028909497 | validation: 1.6007196157535455]
	TIME [epoch: 0.697 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5519502768333013		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.5519502768333013 | validation: 0.8658651446132885]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4327484312230638		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.4327484312230638 | validation: 1.5642578686469684]
	TIME [epoch: 0.695 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5311328640286201		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.5311328640286201 | validation: 0.857998416797376]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4220724566831455		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.4220724566831455 | validation: 1.5116658683960278]
	TIME [epoch: 0.694 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4843528105320405		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.4843528105320405 | validation: 0.8477765169157696]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4092514947387416		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.4092514947387416 | validation: 1.514107102226327]
	TIME [epoch: 0.692 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475599449035895		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.475599449035895 | validation: 0.9164028690701116]
	TIME [epoch: 0.83 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4057885466429207		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.4057885466429207 | validation: 1.4856948952974136]
	TIME [epoch: 0.7 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4549419918098745		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.4549419918098745 | validation: 0.8618661749691032]
	TIME [epoch: 0.695 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3511796625064982		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.3511796625064982 | validation: 1.4716211932473904]
	TIME [epoch: 0.689 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4177386739756455		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4177386739756455 | validation: 0.7915916079608717]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.360410862222081		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.360410862222081 | validation: 1.4277515348994836]
	TIME [epoch: 0.693 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4069662524913442		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.4069662524913442 | validation: 1.0612907139400372]
	TIME [epoch: 0.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.64885604010588		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.64885604010588 | validation: 1.146046646705211]
	TIME [epoch: 0.697 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.252503140294836		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.252503140294836 | validation: 0.7881656152980643]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3634779693856012		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.3634779693856012 | validation: 1.520935369653088]
	TIME [epoch: 0.693 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4816645288411507		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.4816645288411507 | validation: 0.8624079384432017]
	TIME [epoch: 0.689 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4049625618278037		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.4049625618278037 | validation: 1.217452522362466]
	TIME [epoch: 0.689 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2898918856453678		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.2898918856453678 | validation: 0.8232112119621422]
	TIME [epoch: 0.692 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3941275975506136		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.3941275975506136 | validation: 1.340447270420638]
	TIME [epoch: 0.691 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341575986687858		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.341575986687858 | validation: 0.8067651871819559]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3407605943297825		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.3407605943297825 | validation: 1.253690671303061]
	TIME [epoch: 0.692 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3028098955647953		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.3028098955647953 | validation: 0.8165597873540316]
	TIME [epoch: 0.693 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3702860380541844		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.3702860380541844 | validation: 1.2601378326682422]
	TIME [epoch: 0.693 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2945411726894578		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.2945411726894578 | validation: 0.8182664945928594]
	TIME [epoch: 0.694 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3434472177455672		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.3434472177455672 | validation: 1.2821275400243197]
	TIME [epoch: 0.695 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3031876019775774		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.3031876019775774 | validation: 0.8167203820430271]
	TIME [epoch: 0.705 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3360203996734725		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.3360203996734725 | validation: 1.219741745557685]
	TIME [epoch: 0.696 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2775923437481618		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.2775923437481618 | validation: 0.8211502659818394]
	TIME [epoch: 0.701 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.324581086779		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.324581086779 | validation: 1.2595501955491495]
	TIME [epoch: 0.695 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2979976256211634		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2979976256211634 | validation: 0.8322439111598108]
	TIME [epoch: 0.694 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3227407246536442		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.3227407246536442 | validation: 1.2111486326218408]
	TIME [epoch: 0.694 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2889515781609742		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.2889515781609742 | validation: 0.8197233136502992]
	TIME [epoch: 0.693 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.33222741969409		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.33222741969409 | validation: 1.2361732167801196]
	TIME [epoch: 0.695 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2738892828743809		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2738892828743809 | validation: 0.7894937989031134]
	TIME [epoch: 0.692 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2865864352775327		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.2865864352775327 | validation: 1.276709534416299]
	TIME [epoch: 0.693 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.280452515928922		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.280452515928922 | validation: 0.8023215216015931]
	TIME [epoch: 0.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.294901602260339		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.294901602260339 | validation: 1.272929746026488]
	TIME [epoch: 0.696 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2742214920426644		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2742214920426644 | validation: 0.8492354485760449]
	TIME [epoch: 0.694 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.289585071032789		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.289585071032789 | validation: 1.4690848320244363]
	TIME [epoch: 0.695 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4292705212129364		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.4292705212129364 | validation: 0.8525709484907655]
	TIME [epoch: 0.704 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235020709614566		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.2235020709614566 | validation: 1.256531760280465]
	TIME [epoch: 0.696 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2648405568326515		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.2648405568326515 | validation: 0.8245591061200783]
	TIME [epoch: 0.693 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3160250801036395		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.3160250801036395 | validation: 1.2741673740259307]
	TIME [epoch: 0.694 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2850162308521278		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.2850162308521278 | validation: 0.7933905708118184]
	TIME [epoch: 0.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.23105203308616		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.23105203308616 | validation: 1.2616163137009768]
	TIME [epoch: 0.693 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2976714898999826		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.2976714898999826 | validation: 0.9399689772321758]
	TIME [epoch: 0.705 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4101050637835169		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.4101050637835169 | validation: 1.1447515425722912]
	TIME [epoch: 0.695 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2449649380513004		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.2449649380513004 | validation: 0.8406486788411943]
	TIME [epoch: 0.694 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2755235850216675		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.2755235850216675 | validation: 1.2471048097105109]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2750366062963732		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.2750366062963732 | validation: 0.8398208579342706]
	TIME [epoch: 0.702 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3033267519962533		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.3033267519962533 | validation: 1.1800869581242812]
	TIME [epoch: 0.692 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2570206220034315		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.2570206220034315 | validation: 0.8325962490235135]
	TIME [epoch: 0.693 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3005191983768722		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.3005191983768722 | validation: 1.1942216619863555]
	TIME [epoch: 0.691 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2673439449534898		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.2673439449534898 | validation: 0.8323593046000713]
	TIME [epoch: 0.69 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3022401501077299		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.3022401501077299 | validation: 1.20110169011057]
	TIME [epoch: 0.69 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2627123866696233		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.2627123866696233 | validation: 0.8294512121562119]
	TIME [epoch: 0.701 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2935580225647625		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.2935580225647625 | validation: 1.1784442322095798]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2431816273350778		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.2431816273350778 | validation: 0.808187837012482]
	TIME [epoch: 0.69 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.258019356374446		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.258019356374446 | validation: 1.1613140540989517]
	TIME [epoch: 0.69 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2464318717533116		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2464318717533116 | validation: 0.8180302903752401]
	TIME [epoch: 0.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2793805801536875		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.2793805801536875 | validation: 1.1919708033177752]
	TIME [epoch: 0.691 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2498439574626998		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2498439574626998 | validation: 0.8240626686013084]
	TIME [epoch: 0.811 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.293282395874919		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.293282395874919 | validation: 1.1914553592757715]
	TIME [epoch: 0.692 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.27047834047293		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.27047834047293 | validation: 0.859543886211715]
	TIME [epoch: 0.701 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.303308708271134		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.303308708271134 | validation: 1.1249879849556204]
	TIME [epoch: 0.7 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2316628887144458		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2316628887144458 | validation: 0.7955645171603196]
	TIME [epoch: 0.692 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.242921310895375		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.242921310895375 | validation: 1.2231256171275968]
	TIME [epoch: 0.692 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2633607512176075		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.2633607512176075 | validation: 0.7950355000673165]
	TIME [epoch: 0.691 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3106782534184998		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.3106782534184998 | validation: 1.2208653337019122]
	TIME [epoch: 0.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2340177436141948		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.2340177436141948 | validation: 0.8655982712358634]
	TIME [epoch: 0.698 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2672106500303881		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2672106500303881 | validation: 1.4545785272467917]
	TIME [epoch: 0.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3877274204812724		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.3877274204812724 | validation: 0.8134695127220231]
	TIME [epoch: 0.694 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2029702250420928		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.2029702250420928 | validation: 1.142916950335368]
	TIME [epoch: 0.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2136822949212265		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.2136822949212265 | validation: 0.8175330772721756]
	TIME [epoch: 0.689 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2855285789125037		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.2855285789125037 | validation: 1.201432829034525]
	TIME [epoch: 0.69 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2480084428305105		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2480084428305105 | validation: 0.8493139019745443]
	TIME [epoch: 0.691 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2716377853359724		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.2716377853359724 | validation: 1.1461179747921262]
	TIME [epoch: 0.698 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2600809027836266		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.2600809027836266 | validation: 0.8337442513961895]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2703059077634336		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.2703059077634336 | validation: 1.1459370261389719]
	TIME [epoch: 0.69 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225231932092443		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.225231932092443 | validation: 0.80963892853511]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455393051381765		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.2455393051381765 | validation: 1.1529385010734208]
	TIME [epoch: 0.693 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2177609486732863		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.2177609486732863 | validation: 0.8046003823537078]
	TIME [epoch: 0.691 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2306652597760526		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.2306652597760526 | validation: 1.210409131650279]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2458316228035542		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2458316228035542 | validation: 0.7699392093859696]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2207491736511227		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.2207491736511227 | validation: 1.277698198605313]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.278716596282882		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.278716596282882 | validation: 0.8831948052352274]
	TIME [epoch: 0.691 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2825431164052685		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.2825431164052685 | validation: 1.2379234123619387]
	TIME [epoch: 0.693 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2632936862760284		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2632936862760284 | validation: 0.7846459821432379]
	TIME [epoch: 0.692 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1961425331208604		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1961425331208604 | validation: 1.2034186241772495]
	TIME [epoch: 0.701 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2219244590000746		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.2219244590000746 | validation: 0.8172612312413077]
	TIME [epoch: 0.692 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2457429142475633		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.2457429142475633 | validation: 1.2023546412035824]
	TIME [epoch: 0.693 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2727954847632499		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2727954847632499 | validation: 0.8760323162492387]
	TIME [epoch: 0.692 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2766422931630117		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.2766422931630117 | validation: 1.0585233865185808]
	TIME [epoch: 0.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1799936697478506		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1799936697478506 | validation: 0.7740844472393528]
	TIME [epoch: 0.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2147957969383862		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.2147957969383862 | validation: 1.2579216068840091]
	TIME [epoch: 0.698 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2602174614108368		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.2602174614108368 | validation: 0.7885040197209678]
	TIME [epoch: 0.692 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.220936872363872		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.220936872363872 | validation: 1.1124106024621607]
	TIME [epoch: 0.69 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194391528743045		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.194391528743045 | validation: 0.8190435476487452]
	TIME [epoch: 0.691 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2441475974197413		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.2441475974197413 | validation: 1.1574972092582951]
	TIME [epoch: 0.691 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.250619786086837		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.250619786086837 | validation: 0.8492121567882429]
	TIME [epoch: 0.691 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2696501045510524		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.2696501045510524 | validation: 1.07683742524614]
	TIME [epoch: 0.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1896836122868373		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.1896836122868373 | validation: 0.7715564195234749]
	TIME [epoch: 0.692 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2230696565816381		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.2230696565816381 | validation: 1.165729603714172]
	TIME [epoch: 0.701 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2134179627901112		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2134179627901112 | validation: 0.801287029193653]
	TIME [epoch: 0.698 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.226179485932759		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.226179485932759 | validation: 1.205583947344976]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2083225028351072		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.2083225028351072 | validation: 0.8204203014887892]
	TIME [epoch: 0.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2297914022418857		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.2297914022418857 | validation: 1.3170624542831906]
	TIME [epoch: 0.691 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2952045283096607		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.2952045283096607 | validation: 0.8151804448405516]
	TIME [epoch: 0.69 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1760191841402001		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1760191841402001 | validation: 1.1789388830492056]
	TIME [epoch: 0.701 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1897394642986883		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1897394642986883 | validation: 0.786286869019107]
	TIME [epoch: 0.693 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2144269208936174		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.2144269208936174 | validation: 1.2042448611623264]
	TIME [epoch: 0.692 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2259514074156639		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2259514074156639 | validation: 0.7761032100817945]
	TIME [epoch: 0.69 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1727658199948627		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.1727658199948627 | validation: 1.1312738464142968]
	TIME [epoch: 0.691 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1908497817570518		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.1908497817570518 | validation: 0.7779072357544634]
	TIME [epoch: 0.695 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2026923704871413		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2026923704871413 | validation: 1.1426296629383061]
	TIME [epoch: 0.705 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2419999043235026		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.2419999043235026 | validation: 0.9126686645002142]
	TIME [epoch: 0.691 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3184838915476749		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3184838915476749 | validation: 1.0459601841033723]
	TIME [epoch: 0.702 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1648918139725968		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1648918139725968 | validation: 0.7737382379994844]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.180004485501269		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.180004485501269 | validation: 1.1380411449007102]
	TIME [epoch: 0.699 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194562468328188		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.194562468328188 | validation: 0.773835162601586]
	TIME [epoch: 0.694 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2203682106361555		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2203682106361555 | validation: 1.1211063382388156]
	TIME [epoch: 0.701 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2062734680943348		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.2062734680943348 | validation: 0.8121885904893814]
	TIME [epoch: 0.691 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2197548897795139		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.2197548897795139 | validation: 1.0653536231587173]
	TIME [epoch: 0.691 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1915907194395328		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.1915907194395328 | validation: 0.8081732972936222]
	TIME [epoch: 0.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2013228189982257		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.2013228189982257 | validation: 1.1091994525200348]
	TIME [epoch: 0.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1910249714027463		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.1910249714027463 | validation: 0.7756377540142474]
	TIME [epoch: 0.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1939010995778325		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1939010995778325 | validation: 1.1448866475321389]
	TIME [epoch: 0.708 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1934611152259205		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.1934611152259205 | validation: 0.7795366751794625]
	TIME [epoch: 0.693 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192312706014335		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.192312706014335 | validation: 1.2318131125092995]
	TIME [epoch: 0.694 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2335309435779167		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2335309435779167 | validation: 0.8456131534890923]
	TIME [epoch: 0.695 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2377731762945319		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.2377731762945319 | validation: 1.2228607694982387]
	TIME [epoch: 0.694 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2236057879309683		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.2236057879309683 | validation: 0.7685628310260283]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1566955474018308		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.1566955474018308 | validation: 1.1172907184756622]
	TIME [epoch: 0.775 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770272532325305		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1770272532325305 | validation: 0.779336092272108]
	TIME [epoch: 0.696 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1986245344713482		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.1986245344713482 | validation: 1.1141637557599418]
	TIME [epoch: 0.695 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1895583685909128		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.1895583685909128 | validation: 0.8158553332674604]
	TIME [epoch: 0.695 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2181392129734874		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.2181392129734874 | validation: 1.0391366273165958]
	TIME [epoch: 0.702 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1851678916198958		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1851678916198958 | validation: 0.8262648355845568]
	TIME [epoch: 0.706 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969437405666		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.1969437405666 | validation: 1.091892572784391]
	TIME [epoch: 0.695 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1812080599644375		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1812080599644375 | validation: 0.7566754813783672]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194029623203295		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.194029623203295 | validation: 1.1226450086803432]
	TIME [epoch: 0.695 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1689403064942447		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.1689403064942447 | validation: 0.7731381558571655]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1670657198808865		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.1670657198808865 | validation: 1.0829628737311647]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165001937264754		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.165001937264754 | validation: 0.7564826490569506]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1738628914475417		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.1738628914475417 | validation: 1.1497311330592683]
	TIME [epoch: 0.697 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1974310731938878		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.1974310731938878 | validation: 0.8031693368317747]
	TIME [epoch: 0.694 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2035309690231952		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2035309690231952 | validation: 1.2186496744249973]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.224353293608853		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.224353293608853 | validation: 0.7847411933840174]
	TIME [epoch: 1.39 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1416566586702062		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.1416566586702062 | validation: 1.1199871690038496]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164608116435082		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.164608116435082 | validation: 0.7566105552268437]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1988044477499542		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.1988044477499542 | validation: 1.0984008675484]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1874760686759478		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.1874760686759478 | validation: 0.8100658425810057]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1957291008390898		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1957291008390898 | validation: 1.086730965393709]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1813587726520864		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1813587726520864 | validation: 0.7758510604182935]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.174103733235796		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.174103733235796 | validation: 1.0800640775191759]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1750432068213559		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.1750432068213559 | validation: 0.766339449414205]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625542403573004		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.1625542403573004 | validation: 1.0649895527390674]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1663254631282152		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.1663254631282152 | validation: 0.7531551759108681]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1459572441842059		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1459572441842059 | validation: 1.0750147478234529]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1657425764397567		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1657425764397567 | validation: 0.7415029497114758]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1636024628194894		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1636024628194894 | validation: 1.0515309226126466]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15360740654634		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.15360740654634 | validation: 0.7552268200446863]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1584732994277798		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.1584732994277798 | validation: 1.0618289022781782]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535127582965063		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.1535127582965063 | validation: 0.7942510127202839]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.209786338315184		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.209786338315184 | validation: 1.0581291569326179]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1921369758254576		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1921369758254576 | validation: 0.794797571448169]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625022761749866		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1625022761749866 | validation: 1.0382605228544737]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1451489031598605		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.1451489031598605 | validation: 0.7379438937031871]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746648328729583		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1746648328729583 | validation: 1.1214398076690366]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1880182495154352		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.1880182495154352 | validation: 0.8017874930560058]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176461897597388		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.176461897597388 | validation: 1.1344803793792948]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1868061844647255		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.1868061844647255 | validation: 0.7715830512726742]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149457802939896		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.149457802939896 | validation: 1.0893378722487155]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.147513827834735		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.147513827834735 | validation: 0.7357939924889796]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1488071273017446		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1488071273017446 | validation: 1.051531671160738]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1558885221737842		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1558885221737842 | validation: 0.775941981178944]
	TIME [epoch: 1.44 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1552619591596665		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1552619591596665 | validation: 1.0712413630242057]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816779932115895		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1816779932115895 | validation: 0.7974595145123606]
	TIME [epoch: 1.39 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1859904372348744		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.1859904372348744 | validation: 1.0251488357728553]
	TIME [epoch: 1.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271379650347764		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1271379650347764 | validation: 0.7461274530438256]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140482573789887		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.140482573789887 | validation: 1.0886985607247697]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1622382682710255		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.1622382682710255 | validation: 0.7604834756676484]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1507015266935785		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1507015266935785 | validation: 1.0998265481301077]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1433407400660875		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1433407400660875 | validation: 0.7778625670135814]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508226785663138		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.1508226785663138 | validation: 1.1179180114895102]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1767193543311452		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1767193543311452 | validation: 0.7669695823275089]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1396521229234033		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.1396521229234033 | validation: 1.0646072686164085]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1360160584416619		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1360160584416619 | validation: 0.7465447577928764]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1520769167423255		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1520769167423255 | validation: 1.049598309130402]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1333205387285632		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1333205387285632 | validation: 0.7563250641493316]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1545536575802524		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1545536575802524 | validation: 1.013634570652451]
	TIME [epoch: 1.41 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1631060437079228		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1631060437079228 | validation: 0.7923341698190663]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1668639727801107		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.1668639727801107 | validation: 1.0147763387136093]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321699380649364		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.1321699380649364 | validation: 0.729038452839994]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1422339520510414		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.1422339520510414 | validation: 1.0681979770321064]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1311182868021172		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1311182868021172 | validation: 0.7514758422239306]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199563437976736		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1199563437976736 | validation: 1.0821371405116456]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1381213385484528		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1381213385484528 | validation: 0.7722576668540796]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575294275945285		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.1575294275945285 | validation: 1.1052783246358382]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1577545533490952		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.1577545533490952 | validation: 0.7636837608400496]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1288597610444684		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.1288597610444684 | validation: 1.0585618460596184]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313737509626312		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.1313737509626312 | validation: 0.7362647061427483]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1441410064487982		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1441410064487982 | validation: 1.0491993195964313]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132957524636029		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.132957524636029 | validation: 0.7407554030087571]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131546725377324		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.131546725377324 | validation: 1.0123766344905971]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142780861669974		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.142780861669974 | validation: 0.7941602088063887]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1731049503805986		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.1731049503805986 | validation: 1.0029591721649262]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366729823490247		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.1366729823490247 | validation: 0.7421734658070962]
	TIME [epoch: 1.37 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1319415206749768		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.1319415206749768 | validation: 1.0121362374272294]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1165279687789584		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1165279687789584 | validation: 0.7316046349612714]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1112113084884985		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.1112113084884985 | validation: 1.052410867628743]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1290528122265713		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.1290528122265713 | validation: 0.756157262651599]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313810714147825		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.1313810714147825 | validation: 1.0838551983922253]
	TIME [epoch: 1.43 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439256557646744		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.1439256557646744 | validation: 0.7704956015169077]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276631913664823		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.1276631913664823 | validation: 1.0577302406578888]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1210705328795496		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.1210705328795496 | validation: 0.7273214293101307]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1136837614615556		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.1136837614615556 | validation: 1.038464677616626]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131587375160307		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.131587375160307 | validation: 0.737121546464396]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108539106145096		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.108539106145096 | validation: 1.0159319369354172]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1279833920795181		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1279833920795181 | validation: 0.782863525880392]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1751160198526842		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.1751160198526842 | validation: 0.9761350957201178]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1272054417950086		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1272054417950086 | validation: 0.7611049864019851]
	TIME [epoch: 1.42 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1218219882794078		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.1218219882794078 | validation: 0.9957016709422991]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.112904405203234		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.112904405203234 | validation: 0.7400795641770216]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1223715818246156		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.1223715818246156 | validation: 1.035265842026217]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115470068794995		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.115470068794995 | validation: 0.729457416657594]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107162316139303		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.107162316139303 | validation: 1.0379394218706965]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1260397725985534		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.1260397725985534 | validation: 0.725860920836764]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303640065436547		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.1303640065436547 | validation: 1.0359338163802032]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1230450994002008		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.1230450994002008 | validation: 0.7377334920897651]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064479635730475		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.1064479635730475 | validation: 1.0174586600377538]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209751892055808		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.1209751892055808 | validation: 0.7372933607070656]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122956684692143		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.122956684692143 | validation: 1.008772009371895]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1238724704797947		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1238724704797947 | validation: 0.7341700282962003]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1172561659107925		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.1172561659107925 | validation: 1.010807454837247]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1312417411627214		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1312417411627214 | validation: 0.7860913166632616]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1597600652648012		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.1597600652648012 | validation: 0.9549305417976657]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0910343922648673		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0910343922648673 | validation: 0.7557717375636298]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0981253350041222		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.0981253350041222 | validation: 0.9999248614835651]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1050597654576737		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.1050597654576737 | validation: 0.7534451834989223]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108671368855499		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.108671368855499 | validation: 1.0186601900719556]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1215081883964582		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.1215081883964582 | validation: 0.7235519922596718]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1061120996647673		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.1061120996647673 | validation: 1.0300447356728826]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1263497493016892		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.1263497493016892 | validation: 0.7693552816430835]
	TIME [epoch: 1.37 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199472949959282		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.1199472949959282 | validation: 0.9977480758787695]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1036976151794		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.1036976151794 | validation: 0.7382501002959281]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0904035124481535		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.0904035124481535 | validation: 1.0196339074783995]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1230869216453863		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.1230869216453863 | validation: 0.7239178642116102]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144349670998768		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.1144349670998768 | validation: 0.9815986583689015]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1120521900017213		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1120521900017213 | validation: 0.7781878698048503]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.141598033807483		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.141598033807483 | validation: 0.9522085701190083]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064242514082148		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1064242514082148 | validation: 0.7390731527934312]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1037934390252984		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.1037934390252984 | validation: 1.008320646199802]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105492825066687		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.105492825066687 | validation: 0.7220160586772351]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097191771544168		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.097191771544168 | validation: 1.0195055919712732]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1123472698533226		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.1123472698533226 | validation: 0.7218255101153289]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0984584006804563		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0984584006804563 | validation: 1.019650543102468]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1235086021229823		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.1235086021229823 | validation: 0.7465856024726834]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1033402425423442		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1033402425423442 | validation: 1.0055647742870473]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1218780766174274		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.1218780766174274 | validation: 0.7257429281594243]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0960885920860846		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.0960885920860846 | validation: 1.0260030920754508]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1042094793932475		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1042094793932475 | validation: 0.7290265298428111]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0970323085083047		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.0970323085083047 | validation: 0.927606762254324]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0975404231467583		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.0975404231467583 | validation: 0.7610812525751884]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117455262287817		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.117455262287817 | validation: 0.993105275590286]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1299748276073511		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.1299748276073511 | validation: 0.7545193464261717]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1184590887263393		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.1184590887263393 | validation: 0.9109122449575372]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0843657747197344		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.0843657747197344 | validation: 0.7379255879768788]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0915277077894678		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.0915277077894678 | validation: 1.0034827848071728]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098891840107221		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.098891840107221 | validation: 0.7255132382267228]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.106216633251109		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.106216633251109 | validation: 1.0071420873102068]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1022927632208004		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.1022927632208004 | validation: 0.7449452440747653]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0981672660238297		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.0981672660238297 | validation: 0.9935501439166871]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1123567112700325		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.1123567112700325 | validation: 0.7148930964037036]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0989757809251262		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.0989757809251262 | validation: 1.020217370903775]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111870110573243		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.111870110573243 | validation: 0.7211573904379007]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0916675821943989		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.0916675821943989 | validation: 0.9638419703910289]
	TIME [epoch: 1.37 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078253866805882		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.078253866805882 | validation: 0.7343971116196727]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0902008611413123		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.0902008611413123 | validation: 0.99469313687083]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0973852585004362		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0973852585004362 | validation: 0.7295645613330635]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0792312595743045		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0792312595743045 | validation: 0.9786895778785095]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086206127299056		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.086206127299056 | validation: 0.7328505019643633]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100999535788856		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.100999535788856 | validation: 0.9549269638426385]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142168443681657		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.1142168443681657 | validation: 0.8191427692918453]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1406599538222688		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.1406599538222688 | validation: 0.887530226706323]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.080473104248865		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.080473104248865 | validation: 0.7097860792134383]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0878609960628522		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0878609960628522 | validation: 0.9749082897449105]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.095327261922756		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.095327261922756 | validation: 0.7361709695290553]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087924842942071		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.087924842942071 | validation: 0.994630779426859]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0864151553945205		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0864151553945205 | validation: 0.7235738934292719]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0836030925538598		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.0836030925538598 | validation: 0.9776302939873432]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0931547186169672		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.0931547186169672 | validation: 0.7161889250343663]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0976519105522449		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.0976519105522449 | validation: 0.9352329293162619]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0882040677830296		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.0882040677830296 | validation: 0.7144761575639441]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0771067266237528		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.0771067266237528 | validation: 0.9293712740413181]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075527232928072		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.075527232928072 | validation: 0.7190478747391151]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0760577898741184		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.0760577898741184 | validation: 0.9693252830560685]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0979552111776227		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.0979552111776227 | validation: 0.7587050507858845]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119949692557139		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.119949692557139 | validation: 0.881422898746528]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098917492956947		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.098917492956947 | validation: 0.7498266957235858]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078635815448553		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.078635815448553 | validation: 0.9670223565108881]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0839590653606919		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.0839590653606919 | validation: 0.7365721941799781]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0827783294970081		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.0827783294970081 | validation: 0.9709456568461153]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0902096033385567		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0902096033385567 | validation: 0.7445057508794277]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0834726636451928		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.0834726636451928 | validation: 0.9410391274181095]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0909076845934862		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.0909076845934862 | validation: 0.7044105164796464]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939804821052643		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.0939804821052643 | validation: 0.9738583859475478]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.080902438605375		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.080902438605375 | validation: 0.7198645665357808]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073812437820225		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.073812437820225 | validation: 0.9630589632008706]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0869898787781869		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.0869898787781869 | validation: 0.7198203459223877]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0729261320088292		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.0729261320088292 | validation: 0.9599811577943069]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0863033425355264		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.0863033425355264 | validation: 0.7177569090892332]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0843807571773898		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.0843807571773898 | validation: 0.9346967571593533]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0926495059087185		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.0926495059087185 | validation: 0.7726842959078337]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1076097305786043		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.1076097305786043 | validation: 0.9180506701939483]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0887248897673567		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.0887248897673567 | validation: 0.7176272301895716]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.083217885409837		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.083217885409837 | validation: 0.948816254045974]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.080621617210178		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.080621617210178 | validation: 0.7191218304579817]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.081697518266686		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.081697518266686 | validation: 0.9589405250877938]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0867437247912097		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.0867437247912097 | validation: 0.7262821728509393]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0946632843534236		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.0946632843534236 | validation: 0.9908962267511477]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0963856159271095		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.0963856159271095 | validation: 0.7380894281825923]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0722200412203582		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.0722200412203582 | validation: 0.9552279705271789]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0735393564771638		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.0735393564771638 | validation: 0.7335218155433044]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0778633061198966		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.0778633061198966 | validation: 0.9179274339338287]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0737660649559297		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0737660649559297 | validation: 0.7415862178532436]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0759156907408147		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.0759156907408147 | validation: 0.9073654509374862]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.08433833778558		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.08433833778558 | validation: 0.7552660972630412]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0853571118800966		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.0853571118800966 | validation: 0.900952006148831]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0735047684843675		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.0735047684843675 | validation: 0.7459829053860028]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0695219718266917		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.0695219718266917 | validation: 0.9264970788248142]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0830545803471994		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.0830545803471994 | validation: 0.7288165636147805]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0807298597318413		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.0807298597318413 | validation: 0.9401515284124248]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0855712226058907		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.0855712226058907 | validation: 0.737207266445513]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0660116344350086		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.0660116344350086 | validation: 0.951612249510692]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0833083318759313		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.0833083318759313 | validation: 0.740634367329589]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082133203690203		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.082133203690203 | validation: 0.9660777093623673]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794321746552036		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.0794321746552036 | validation: 0.72738941727457]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0543067040461802		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.0543067040461802 | validation: 0.886525486420627]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0702901897317885		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.0702901897317885 | validation: 0.7138124205550214]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0827883391426643		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.0827883391426643 | validation: 0.9135091545914888]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794372950559483		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.0794372950559483 | validation: 0.7606829739115814]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0786991180770857		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.0786991180770857 | validation: 0.9035366596289001]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0912436062775468		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.0912436062775468 | validation: 0.7378977350976763]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079292696532998		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.079292696532998 | validation: 0.8928763583916336]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0593373735211595		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.0593373735211595 | validation: 0.7340462992235581]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0771382376966914		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.0771382376966914 | validation: 0.9557695039241563]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077530762281905		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.077530762281905 | validation: 0.718858449596245]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0740498270316932		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.0740498270316932 | validation: 0.9569890042613833]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0801062750973582		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.0801062750973582 | validation: 0.720689123831072]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0932780036781413		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.0932780036781413 | validation: 0.9451884543842625]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.083198266669368		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.083198266669368 | validation: 0.7545607428191713]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064374945591512		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.064374945591512 | validation: 0.9179910811807885]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0704416703909763		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.0704416703909763 | validation: 0.7278540060376617]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0710535104566674		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.0710535104566674 | validation: 0.9184763589978093]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0742032160695858		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.0742032160695858 | validation: 0.7402462726100195]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0873574594416		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.0873574594416 | validation: 0.9261497931993179]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0888253393280554		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.0888253393280554 | validation: 0.742690311959218]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0744159794364294		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.0744159794364294 | validation: 0.8925935178689763]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0668678948468862		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.0668678948468862 | validation: 0.7485113469026357]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0683512259417698		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.0683512259417698 | validation: 0.9269833295100233]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063572845029859		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.063572845029859 | validation: 0.7323125736851455]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0759243630538007		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.0759243630538007 | validation: 0.9307464657802467]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0761307017049233		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.0761307017049233 | validation: 0.740932548550438]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0719250345242204		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.0719250345242204 | validation: 0.940559436741996]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0713432613539247		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.0713432613539247 | validation: 0.7356744531600241]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0748470186105876		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.0748470186105876 | validation: 0.9065021147995458]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0693788337517909		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.0693788337517909 | validation: 0.752019431111164]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0611429580764564		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.0611429580764564 | validation: 0.9050614842054236]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073286816424845		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.073286816424845 | validation: 0.7171735426476913]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.074023398026498		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.074023398026498 | validation: 0.8856665217378968]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0652758264579578		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.0652758264579578 | validation: 0.7505334434256826]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.083524531474297		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.083524531474297 | validation: 0.9020804636009747]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0669837771005837		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.0669837771005837 | validation: 0.7448310614897552]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.07529616505182		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.07529616505182 | validation: 0.8726170828040751]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0529426352057107		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.0529426352057107 | validation: 0.7071872238086911]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0608559374867097		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.0608559374867097 | validation: 0.8966044538315711]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0647432498669025		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.0647432498669025 | validation: 0.7401277917596258]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0765280982417311		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.0765280982417311 | validation: 0.9129100209800696]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0691099294794026		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.0691099294794026 | validation: 0.7201608430177426]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0637091088782373		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.0637091088782373 | validation: 0.9305153720882129]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066290861988714		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.066290861988714 | validation: 0.7321874845084242]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.062012338196966		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.062012338196966 | validation: 0.9066269554783158]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0594151673545917		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.0594151673545917 | validation: 0.7242815316386745]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0598230369989556		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.0598230369989556 | validation: 0.8944089823063305]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0567761389998187		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.0567761389998187 | validation: 0.7446703063846446]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.065028721181844		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.065028721181844 | validation: 0.8882780302564468]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634626221464556		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.0634626221464556 | validation: 0.7583757788026111]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0607549703085062		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.0607549703085062 | validation: 0.8953316516323547]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0718296819958553		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.0718296819958553 | validation: 0.7573807926151174]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0542314263702333		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.0542314263702333 | validation: 0.9284226721665902]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630718125642467		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.0630718125642467 | validation: 0.7302277582736358]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.069108937726477		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.069108937726477 | validation: 0.8829361028192646]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0565288096346657		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.0565288096346657 | validation: 0.7384831770105982]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0485047773334335		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.0485047773334335 | validation: 0.9208167229369139]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0712736105692795		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.0712736105692795 | validation: 0.7272126452496964]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0790717097212261		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.0790717097212261 | validation: 0.8890878861340625]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0730997531128597		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.0730997531128597 | validation: 0.7195981899433594]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0470810350888238		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.0470810350888238 | validation: 0.9109149480333495]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0635196158033438		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.0635196158033438 | validation: 0.7318704764765839]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0534355729763187		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.0534355729763187 | validation: 0.8717374126531849]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0562409763939669		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.0562409763939669 | validation: 0.7254957847195407]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0675136501639282		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.0675136501639282 | validation: 0.8830488624604632]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0638993238268266		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.0638993238268266 | validation: 0.7472572581425394]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0660496544145839		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.0660496544145839 | validation: 0.9052822410169625]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061913192866465		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.061913192866465 | validation: 0.7181310069671962]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.056816911182103		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.056816911182103 | validation: 0.9206762029350763]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0580226998472058		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.0580226998472058 | validation: 0.7319518004591915]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0582152918415335		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.0582152918415335 | validation: 0.915160265218465]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634400789397125		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.0634400789397125 | validation: 0.7318666419240233]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0630777945960954		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.0630777945960954 | validation: 0.8887669585320277]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0556916894986528		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.0556916894986528 | validation: 0.7359968413925183]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506017119368964		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.0506017119368964 | validation: 0.8646305537921715]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475913258483698		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.0475913258483698 | validation: 0.7341469619761996]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0610925622140814		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.0610925622140814 | validation: 0.9234638669545827]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077102545547232		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.077102545547232 | validation: 0.7433555757019102]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0597514656850715		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.0597514656850715 | validation: 0.8388853455560439]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540449098880649		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.0540449098880649 | validation: 0.7623344841738796]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0561646216748386		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.0561646216748386 | validation: 0.8825750609416737]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053199302315476		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.053199302315476 | validation: 0.7395053514449765]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0650872894484704		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.0650872894484704 | validation: 0.8562034958104574]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057004816637115		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.057004816637115 | validation: 0.7578009802565504]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609957827408731		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.0609957827408731 | validation: 0.8981697911282764]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061221373680573		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.061221373680573 | validation: 0.744153865876403]
	TIME [epoch: 1.36 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053995629079971		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.053995629079971 | validation: 0.8974398351748077]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0477660196479153		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.0477660196479153 | validation: 0.755269869108527]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0462985516729715		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.0462985516729715 | validation: 0.8877054437213385]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0633347684982122		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.0633347684982122 | validation: 0.7253005687133242]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.062049390789367		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.062049390789367 | validation: 0.8906170004130791]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050831173705674		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.050831173705674 | validation: 0.7453277234989182]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0406639370192696		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.0406639370192696 | validation: 0.8854802906672132]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0509151405529762		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.0509151405529762 | validation: 0.7471399532516537]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.060556522262943		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.060556522262943 | validation: 0.8542701801449003]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0595011928208498		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.0595011928208498 | validation: 0.7606729657104618]
	TIME [epoch: 1.64 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.060236121164733		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.060236121164733 | validation: 0.8714159816703332]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0555246116185177		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.0555246116185177 | validation: 0.7457294175813038]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0557833342553604		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.0557833342553604 | validation: 0.8767915991255587]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0497495196300886		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.0497495196300886 | validation: 0.7394218217142369]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0526369575306094		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.0526369575306094 | validation: 0.9190619799957628]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0713454179767377		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.0713454179767377 | validation: 0.7419022983514698]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0607413307155253		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.0607413307155253 | validation: 0.8996737809534946]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0553007460250614		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.0553007460250614 | validation: 0.7468166388815152]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0445224975554839		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.0445224975554839 | validation: 0.861032793649385]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508054584471502		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.0508054584471502 | validation: 0.7238498671447691]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0571996436369662		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.0571996436369662 | validation: 0.9000861831719068]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0645748903566514		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.0645748903566514 | validation: 0.7521876313491614]
	TIME [epoch: 1.45 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049317655106297		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.049317655106297 | validation: 0.8462234687953082]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0407140110807596		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.0407140110807596 | validation: 0.7317180094724216]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_4_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_4_v_mmd4_502.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1137.338 seconds.
