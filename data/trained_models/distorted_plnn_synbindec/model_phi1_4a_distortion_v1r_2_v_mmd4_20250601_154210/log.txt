Args:
Namespace(name='model_phi1_4a_distortion_v1r_2_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1r_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.06710146, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2789655695

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.400113981876068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.400113981876068 | validation: 7.665165768022712]
	TIME [epoch: 159 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.168849787949066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.168849787949066 | validation: 7.543507892253411]
	TIME [epoch: 0.791 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.058457874581157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.058457874581157 | validation: 7.521878531832027]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.66304384440145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.66304384440145 | validation: 7.394640045098054]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.641022313306768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.641022313306768 | validation: 7.095648911562413]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.493528339385074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.493528339385074 | validation: 6.428025524641598]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.133046659397017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.133046659397017 | validation: 4.240215148502292]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.64386112291036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.64386112291036 | validation: 8.05392689752505]
	TIME [epoch: 0.697 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.405457368963562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.405457368963562 | validation: 8.2173457761309]
	TIME [epoch: 0.694 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.856955568625268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.856955568625268 | validation: 7.723209163274513]
	TIME [epoch: 0.693 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.975224902753573		[learning rate: 0.01]
nan encountered in epoch 11 (validation loss).
	Learning Rate: 0.01
	LOSS [training: 6.975224902753573 | validation: nan]
	TIME [epoch: 0.69 sec]
EPOCH 12/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 1/1] avg loss: 8.529575791411657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.529575791411657 | validation: 7.520208215736034]
	TIME [epoch: 167 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.197015463727937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.197015463727937 | validation: 4.485099867463545]
	TIME [epoch: 0.722 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.399123400258636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.399123400258636 | validation: 8.250106550924938]
	TIME [epoch: 0.693 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.13801813959362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.13801813959362 | validation: 8.34700765375173]
	TIME [epoch: 0.694 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.233983608300482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.233983608300482 | validation: 5.646135457708654]
	TIME [epoch: 0.692 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.704964779337359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.704964779337359 | validation: 6.972652028892189]
	TIME [epoch: 0.691 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.837919754896407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.837919754896407 | validation: 6.049581864646901]
	TIME [epoch: 0.69 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.083747018397428		[learning rate: 0.01]
nan encountered in epoch 19 (validation loss).
	Learning Rate: 0.01
	LOSS [training: 6.083747018397428 | validation: nan]
	TIME [epoch: 0.692 sec]
EPOCH 20/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.0010000000000000002
		[batch 1/1] avg loss: 8.817659266348944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.817659266348944 | validation: 8.827138391192062]
	TIME [epoch: 168 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.755204180307071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.755204180307071 | validation: 8.814962877798633]
	TIME [epoch: 0.75 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.741254008022464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.741254008022464 | validation: 8.804695156400095]
	TIME [epoch: 0.692 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.729186390796709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.729186390796709 | validation: 8.79194678394585]
	TIME [epoch: 0.695 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.71603983732551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.71603983732551 | validation: 8.783607484160141]
	TIME [epoch: 0.692 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.705197429458375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.705197429458375 | validation: 8.766069080654116]
	TIME [epoch: 0.696 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.691435115257585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.691435115257585 | validation: 8.751363349497714]
	TIME [epoch: 0.692 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.678047945526075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.678047945526075 | validation: 8.736657447756915]
	TIME [epoch: 0.693 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.660323544635908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.660323544635908 | validation: 8.714950076913537]
	TIME [epoch: 0.693 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.641869568273979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.641869568273979 | validation: 8.689234326407023]
	TIME [epoch: 0.692 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.611740570666306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.611740570666306 | validation: 8.646581684167941]
	TIME [epoch: 0.692 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.572300380288167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.572300380288167 | validation: 8.573161677419455]
	TIME [epoch: 0.693 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.499342767262416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.499342767262416 | validation: 7.831692861006698]
	TIME [epoch: 0.692 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.731144733193586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.731144733193586 | validation: 8.67998697759188]
	TIME [epoch: 0.752 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.60299726659696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.60299726659696 | validation: 8.699018253732097]
	TIME [epoch: 0.694 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.621488021884995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.621488021884995 | validation: 8.692599052011436]
	TIME [epoch: 0.692 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.614118810692123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.614118810692123 | validation: 8.676542728174075]
	TIME [epoch: 0.695 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.599070415014683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.599070415014683 | validation: 8.647951205968747]
	TIME [epoch: 0.693 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.570101501847336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.570101501847336 | validation: 8.60076515839791]
	TIME [epoch: 0.689 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.530689310770267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.530689310770267 | validation: 8.544251549735879]
	TIME [epoch: 0.693 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.471366332726767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.471366332726767 | validation: 8.47801450453508]
	TIME [epoch: 0.694 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.406561593717562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.406561593717562 | validation: 8.274109761047098]
	TIME [epoch: 0.691 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.198913655652357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.198913655652357 | validation: 8.4358453166028]
	TIME [epoch: 0.69 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.354726767361328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.354726767361328 | validation: 8.425898191514738]
	TIME [epoch: 0.691 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.352812320121076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.352812320121076 | validation: 8.351516958766416]
	TIME [epoch: 0.703 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.274732090129623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.274732090129623 | validation: 8.108541892771203]
	TIME [epoch: 0.703 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.029841356227037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.029841356227037 | validation: 8.250126011162696]
	TIME [epoch: 0.704 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.181889486474182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.181889486474182 | validation: 8.019960478377781]
	TIME [epoch: 0.692 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.941943081302293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.941943081302293 | validation: 7.680657745893203]
	TIME [epoch: 0.694 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.594535138165067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.594535138165067 | validation: 4.174230712109275]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.069288300603057		[learning rate: 0.01]
nan encountered in epoch 50 (validation loss).
	Learning Rate: 0.01
	LOSS [training: 4.069288300603057 | validation: nan]
	TIME [epoch: 0.77 sec]
EPOCH 51/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.00010000000000000003
		[batch 1/1] avg loss: 7.287506657054218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.287506657054218 | validation: 8.152091011540994]
	TIME [epoch: 170 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.073369635991407		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 8.073369635991407 | validation: 7.448766106193142]
	TIME [epoch: 0.74 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.329045352076449		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 7.329045352076449 | validation: 8.28026742375635]
	TIME [epoch: 0.691 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.199216684228432		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 8.199216684228432 | validation: 8.281505821719927]
	TIME [epoch: 0.692 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.196445484873356		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 8.196445484873356 | validation: 8.184610858191823]
	TIME [epoch: 0.688 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.112232898438052		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 8.112232898438052 | validation: 7.744178899724308]
	TIME [epoch: 0.689 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.661466354952211		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 7.661466354952211 | validation: 8.250957827460448]
	TIME [epoch: 0.689 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.180563060519136		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 8.180563060519136 | validation: 8.220756199060858]
	TIME [epoch: 0.691 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.145772469501335		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 8.145772469501335 | validation: 8.100522851269131]
	TIME [epoch: 0.69 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.028850682352232		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 8.028850682352232 | validation: 6.9447879006134805]
	TIME [epoch: 0.687 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.851291676791829		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 6.851291676791829 | validation: 7.637261285190771]
	TIME [epoch: 0.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.535791610343063		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 7.535791610343063 | validation: 7.996041277520231]
	TIME [epoch: 0.691 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.92353385700729		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 7.92353385700729 | validation: 7.912270789185531]
	TIME [epoch: 0.691 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.845255741370267		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 7.845255741370267 | validation: 7.070694561377852]
	TIME [epoch: 0.688 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.966641657728746		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 6.966641657728746 | validation: 7.954668912906192]
	TIME [epoch: 0.688 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.883939884130876		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 7.883939884130876 | validation: 7.833921652059819]
	TIME [epoch: 0.688 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.767544401406512		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 7.767544401406512 | validation: 6.96658396453575]
	TIME [epoch: 0.69 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.860236089597077		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 6.860236089597077 | validation: 7.841100675737227]
	TIME [epoch: 0.688 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.769487260640733		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 7.769487260640733 | validation: 7.7395479131185745]
	TIME [epoch: 0.69 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.667009820990556		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 7.667009820990556 | validation: 6.793851924607161]
	TIME [epoch: 0.689 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.672246763823275		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 6.672246763823275 | validation: 7.671627474318594]
	TIME [epoch: 0.691 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.588261537583235		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 7.588261537583235 | validation: 7.605392401442652]
	TIME [epoch: 0.691 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.51691366766749		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 7.51691366766749 | validation: 7.874634462744968]
	TIME [epoch: 0.688 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.796314925235784		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 7.796314925235784 | validation: 7.806542212182807]
	TIME [epoch: 0.69 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.71694446212187		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 7.71694446212187 | validation: 7.687036990696825]
	TIME [epoch: 0.691 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.606485133071174		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 7.606485133071174 | validation: 7.19883946426867]
	TIME [epoch: 0.701 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.122981463394017		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 7.122981463394017 | validation: 4.332000487353425]
	TIME [epoch: 0.688 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.252065429158253		[learning rate: 0.0090879]
nan encountered in epoch 78 (validation loss).
	Learning Rate: 0.00908786
	LOSS [training: 4.252065429158253 | validation: nan]
	TIME [epoch: 0.689 sec]
EPOCH 79/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 1.0000000000000004e-05
		[batch 1/1] avg loss: 7.767908179288768		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 7.767908179288768 | validation: 7.812190723741347]
	TIME [epoch: 172 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.736425214071919		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 7.736425214071919 | validation: 7.56999790681161]
	TIME [epoch: 0.761 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.49348593252801		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 7.49348593252801 | validation: 8.0785711724773]
	TIME [epoch: 0.695 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 8.007352951031267		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 8.007352951031267 | validation: 7.234694035778439]
	TIME [epoch: 0.694 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.1694327372261135		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 7.1694327372261135 | validation: 7.8534966529561645]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.770621576088372		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 7.770621576088372 | validation: 7.816589846996989]
	TIME [epoch: 0.692 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.7378317221307364		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 7.7378317221307364 | validation: 7.7194443878923344]
	TIME [epoch: 0.693 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.628634613957505		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 7.628634613957505 | validation: 7.123480031508314]
	TIME [epoch: 0.697 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.040681188578542		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 7.040681188578542 | validation: 6.692359432564806]
	TIME [epoch: 0.694 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.595824155556843		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 6.595824155556843 | validation: 7.877426582035906]
	TIME [epoch: 0.695 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.806329196328274		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 7.806329196328274 | validation: 7.172851991626544]
	TIME [epoch: 0.694 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.089190825522997		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 7.089190825522997 | validation: 7.799114300459862]
	TIME [epoch: 0.695 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.716048171224568		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 7.716048171224568 | validation: 6.844817093907387]
	TIME [epoch: 0.694 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.75550780915858		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 6.75550780915858 | validation: 7.459330459980372]
	TIME [epoch: 0.691 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.396860517347431		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 7.396860517347431 | validation: 7.525241501997647]
	TIME [epoch: 0.699 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.4426477263101765		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 7.4426477263101765 | validation: 7.560842218955384]
	TIME [epoch: 0.694 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.476607637044538		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 7.476607637044538 | validation: 7.33492877798852]
	TIME [epoch: 0.693 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.267980371572567		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 7.267980371572567 | validation: 7.9246138199287515]
	TIME [epoch: 0.691 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.848198890637848		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 7.848198890637848 | validation: 6.49203814952802]
	TIME [epoch: 0.692 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.399276861409446		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 6.399276861409446 | validation: 7.8025987067228275]
	TIME [epoch: 0.694 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.728720114670681		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 7.728720114670681 | validation: 7.952514350009469]
	TIME [epoch: 0.694 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.866692551792752		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 7.866692551792752 | validation: 7.460020573368834]
	TIME [epoch: 0.693 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.386555430785846		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 7.386555430785846 | validation: 7.462673134907874]
	TIME [epoch: 0.694 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.386668271206652		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 7.386668271206652 | validation: 7.267946283945611]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.201607764954924		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 7.201607764954924 | validation: 7.852019565901961]
	TIME [epoch: 0.688 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.769089976400959		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 7.769089976400959 | validation: 6.693154696177489]
	TIME [epoch: 0.687 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.621839814085242		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 6.621839814085242 | validation: 7.884917684592594]
	TIME [epoch: 0.702 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.8076979820802626		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 7.8076979820802626 | validation: 7.474156437736176]
	TIME [epoch: 0.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.4001767857084495		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 7.4001767857084495 | validation: 7.309976093464667]
	TIME [epoch: 0.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.226476950664289		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 7.226476950664289 | validation: 7.295101205365551]
	TIME [epoch: 0.688 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.231961959897244		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 7.231961959897244 | validation: 6.978711391242219]
	TIME [epoch: 0.688 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.913889026574493		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 6.913889026574493 | validation: 7.469964004762758]
	TIME [epoch: 0.691 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.3904410802388805		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 7.3904410802388805 | validation: 6.971472184076102]
	TIME [epoch: 0.688 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.901714785501367		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 6.901714785501367 | validation: 6.913055998525046]
	TIME [epoch: 0.701 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.829873861618495		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 6.829873861618495 | validation: 7.631058090450543]
	TIME [epoch: 0.688 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.5622309335107865		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 7.5622309335107865 | validation: 6.475135166738628]
	TIME [epoch: 0.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.410792921686826		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 6.410792921686826 | validation: 7.44352893028227]
	TIME [epoch: 0.692 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.372161736193193		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 7.372161736193193 | validation: 5.940726374045051]
	TIME [epoch: 0.687 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.842948899681883		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 5.842948899681883 | validation: 7.322027563392645]
	TIME [epoch: 0.688 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.248598467623225		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 7.248598467623225 | validation: 6.743112675011496]
	TIME [epoch: 0.69 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.685215767839784		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 6.685215767839784 | validation: 6.498183385732364]
	TIME [epoch: 0.701 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.439855597688499		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 6.439855597688499 | validation: 7.318653213800208]
	TIME [epoch: 0.689 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.2436869283667304		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 7.2436869283667304 | validation: 6.175607040577671]
	TIME [epoch: 0.689 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.101373937359633		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 6.101373937359633 | validation: 7.2551237079226185]
	TIME [epoch: 0.69 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.18384170748783		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 7.18384170748783 | validation: 6.185943479285525]
	TIME [epoch: 0.701 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.109874518352649		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 6.109874518352649 | validation: 7.270741844664727]
	TIME [epoch: 0.687 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.192077828717607		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 7.192077828717607 | validation: 7.254769878758797]
	TIME [epoch: 0.688 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.182946104488413		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 7.182946104488413 | validation: 5.702121826750731]
	TIME [epoch: 0.692 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.611479866828345		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 5.611479866828345 | validation: 7.135646082498209]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.063054767647079		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 7.063054767647079 | validation: 6.350262972544899]
	TIME [epoch: 0.689 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.277968259779137		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 6.277968259779137 | validation: 5.6055044514815595]
	TIME [epoch: 0.691 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.52467253331514		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 5.52467253331514 | validation: 5.677762013230134]
	TIME [epoch: 0.695 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.59219638841521		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 5.59219638841521 | validation: 6.5977366516709]
	TIME [epoch: 0.688 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.512257249900686		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 6.512257249900686 | validation: 6.233754873861854]
	TIME [epoch: 0.696 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.155234913335221		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 6.155234913335221 | validation: 7.10471946544302]
	TIME [epoch: 0.705 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.038840051101082		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 7.038840051101082 | validation: 7.158203790558956]
	TIME [epoch: 0.702 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.091324116533855		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 7.091324116533855 | validation: 6.44200106816199]
	TIME [epoch: 0.688 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.366205027707791		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 6.366205027707791 | validation: 7.007747591990054]
	TIME [epoch: 0.688 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.938594802106346		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 6.938594802106346 | validation: 6.219040240918962]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.144064796364934		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 6.144064796364934 | validation: 5.548145804868411]
	TIME [epoch: 0.69 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.472338803266581		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 5.472338803266581 | validation: 6.06246467020064]
	TIME [epoch: 0.691 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.979543706413336		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 5.979543706413336 | validation: 6.4894119815629985]
	TIME [epoch: 0.688 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.406668176577993		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 6.406668176577993 | validation: 6.503065786658441]
	TIME [epoch: 0.7 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.419138552987797		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 6.419138552987797 | validation: 5.968529877732333]
	TIME [epoch: 0.691 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.877410265691688		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 5.877410265691688 | validation: 6.252378301098716]
	TIME [epoch: 0.688 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.163841415657108		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 6.163841415657108 | validation: 5.762409997332519]
	TIME [epoch: 0.705 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.686568958351968		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 5.686568958351968 | validation: 4.774860436853079]
	TIME [epoch: 0.689 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.701407692887784		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 4.701407692887784 | validation: 4.8766880900330705]
	TIME [epoch: 0.689 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.803742518003106		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 4.803742518003106 | validation: 4.8408780259174256]
	TIME [epoch: 0.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.758245796558433		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 4.758245796558433 | validation: 4.73167654040594]
	TIME [epoch: 0.688 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.647630868573068		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 4.647630868573068 | validation: 4.16559387609633]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.108089698832833		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 4.108089698832833 | validation: 4.8875109701372175]
	TIME [epoch: 0.697 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.807911992755794		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 4.807911992755794 | validation: 4.850681749294061]
	TIME [epoch: 0.696 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.785441605806998		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 4.785441605806998 | validation: 4.747367425640834]
	TIME [epoch: 0.695 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.692326124297791		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 4.692326124297791 | validation: 4.336229543188108]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.30134254272566		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 4.30134254272566 | validation: 4.86202339138247]
	TIME [epoch: 0.693 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.784428699041753		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 4.784428699041753 | validation: 4.821330618590487]
	TIME [epoch: 0.691 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.750462704990521		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 4.750462704990521 | validation: 4.7482060546310985]
	TIME [epoch: 0.693 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.663957479388735		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 4.663957479388735 | validation: 4.627746565934031]
	TIME [epoch: 0.698 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.550059303550218		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 4.550059303550218 | validation: 4.338165546072909]
	TIME [epoch: 0.702 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.230049441807891		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 4.230049441807891 | validation: 4.825336877778927]
	TIME [epoch: 0.692 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.745006500652967		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 4.745006500652967 | validation: 4.8563809806127045]
	TIME [epoch: 0.689 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.768842764318242		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 4.768842764318242 | validation: 4.823762262339592]
	TIME [epoch: 0.688 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.754408507922836		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 4.754408507922836 | validation: 4.802193524065531]
	TIME [epoch: 0.688 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.732752833736982		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 4.732752833736982 | validation: 4.778767659616477]
	TIME [epoch: 0.687 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.711544687389457		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 4.711544687389457 | validation: 4.766788965556851]
	TIME [epoch: 0.692 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.692853172742988		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 4.692853172742988 | validation: 4.7473143151334884]
	TIME [epoch: 0.701 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.675417694892391		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 4.675417694892391 | validation: 4.718120583309697]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.653152428050945		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 4.653152428050945 | validation: 4.681973394680958]
	TIME [epoch: 0.69 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.631224529009587		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 4.631224529009587 | validation: 4.646820309810999]
	TIME [epoch: 0.691 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.589693045860627		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 4.589693045860627 | validation: 4.580943260711446]
	TIME [epoch: 0.719 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.523552160135693		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 4.523552160135693 | validation: 4.578534697821879]
	TIME [epoch: 0.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5489361522413505		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 4.5489361522413505 | validation: 4.736483244550713]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.676961633092597		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 4.676961633092597 | validation: 4.709102240537894]
	TIME [epoch: 0.693 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.631983240381912		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 4.631983240381912 | validation: 4.589123187228615]
	TIME [epoch: 0.694 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.510636228250274		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 4.510636228250274 | validation: 4.512453155477607]
	TIME [epoch: 0.69 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.426402835931443		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 4.426402835931443 | validation: 5.413603185228637]
	TIME [epoch: 0.69 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.3419355526326875		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 5.3419355526326875 | validation: 6.162581699905461]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.089024743951198		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 6.089024743951198 | validation: 6.223554345963255]
	TIME [epoch: 0.704 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.148600087234627		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 6.148600087234627 | validation: 6.139732141568583]
	TIME [epoch: 0.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.06816138209472		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 6.06816138209472 | validation: 5.824417846991385]
	TIME [epoch: 0.691 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.740469956293218		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 5.740469956293218 | validation: 6.305450101875741]
	TIME [epoch: 0.692 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.2206140021373315		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 6.2206140021373315 | validation: 5.619040945645519]
	TIME [epoch: 0.703 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.54235707716962		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 5.54235707716962 | validation: 5.453863503240401]
	TIME [epoch: 0.689 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.392690426087324		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 5.392690426087324 | validation: 6.458632725249378]
	TIME [epoch: 0.732 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.379427147666875		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 6.379427147666875 | validation: 5.02068603340015]
	TIME [epoch: 0.699 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.932590030634095		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 4.932590030634095 | validation: 5.904896755252613]
	TIME [epoch: 0.699 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.817695029789684		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 5.817695029789684 | validation: 5.870013604763461]
	TIME [epoch: 0.69 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.787975639893605		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 5.787975639893605 | validation: 5.672471791056873]
	TIME [epoch: 0.693 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.597503569316573		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 5.597503569316573 | validation: 4.865686050350845]
	TIME [epoch: 0.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.792058155993682		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 4.792058155993682 | validation: 4.974994479418078]
	TIME [epoch: 0.69 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.89845018589648		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 4.89845018589648 | validation: 5.841753103929091]
	TIME [epoch: 0.747 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.776263196410317		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 5.776263196410317 | validation: 5.4538822478719595]
	TIME [epoch: 0.691 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.369747769490991		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 5.369747769490991 | validation: 5.734781432471999]
	TIME [epoch: 0.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.639176328385311		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 5.639176328385311 | validation: 5.237451451821091]
	TIME [epoch: 0.692 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.157050527823312		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 5.157050527823312 | validation: 5.375286753918979]
	TIME [epoch: 0.697 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.308872274654773		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 5.308872274654773 | validation: 5.263213336653437]
	TIME [epoch: 0.7 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.19038025362428		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 5.19038025362428 | validation: 5.353199848834397]
	TIME [epoch: 0.69 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2764252094149455		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 5.2764252094149455 | validation: 4.806329317422693]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.732165058871393		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 4.732165058871393 | validation: 5.047598886786512]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.974389654659117		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 4.974389654659117 | validation: 5.460407544940779]
	TIME [epoch: 0.693 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.36738492531804		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 5.36738492531804 | validation: 5.375066796118805]
	TIME [epoch: 0.693 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.9687197978077435		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 5.9687197978077435 | validation: 6.007320900366773]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.93010137948135		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 5.93010137948135 | validation: 5.268091770585368]
	TIME [epoch: 1.44 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.197650320056421		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 5.197650320056421 | validation: 6.952738402497968]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.868128258282742		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 6.868128258282742 | validation: 5.765901573670567]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.683447344523578		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 5.683447344523578 | validation: 5.68260455433604]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.577646040394938		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 5.577646040394938 | validation: 6.504149581848819]
	TIME [epoch: 1.37 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.446907580524634		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 6.446907580524634 | validation: 5.278379717414983]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.204036547763523		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 5.204036547763523 | validation: 5.907657165286159]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.852392311717519		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 5.852392311717519 | validation: 5.756742252720587]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.682721392717476		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 5.682721392717476 | validation: 5.649153033844975]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.582598903513654		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 5.582598903513654 | validation: 4.854077621179334]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.781156965264444		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 4.781156965264444 | validation: 6.750159869687619]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.681049617931012		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 6.681049617931012 | validation: 6.310012911247353]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.214241430171271		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 6.214241430171271 | validation: 5.860445526022696]
	TIME [epoch: 1.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.788504581334082		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 5.788504581334082 | validation: 5.887563522760338]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.80414602430957		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 5.80414602430957 | validation: 5.626469695380433]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.555712457847624		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 5.555712457847624 | validation: 5.963728427525843]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.870345913859276		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 5.870345913859276 | validation: 5.392451307447965]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.31169206340435		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 5.31169206340435 | validation: 4.94521174551302]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.852438623500119		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 4.852438623500119 | validation: 6.503296452337842]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.4288060244976535		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 6.4288060244976535 | validation: 5.059714730221081]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.972578116263336		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 4.972578116263336 | validation: 5.6553753540948]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.564842426431328		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 5.564842426431328 | validation: 5.570778681969593]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.493611150710775		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 5.493611150710775 | validation: 4.9018968714323226]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.831894063003271		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 4.831894063003271 | validation: 6.3982255830547405]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.341525878371717		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 6.341525878371717 | validation: 5.234251068850228]
	TIME [epoch: 1.36 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.170832373562089		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 5.170832373562089 | validation: 4.996932727145362]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.939994328424382		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 4.939994328424382 | validation: 6.350908768384323]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.28063246599138		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 6.28063246599138 | validation: 6.1779171778208095]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.088886189125585		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 6.088886189125585 | validation: 5.193088727013381]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.12008115443654		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 5.12008115443654 | validation: 4.899437859886286]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.832763410478462		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 4.832763410478462 | validation: 6.237204614839124]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.165803078496199		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 6.165803078496199 | validation: 6.064835497924372]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.996453250532982		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 5.996453250532982 | validation: 5.037865699509332]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.973205967791491		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 4.973205967791491 | validation: 4.791607536779774]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.710233327535299		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 4.710233327535299 | validation: 6.1038593227951585]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.023723374477375		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 6.023723374477375 | validation: 4.705551036999102]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6331438884377665		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 4.6331438884377665 | validation: 4.930794736822491]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.842193249448559		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 4.842193249448559 | validation: 4.691621916431138]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.618321380108357		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 4.618321380108357 | validation: 5.020105939750348]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.943213172956482		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 4.943213172956482 | validation: 4.741144174211863]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.668959784314078		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 4.668959784314078 | validation: 4.690880451556626]
	TIME [epoch: 1.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.600684660948314		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 4.600684660948314 | validation: 4.690802363781231]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.611587849913072		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 4.611587849913072 | validation: 4.621116646117936]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.54288612485618		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 4.54288612485618 | validation: 4.721336930192073]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.640517758007698		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 4.640517758007698 | validation: 4.6105881494805585]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.532533377812152		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 4.532533377812152 | validation: 4.728756773159637]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.656768243453343		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 4.656768243453343 | validation: 4.602001147058123]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.524798937979224		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 4.524798937979224 | validation: 4.73068963866401]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6564135439406265		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 4.6564135439406265 | validation: 4.609604093876131]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.534854704536106		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 4.534854704536106 | validation: 4.689918393579418]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.610240649736409		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 4.610240649736409 | validation: 4.587357693943455]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.515208652402906		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 4.515208652402906 | validation: 4.665807288992774]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.591939078892192		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 4.591939078892192 | validation: 4.580374457024831]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.506382344795352		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 4.506382344795352 | validation: 4.647231913243309]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.573004220494893		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 4.573004220494893 | validation: 4.566932427746972]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.491063244156159		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 4.491063244156159 | validation: 4.63817249226495]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.553462711149353		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 4.553462711149353 | validation: 4.558205975138015]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.482137482109388		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 4.482137482109388 | validation: 4.607394140643424]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.53586864416073		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 4.53586864416073 | validation: 4.540242699657728]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.466113877793407		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 4.466113877793407 | validation: 4.593688346204695]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.516536049347541		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 4.516536049347541 | validation: 4.531335498021817]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.455009168905912		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 4.455009168905912 | validation: 4.5614386692602835]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4934211308443075		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 4.4934211308443075 | validation: 4.514410273225495]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.442595658268902		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 4.442595658268902 | validation: 4.531392188373149]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.453292321591964		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 4.453292321591964 | validation: 4.516481261015207]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.43711819008761		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 4.43711819008761 | validation: 4.509301840423361]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.429697875278708		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 4.429697875278708 | validation: 4.498345548168945]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.424386509680688		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 4.424386509680688 | validation: 4.498046148784262]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.419221409582242		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 4.419221409582242 | validation: 4.489804002826296]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.416780547701866		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 4.416780547701866 | validation: 4.485811388843776]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.410036465839416		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 4.410036465839416 | validation: 4.477381792906854]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.405321094949864		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 4.405321094949864 | validation: 4.478836102310673]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.403315553893553		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 4.403315553893553 | validation: 4.476311861347155]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.398642389847698		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 4.398642389847698 | validation: 4.468699913324864]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.395802561036095		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 4.395802561036095 | validation: 4.4592581793135135]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.390177451263355		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 4.390177451263355 | validation: 4.459758678771443]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.38631073222902		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 4.38631073222902 | validation: 4.461853957122642]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.384340355293331		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 4.384340355293331 | validation: 4.456049368816479]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.375101852254328		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 4.375101852254328 | validation: 4.448693653432088]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.373275665340816		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 4.373275665340816 | validation: 4.4439357292844885]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.367515276495147		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 4.367515276495147 | validation: 4.4391405854361325]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.362328571702624		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 4.362328571702624 | validation: 4.429644931922684]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.358592374460996		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 4.358592374460996 | validation: 4.427856168177034]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.352742838510224		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 4.352742838510224 | validation: 4.423585830887385]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.348631814454513		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 4.348631814454513 | validation: 4.419025330131762]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342666474898399		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 4.342666474898399 | validation: 4.418415387848829]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.336939121344318		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 4.336939121344318 | validation: 4.403318796808569]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.330896756093455		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 4.330896756093455 | validation: 4.400749319000309]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.326888423192818		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 4.326888423192818 | validation: 4.393323808609219]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.320520456788383		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 4.320520456788383 | validation: 4.385049377849046]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.310405841838558		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 4.310405841838558 | validation: 4.377256268798006]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3012528153352365		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 4.3012528153352365 | validation: 4.363028050886105]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.29270194685196		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 4.29270194685196 | validation: 4.351490699845296]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.27550974650599		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 4.27550974650599 | validation: 4.323994724714301]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2462946023437995		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 4.2462946023437995 | validation: 4.189801152046395]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.10928866362479		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 4.10928866362479 | validation: 4.1756297009228245]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.100620131010215		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 4.100620131010215 | validation: 4.167314916242931]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.091794466786743		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 4.091794466786743 | validation: 4.158077702508253]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.083452128883294		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 4.083452128883294 | validation: 4.15150454115466]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.076240703383248		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 4.076240703383248 | validation: 4.142325381781832]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.067095702372592		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 4.067095702372592 | validation: 4.137414515662869]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.06111902475307		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 4.06111902475307 | validation: 4.130407647386919]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.055877199859399		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 4.055877199859399 | validation: 4.125668639099236]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.05058370149179		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 4.05058370149179 | validation: 4.122218955672213]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.046910976063436		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 4.046910976063436 | validation: 4.119708985531702]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0444297780012715		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 4.0444297780012715 | validation: 4.118099185025119]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.042442944479328		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 4.042442944479328 | validation: 4.114973505315393]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.04069217587855		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 4.04069217587855 | validation: 4.113428667557848]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.039656979592114		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 4.039656979592114 | validation: 4.11685749840801]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.040938245465588		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 4.040938245465588 | validation: 4.111385165970581]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0368709781010885		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 4.0368709781010885 | validation: 4.111558711400237]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.035732857557025		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 4.035732857557025 | validation: 4.110421130651168]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.034605759961022		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 4.034605759961022 | validation: 4.109963109108212]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.034464154514216		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 4.034464154514216 | validation: 4.109918247472133]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.034868248181918		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 4.034868248181918 | validation: 4.108951397897402]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.032632181258639		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 4.032632181258639 | validation: 4.106662887364746]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.031939787859783		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 4.031939787859783 | validation: 4.106835055195286]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.030987046466904		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 4.030987046466904 | validation: 4.10581994521157]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.030523378244405		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 4.030523378244405 | validation: 4.105901343106092]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02992415725004		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 4.02992415725004 | validation: 4.105414565648084]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.030254245901558		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 4.030254245901558 | validation: 4.108040786525104]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.032705912934444		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 4.032705912934444 | validation: 4.1031810776286735]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02836763277687		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 4.02836763277687 | validation: 4.105798955017106]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.03196532032142		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 4.03196532032142 | validation: 4.108760035572676]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0329079971059665		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 4.0329079971059665 | validation: 4.107685478943486]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.031565947414483		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 4.031565947414483 | validation: 4.101611687801955]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026680411425385		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 4.026680411425385 | validation: 4.103238437233027]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028801239703579		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 4.028801239703579 | validation: 4.103857283310174]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0278689050242695		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 4.0278689050242695 | validation: 4.1024483140019985]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026575558259711		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 4.026575558259711 | validation: 4.100035893084065]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025481034040879		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 4.025481034040879 | validation: 4.0990940430442935]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.023986176019132		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 4.023986176019132 | validation: 4.10127499428515]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024120648938387		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 4.024120648938387 | validation: 4.099059514125886]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.023381274639288		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 4.023381274639288 | validation: 4.099032063867969]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.022326637386542		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 4.022326637386542 | validation: 4.098346222612054]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02243365470791		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 4.02243365470791 | validation: 4.09771330619034]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021731140240488		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 4.021731140240488 | validation: 4.097005639129139]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021656560595598		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 4.021656560595598 | validation: 4.097228712712031]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021302396089232		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 4.021302396089232 | validation: 4.098593637849436]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.022079976638001		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 4.022079976638001 | validation: 4.096820246041575]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020613157882364		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 4.020613157882364 | validation: 4.096333084355546]
	TIME [epoch: 1.46 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.019954269148197		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 4.019954269148197 | validation: 4.095596278251495]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01968900466435		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 4.01968900466435 | validation: 4.096491294464321]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020311462044206		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 4.020311462044206 | validation: 4.094805091765612]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018655677957125		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 4.018655677957125 | validation: 4.0938561218175]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018122786772713		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 4.018122786772713 | validation: 4.0931166742317675]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017654741010589		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 4.017654741010589 | validation: 4.093989880134206]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017732805762589		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 4.017732805762589 | validation: 4.093433493186176]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017190707745251		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 4.017190707745251 | validation: 4.093538581508326]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01877923398733		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 4.01877923398733 | validation: 4.096905880737314]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021374594143441		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 4.021374594143441 | validation: 4.093227148641392]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017523763672493		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 4.017523763672493 | validation: 4.096071762462745]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0200604376434725		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 4.0200604376434725 | validation: 4.093145183727053]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0179004346216525		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 4.0179004346216525 | validation: 4.092064962835461]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016605079791088		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 4.016605079791088 | validation: 4.092405259891529]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016017810565058		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 4.016017810565058 | validation: 4.0901574782396235]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014367399546753		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 4.014367399546753 | validation: 4.090741678860413]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014554274534843		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 4.014554274534843 | validation: 4.089868384067569]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013729640434512		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 4.013729640434512 | validation: 4.092789525612769]
	TIME [epoch: 1.42 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016976857555975		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 4.016976857555975 | validation: 4.090735553068955]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0151306165778236		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 4.0151306165778236 | validation: 4.089419051280895]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014003690354342		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 4.014003690354342 | validation: 4.089041466540962]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013401389147394		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 4.013401389147394 | validation: 4.088065886526145]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0125860202452746		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 4.0125860202452746 | validation: 4.087672439383368]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012789905216896		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 4.012789905216896 | validation: 4.08819951284572]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012037307114829		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 4.012037307114829 | validation: 4.088635438886306]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012996847733549		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 4.012996847733549 | validation: 4.086962938186604]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011329713295102		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 4.011329713295102 | validation: 4.087335750126559]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011195989285232		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 4.011195989285232 | validation: 4.086771691545607]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010719334901898		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 4.010719334901898 | validation: 4.086250362375925]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010214606705839		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 4.010214606705839 | validation: 4.085080955584212]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010557622810812		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 4.010557622810812 | validation: 4.085736507359913]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010032962519233		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 4.010032962519233 | validation: 4.0872204793563345]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011017352979411		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 4.011017352979411 | validation: 4.086443125292702]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010496521981394		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 4.010496521981394 | validation: 4.085197384491476]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009203496845884		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 4.009203496845884 | validation: 4.085691305253846]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0095731169609925		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 4.0095731169609925 | validation: 4.085928983392515]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011189358091549		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 4.011189358091549 | validation: 4.084413715320548]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008549161776795		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 4.008549161776795 | validation: 4.084436705814191]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009163576156971		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 4.009163576156971 | validation: 4.093479060122945]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017638640040865		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 4.017638640040865 | validation: 4.091407779805091]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0148235704976205		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 4.0148235704976205 | validation: 4.090599829633648]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014406618573422		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 4.014406618573422 | validation: 4.0924459924871845]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016749087919248		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 4.016749087919248 | validation: 4.092028523848633]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016830585948126		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 4.016830585948126 | validation: 4.086346122107868]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010610765684606		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 4.010610765684606 | validation: 4.084598558270325]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009129463030979		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 4.009129463030979 | validation: 4.085097930023095]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009464935602365		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 4.009464935602365 | validation: 4.084479085260009]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008073833422871		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 4.008073833422871 | validation: 4.083408497401586]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007134011670654		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 4.007134011670654 | validation: 4.0830381743319135]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006633080808989		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 4.006633080808989 | validation: 4.082121994673483]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0068034125558505		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 4.0068034125558505 | validation: 4.082746359131206]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006941170851629		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 4.006941170851629 | validation: 4.0873657306459865]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011408989030466		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 4.011408989030466 | validation: 4.09425994596537]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018083330869432		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 4.018083330869432 | validation: 4.087254284227656]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01084108518748		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 4.01084108518748 | validation: 4.08356651640235]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007085436272795		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 4.007085436272795 | validation: 4.084869180432613]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008386198196839		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 4.008386198196839 | validation: 4.084124119654642]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009367438194832		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 4.009367438194832 | validation: 4.082189237160894]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00608912832172		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 4.00608912832172 | validation: 4.082200100081525]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005874227396905		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 4.005874227396905 | validation: 4.08168214516886]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005530403719438		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 4.005530403719438 | validation: 4.081745589256222]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006632236553408		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 4.006632236553408 | validation: 4.081979317543184]
	TIME [epoch: 1.45 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005649900645852		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 4.005649900645852 | validation: 4.081645211828035]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0057687151859165		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 4.0057687151859165 | validation: 4.088517848249012]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013028892328244		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 4.013028892328244 | validation: 4.091792241454672]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014435708372615		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 4.014435708372615 | validation: 4.09017830392005]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013277863946218		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 4.013277863946218 | validation: 4.0817748731799695]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006377140522721		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 4.006377140522721 | validation: 4.082250100370266]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006283387472619		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 4.006283387472619 | validation: 4.084131151485736]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008309971617745		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 4.008309971617745 | validation: 4.084753363816168]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009325948010505		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 4.009325948010505 | validation: 4.085745367962406]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009394016015906		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 4.009394016015906 | validation: 4.085260605464046]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010693498462123		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 4.010693498462123 | validation: 4.086513749713322]
	TIME [epoch: 1.35 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010711862342957		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 4.010711862342957 | validation: 4.086367154561427]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0110521439960465		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 4.0110521439960465 | validation: 4.086257761621316]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010847513886907		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 4.010847513886907 | validation: 4.086583919249035]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0109407391065925		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 4.0109407391065925 | validation: 4.085758051663507]
	TIME [epoch: 1.35 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009437267589251		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 4.009437267589251 | validation: 4.084903622566889]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00870652958341		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 4.00870652958341 | validation: 4.084392056826481]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0085540281955945		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 4.0085540281955945 | validation: 4.088196755888712]
	TIME [epoch: 1.35 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013129428302244		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 4.013129428302244 | validation: 4.092620948086259]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016293096172489		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 4.016293096172489 | validation: 4.094229537377692]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018178311094923		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 4.018178311094923 | validation: 4.0934627376877915]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017579089352036		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 4.017579089352036 | validation: 4.09341248514615]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0173041785641255		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 4.0173041785641255 | validation: 4.09293962518814]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017356608988668		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 4.017356608988668 | validation: 4.092703194328754]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016852125090209		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 4.016852125090209 | validation: 4.093405740576732]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017193123424225		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 4.017193123424225 | validation: 4.092688855107144]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01695633310182		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 4.01695633310182 | validation: 4.092379335133582]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016570618695698		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 4.016570618695698 | validation: 4.091463011770357]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016203531511795		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 4.016203531511795 | validation: 4.091420599348387]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015841658743254		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 4.015841658743254 | validation: 4.090260952747939]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015463676297741		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 4.015463676297741 | validation: 4.090244248565736]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015018014877159		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 4.015018014877159 | validation: 4.090488370097036]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014916500741733		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 4.014916500741733 | validation: 4.090373683827082]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0146435177305		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 4.0146435177305 | validation: 4.089332676856555]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014106281728995		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 4.014106281728995 | validation: 4.089133008924578]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013724475549795		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 4.013724475549795 | validation: 4.08805971988736]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0131771389867765		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 4.0131771389867765 | validation: 4.0903102478027815]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014830177119751		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 4.014830177119751 | validation: 4.091838675607794]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015855986932939		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 4.015855986932939 | validation: 4.091441648716996]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016209545347925		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 4.016209545347925 | validation: 4.091844669367639]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016126462103588		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 4.016126462103588 | validation: 4.0917939325138155]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016012373758831		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 4.016012373758831 | validation: 4.091420850222998]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015638295435579		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 4.015638295435579 | validation: 4.090406451204887]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015388304907314		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 4.015388304907314 | validation: 4.090320285611123]
	TIME [epoch: 1.35 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0153719290940515		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 4.0153719290940515 | validation: 4.090218078417112]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014790800905058		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 4.014790800905058 | validation: 4.089939356751806]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013977427185715		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 4.013977427185715 | validation: 4.089802755241365]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013619171096651		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 4.013619171096651 | validation: 4.088246295758675]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013252211095074		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 4.013252211095074 | validation: 4.087365288241419]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012838179956149		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 4.012838179956149 | validation: 4.088083143811659]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012580782267567		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 4.012580782267567 | validation: 4.083823376643353]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008226019725002		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 4.008226019725002 | validation: 4.0811489079245575]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005714009994049		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 4.005714009994049 | validation: 4.080744126445006]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004810149196952		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 4.004810149196952 | validation: 4.0811848995207525]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005710517764831		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 4.005710517764831 | validation: 4.082540930076741]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006421586947744		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 4.006421586947744 | validation: 4.082587827472641]
	TIME [epoch: 1.43 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00658165013566		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 4.00658165013566 | validation: 4.086459977389547]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010501280042026		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 4.010501280042026 | validation: 4.088987082234675]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01303668186455		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 4.01303668186455 | validation: 4.088915235831778]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0135410845974135		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 4.0135410845974135 | validation: 4.088862254429289]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0135833440156565		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 4.0135833440156565 | validation: 4.089873705464027]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01455152594106		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 4.01455152594106 | validation: 4.093287871662233]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018389425976269		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 4.018389425976269 | validation: 4.096199913602874]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020329802707458		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 4.020329802707458 | validation: 4.0971013000043355]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021288104097634		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 4.021288104097634 | validation: 4.096304563666343]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021287199408915		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 4.021287199408915 | validation: 4.096078906717392]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020698678454212		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 4.020698678454212 | validation: 4.095593478446463]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020471243401963		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 4.020471243401963 | validation: 4.094934179231126]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.019708812544894		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 4.019708812544894 | validation: 4.094613523987825]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0192896422326365		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 4.0192896422326365 | validation: 4.095058977681532]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01883852806517		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 4.01883852806517 | validation: 4.093354398452369]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018013405791448		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 4.018013405791448 | validation: 4.092539239464865]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0175441205926505		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 4.0175441205926505 | validation: 4.092079825216359]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017102750020953		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 4.017102750020953 | validation: 4.09222039456312]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01626648370683		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 4.01626648370683 | validation: 4.0909004897332]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015765590191654		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 4.015765590191654 | validation: 4.090079713361274]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01485921425718		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 4.01485921425718 | validation: 4.089798283811617]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013952267599699		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 4.013952267599699 | validation: 4.088141873110146]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013328345476936		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 4.013328345476936 | validation: 4.088144043190539]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0121138331468895		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 4.0121138331468895 | validation: 4.086165650710569]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011370867129411		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 4.011370867129411 | validation: 4.0852962109458035]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0104362312943		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 4.0104362312943 | validation: 4.085569808305511]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0102268636229805		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 4.0102268636229805 | validation: 4.083835886954481]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008727430083217		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 4.008727430083217 | validation: 4.084213819307778]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008489993034389		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 4.008489993034389 | validation: 4.080460758471793]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005916072028721		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 4.005916072028721 | validation: 4.080874283127427]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00468782741707		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 4.00468782741707 | validation: 4.083827739927297]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008838641215163		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 4.008838641215163 | validation: 4.086762211646255]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011390363439994		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 4.011390363439994 | validation: 4.088658620031116]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012601328597364		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 4.012601328597364 | validation: 4.088313325002886]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01313172452599		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 4.01313172452599 | validation: 4.088716038862846]
	TIME [epoch: 1.38 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0137450022767585		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 4.0137450022767585 | validation: 4.089521287076006]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013926013909717		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 4.013926013909717 | validation: 4.0895857141812755]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014047492517759		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 4.014047492517759 | validation: 4.088539687193644]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013479315868093		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 4.013479315868093 | validation: 4.08906563184699]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013542909411545		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 4.013542909411545 | validation: 4.08844205668992]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00650266296398		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 4.00650266296398 | validation: 4.0868718035816745]
	TIME [epoch: 175 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010377583627509		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 4.010377583627509 | validation: 4.090028394011377]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.014350580491482		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 4.014350580491482 | validation: 4.093397413432717]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017489227535155		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 4.017489227535155 | validation: 4.097299966006182]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020792098821362		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 4.020792098821362 | validation: 4.098265987015269]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021664582507549		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 4.021664582507549 | validation: 4.096955355231326]
	TIME [epoch: 2.66 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021846538079756		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 4.021846538079756 | validation: 4.097631986018727]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.021736948454499		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 4.021736948454499 | validation: 4.097615670987337]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0215498778545715		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 4.0215498778545715 | validation: 4.099341502963263]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024196052514021		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 4.024196052514021 | validation: 4.101147441754963]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025316848352709		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 4.025316848352709 | validation: 4.101959279152321]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025974978104424		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 4.025974978104424 | validation: 4.10231464505518]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02640229964572		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 4.02640229964572 | validation: 4.102648691255758]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025916832786655		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 4.025916832786655 | validation: 4.102895430031948]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0262495174049615		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 4.0262495174049615 | validation: 4.101838370921492]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026018270028207		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 4.026018270028207 | validation: 4.101296710893841]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025786115220135		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 4.025786115220135 | validation: 4.101620551180373]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025482147025512		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 4.025482147025512 | validation: 4.101056591427478]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025525167155023		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 4.025525167155023 | validation: 4.100653800872686]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024784616763372		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 4.024784616763372 | validation: 4.101191448810162]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025140368180856		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 4.025140368180856 | validation: 4.100717093071642]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024965428117409		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 4.024965428117409 | validation: 4.1010110735706204]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0248272222812655		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 4.0248272222812655 | validation: 4.100718043245844]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024439227920685		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 4.024439227920685 | validation: 4.099961114502456]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024169430524117		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 4.024169430524117 | validation: 4.10287899131664]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0268195460071805		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 4.0268195460071805 | validation: 4.102449061327033]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027437929174639		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 4.027437929174639 | validation: 4.103113911725538]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027748014906782		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 4.027748014906782 | validation: 4.10281348937244]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0279844690999065		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 4.0279844690999065 | validation: 4.104245098442689]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027488129645101		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 4.027488129645101 | validation: 4.103272445342344]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028294194623381		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 4.028294194623381 | validation: 4.102988920496779]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028219245611906		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 4.028219245611906 | validation: 4.103525452271772]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027342353641614		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 4.027342353641614 | validation: 4.102419357355816]
	TIME [epoch: 2.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027735101057809		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 4.027735101057809 | validation: 4.102347148691126]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.027406875742075		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 4.027406875742075 | validation: 4.102220605572438]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026865229513366		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 4.026865229513366 | validation: 4.101297331650941]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026333999126283		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 4.026333999126283 | validation: 4.101856808559155]
	TIME [epoch: 2.73 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026362722873967		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 4.026362722873967 | validation: 4.100809747558636]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025535580080881		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 4.025535580080881 | validation: 4.101091549193761]
	TIME [epoch: 2.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024679597780143		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 4.024679597780143 | validation: 4.099364974792988]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024039271769616		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 4.024039271769616 | validation: 4.097498455460353]
	TIME [epoch: 2.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.022332753023732		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 4.022332753023732 | validation: 4.096644125285436]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020578148991134		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 4.020578148991134 | validation: 4.096373110842427]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020840791085291		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 4.020840791085291 | validation: 4.094564392577071]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.019929580173554		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 4.019929580173554 | validation: 4.094275182291156]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.019020240845828		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 4.019020240845828 | validation: 4.093355312345293]
	TIME [epoch: 2.71 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0176236547730495		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 4.0176236547730495 | validation: 4.0937850025582545]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01785508469167		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 4.01785508469167 | validation: 4.093434407299752]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.017185095786447		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 4.017185095786447 | validation: 4.092145736335001]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.016349416667372		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 4.016349416667372 | validation: 4.084623447564135]
	TIME [epoch: 2.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00915602005728		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 4.00915602005728 | validation: 4.082290666171286]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007199852100889		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 4.007199852100889 | validation: 4.081464572243125]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005521410517363		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 4.005521410517363 | validation: 4.082693596845133]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006856286998046		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 4.006856286998046 | validation: 4.082784695036173]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0072105630977575		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 4.0072105630977575 | validation: 4.084073711217515]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008696264880994		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 4.008696264880994 | validation: 4.0850291918272585]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009386971216016		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 4.009386971216016 | validation: 4.08507595730482]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009889112426405		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 4.009889112426405 | validation: 4.085407318398051]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009776467605201		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 4.009776467605201 | validation: 4.085262246163584]
	TIME [epoch: 2.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009515186258377		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 4.009515186258377 | validation: 4.084146860194359]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008941226391302		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 4.008941226391302 | validation: 4.084015171191001]
	TIME [epoch: 2.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009285257506536		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 4.009285257506536 | validation: 4.084383307341679]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009242990275813		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 4.009242990275813 | validation: 4.083352028650132]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008853824719474		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 4.008853824719474 | validation: 4.084393648226793]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008920214170096		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 4.008920214170096 | validation: 4.084324645247689]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008799008978363		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 4.008799008978363 | validation: 4.0836790899899285]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0084900241388945		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 4.0084900241388945 | validation: 4.083524351332344]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008262899417795		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 4.008262899417795 | validation: 4.0838286387472795]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008283987786062		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 4.008283987786062 | validation: 4.083586664580924]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008386604009781		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 4.008386604009781 | validation: 4.08351641207417]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008157538750355		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 4.008157538750355 | validation: 4.083299734901046]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007968179012986		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 4.007968179012986 | validation: 4.082911086053012]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007563725434158		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 4.007563725434158 | validation: 4.083250889809164]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007732687545763		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 4.007732687545763 | validation: 4.082840186414442]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0072574797020435		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 4.0072574797020435 | validation: 4.082533231918208]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0070635354886255		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 4.0070635354886255 | validation: 4.082717900034754]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007107589196659		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 4.007107589196659 | validation: 4.082439387664455]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006482385933859		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 4.006482385933859 | validation: 4.081659084642619]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0063426154148045		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 4.0063426154148045 | validation: 4.082101004400214]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006113856179426		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 4.006113856179426 | validation: 4.081385693627306]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006101707457902		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 4.006101707457902 | validation: 4.080855614740016]
	TIME [epoch: 2.67 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006009753667572		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 4.006009753667572 | validation: 4.080757705120125]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0056400787648565		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 4.0056400787648565 | validation: 4.080603391504616]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0056293990400595		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 4.0056293990400595 | validation: 4.081161538884867]
	TIME [epoch: 2.67 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0056698672271		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 4.0056698672271 | validation: 4.080799090363306]
	TIME [epoch: 2.78 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005270366620835		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 4.005270366620835 | validation: 4.080529339712611]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005305437135071		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 4.005305437135071 | validation: 4.0808535910205235]
	TIME [epoch: 2.69 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004971215590076		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 4.004971215590076 | validation: 4.080152001397602]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004976046765682		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 4.004976046765682 | validation: 4.080202103857839]
	TIME [epoch: 2.67 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005081909809465		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 4.005081909809465 | validation: 4.080431890621251]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005773201777142		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 4.005773201777142 | validation: 4.083921627093517]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00794690572239		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 4.00794690572239 | validation: 4.085113482884317]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009389973617541		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 4.009389973617541 | validation: 4.084914294339086]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009746518558749		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 4.009746518558749 | validation: 4.085931369353067]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010596117155857		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 4.010596117155857 | validation: 4.0851986999139625]
	TIME [epoch: 2.77 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0106030627338285		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 4.0106030627338285 | validation: 4.085535322115072]
	TIME [epoch: 2.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0102909819649675		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 4.0102909819649675 | validation: 4.084519159721603]
	TIME [epoch: 2.67 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009311450526557		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 4.009311450526557 | validation: 4.084425867082224]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008709976678517		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 4.008709976678517 | validation: 4.082824217532874]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007079524812886		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 4.007079524812886 | validation: 4.082159392274149]
	TIME [epoch: 2.79 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0071573633103075		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 4.0071573633103075 | validation: 4.082363198974302]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006861934778282		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 4.006861934778282 | validation: 4.082262536747051]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006861632049231		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 4.006861632049231 | validation: 4.082003259590237]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006230799927916		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 4.006230799927916 | validation: 4.082200252757014]
	TIME [epoch: 2.8 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006569656025291		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 4.006569656025291 | validation: 4.0801090078211715]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0042761851137145		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 4.0042761851137145 | validation: 4.0797739212291715]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003968734765828		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 4.003968734765828 | validation: 4.079278122105616]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0036883777894285		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 4.0036883777894285 | validation: 4.079830588764527]
	TIME [epoch: 2.67 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003666810026029		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 4.003666810026029 | validation: 4.079059728572839]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003437282861178		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 4.003437282861178 | validation: 4.078524740144971]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003551981092379		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 4.003551981092379 | validation: 4.07978478111057]
	TIME [epoch: 2.75 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00346616028029		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 4.00346616028029 | validation: 4.079295803281939]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0039169216642145		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 4.0039169216642145 | validation: 4.080122958280863]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00389055211198		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 4.00389055211198 | validation: 4.079615245182745]
	TIME [epoch: 2.67 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004066151009215		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 4.004066151009215 | validation: 4.080805611168252]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004378357023988		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 4.004378357023988 | validation: 4.08140008734562]
	TIME [epoch: 2.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005297347919317		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 4.005297347919317 | validation: 4.081143730233001]
	TIME [epoch: 2.68 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004788044474769		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 4.004788044474769 | validation: 4.080918340136104]
	TIME [epoch: 2.67 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004454220064916		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 4.004454220064916 | validation: 4.080208142477538]
	TIME [epoch: 2.67 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004464319617888		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 4.004464319617888 | validation: 4.080474405410883]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004732780118693		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 4.004732780118693 | validation: 4.079867408963549]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004491613192565		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 4.004491613192565 | validation: 4.080274838530059]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003728539966344		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 4.003728539966344 | validation: 4.079204289944147]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003517440903642		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 4.003517440903642 | validation: 4.079209343330409]
	TIME [epoch: 2.68 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002682125742378		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 4.002682125742378 | validation: 4.078456370404981]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002644993669365		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 4.002644993669365 | validation: 4.0786526319204865]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002841971141492		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 4.002841971141492 | validation: 4.079101393483055]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003246591096118		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 4.003246591096118 | validation: 4.078783622656503]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002894954785612		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 4.002894954785612 | validation: 4.077846466326335]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002785329045713		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 4.002785329045713 | validation: 4.078435351931569]
	TIME [epoch: 2.76 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0027140345031516		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 4.0027140345031516 | validation: 4.078088826662257]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002697958622349		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 4.002697958622349 | validation: 4.078003125453433]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0024972465355795		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 4.0024972465355795 | validation: 4.078818878549646]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002690868236612		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 4.002690868236612 | validation: 4.07834463540358]
	TIME [epoch: 2.76 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002646783921163		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 4.002646783921163 | validation: 4.079264426479674]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003149271491718		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 4.003149271491718 | validation: 4.080587510577591]
	TIME [epoch: 2.66 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005156847462514		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 4.005156847462514 | validation: 4.082152745036287]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00627885775462		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 4.00627885775462 | validation: 4.080822232809699]
	TIME [epoch: 2.67 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005889978275784		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 4.005889978275784 | validation: 4.079354912156646]
	TIME [epoch: 2.67 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003386426315811		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 4.003386426315811 | validation: 4.078711961662333]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003103244332701		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 4.003103244332701 | validation: 4.079059124422611]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002599398013594		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 4.002599398013594 | validation: 4.080058993920669]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003962953162519		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 4.003962953162519 | validation: 4.081261803245055]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0050847572730435		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 4.0050847572730435 | validation: 4.081333133970947]
	TIME [epoch: 2.66 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005434929293594		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 4.005434929293594 | validation: 4.081228450423097]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005488040421715		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 4.005488040421715 | validation: 4.084961001208128]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008881203161679		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 4.008881203161679 | validation: 4.087129744848513]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010450778540909		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 4.010450778540909 | validation: 4.087798595081973]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01129603970649		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 4.01129603970649 | validation: 4.086869393016507]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01113285095955		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 4.01113285095955 | validation: 4.087350660480629]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011026224783178		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 4.011026224783178 | validation: 4.087097817154012]
	TIME [epoch: 2.66 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0107289486277145		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 4.0107289486277145 | validation: 4.087747131129798]
	TIME [epoch: 2.66 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011026811583193		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 4.011026811583193 | validation: 4.085905147304951]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010566051471021		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 4.010566051471021 | validation: 4.085234795119122]
	TIME [epoch: 2.66 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009106356670383		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 4.009106356670383 | validation: 4.084056028546414]
	TIME [epoch: 2.75 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008578995579042		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 4.008578995579042 | validation: 4.0844107519035875]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008699936903146		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 4.008699936903146 | validation: 4.084903052320553]
	TIME [epoch: 2.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008671411341367		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 4.008671411341367 | validation: 4.084300651576646]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008152107348196		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 4.008152107348196 | validation: 4.08447420234426]
	TIME [epoch: 2.67 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007712983751812		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 4.007712983751812 | validation: 4.083220874055108]
	TIME [epoch: 2.67 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007854253640314		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 4.007854253640314 | validation: 4.084235903250612]
	TIME [epoch: 2.67 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007142010922031		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 4.007142010922031 | validation: 4.083152811874881]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006932129274469		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 4.006932129274469 | validation: 4.082879927085631]
	TIME [epoch: 2.79 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006977287702726		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 4.006977287702726 | validation: 4.0827784773223215]
	TIME [epoch: 2.76 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.007078888828292		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 4.007078888828292 | validation: 4.083044960641786]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006655156721666		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 4.006655156721666 | validation: 4.082429488327581]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006057117233781		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 4.006057117233781 | validation: 4.083162957104131]
	TIME [epoch: 2.66 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006473267946083		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 4.006473267946083 | validation: 4.081701351282072]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006115836582506		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 4.006115836582506 | validation: 4.082092101021882]
	TIME [epoch: 2.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005733378686284		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 4.005733378686284 | validation: 4.080345332908459]
	TIME [epoch: 2.67 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005174891265786		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 4.005174891265786 | validation: 4.080781290424734]
	TIME [epoch: 2.66 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0048513138397634		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 4.0048513138397634 | validation: 4.080095482282778]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0044319121692435		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 4.0044319121692435 | validation: 4.080635526201536]
	TIME [epoch: 2.67 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003885142608467		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 4.003885142608467 | validation: 4.0791942001805515]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003328964265941		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 4.003328964265941 | validation: 4.079637066418925]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0030741994812775		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 4.0030741994812775 | validation: 4.078789551731389]
	TIME [epoch: 2.69 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0032888802756315		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 4.0032888802756315 | validation: 4.078764552366471]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002505759104152		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 4.002505759104152 | validation: 4.0774722795890685]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002585100613766		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 4.002585100613766 | validation: 4.077637134969464]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002395765877764		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 4.002395765877764 | validation: 4.077593798172197]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001983436206135		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 4.001983436206135 | validation: 4.077693661088267]
	TIME [epoch: 2.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001962853931643		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 4.001962853931643 | validation: 4.078490135141032]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001909108885066		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 4.001909108885066 | validation: 4.077797101232148]
	TIME [epoch: 2.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002031765310829		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 4.002031765310829 | validation: 4.078175074271289]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002011438829222		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 4.002011438829222 | validation: 4.077992406637899]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001860336797457		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 4.001860336797457 | validation: 4.078092352992284]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001871360879018		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 4.001871360879018 | validation: 4.078081077001147]
	TIME [epoch: 2.67 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001926453290026		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 4.001926453290026 | validation: 4.077815711291356]
	TIME [epoch: 2.77 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001963016886411		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 4.001963016886411 | validation: 4.077833860198218]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002162250527283		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 4.002162250527283 | validation: 4.078313376002822]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002192909256699		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 4.002192909256699 | validation: 4.0783117688383514]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0018188047154135		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 4.0018188047154135 | validation: 4.0776652664939235]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002004067538556		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 4.002004067538556 | validation: 4.077897470744172]
	TIME [epoch: 2.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00170983084524		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 4.00170983084524 | validation: 4.077150553256302]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001399150737659		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 4.001399150737659 | validation: 4.078182562112753]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002053503307632		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 4.002053503307632 | validation: 4.078215332772774]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002681386150801		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 4.002681386150801 | validation: 4.0786783562535005]
	TIME [epoch: 2.76 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002581550788557		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 4.002581550788557 | validation: 4.078768926584091]
	TIME [epoch: 2.77 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002649035668538		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 4.002649035668538 | validation: 4.078533838594161]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002813281878799		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 4.002813281878799 | validation: 4.0783422317743545]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002974348045102		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 4.002974348045102 | validation: 4.078880143673742]
	TIME [epoch: 2.76 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002791431322819		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 4.002791431322819 | validation: 4.079539953337803]
	TIME [epoch: 2.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003712482434976		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 4.003712482434976 | validation: 4.0798125763205775]
	TIME [epoch: 2.76 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004268555053811		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 4.004268555053811 | validation: 4.079779003955155]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004346509879417		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 4.004346509879417 | validation: 4.079461840488389]
	TIME [epoch: 2.66 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003883146350968		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 4.003883146350968 | validation: 4.07922590481782]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0034278784425785		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 4.0034278784425785 | validation: 4.078340923218035]
	TIME [epoch: 2.66 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003085359201791		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 4.003085359201791 | validation: 4.078938807737036]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002550150722336		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 4.002550150722336 | validation: 4.077826468605875]
	TIME [epoch: 2.66 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002610524905444		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 4.002610524905444 | validation: 4.0806692715271256]
	TIME [epoch: 2.66 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004671651124332		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 4.004671651124332 | validation: 4.082016091512699]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005671620881755		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 4.005671620881755 | validation: 4.081478123731965]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006398988563355		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 4.006398988563355 | validation: 4.081623541725069]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005891950032052		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 4.005891950032052 | validation: 4.08165430085829]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0062784607083834		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 4.0062784607083834 | validation: 4.081188985823789]
	TIME [epoch: 2.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005600105440792		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 4.005600105440792 | validation: 4.081257506806476]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00528434987548		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 4.00528434987548 | validation: 4.0814322345256455]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.005209052790462		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 4.005209052790462 | validation: 4.08099198724475]
	TIME [epoch: 2.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004810575368241		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 4.004810575368241 | validation: 4.079478904233289]
	TIME [epoch: 2.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.004703367230354		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 4.004703367230354 | validation: 4.079685061506966]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003987203522968		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 4.003987203522968 | validation: 4.079129539275025]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0033352291021975		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 4.0033352291021975 | validation: 4.078588935086659]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003179203774808		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 4.003179203774808 | validation: 4.078213679070119]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002936800474821		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 4.002936800474821 | validation: 4.078743569941259]
	TIME [epoch: 2.66 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002509917131978		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 4.002509917131978 | validation: 4.077562356958643]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002216059504658		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 4.002216059504658 | validation: 4.077507254020342]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0019327507996785		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 4.0019327507996785 | validation: 4.077520166237556]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001790728888158		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 4.001790728888158 | validation: 4.077059071578654]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001423067663054		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 4.001423067663054 | validation: 4.077459887576562]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001565269534269		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 4.001565269534269 | validation: 4.077025211246464]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001472373284922		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 4.001472373284922 | validation: 4.076748883191478]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001338704113408		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 4.001338704113408 | validation: 4.076338409534525]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001041779605982		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 4.001041779605982 | validation: 4.076798495991386]
	TIME [epoch: 2.75 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001221129849414		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 4.001221129849414 | validation: 4.076920507166501]
	TIME [epoch: 2.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001486959914022		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 4.001486959914022 | validation: 4.077112286886176]
	TIME [epoch: 2.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001291474081939		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 4.001291474081939 | validation: 4.076791679404749]
	TIME [epoch: 2.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001120860186484		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 4.001120860186484 | validation: 4.077153596562459]
	TIME [epoch: 2.69 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001234756167115		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 4.001234756167115 | validation: 4.077024094902712]
	TIME [epoch: 2.68 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001310905306626		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 4.001310905306626 | validation: 4.076424154417929]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001152730307772		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 4.001152730307772 | validation: 4.077037286662781]
	TIME [epoch: 2.69 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000955160026554		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 4.000955160026554 | validation: 4.076147435991649]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001048080975023		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 4.001048080975023 | validation: 4.076854185780224]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000803631908062		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 4.000803631908062 | validation: 4.077143760323432]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000803975655256		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 4.000803975655256 | validation: 4.077077049556762]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001235066795243		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 4.001235066795243 | validation: 4.077160163057342]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001404543782822		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 4.001404543782822 | validation: 4.077022132574036]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000901734760143		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 4.000901734760143 | validation: 4.07716891082289]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001188416973306		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 4.001188416973306 | validation: 4.077814230600142]
	TIME [epoch: 2.66 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001665181875566		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 4.001665181875566 | validation: 4.078174531790571]
	TIME [epoch: 2.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001500904267948		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 4.001500904267948 | validation: 4.076897286188445]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001068778725028		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 4.001068778725028 | validation: 4.077652039120699]
	TIME [epoch: 2.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001299205325224		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 4.001299205325224 | validation: 4.0771047368806554]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000952437857451		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 4.000952437857451 | validation: 4.076486241840486]
	TIME [epoch: 2.66 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000768650139812		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 4.000768650139812 | validation: 4.076335432735813]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000633803869842		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 4.000633803869842 | validation: 4.076143092853407]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000621953238562		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 4.000621953238562 | validation: 4.076226063644149]
	TIME [epoch: 2.78 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000455803601023		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 4.000455803601023 | validation: 4.076573892893875]
	TIME [epoch: 2.79 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0005758764691395		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 4.0005758764691395 | validation: 4.075930372253757]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000622847571116		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 4.000622847571116 | validation: 4.076489560950754]
	TIME [epoch: 2.75 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000851099567215		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 4.000851099567215 | validation: 4.076779013928266]
	TIME [epoch: 2.69 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000641813252531		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 4.000641813252531 | validation: 4.076896964066754]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000604638962126		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 4.000604638962126 | validation: 4.075805779125076]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000517038225178		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 4.000517038225178 | validation: 4.076423459148791]
	TIME [epoch: 2.66 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000509645846348		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 4.000509645846348 | validation: 4.075995503325414]
	TIME [epoch: 2.76 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000377658970576		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 4.000377658970576 | validation: 4.0763448750401095]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0007210440998415		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 4.0007210440998415 | validation: 4.076440902962881]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000784342529224		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 4.000784342529224 | validation: 4.076039325366028]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000492434649567		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 4.000492434649567 | validation: 4.0761045495855335]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00025195685851		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 4.00025195685851 | validation: 4.07613257989775]
	TIME [epoch: 2.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000410820195684		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 4.000410820195684 | validation: 4.0754178810017185]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000381749272419		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 4.000381749272419 | validation: 4.076113767264707]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0003371981846545		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 4.0003371981846545 | validation: 4.075919455247968]
	TIME [epoch: 2.68 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000173124701142		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 4.000173124701142 | validation: 4.076047414279271]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000027845495796		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 4.000027845495796 | validation: 4.075756111937595]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000066559768659		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 4.000066559768659 | validation: 4.075330641551196]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999907355950511		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 3.999907355950511 | validation: 4.075754045181996]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0006554153867135		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 4.0006554153867135 | validation: 4.077521233093029]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001628408302368		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 4.001628408302368 | validation: 4.077639204783112]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001974981215424		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 4.001974981215424 | validation: 4.078676551742757]
	TIME [epoch: 2.66 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002579189328265		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 4.002579189328265 | validation: 4.078226225222083]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002481262201201		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 4.002481262201201 | validation: 4.078300824400721]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002947590531184		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 4.002947590531184 | validation: 4.078168448594401]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001948344615478		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 4.001948344615478 | validation: 4.076220615774054]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0003448905027605		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 4.0003448905027605 | validation: 4.075661024388362]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000075123323106		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 4.000075123323106 | validation: 4.075257331615832]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000033491121392		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 4.000033491121392 | validation: 4.075903985221244]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9999405991176897		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 3.9999405991176897 | validation: 4.07569196417784]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000274486240436		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 4.000274486240436 | validation: 4.075925983174859]
	TIME [epoch: 2.68 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00011233904022		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 4.00011233904022 | validation: 4.075460214227358]
	TIME [epoch: 2.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9999852226569783		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 3.9999852226569783 | validation: 4.075395385311856]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999756164719827		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 3.999756164719827 | validation: 4.075648406993012]
	TIME [epoch: 2.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000099360198747		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 4.000099360198747 | validation: 4.0775787466014135]
	TIME [epoch: 2.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001767134728317		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 4.001767134728317 | validation: 4.077547711414645]
	TIME [epoch: 2.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002311039132502		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 4.002311039132502 | validation: 4.078162184978225]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002705522475388		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 4.002705522475388 | validation: 4.0793091033796705]
	TIME [epoch: 2.68 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003524534851785		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 4.003524534851785 | validation: 4.07935759978662]
	TIME [epoch: 2.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003638151740825		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 4.003638151740825 | validation: 4.078790057718861]
	TIME [epoch: 2.73 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003106825043305		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 4.003106825043305 | validation: 4.076793481550777]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001354084700197		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 4.001354084700197 | validation: 4.076369648895441]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000398919767003		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 4.000398919767003 | validation: 4.0756984588409795]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000149282907569		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 4.000149282907569 | validation: 4.075798183468977]
	TIME [epoch: 2.67 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999825738502986		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 3.999825738502986 | validation: 4.075960400290007]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9998341619052407		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 3.9998341619052407 | validation: 4.0753888260715545]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999771308602521		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 3.999771308602521 | validation: 4.075645140973522]
	TIME [epoch: 2.66 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999537022604044		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 3.999537022604044 | validation: 4.075318733685506]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995276493634324		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 3.9995276493634324 | validation: 4.075051503096717]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999600910661089		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 3.999600910661089 | validation: 4.0759045171006525]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999804373580037		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 3.999804373580037 | validation: 4.075176466821937]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999632410905152		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 3.999632410905152 | validation: 4.076088939055508]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996163893254972		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 3.9996163893254972 | validation: 4.075107596938688]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999981779866563		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 3.999981779866563 | validation: 4.0757834384351685]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999690004530421		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 3.999690004530421 | validation: 4.075306210875794]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999661646618149		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 3.999661646618149 | validation: 4.075223165960856]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995073051195007		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 3.9995073051195007 | validation: 4.075406253910414]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9993978654023716		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 3.9993978654023716 | validation: 4.0748600325602276]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999656930793127		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 3.999656930793127 | validation: 4.076218902060414]
	TIME [epoch: 2.66 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999903064766862		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 3.999903064766862 | validation: 4.076072525603506]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9994469316277583		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 3.9994469316277583 | validation: 4.0758561328717295]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996099182181117		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 3.9996099182181117 | validation: 4.075954455609696]
	TIME [epoch: 2.67 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999874777725247		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 3.999874777725247 | validation: 4.075881590770076]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9997040400294477		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 3.9997040400294477 | validation: 4.075608970555862]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999611517062646		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 3.999611517062646 | validation: 4.0752735606100705]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995616026790457		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 3.9995616026790457 | validation: 4.075255942747954]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999733380196401		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 3.999733380196401 | validation: 4.076228919880573]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000461962627778		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 4.000461962627778 | validation: 4.0761264483803545]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000154964198613		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 4.000154964198613 | validation: 4.076886938961566]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000981757907063		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 4.000981757907063 | validation: 4.076231668352378]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000896225161432		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 4.000896225161432 | validation: 4.076652727692107]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001303267209504		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 4.001303267209504 | validation: 4.07799542993276]
	TIME [epoch: 2.66 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001500803747231		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 4.001500803747231 | validation: 4.077451611212732]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0017299008191864		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 4.0017299008191864 | validation: 4.076855533206097]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0017188460894735		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 4.0017188460894735 | validation: 4.077864653564476]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0017189104963204		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 4.0017189104963204 | validation: 4.077376421158095]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002182671336692		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 4.002182671336692 | validation: 4.078032284272851]
	TIME [epoch: 2.66 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001819512139392		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 4.001819512139392 | validation: 4.077640947672497]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0019417586707435		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 4.0019417586707435 | validation: 4.07712026128968]
	TIME [epoch: 2.66 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001951280523239		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 4.001951280523239 | validation: 4.077141741480415]
	TIME [epoch: 2.79 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001494974020403		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 4.001494974020403 | validation: 4.077780379722017]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001236772315064		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 4.001236772315064 | validation: 4.077547695210072]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001393542301933		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 4.001393542301933 | validation: 4.077593837236427]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001114678974014		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 4.001114678974014 | validation: 4.076659396741281]
	TIME [epoch: 2.66 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001405698356581		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 4.001405698356581 | validation: 4.076986779401943]
	TIME [epoch: 2.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001452099573992		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 4.001452099573992 | validation: 4.076650329908278]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001252593699912		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 4.001252593699912 | validation: 4.07748467404049]
	TIME [epoch: 2.66 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001203431597896		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 4.001203431597896 | validation: 4.0775611458186445]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001102991233461		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 4.001102991233461 | validation: 4.076434685782064]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00112789482547		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 4.00112789482547 | validation: 4.076438079659036]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00092928611743		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 4.00092928611743 | validation: 4.077059452851531]
	TIME [epoch: 2.66 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00103894601579		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 4.00103894601579 | validation: 4.076706765162371]
	TIME [epoch: 2.71 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000380292959396		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 4.000380292959396 | validation: 4.076669162188877]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00023302519959		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 4.00023302519959 | validation: 4.076599457763421]
	TIME [epoch: 2.67 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000332162128834		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 4.000332162128834 | validation: 4.07632064749865]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000051295317046		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 4.000051295317046 | validation: 4.075324703149559]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.99964884486649		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 3.99964884486649 | validation: 4.075261556037242]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999773968798597		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 3.999773968798597 | validation: 4.075433455379612]
	TIME [epoch: 2.66 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995639232715288		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 3.9995639232715288 | validation: 4.0751612889818025]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.99973544800455		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 3.99973544800455 | validation: 4.075362948559351]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999609040066972		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 3.999609040066972 | validation: 4.075304964238049]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999794515081806		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 3.999794515081806 | validation: 4.075279961589399]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999633356603997		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 3.999633356603997 | validation: 4.074960023054091]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999947471737912		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 3.999947471737912 | validation: 4.075550829991466]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996288000396336		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 3.9996288000396336 | validation: 4.0752982731230825]
	TIME [epoch: 2.73 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999900292416275		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 3.999900292416275 | validation: 4.075529722065622]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999732715999876		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 3.999732715999876 | validation: 4.0755772049349535]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999814737465219		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 3.999814737465219 | validation: 4.076094975428314]
	TIME [epoch: 2.66 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996324864623065		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 3.9996324864623065 | validation: 4.075128083111351]
	TIME [epoch: 2.67 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996747853658077		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 3.9996747853658077 | validation: 4.075238413597212]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995409828881585		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 3.9995409828881585 | validation: 4.075446996861582]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999572648386585		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 3.999572648386585 | validation: 4.075286092038959]
	TIME [epoch: 2.66 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996137613772365		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 3.9996137613772365 | validation: 4.075015614095899]
	TIME [epoch: 2.66 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999490634631676		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 3.999490634631676 | validation: 4.075788109753651]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9996699362423884		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 3.9996699362423884 | validation: 4.075441209010457]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995712975188007		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 3.9995712975188007 | validation: 4.07520739454043]
	TIME [epoch: 2.66 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9994227169075054		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 3.9994227169075054 | validation: 4.07537416599957]
	TIME [epoch: 2.66 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995911887645184		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 3.9995911887645184 | validation: 4.0752655289444455]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9992835554522377		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 3.9992835554522377 | validation: 4.075517423200034]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9995280390074868		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 3.9995280390074868 | validation: 4.07535319096174]
	TIME [epoch: 2.67 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999559180304712		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 3.999559180304712 | validation: 4.0756362320833395]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9991670610822347		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 3.9991670610822347 | validation: 4.075228699845075]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999631464010002		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 3.999631464010002 | validation: 4.0750335998083]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.99978091414295		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 3.99978091414295 | validation: 4.0751492666775695]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.999337164360053		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 3.999337164360053 | validation: 4.074953227635244]
	TIME [epoch: 2.67 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.99952709979117		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 3.99952709979117 | validation: 4.076028366203712]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000533662621206		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 4.000533662621206 | validation: 4.076841194087379]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000777574692622		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 4.000777574692622 | validation: 4.077347290594689]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00151914236745		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 4.00151914236745 | validation: 4.078205400905004]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001810593741871		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 4.001810593741871 | validation: 4.078241242374562]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001735359441892		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 4.001735359441892 | validation: 4.077924814759962]
	TIME [epoch: 2.67 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0021352326823925		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 4.0021352326823925 | validation: 4.077960115851709]
	TIME [epoch: 2.66 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0020492242285695		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 4.0020492242285695 | validation: 4.078712637673951]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002675013319773		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 4.002675013319773 | validation: 4.07875021402745]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003171622889364		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 4.003171622889364 | validation: 4.078889980669756]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003241027410096		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 4.003241027410096 | validation: 4.079107323118942]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003261206925687		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 4.003261206925687 | validation: 4.0783409811177105]
	TIME [epoch: 2.79 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002900072043991		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 4.002900072043991 | validation: 4.0789069093543375]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003079914683899		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 4.003079914683899 | validation: 4.078727364708571]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00249202219601		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 4.00249202219601 | validation: 4.078747239852406]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0026142973637		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 4.0026142973637 | validation: 4.0778768182549685]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002721011704871		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 4.002721011704871 | validation: 4.078474762687269]
	TIME [epoch: 2.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002325709904598		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 4.002325709904598 | validation: 4.078729267808682]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002500594136866		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 4.002500594136866 | validation: 4.077760430714403]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0020719972415195		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 4.0020719972415195 | validation: 4.078992041992019]
	TIME [epoch: 2.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002117802413526		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 4.002117802413526 | validation: 4.078242799088918]
	TIME [epoch: 2.66 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002739852000835		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 4.002739852000835 | validation: 4.079419856240747]
	TIME [epoch: 2.71 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0032429335737		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 4.0032429335737 | validation: 4.079426833951003]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002734684461298		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 4.002734684461298 | validation: 4.079236846342233]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003268150081259		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 4.003268150081259 | validation: 4.079320312236648]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003185328554124		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 4.003185328554124 | validation: 4.079358407761947]
	TIME [epoch: 2.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003495569399741		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 4.003495569399741 | validation: 4.079918728364549]
	TIME [epoch: 2.67 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003320185839242		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 4.003320185839242 | validation: 4.078482213673759]
	TIME [epoch: 2.67 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0032658392286145		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 4.0032658392286145 | validation: 4.079030455583111]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.003102708652973		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 4.003102708652973 | validation: 4.078843175556815]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.002721721211617		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 4.002721721211617 | validation: 4.077819102735606]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001734682782168		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 4.001734682782168 | validation: 4.077248192962272]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001873057788411		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 4.001873057788411 | validation: 4.077171580386216]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001689196638382		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 4.001689196638382 | validation: 4.0781038364378706]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1r_2_v_mmd4_20250601_154210/states/model_phi1_4a_distortion_v1r_2_v_mmd4_916.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3090.676 seconds.
