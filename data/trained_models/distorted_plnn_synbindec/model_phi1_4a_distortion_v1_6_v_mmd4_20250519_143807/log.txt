Args:
Namespace(name='model_phi1_4a_distortion_v1_6_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_6/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_6/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.05487167835235596, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3228386042

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.0569131631672795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0569131631672795 | validation: 6.426003128527623]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.150296509419562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.150296509419562 | validation: 6.178056508763172]
	TIME [epoch: 1.05 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.714523635208175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.714523635208175 | validation: 6.116896359429357]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6543322548492805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6543322548492805 | validation: 6.009995509127705]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.566683068576757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.566683068576757 | validation: 5.924785151617852]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.506818928214491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.506818928214491 | validation: 5.86274227109557]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.464108639647501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.464108639647501 | validation: 5.827112414505908]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.404572272359214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.404572272359214 | validation: 5.802245648281247]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.352478632500611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.352478632500611 | validation: 5.738598809890592]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.297916849431253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.297916849431253 | validation: 5.661963826102026]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.235047561480902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.235047561480902 | validation: 5.638285972772813]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.173495687329994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.173495687329994 | validation: 5.584584591716737]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.181660534012755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.181660534012755 | validation: 5.766136800700373]
	TIME [epoch: 0.702 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.370828938969315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.370828938969315 | validation: 5.583174998137443]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.028756235611091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.028756235611091 | validation: 5.519069653489696]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.191258390262203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.191258390262203 | validation: 5.474679063909162]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.172379198293336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.172379198293336 | validation: 5.575607242599361]
	TIME [epoch: 0.7 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.8928484191211306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8928484191211306 | validation: 5.587732559328656]
	TIME [epoch: 0.699 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.773556764325629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.773556764325629 | validation: 5.393210424204769]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.72047251621907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.72047251621907 | validation: 5.914038545283223]
	TIME [epoch: 0.698 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.679512840767244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.679512840767244 | validation: 5.273524098891958]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.848892367417279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.848892367417279 | validation: 5.2826184678105985]
	TIME [epoch: 0.701 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.37006033665447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.37006033665447 | validation: 6.15769863409612]
	TIME [epoch: 0.7 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.019501534893628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.019501534893628 | validation: 5.382767115049173]
	TIME [epoch: 0.701 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.417439520631568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.417439520631568 | validation: 5.184231132193253]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.166842063326979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.166842063326979 | validation: 5.863412281554732]
	TIME [epoch: 0.699 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.301468965799427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.301468965799427 | validation: 5.366666455243468]
	TIME [epoch: 0.699 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.158053999028163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.158053999028163 | validation: 5.178650831072545]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8130155997933537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8130155997933537 | validation: 5.720910835114982]
	TIME [epoch: 0.695 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.13298900476001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.13298900476001 | validation: 5.278058192798525]
	TIME [epoch: 0.694 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.745384873332782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.745384873332782 | validation: 4.939530744065949]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.541255709254772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.541255709254772 | validation: 5.608771666543387]
	TIME [epoch: 0.692 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.121947830773293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121947830773293 | validation: 5.198478547681791]
	TIME [epoch: 0.692 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3539186061025212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3539186061025212 | validation: 5.244441565523873]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.220323633370454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.220323633370454 | validation: 5.199336853502642]
	TIME [epoch: 0.689 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4427872041715104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4427872041715104 | validation: 5.185375446839917]
	TIME [epoch: 0.689 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.346095445482601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.346095445482601 | validation: 4.835102770774199]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.386862933608445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.386862933608445 | validation: 5.104200424112483]
	TIME [epoch: 0.695 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1884759127895332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1884759127895332 | validation: 4.970504664903544]
	TIME [epoch: 0.693 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.165690631044823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.165690631044823 | validation: 5.007422499483857]
	TIME [epoch: 0.697 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3114307376719245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3114307376719245 | validation: 5.058313600042986]
	TIME [epoch: 0.697 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2999112102934465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2999112102934465 | validation: 4.998074941109218]
	TIME [epoch: 0.696 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.250518683513069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.250518683513069 | validation: 4.776851111835241]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0617173331176417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0617173331176417 | validation: 4.956927070740372]
	TIME [epoch: 0.695 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.090274403707287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.090274403707287 | validation: 4.712286125111311]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139136767663281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139136767663281 | validation: 4.9803451984848]
	TIME [epoch: 0.698 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1131406730578277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1131406730578277 | validation: 4.73437276304894]
	TIME [epoch: 0.695 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.093977068255331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093977068255331 | validation: 4.905653980976499]
	TIME [epoch: 0.695 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1030222693080063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1030222693080063 | validation: 4.775123733118747]
	TIME [epoch: 0.692 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1273539023772505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1273539023772505 | validation: 4.700608050261782]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9549889245946197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9549889245946197 | validation: 4.742916708183489]
	TIME [epoch: 0.698 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.978186685809469		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.978186685809469 | validation: 4.6125904500477715]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9653497820476686		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.9653497820476686 | validation: 4.747358503082105]
	TIME [epoch: 0.698 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.954597482013344		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.954597482013344 | validation: 4.5867130182577425]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8945274897428668		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.8945274897428668 | validation: 4.664615266447912]
	TIME [epoch: 0.698 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9014288411031606		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.9014288411031606 | validation: 4.55304728450842]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0100410360678804		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.0100410360678804 | validation: 4.707415189101212]
	TIME [epoch: 0.697 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.985007315316411		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.985007315316411 | validation: 4.524193298815308]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9483194210899732		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.9483194210899732 | validation: 4.50289845669508]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.824590659913832		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.824590659913832 | validation: 4.448712929272068]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7818835981451433		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.7818835981451433 | validation: 4.456298926815427]
	TIME [epoch: 0.697 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7679025326534155		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.7679025326534155 | validation: 4.429132096435782]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7592458988232273		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.7592458988232273 | validation: 4.4322292111938175]
	TIME [epoch: 0.693 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7753462882602373		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.7753462882602373 | validation: 4.438276896396265]
	TIME [epoch: 0.692 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.854267427511958		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.854267427511958 | validation: 4.474411384731101]
	TIME [epoch: 0.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9300977981165217		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.9300977981165217 | validation: 4.411083030135884]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9041743512309814		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.9041743512309814 | validation: 4.257634969422618]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7094342377644933		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.7094342377644933 | validation: 4.237899420104244]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7134037549121657		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.7134037549121657 | validation: 4.270643210723185]
	TIME [epoch: 0.695 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.765367032036304		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.765367032036304 | validation: 4.176143473356037]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7170099811371764		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.7170099811371764 | validation: 4.183846973897023]
	TIME [epoch: 0.692 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6984633548763997		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.6984633548763997 | validation: 4.088884796460756]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6556312282362406		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.6556312282362406 | validation: 4.080756919925462]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6307847450805766		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.6307847450805766 | validation: 3.991590537068094]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6790400054723795		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.6790400054723795 | validation: 4.385763842038192]
	TIME [epoch: 0.694 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1594510638821296		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.1594510638821296 | validation: 4.013533389317767]
	TIME [epoch: 0.69 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7939861961839543		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.7939861961839543 | validation: 3.853807331276319]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.653115799090463		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.653115799090463 | validation: 3.8003644143267468]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.583390373406286		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.583390373406286 | validation: 3.7901381550890054]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5645099518429357		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.5645099518429357 | validation: 3.762931927321006]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.542317511676466		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.542317511676466 | validation: 3.721790957278468]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.519914826607033		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.519914826607033 | validation: 3.643583938896448]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.496547645840905		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.496547645840905 | validation: 3.6051012889112934]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.476531862411596		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.476531862411596 | validation: 3.49486480715511]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4512179835786925		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.4512179835786925 | validation: 3.4139604540417268]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4416540567112968		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.4416540567112968 | validation: 3.422913436458646]
	TIME [epoch: 0.698 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.454647834631475		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.454647834631475 | validation: 3.325300510601233]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.667432506996287		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.667432506996287 | validation: 3.480386242856588]
	TIME [epoch: 0.697 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7645702632035403		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.7645702632035403 | validation: 2.967605402000104]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3360444720048235		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.3360444720048235 | validation: 3.663135662944977]
	TIME [epoch: 0.696 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.69776322513272		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.69776322513272 | validation: 2.8866735430532837]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3068894074422492		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.3068894074422492 | validation: 2.9889946779278063]
	TIME [epoch: 0.697 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5133799227757403		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.5133799227757403 | validation: 3.3871135644251513]
	TIME [epoch: 0.694 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.546289784760143		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.546289784760143 | validation: 2.6046928603198864]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2441735362710102		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.2441735362710102 | validation: 2.52131255924969]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1360601300994357		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.1360601300994357 | validation: 2.683291232379096]
	TIME [epoch: 0.692 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1592374890787047		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.1592374890787047 | validation: 2.421271907322455]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.178241897446739		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.178241897446739 | validation: 2.7087675875939246]
	TIME [epoch: 0.695 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1973729762844405		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.1973729762844405 | validation: 2.0643341035509772]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0472622270423337		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.0472622270423337 | validation: 2.180919092887045]
	TIME [epoch: 0.695 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.977834668148667		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.977834668148667 | validation: 1.7538082054526964]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8106953111246555		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.8106953111246555 | validation: 2.0871843717847254]
	TIME [epoch: 0.696 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8672168456241576		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8672168456241576 | validation: 2.3651878780397064]
	TIME [epoch: 0.694 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2618581231028627		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.2618581231028627 | validation: 1.9952284534646534]
	TIME [epoch: 0.694 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.803246784685656		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.803246784685656 | validation: 1.4625527704563366]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6107200084123867		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.6107200084123867 | validation: 1.5084003793058565]
	TIME [epoch: 0.696 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5171146499473123		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.5171146499473123 | validation: 1.641905067073247]
	TIME [epoch: 0.696 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.55997093913364		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.55997093913364 | validation: 1.9562214425449653]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8809774922935532		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.8809774922935532 | validation: 1.600938002182609]
	TIME [epoch: 0.695 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5391387228082465		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.5391387228082465 | validation: 1.408910195159453]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6266988422214126		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.6266988422214126 | validation: 1.3662493437078795]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.485494340240248		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.485494340240248 | validation: 1.4289551097738136]
	TIME [epoch: 0.697 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5317753849251012		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5317753849251012 | validation: 1.5849337516664996]
	TIME [epoch: 0.695 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.469115113223359		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.469115113223359 | validation: 1.3657667377917266]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5359034366245612		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.5359034366245612 | validation: 1.7671648407698148]
	TIME [epoch: 0.697 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5554779660352192		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.5554779660352192 | validation: 1.2940640127278131]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4287343180915661		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.4287343180915661 | validation: 1.3737686744254398]
	TIME [epoch: 0.693 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3445260344412713		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.3445260344412713 | validation: 1.237308283919802]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.358869123653869		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.358869123653869 | validation: 1.4616267427595475]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4508630229336508		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.4508630229336508 | validation: 1.4440012596801504]
	TIME [epoch: 0.692 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5607894616924665		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.5607894616924665 | validation: 1.3545830779041654]
	TIME [epoch: 0.691 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.537658071663235		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.537658071663235 | validation: 1.2770526811647493]
	TIME [epoch: 0.691 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3761290339764478		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.3761290339764478 | validation: 1.2544835879128806]
	TIME [epoch: 0.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3523337870595162		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.3523337870595162 | validation: 1.1124871514373855]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2276425542490066		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.2276425542490066 | validation: 1.2345391361583788]
	TIME [epoch: 0.697 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2642947236108009		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2642947236108009 | validation: 1.355305846425006]
	TIME [epoch: 0.696 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.391732784816063		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.391732784816063 | validation: 1.6509757064363086]
	TIME [epoch: 0.695 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4106344880862727		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.4106344880862727 | validation: 1.2245722266060266]
	TIME [epoch: 0.694 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5073733641707046		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.5073733641707046 | validation: 1.5680018467137107]
	TIME [epoch: 0.694 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4309575818807954		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.4309575818807954 | validation: 1.0199449271539747]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.248419171587514		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.248419171587514 | validation: 1.1487218294709505]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.186434054504052		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.186434054504052 | validation: 1.1461033686543283]
	TIME [epoch: 0.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207937117240926		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.207937117240926 | validation: 1.1932667112002482]
	TIME [epoch: 0.691 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2290029709966774		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.2290029709966774 | validation: 1.1875039491068728]
	TIME [epoch: 0.691 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.269879072704489		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.269879072704489 | validation: 1.3867279705635953]
	TIME [epoch: 0.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3265072717399513		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.3265072717399513 | validation: 1.240261058523056]
	TIME [epoch: 0.69 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.503674795514724		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.503674795514724 | validation: 1.0427525034054406]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009277834462357		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.2009277834462357 | validation: 1.1691708104554461]
	TIME [epoch: 0.692 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356120845819242		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.356120845819242 | validation: 1.202275750079787]
	TIME [epoch: 0.691 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3007557663386375		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3007557663386375 | validation: 1.3161699272311456]
	TIME [epoch: 0.695 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3325213234796645		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.3325213234796645 | validation: 1.3081136718650321]
	TIME [epoch: 0.694 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2379462531904355		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2379462531904355 | validation: 0.9506492753528519]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1526531781074396		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.1526531781074396 | validation: 1.2168505460815495]
	TIME [epoch: 0.696 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1884341787590211		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.1884341787590211 | validation: 0.9034858294666219]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1721419407480076		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.1721419407480076 | validation: 1.4100622359178627]
	TIME [epoch: 0.693 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2332478775290603		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2332478775290603 | validation: 0.9822409080264404]
	TIME [epoch: 0.691 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1863874146593754		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.1863874146593754 | validation: 1.2476837064733677]
	TIME [epoch: 0.698 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321141206711745		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.1321141206711745 | validation: 0.9442281020073274]
	TIME [epoch: 0.691 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1897842680565542		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.1897842680565542 | validation: 1.2255989605175506]
	TIME [epoch: 0.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.21377880095763		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.21377880095763 | validation: 1.157471298203207]
	TIME [epoch: 0.689 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3453529385391538		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.3453529385391538 | validation: 1.0781238575740748]
	TIME [epoch: 0.691 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575644744439992		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.1575644744439992 | validation: 1.342852802620222]
	TIME [epoch: 0.691 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2111077584809844		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.2111077584809844 | validation: 0.92704908194589]
	TIME [epoch: 0.691 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554585302109899		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1554585302109899 | validation: 0.9541827426518208]
	TIME [epoch: 0.69 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.041466505440532		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.041466505440532 | validation: 0.9375654339330541]
	TIME [epoch: 0.691 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0455204824767652		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.0455204824767652 | validation: 0.9393235610785237]
	TIME [epoch: 0.691 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0557413625486343		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.0557413625486343 | validation: 1.0015725976815626]
	TIME [epoch: 0.692 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.026992921064552		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.026992921064552 | validation: 0.8668390547126752]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038440913055273		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.038440913055273 | validation: 1.065474040212658]
	TIME [epoch: 0.695 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0518372816454866		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.0518372816454866 | validation: 0.9826166321144321]
	TIME [epoch: 0.693 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.301051280929797		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.301051280929797 | validation: 1.0926116131045098]
	TIME [epoch: 0.691 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1320745162871912		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.1320745162871912 | validation: 1.2192096000323496]
	TIME [epoch: 0.69 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1098687295410599		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.1098687295410599 | validation: 1.009182250683833]
	TIME [epoch: 0.691 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2697468174705722		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.2697468174705722 | validation: 1.3111925622338931]
	TIME [epoch: 0.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1771383660631582		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.1771383660631582 | validation: 0.8421553945375794]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9832702442398781		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.9832702442398781 | validation: 0.9056603716715019]
	TIME [epoch: 0.692 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579233978975503		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.9579233978975503 | validation: 0.8780677389285654]
	TIME [epoch: 0.69 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9514803446583195		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9514803446583195 | validation: 0.8724232248290025]
	TIME [epoch: 0.691 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.955044026110042		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.955044026110042 | validation: 0.9391418366271701]
	TIME [epoch: 0.691 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199890871866597		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.0199890871866597 | validation: 1.2550450295030782]
	TIME [epoch: 0.692 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1818962758273381		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.1818962758273381 | validation: 1.224133743511106]
	TIME [epoch: 0.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4831193430653575		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.4831193430653575 | validation: 0.9447104611611604]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1614127185196401		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.1614127185196401 | validation: 1.158121875230962]
	TIME [epoch: 0.69 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0534869777652554		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0534869777652554 | validation: 0.8418006872976709]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029902604250885		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.029902604250885 | validation: 1.0318636403717545]
	TIME [epoch: 0.693 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.975645971364859		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.975645971364859 | validation: 0.8223801725225016]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9527881377406362		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.9527881377406362 | validation: 0.9882832213365044]
	TIME [epoch: 0.698 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9523112060211655		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9523112060211655 | validation: 0.7883928295036843]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0560873852192438		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.0560873852192438 | validation: 1.0443028103036067]
	TIME [epoch: 0.696 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555189394597176		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.9555189394597176 | validation: 0.7348701625266809]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725549841857251		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.9725549841857251 | validation: 1.1218018987606933]
	TIME [epoch: 0.695 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9891786767476223		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9891786767476223 | validation: 0.7905120331798988]
	TIME [epoch: 0.696 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006814536253579		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.006814536253579 | validation: 1.067226189616693]
	TIME [epoch: 0.696 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9399455926348972		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9399455926348972 | validation: 0.7314065650067212]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074109367344848		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.9074109367344848 | validation: 0.9031409198858559]
	TIME [epoch: 0.692 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936460182962657		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8936460182962657 | validation: 0.9752537244227373]
	TIME [epoch: 0.697 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053407635908454		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.053407635908454 | validation: 1.1566472246665909]
	TIME [epoch: 0.694 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1224394359063417		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.1224394359063417 | validation: 0.8701694377856655]
	TIME [epoch: 0.697 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273161174651239		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.9273161174651239 | validation: 0.7889347148612128]
	TIME [epoch: 0.696 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325952382482948		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8325952382482948 | validation: 0.7815013191008324]
	TIME [epoch: 0.695 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958722308411464		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7958722308411464 | validation: 0.721217732810755]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914302013868756		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7914302013868756 | validation: 0.9411576666710927]
	TIME [epoch: 0.692 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8354430820630213		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8354430820630213 | validation: 0.8694572196319932]
	TIME [epoch: 0.691 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1827774247550213		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1827774247550213 | validation: 1.2601093924400946]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0642284430818		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0642284430818 | validation: 0.6845823629160028]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089519631753873		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.8089519631753873 | validation: 0.823863433086062]
	TIME [epoch: 0.699 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873728007286151		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.8873728007286151 | validation: 1.1231460568463487]
	TIME [epoch: 0.695 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0898049989015015		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.0898049989015015 | validation: 0.9155820906267135]
	TIME [epoch: 0.695 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913559905710317		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.913559905710317 | validation: 0.7682804012579544]
	TIME [epoch: 0.696 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702714508600533		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7702714508600533 | validation: 0.6920705633724838]
	TIME [epoch: 0.695 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435543550975191		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7435543550975191 | validation: 0.7352663054961266]
	TIME [epoch: 175 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366834629531087		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7366834629531087 | validation: 0.7211309734200033]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877334524415062		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7877334524415062 | validation: 1.19539557516448]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1210125920856362		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.1210125920856362 | validation: 0.9802455016301892]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2496728685062075		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.2496728685062075 | validation: 0.7736267634270827]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8086399278422238		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8086399278422238 | validation: 1.2326981451698815]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1724349736438346		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.1724349736438346 | validation: 0.712350656752566]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464235121068232		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8464235121068232 | validation: 1.0126527218522738]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8421885608280976		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.8421885608280976 | validation: 0.8243481661057604]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353688276453175		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8353688276453175 | validation: 0.7407403921784694]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015528686970123		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7015528686970123 | validation: 0.7591580221426574]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7042769719019626		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7042769719019626 | validation: 0.656995145394091]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918303748629903		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6918303748629903 | validation: 0.8313050412884622]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151947171211478		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7151947171211478 | validation: 0.6905892316958657]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8395691804407531		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8395691804407531 | validation: 1.1331029605100944]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111421565996698		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9111421565996698 | validation: 0.6044403290853535]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181890242809074		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7181890242809074 | validation: 0.8902250564060119]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902099706770841		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7902099706770841 | validation: 0.9681290667218652]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0913076672371003		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.0913076672371003 | validation: 0.8555603247652087]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740312885191719		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7740312885191719 | validation: 0.6678083278231036]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475576942667993		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6475576942667993 | validation: 0.6877374138328065]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236310411945943		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6236310411945943 | validation: 0.6398453504631155]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223104446119896		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6223104446119896 | validation: 0.7752572961709322]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323647063319396		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6323647063319396 | validation: 0.682727620763789]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096295653353198		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7096295653353198 | validation: 1.0627491355831338]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427031130612177		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.8427031130612177 | validation: 0.6438282338314084]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373055710729912		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7373055710729912 | validation: 0.8147204442444624]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363030804987052		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6363030804987052 | validation: 0.6512437241588231]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728112941107389		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7728112941107389 | validation: 1.3343627205310895]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366076853434255		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1366076853434255 | validation: 0.8153112293266]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135297297510536		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.8135297297510536 | validation: 0.6358952206957952]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6211995150911392		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6211995150911392 | validation: 0.7743012249649471]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244782600410281		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6244782600410281 | validation: 0.6786524168769703]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111586918522772		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7111586918522772 | validation: 0.9130826314579652]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993282642702073		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7993282642702073 | validation: 0.6737609833763835]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7241589100597989		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7241589100597989 | validation: 0.7770863275424253]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205711224251499		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6205711224251499 | validation: 0.6113009251745973]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719073510823589		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.5719073510823589 | validation: 0.7048251621624634]
	TIME [epoch: 1.35 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5615781066524518		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.5615781066524518 | validation: 0.587283435218071]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544947919930047		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.5544947919930047 | validation: 0.7832409857262723]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6004632644224349		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6004632644224349 | validation: 0.64865993545627]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7387189414153948		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7387189414153948 | validation: 0.9590774776796405]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052323183793837		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7052323183793837 | validation: 0.5235927670028309]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381795676370261		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5381795676370261 | validation: 0.7714865640368868]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419253330394483		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6419253330394483 | validation: 1.0364518323516887]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1704877259518305		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1704877259518305 | validation: 0.7646647830498268]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314000506649272		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8314000506649272 | validation: 1.0484700101486533]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038806121127305		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8038806121127305 | validation: 0.7599284636017414]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907271516875332		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7907271516875332 | validation: 0.6546976140637186]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651238578037224		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.5651238578037224 | validation: 0.624981957843995]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5283408278360282		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.5283408278360282 | validation: 0.5619315894945923]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5024094602486959		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.5024094602486959 | validation: 0.6837747702001477]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316971390842493		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5316971390842493 | validation: 0.5832436719963253]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408480665719068		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6408480665719068 | validation: 1.076187391516894]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939199778660304		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7939199778660304 | validation: 0.569816630074904]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5304341501192235		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5304341501192235 | validation: 0.6090760284334333]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6294657523118372		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6294657523118372 | validation: 0.8615826595360603]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345700052498734		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7345700052498734 | validation: 0.6945858280002318]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5814382923565277		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.5814382923565277 | validation: 0.5840435402337081]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5991963801870515		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5991963801870515 | validation: 0.9349669862499161]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6593865122591563		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6593865122591563 | validation: 0.6169171581209869]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854706948631376		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6854706948631376 | validation: 0.7635255466621302]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508974048122823		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5508974048122823 | validation: 0.5393882550643477]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46618487779470275		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.46618487779470275 | validation: 0.5700325658650863]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4702199109617392		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.4702199109617392 | validation: 0.6134720638449473]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46355738638281224		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.46355738638281224 | validation: 0.5729690269718982]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846162689012078		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.5846162689012078 | validation: 0.8972278140048382]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221927462522446		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.7221927462522446 | validation: 0.7128683623359888]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562013780295304		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7562013780295304 | validation: 0.5587486329177483]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47983973878878033		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.47983973878878033 | validation: 0.6069387828989772]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4640556577801583		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.4640556577801583 | validation: 0.5298720563994411]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.525731233103947		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.525731233103947 | validation: 0.8601387609695771]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5914883567018082		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5914883567018082 | validation: 0.5017826072595422]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49666605148748416		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.49666605148748416 | validation: 0.5617393176273636]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.432143000363695		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.432143000363695 | validation: 0.46999839441772545]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4148246811891254		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.4148246811891254 | validation: 0.6077968231645413]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.425983387481622		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.425983387481622 | validation: 0.5844314342790126]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6507035777818376		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6507035777818376 | validation: 1.3664608864603753]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526509827082404		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.2526509827082404 | validation: 0.8408407159858801]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800625157454547		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6800625157454547 | validation: 0.6813706472754655]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824004764077566		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6824004764077566 | validation: 0.5565398089369591]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383991526605761		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.4383991526605761 | validation: 0.6110146391159861]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4379238396036179		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.4379238396036179 | validation: 0.5072504031894296]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46348141148102584		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.46348141148102584 | validation: 0.5736243214503256]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415244874272989		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.415244874272989 | validation: 0.4714674565686428]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907210422713524		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.3907210422713524 | validation: 0.48654735145469896]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37193034620134036		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.37193034620134036 | validation: 0.4694040106015046]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39130302173787995		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.39130302173787995 | validation: 0.7071529658862523]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040579950813423		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.7040579950813423 | validation: 0.9725835493912272]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325064966211205		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.9325064966211205 | validation: 0.6493850150313962]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759514603787733		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6759514603787733 | validation: 0.7110459619027916]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4737331829312501		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.4737331829312501 | validation: 0.47915765906235797]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4195767441966649		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.4195767441966649 | validation: 0.5278741546592491]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42423578788100075		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.42423578788100075 | validation: 0.5299209763221302]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44940243979496297		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.44940243979496297 | validation: 0.5529566062429891]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4775656406824447		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.4775656406824447 | validation: 0.7231246019229297]
	TIME [epoch: 1.35 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5691619119448657		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5691619119448657 | validation: 0.5783540736547716]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632651675789262		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.5632651675789262 | validation: 0.5937106474437558]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47282903773823026		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.47282903773823026 | validation: 0.5207854624743847]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.408584527288225		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.408584527288225 | validation: 0.5321797254798231]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.416452160395277		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.416452160395277 | validation: 0.5105335353563022]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46109595922888147		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.46109595922888147 | validation: 0.8168489184811286]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845017094884168		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.5845017094884168 | validation: 0.5587117750874685]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106597484062769		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5106597484062769 | validation: 0.5432975368984829]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40930892020498844		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.40930892020498844 | validation: 0.4922606512923362]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37819267506067744		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.37819267506067744 | validation: 0.4508277782824693]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40331702748648396		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.40331702748648396 | validation: 0.636492782986616]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47828438314927757		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.47828438314927757 | validation: 0.5289169121646546]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5692806487454312		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.5692806487454312 | validation: 0.5737892816018143]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40226448574875334		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.40226448574875334 | validation: 0.4286434177672894]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3516422988385216		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.3516422988385216 | validation: 0.50369646511786]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3531492409603088		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.3531492409603088 | validation: 0.4880298602024191]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42705270353833935		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.42705270353833935 | validation: 0.9884809784335158]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055319703114665		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8055319703114665 | validation: 0.6114773528543759]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214176451140019		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6214176451140019 | validation: 0.505278550509173]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37631699956088843		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.37631699956088843 | validation: 0.44345739550377294]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34155776424802825		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.34155776424802825 | validation: 0.4669883295733723]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33895103244412045		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.33895103244412045 | validation: 0.40053113029310894]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34415829198113057		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.34415829198113057 | validation: 0.5742737459486902]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4303260518189192		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.4303260518189192 | validation: 0.5008134402575627]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530882316326554		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.5530882316326554 | validation: 0.6219351232394854]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414187189513298		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.4414187189513298 | validation: 0.40177298136487716]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420769588462032		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.3420769588462032 | validation: 0.4970942783863599]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33704439956234966		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.33704439956234966 | validation: 0.47143223860416655]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40054133033074046		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.40054133033074046 | validation: 0.9771959726867412]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209172514664204		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.8209172514664204 | validation: 0.6004403654504807]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.512345700264671		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.512345700264671 | validation: 0.4407439086658901]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3568373610258044		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3568373610258044 | validation: 0.47389928262395786]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40396371429948813		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.40396371429948813 | validation: 0.6329118677031441]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45627206095174594		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.45627206095174594 | validation: 0.5613610296929691]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43761574472405895		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.43761574472405895 | validation: 0.6057975633247299]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4171297261528777		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.4171297261528777 | validation: 0.46498275330951994]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36879999103667116		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.36879999103667116 | validation: 0.49926239344575435]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4198662872692445		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.4198662872692445 | validation: 0.49263100756035616]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4052022116280538		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.4052022116280538 | validation: 0.5000975750115965]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3877711641497824		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.3877711641497824 | validation: 0.47440866568498125]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39617407642747476		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.39617407642747476 | validation: 0.6428997032393577]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44817890879864364		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.44817890879864364 | validation: 0.4795130371772956]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029521820216604		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.4029521820216604 | validation: 0.5173845607059153]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3849427036007681		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3849427036007681 | validation: 0.4897235727632205]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40014531837133716		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.40014531837133716 | validation: 0.4713543934170528]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38976113858206474		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.38976113858206474 | validation: 0.5048827607194197]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.394036463965498		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.394036463965498 | validation: 0.4610571319226804]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3562841741538209		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.3562841741538209 | validation: 0.46077990797908513]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34013579601584526		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.34013579601584526 | validation: 0.39995108542405106]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35649289137007373		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.35649289137007373 | validation: 0.5708109394909027]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39931558567332304		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.39931558567332304 | validation: 0.5081412328999175]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894529759455979		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.4894529759455979 | validation: 0.6663110001405261]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4236925752673352		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.4236925752673352 | validation: 0.41816919341448106]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3279958831663364		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.3279958831663364 | validation: 0.4348953691058441]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3834307228313954		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.3834307228313954 | validation: 0.5215513142745299]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.450844474922259		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.450844474922259 | validation: 0.6601265780640722]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4772307641993595		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.4772307641993595 | validation: 0.5339990763009077]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47223677976233375		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.47223677976233375 | validation: 0.5105000896437909]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38212889861783683		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.38212889861783683 | validation: 0.44653446704856137]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3061768539596605		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.3061768539596605 | validation: 0.38214792466882347]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29666600262850296		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.29666600262850296 | validation: 0.4534516965899739]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178474871140163		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.3178474871140163 | validation: 0.3970776679716709]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34496695215488976		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.34496695215488976 | validation: 0.5438941605013566]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3655530061797157		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3655530061797157 | validation: 0.3986222863001878]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37173838410064586		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.37173838410064586 | validation: 0.48550322673618107]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37497432928497043		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.37497432928497043 | validation: 0.3795505910641303]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259572094152473		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.3259572094152473 | validation: 0.40301671163081676]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007459983430922		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.3007459983430922 | validation: 0.5328918767407588]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4082941875809257		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.4082941875809257 | validation: 0.7131127448368244]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472668524724927		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7472668524724927 | validation: 0.52867862690014]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753804478523455		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.3753804478523455 | validation: 0.406348273467628]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2872537431188923		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2872537431188923 | validation: 0.377975303525241]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713305052362165		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.2713305052362165 | validation: 0.374294518423897]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26504596844492595		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.26504596844492595 | validation: 0.39214098837028577]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28298214774380176		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.28298214774380176 | validation: 0.5275628903541969]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4043695167016293		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4043695167016293 | validation: 0.6885763231875413]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783848551359993		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.6783848551359993 | validation: 0.5212986458169656]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36024229549429004		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.36024229549429004 | validation: 0.3930350902230282]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28000010095715655		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.28000010095715655 | validation: 0.41365910471817813]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714987091707906		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2714987091707906 | validation: 0.3686495968848671]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2872326584187674		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.2872326584187674 | validation: 0.5549207558276826]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415150531089161		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.3415150531089161 | validation: 0.4582155883789428]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37246904269315295		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.37246904269315295 | validation: 0.5478933381651464]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.343193353348731		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.343193353348731 | validation: 0.37038639481933294]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28472457668674234		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.28472457668674234 | validation: 0.3733535395452003]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25226159736021475		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.25226159736021475 | validation: 0.3480059784196106]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542644599952395		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.2542644599952395 | validation: 0.4583930192595954]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286252582217188		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.286252582217188 | validation: 0.43106129538794175]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32362019824807314		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.32362019824807314 | validation: 0.6553252000132597]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43995958979133976		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.43995958979133976 | validation: 0.7733660994934374]
	TIME [epoch: 1.37 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456099341466056		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.8456099341466056 | validation: 0.38884739379611916]
	TIME [epoch: 1.98 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3293554315490495		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3293554315490495 | validation: 0.6072759282934528]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5230788079909351		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5230788079909351 | validation: 0.6188612754968261]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5205873344291382		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5205873344291382 | validation: 0.3506681821749159]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28230908282782824		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.28230908282782824 | validation: 0.4147690249813707]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2825131344065976		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.2825131344065976 | validation: 0.35496961811580613]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2626638821843983		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.2626638821843983 | validation: 0.4082584444599586]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27385558221040074		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.27385558221040074 | validation: 0.33747613591500064]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2699967730752454		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.2699967730752454 | validation: 0.3881962753084341]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812227391838688		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.2812227391838688 | validation: 0.33779592745391823]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31491684744437715		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.31491684744437715 | validation: 0.4513454401587163]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38419542763069225		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.38419542763069225 | validation: 0.3886331978274944]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33647744506178684		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.33647744506178684 | validation: 0.4535356153354668]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.345538368163967		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.345538368163967 | validation: 0.718572914627112]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4700790466096765		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.4700790466096765 | validation: 0.4438954257894609]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504968320872362		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.3504968320872362 | validation: 0.4024232406732699]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29748618077857025		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.29748618077857025 | validation: 0.3738053682856788]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742910289280433		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2742910289280433 | validation: 0.4144313576336853]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775221571247725		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.2775221571247725 | validation: 0.4067371528689878]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889740557412925		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.2889740557412925 | validation: 0.4792647373282353]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3237916585783229		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.3237916585783229 | validation: 0.4191955178976059]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30923817198297476		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.30923817198297476 | validation: 0.42992477030220366]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351717381251707		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.3351717381251707 | validation: 0.44113313925289455]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34404828116937664		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.34404828116937664 | validation: 0.43236241062339364]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3188376688642643		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.3188376688642643 | validation: 0.40233922108027936]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2813579026563936		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2813579026563936 | validation: 0.34456048334899214]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848578724134942		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.2848578724134942 | validation: 0.3840110685380705]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26808166287878527		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.26808166287878527 | validation: 0.33034828513013526]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2680525225974821		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2680525225974821 | validation: 0.3745974697865049]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2494490844948083		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2494490844948083 | validation: 0.3190969804444393]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26774976195123956		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.26774976195123956 | validation: 0.41669325058654155]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3076825334152731		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.3076825334152731 | validation: 0.3856923218743806]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33505403066514283		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.33505403066514283 | validation: 0.5446317125059424]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44056411514474536		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.44056411514474536 | validation: 0.6714832855878147]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4347220290806392		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.4347220290806392 | validation: 0.39310860220508154]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2656975854271373		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2656975854271373 | validation: 0.3117895965359913]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23416449296001027		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.23416449296001027 | validation: 0.33018661539229116]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2268481603721352		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.2268481603721352 | validation: 0.3267788948659586]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2171310124029592		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2171310124029592 | validation: 0.36020911295198466]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22344496010548898		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.22344496010548898 | validation: 0.3815324933634239]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660514826131292		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.2660514826131292 | validation: 0.5395676344970065]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.445116564972892		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.445116564972892 | validation: 0.6823551000455015]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.494680876690775		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.494680876690775 | validation: 0.38412785056382887]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26366211379968457		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.26366211379968457 | validation: 0.31708722387250776]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22727395836307324		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.22727395836307324 | validation: 0.3563337868069037]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2234047528515967		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2234047528515967 | validation: 0.3508983710328683]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24750649069944813		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.24750649069944813 | validation: 0.49506191649901504]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.332030179643235		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.332030179643235 | validation: 0.3499668801174846]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.310753583173926		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.310753583173926 | validation: 0.38331855593825903]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693361077867027		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.2693361077867027 | validation: 0.3141915338204946]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24844000163293894		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.24844000163293894 | validation: 0.35290669653058776]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24548474392766959		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.24548474392766959 | validation: 0.3293456869687841]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.289115925924714		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.289115925924714 | validation: 0.4755613441154948]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40351755406294504		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.40351755406294504 | validation: 0.7711024868831562]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4970640949385339		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.4970640949385339 | validation: 0.34648811798723317]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24407468118719514		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.24407468118719514 | validation: 0.3015638126556594]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21896902713987657		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.21896902713987657 | validation: 0.3735590184340456]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22778123016679927		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.22778123016679927 | validation: 0.3739634179670871]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25646639823188566		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.25646639823188566 | validation: 0.4420673571656504]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2660015472102713		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.2660015472102713 | validation: 0.39934927239937307]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28201124424415486		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.28201124424415486 | validation: 0.4538951163577701]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3818679636789342		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.3818679636789342 | validation: 0.41810272067913723]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2977213456989602		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.2977213456989602 | validation: 0.33928234780180333]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24728444078966583		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.24728444078966583 | validation: 0.3369812167431248]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2232728756180242		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2232728756180242 | validation: 0.31560502402664214]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201347735076907		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.2201347735076907 | validation: 0.4113307521255279]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2934821922021185		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.2934821922021185 | validation: 0.5600565551223367]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4085094723439623		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.4085094723439623 | validation: 0.3923755206499322]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702071207016672		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2702071207016672 | validation: 0.3046742978254669]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22276576939460122		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.22276576939460122 | validation: 0.3377332929960233]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22568791288223172		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.22568791288223172 | validation: 0.29919960718228866]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2295814237926269		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2295814237926269 | validation: 0.34283842865447856]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21988157237300954		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.21988157237300954 | validation: 0.312303519253507]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2321408407000799		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.2321408407000799 | validation: 0.3863805846245836]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26835888935855595		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.26835888935855595 | validation: 0.3338882277178204]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2877842316473903		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.2877842316473903 | validation: 0.3718397759691372]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22549105850842638		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.22549105850842638 | validation: 0.3671475134644113]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23608286617250657		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.23608286617250657 | validation: 0.5285482680166019]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491913488305262		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.3491913488305262 | validation: 0.46599363492158474]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36691313362510586		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.36691313362510586 | validation: 0.40966103814558574]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3230958364464685		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.3230958364464685 | validation: 0.36839041912889475]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24757554817673652		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.24757554817673652 | validation: 0.33307893660458254]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22915287569893886		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.22915287569893886 | validation: 0.34186253453313625]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21430660534260632		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.21430660534260632 | validation: 0.3582308595633495]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22577810838333456		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.22577810838333456 | validation: 0.3686477696521133]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22839340965921398		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.22839340965921398 | validation: 0.37701087634558805]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783338235591621		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.2783338235591621 | validation: 0.39612010592376423]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30203319556662445		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.30203319556662445 | validation: 0.39465119512009345]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.315482913623924		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.315482913623924 | validation: 0.34289528238382966]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21920696508259757		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.21920696508259757 | validation: 0.3287225437413734]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21143659881247168		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.21143659881247168 | validation: 0.3239319008998968]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20182893330257315		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.20182893330257315 | validation: 0.38838122160363864]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22136075352727724		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.22136075352727724 | validation: 0.3165915623920945]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21696978541703701		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.21696978541703701 | validation: 0.372778918891955]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22602389967617215		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.22602389967617215 | validation: 0.29304099386759563]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19079226280590977		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.19079226280590977 | validation: 0.2835545646629329]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18784642754275918		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.18784642754275918 | validation: 0.3948111902970489]
	TIME [epoch: 1.36 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31259502630076624		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.31259502630076624 | validation: 0.7492020488653488]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792396203876828		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.6792396203876828 | validation: 0.32417882057421843]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23631302976245208		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.23631302976245208 | validation: 0.3692462306676009]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26205129944262817		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.26205129944262817 | validation: 0.4265185272211411]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37869691084578416		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.37869691084578416 | validation: 0.3493449429777742]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23015850918041486		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.23015850918041486 | validation: 0.29001124740725626]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18731132844573878		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.18731132844573878 | validation: 0.3181693696483139]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19289586242451376		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.19289586242451376 | validation: 0.3153720226646531]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1907862009708887		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1907862009708887 | validation: 0.3203872675300253]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2030578445070031		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.2030578445070031 | validation: 0.3842820795581906]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22823195550009104		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.22823195550009104 | validation: 0.3812067195692874]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23828876250048303		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.23828876250048303 | validation: 0.3446330214598876]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29157218651119016		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.29157218651119016 | validation: 0.3806752859305667]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733490176955017		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.2733490176955017 | validation: 0.27651869045967703]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2219478732717024		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2219478732717024 | validation: 0.337990062700086]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23472181914996745		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.23472181914996745 | validation: 0.35801540392824127]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2396434829531258		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2396434829531258 | validation: 0.36923357192642714]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305886140662215		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2305886140662215 | validation: 0.3399487637154532]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22003385661225497		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.22003385661225497 | validation: 0.3342005570548829]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987026689557085		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1987026689557085 | validation: 0.31445816968899276]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20886590794398505		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.20886590794398505 | validation: 0.3514257623889019]
	TIME [epoch: 2.68 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25914805425121884		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.25914805425121884 | validation: 0.3237815833520207]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27134437412157014		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.27134437412157014 | validation: 0.3802860375285888]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24774731475942255		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.24774731475942255 | validation: 0.3921879016225229]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23837329126217627		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.23837329126217627 | validation: 0.30632697400547465]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18845383045934447		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.18845383045934447 | validation: 0.2783092277201436]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17484866214740813		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.17484866214740813 | validation: 0.28064919934260524]
	TIME [epoch: 2.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16889707395968936		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.16889707395968936 | validation: 0.28161685421465793]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17091115610606022		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.17091115610606022 | validation: 0.3332791721199337]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20429803396448257		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.20429803396448257 | validation: 0.4740644582783531]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2766329485877104		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.2766329485877104 | validation: 0.38206175304125334]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634344175030914		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2634344175030914 | validation: 0.34243327699544546]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952304155865032		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.3952304155865032 | validation: 0.3143608215202922]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997533999607841		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.1997533999607841 | validation: 0.27179205445365334]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16646508449522812		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.16646508449522812 | validation: 0.28343883222588284]
	TIME [epoch: 2.68 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17507415798924522		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.17507415798924522 | validation: 0.3317102363377217]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19964108478008832		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.19964108478008832 | validation: 0.3031137748786912]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19034930960771818		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.19034930960771818 | validation: 0.29636362145797535]
	TIME [epoch: 2.68 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18716277793865807		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.18716277793865807 | validation: 0.313302161622102]
	TIME [epoch: 2.68 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22126655934774878		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.22126655934774878 | validation: 0.33150405276880646]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23079463951295182		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.23079463951295182 | validation: 0.24886343019435186]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24348296785797136		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.24348296785797136 | validation: 0.3807935825934465]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628101690370701		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.2628101690370701 | validation: 0.6363423464500432]
	TIME [epoch: 2.68 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45423852841483453		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.45423852841483453 | validation: 0.2980083926808616]
	TIME [epoch: 2.68 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18705541573901063		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.18705541573901063 | validation: 0.27613306543992444]
	TIME [epoch: 2.69 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17640341355844938		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.17640341355844938 | validation: 0.3377624087071571]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206227619187388		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.2206227619187388 | validation: 0.3493011179992639]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528964389247134		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.2528964389247134 | validation: 0.30922836706825946]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23432101196079402		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.23432101196079402 | validation: 0.3047956667252132]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18456497350997622		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.18456497350997622 | validation: 0.2640371343707523]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709654589846538		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1709654589846538 | validation: 0.3045354256867616]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715564059858228		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.1715564059858228 | validation: 0.3052847126739243]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2115835285566971		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.2115835285566971 | validation: 0.36828465857427434]
	TIME [epoch: 2.68 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25949806260869523		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.25949806260869523 | validation: 0.33958594779452755]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27413402235317064		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.27413402235317064 | validation: 0.34332282321690033]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21324773814759176		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.21324773814759176 | validation: 0.2815632052509748]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1881314719976801		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.1881314719976801 | validation: 0.2844582587410134]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16889265131786171		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.16889265131786171 | validation: 0.2664251726216866]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16860116586432056		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.16860116586432056 | validation: 0.31700826299466744]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1811130409352625		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.1811130409352625 | validation: 0.3378449104947721]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2303218744771586		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.2303218744771586 | validation: 0.34590668402273866]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23054568599046094		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.23054568599046094 | validation: 0.2592273249161203]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23110902744554865		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.23110902744554865 | validation: 0.2994570822439969]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19367371661395616		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.19367371661395616 | validation: 0.3065161224821757]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21895248926432723		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.21895248926432723 | validation: 0.3084134066376299]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875351735327233		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.1875351735327233 | validation: 0.27430024477490333]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1902843880349687		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.1902843880349687 | validation: 0.2978649865558924]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17711711063492824		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.17711711063492824 | validation: 0.23030278186021713]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19049042451785855		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.19049042451785855 | validation: 0.27744129015084823]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1783639711977954		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1783639711977954 | validation: 0.24989511965599426]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19226017371185505		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.19226017371185505 | validation: 0.4419103704614303]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238457696205888		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.3238457696205888 | validation: 0.4240131780307659]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771335227565787		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.2771335227565787 | validation: 0.29813691144821247]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19555439733450913		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.19555439733450913 | validation: 0.27064593034550527]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18054246294772128		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.18054246294772128 | validation: 0.2752569184693618]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16507967311426355		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.16507967311426355 | validation: 0.268555180508708]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15480161892175903		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.15480161892175903 | validation: 0.26590349588026535]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622348760842035		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.1622348760842035 | validation: 0.35207188987262805]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.218000980443276		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.218000980443276 | validation: 0.3640973672880485]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2655135958478861		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.2655135958478861 | validation: 0.302923830556535]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831648225279949		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1831648225279949 | validation: 0.24771007988645988]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15630201109051864		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.15630201109051864 | validation: 0.25606646963691654]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14565551645651326		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.14565551645651326 | validation: 0.2526731334408056]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1626361747599263		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.1626361747599263 | validation: 0.37776218160487546]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519313321356327		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.2519313321356327 | validation: 0.4460835468706118]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281113166743991		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.3281113166743991 | validation: 0.3051005190945872]
	TIME [epoch: 2.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19379925100212356		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.19379925100212356 | validation: 0.2528023747541607]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14698097184755107		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.14698097184755107 | validation: 0.260159782305648]
	TIME [epoch: 2.68 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540130129874368		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.1540130129874368 | validation: 0.2575493828667363]
	TIME [epoch: 2.68 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612006287200606		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.1612006287200606 | validation: 0.2865137974487498]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16970655069067767		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.16970655069067767 | validation: 0.3008499644923356]
	TIME [epoch: 2.68 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659917827363762		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.1659917827363762 | validation: 0.26316343371963985]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16775391943887716		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.16775391943887716 | validation: 0.3400994007728197]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20272449255640257		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.20272449255640257 | validation: 0.32430463373071616]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29598949042145284		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.29598949042145284 | validation: 0.3365485885038101]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21248728164694364		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.21248728164694364 | validation: 0.26272783952250206]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1745213294242322		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.1745213294242322 | validation: 0.27419326306694897]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15400193901460013		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.15400193901460013 | validation: 0.23675107931725128]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1616100844431317		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1616100844431317 | validation: 0.2869710437224002]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17181599031945355		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.17181599031945355 | validation: 0.2521248254199046]
	TIME [epoch: 2.69 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20279255310395353		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.20279255310395353 | validation: 0.3288986827909007]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20750519781699292		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.20750519781699292 | validation: 0.29313750584109005]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2082962746787775		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.2082962746787775 | validation: 0.2721951900659731]
	TIME [epoch: 2.68 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15705020723857868		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.15705020723857868 | validation: 0.24019207975825618]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15417854103901465		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.15417854103901465 | validation: 0.2967070097624769]
	TIME [epoch: 2.68 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18224932218008305		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.18224932218008305 | validation: 0.3276743571716709]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25633178483217356		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.25633178483217356 | validation: 0.3066757285237227]
	TIME [epoch: 2.68 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1815719073175749		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.1815719073175749 | validation: 0.24343895392798337]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150771795519842		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.150771795519842 | validation: 0.25629634295423187]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13538888391302142		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.13538888391302142 | validation: 0.2344298598807368]
	TIME [epoch: 2.68 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373152385930327		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.14373152385930327 | validation: 0.3019315615089391]
	TIME [epoch: 2.69 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17271034664588414		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.17271034664588414 | validation: 0.3464821374884647]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24549223819444757		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.24549223819444757 | validation: 0.3270502240932464]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22069024930386277		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.22069024930386277 | validation: 0.23394630107558845]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19386350390876148		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.19386350390876148 | validation: 0.2636656657381473]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14752812768468324		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.14752812768468324 | validation: 0.23080623171462442]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13157760992102924		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.13157760992102924 | validation: 0.23549999566186544]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12726800616214753		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.12726800616214753 | validation: 0.2224104668800583]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13940179241592854		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.13940179241592854 | validation: 0.32626163757628235]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997474335950798		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.1997474335950798 | validation: 0.417500757011481]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30371222221151056		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.30371222221151056 | validation: 0.29371478171234616]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728215161967051		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.1728215161967051 | validation: 0.22415623327704282]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375512126888718		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.1375512126888718 | validation: 0.23401320637941458]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12334873125280821		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.12334873125280821 | validation: 0.2256756104397952]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272528723738887		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1272528723738887 | validation: 0.25213894826590855]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14570647887446625		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.14570647887446625 | validation: 0.21788397374683433]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031977760571642		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.2031977760571642 | validation: 0.3535970300580993]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24135716866459972		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.24135716866459972 | validation: 0.38195431288965187]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566850689498802		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.2566850689498802 | validation: 0.2755155924344412]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15054366044780493		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.15054366044780493 | validation: 0.21648842974725466]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335635610338135		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1335635610338135 | validation: 0.24566720993785657]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14380185236420107		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.14380185236420107 | validation: 0.2601702973749978]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16918689643722337		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.16918689643722337 | validation: 0.3065950793232339]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19008893910388572		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.19008893910388572 | validation: 0.25314710317113587]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1989292028036911		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.1989292028036911 | validation: 0.26980653154935313]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15581693842833974		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.15581693842833974 | validation: 0.21109600755431535]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14203130559192684		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.14203130559192684 | validation: 0.25433725234882404]
	TIME [epoch: 2.68 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13837728610852357		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.13837728610852357 | validation: 0.22886245019522977]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14723325964593595		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.14723325964593595 | validation: 0.36910148879172144]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23772490621121958		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.23772490621121958 | validation: 0.2784796657953139]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22790631962775834		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.22790631962775834 | validation: 0.24573403413047182]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14771815318341705		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.14771815318341705 | validation: 0.22099175337383709]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276199371452781		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.1276199371452781 | validation: 0.2245948318913107]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11962557011423489		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.11962557011423489 | validation: 0.21611011710939812]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12043524380257015		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.12043524380257015 | validation: 0.24783767783231436]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1352658749590142		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.1352658749590142 | validation: 0.26124787009570183]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15162473891834893		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15162473891834893 | validation: 0.25323892631202044]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449519077346257		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.14449519077346257 | validation: 0.23104527577476378]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14198790046867685		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.14198790046867685 | validation: 0.25072374681118087]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16104847452272894		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.16104847452272894 | validation: 0.21354301948889912]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18062340000713686		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.18062340000713686 | validation: 0.3648447183313228]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24722562890010744		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.24722562890010744 | validation: 0.36501162125065445]
	TIME [epoch: 2.67 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613100096734163		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.2613100096734163 | validation: 0.2408526572219893]
	TIME [epoch: 2.67 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13843816976244114		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.13843816976244114 | validation: 0.21158120669982688]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12701047859154893		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.12701047859154893 | validation: 0.21440364683980057]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11521801755688425		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.11521801755688425 | validation: 0.2039454861034633]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11172160005383208		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.11172160005383208 | validation: 0.22523756001451956]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.113129095715674		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.113129095715674 | validation: 0.20579808628844043]
	TIME [epoch: 2.67 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305865394785556		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.1305865394785556 | validation: 0.2983537213450597]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1767738110665313		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.1767738110665313 | validation: 0.36115200939458064]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298612239137979		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.298612239137979 | validation: 0.2830620831920738]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17339127698241164		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.17339127698241164 | validation: 0.20577980735894486]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14831026996463853		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.14831026996463853 | validation: 0.2781760318527637]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14537361678683244		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.14537361678683244 | validation: 0.23666887158494762]
	TIME [epoch: 2.67 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1704219560682133		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.1704219560682133 | validation: 0.2795273714486634]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.156739411149528		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.156739411149528 | validation: 0.2112751245926622]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15089308881343072		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.15089308881343072 | validation: 0.26254113866313444]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364088934859183		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.1364088934859183 | validation: 0.240008700315218]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16214386915506487		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.16214386915506487 | validation: 0.26395052022705023]
	TIME [epoch: 2.67 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14429160136510905		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.14429160136510905 | validation: 0.20906465487767384]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14837172127568113		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.14837172127568113 | validation: 0.24304857099593813]
	TIME [epoch: 2.67 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13760413838357496		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.13760413838357496 | validation: 0.21493040463442387]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13364501914040372		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.13364501914040372 | validation: 0.2732131624945587]
	TIME [epoch: 2.67 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15004789162224272		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.15004789162224272 | validation: 0.25026018221104096]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1743838014876177		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.1743838014876177 | validation: 0.29686899651246956]
	TIME [epoch: 2.67 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19116310827767788		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.19116310827767788 | validation: 0.21380759241116964]
	TIME [epoch: 2.68 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16054635477774704		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.16054635477774704 | validation: 0.2335904930096513]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12832563048045942		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.12832563048045942 | validation: 0.19521381148813777]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12243100674654402		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.12243100674654402 | validation: 0.23538393727208087]
	TIME [epoch: 2.67 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307511985037215		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.1307511985037215 | validation: 0.21250399490380956]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15662869444286612		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.15662869444286612 | validation: 0.2709307153197565]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15704014261658863		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.15704014261658863 | validation: 0.21799515169838452]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494036081821121		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1494036081821121 | validation: 0.26508013003053743]
	TIME [epoch: 2.67 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306605340842551		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.1306605340842551 | validation: 0.20786846830483238]
	TIME [epoch: 2.67 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117497939680664		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.13117497939680664 | validation: 0.271138881990046]
	TIME [epoch: 2.67 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15834176620796098		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.15834176620796098 | validation: 0.24245337160570088]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178335063584643		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.178335063584643 | validation: 0.2524855133513194]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13952545206350273		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.13952545206350273 | validation: 0.20132021918257423]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262922105783539		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.12262922105783539 | validation: 0.21239261428810519]
	TIME [epoch: 2.67 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11051155809120163		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.11051155809120163 | validation: 0.19775423838123182]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12044494614271405		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.12044494614271405 | validation: 0.2716013404106174]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14214493009592205		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.14214493009592205 | validation: 0.25986498134388347]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18287635382709616		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.18287635382709616 | validation: 0.265430848640142]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17189531770060237		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.17189531770060237 | validation: 0.19599639969528102]
	TIME [epoch: 2.67 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15302725918654855		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.15302725918654855 | validation: 0.25308566393164106]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13329613042458646		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.13329613042458646 | validation: 0.22718778427906328]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14336198439829265		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.14336198439829265 | validation: 0.25484884213938647]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307939916640097		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.1307939916640097 | validation: 0.19108982282774917]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12874507764101642		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.12874507764101642 | validation: 0.2430527980494687]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12265545526974742		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.12265545526974742 | validation: 0.1961606038296337]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12697532569633052		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.12697532569633052 | validation: 0.244786559752721]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12917351784254577		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.12917351784254577 | validation: 0.2158382356116972]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16120205552870343		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.16120205552870343 | validation: 0.28927395312587206]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18090714241069136		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.18090714241069136 | validation: 0.21589930065047805]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15389669369036496		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.15389669369036496 | validation: 0.2123831144311975]
	TIME [epoch: 2.67 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1196173287156983		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.1196173287156983 | validation: 0.18303622706394376]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11017887025061983		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.11017887025061983 | validation: 0.20860462121926898]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10741596564347265		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.10741596564347265 | validation: 0.18332683839040917]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11349765095957351		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.11349765095957351 | validation: 0.2614304627922803]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14429865887808452		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.14429865887808452 | validation: 0.2630824326555051]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24086027268784674		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.24086027268784674 | validation: 0.2425144660918746]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12358071506290606		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.12358071506290606 | validation: 0.205686353232864]
	TIME [epoch: 2.67 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10403006496182414		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.10403006496182414 | validation: 0.2018628864438674]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10062525852489194		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.10062525852489194 | validation: 0.19827050435836968]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09918459098241243		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.09918459098241243 | validation: 0.1913916392894101]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10588146634805945		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.10588146634805945 | validation: 0.20479134685744071]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10509130755676199		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.10509130755676199 | validation: 0.18993982793271438]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12952781144618775		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.12952781144618775 | validation: 0.2637568458283717]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16846058398199418		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.16846058398199418 | validation: 0.31438657465046]
	TIME [epoch: 2.67 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22125698621172757		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.22125698621172757 | validation: 0.29122015959372016]
	TIME [epoch: 2.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16511789005044775		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.16511789005044775 | validation: 0.19363698943169114]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13922949452627698		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.13922949452627698 | validation: 0.24880326709278]
	TIME [epoch: 2.68 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272241035928967		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.1272241035928967 | validation: 0.20092254127367368]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13293684676401116		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.13293684676401116 | validation: 0.24113624813516435]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12926771610951052		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.12926771610951052 | validation: 0.20009788237267745]
	TIME [epoch: 2.69 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13008017674829742		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.13008017674829742 | validation: 0.23332395374942738]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11137306200003474		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.11137306200003474 | validation: 0.19217224778717812]
	TIME [epoch: 2.69 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696730266227185		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.10696730266227185 | validation: 0.2104689130258696]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212192633392254		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.11212192633392254 | validation: 0.17932061359561555]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13455180610252834		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.13455180610252834 | validation: 0.2777103388888634]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491991174380419		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.1491991174380419 | validation: 0.2111513304238551]
	TIME [epoch: 2.68 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15532448714321517		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.15532448714321517 | validation: 0.22736230676896657]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12695328017404267		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.12695328017404267 | validation: 0.1877229432483588]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1191384741946769		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.1191384741946769 | validation: 0.2124381863034504]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10915471878346374		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.10915471878346374 | validation: 0.19573739208525343]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754313801407804		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.11754313801407804 | validation: 0.2344012063140809]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12067724410542852		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.12067724410542852 | validation: 0.19292822486838437]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14975529074992705		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.14975529074992705 | validation: 0.24928481506302844]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15769417066755945		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.15769417066755945 | validation: 0.17655405676183247]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440474501079325		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.12440474501079325 | validation: 0.23596385162492767]
	TIME [epoch: 2.68 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12195013749706422		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.12195013749706422 | validation: 0.18564374947952955]
	TIME [epoch: 2.68 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12875492040074984		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.12875492040074984 | validation: 0.21958982087383055]
	TIME [epoch: 2.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11160168104676654		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.11160168104676654 | validation: 0.1763374048841628]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222742735121241		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.1222742735121241 | validation: 0.23365638133700892]
	TIME [epoch: 2.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12787161190283294		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.12787161190283294 | validation: 0.18773296520354313]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14001358921608545		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.14001358921608545 | validation: 0.22379511707976163]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097084265802181		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.12097084265802181 | validation: 0.17238618174178244]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11446788617626219		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.11446788617626219 | validation: 0.20351700729540712]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10322970869689237		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.10322970869689237 | validation: 0.17559049241940886]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10129869361698035		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.10129869361698035 | validation: 0.23198690612616915]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11859843308022437		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.11859843308022437 | validation: 0.21678679739222254]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15240454007993173		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.15240454007993173 | validation: 0.24882306094118778]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14036796285400968		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.14036796285400968 | validation: 0.1730591658391857]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251990773168622		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.1251990773168622 | validation: 0.21072259852132724]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09649853453003736		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.09649853453003736 | validation: 0.16854636713941737]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758037484846149		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.09758037484846149 | validation: 0.2253943622463849]
	TIME [epoch: 2.69 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11075556008760593		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.11075556008760593 | validation: 0.19614437103094795]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13678878147168472		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.13678878147168472 | validation: 0.24252246620224394]
	TIME [epoch: 2.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797842341714175		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.13797842341714175 | validation: 0.17893226886925162]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13238682323887235		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.13238682323887235 | validation: 0.21337667861059895]
	TIME [epoch: 2.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10667183789798629		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.10667183789798629 | validation: 0.1777006929114365]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10992322110353082		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.10992322110353082 | validation: 0.24175244605636437]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11660081074431819		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.11660081074431819 | validation: 0.1872423418480778]
	TIME [epoch: 2.69 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13477267514390223		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.13477267514390223 | validation: 0.23097093232070334]
	TIME [epoch: 2.68 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894339322272572		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.12894339322272572 | validation: 0.1818685354122648]
	TIME [epoch: 2.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11343356668299398		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.11343356668299398 | validation: 0.19867942281810239]
	TIME [epoch: 2.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1026335441949566		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.1026335441949566 | validation: 0.1688035184377312]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101314612738093		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.101314612738093 | validation: 0.20853931155680466]
	TIME [epoch: 2.68 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09661914198740697		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.09661914198740697 | validation: 0.16990180248801323]
	TIME [epoch: 2.68 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019021293967473		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.1019021293967473 | validation: 0.2677172186748997]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444137422310463		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.1444137422310463 | validation: 0.24296659216311461]
	TIME [epoch: 2.68 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18928413402540245		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.18928413402540245 | validation: 0.20891019958402424]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10530283521633614		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.10530283521633614 | validation: 0.19343847189193294]
	TIME [epoch: 2.68 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940390050564804		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.0940390050564804 | validation: 0.17159964322011922]
	TIME [epoch: 2.69 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09125856062798113		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.09125856062798113 | validation: 0.1970608178510166]
	TIME [epoch: 2.68 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004269073547253		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.1004269073547253 | validation: 0.1823272164126802]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11196558814766061		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.11196558814766061 | validation: 0.2492470853874371]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12881307083262808		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.12881307083262808 | validation: 0.18320441366270473]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13477300598820965		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.13477300598820965 | validation: 0.20353941855569088]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967143737498672		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.10967143737498672 | validation: 0.1675325724630955]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970238012913907		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0970238012913907 | validation: 0.20380169192610964]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015781085373533		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.10015781085373533 | validation: 0.17071554018593887]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11439441963781062		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.11439441963781062 | validation: 0.22652395473975595]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12176088504320909		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.12176088504320909 | validation: 0.17542818821716594]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13750754795621958		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.13750754795621958 | validation: 0.2150063123767657]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10649483468095419		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.10649483468095419 | validation: 0.17319089980058747]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09444119770453986		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.09444119770453986 | validation: 0.18585869721833093]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08944236601478182		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.08944236601478182 | validation: 0.16850136113697445]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09262717419607579		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.09262717419607579 | validation: 0.2176736540856596]
	TIME [epoch: 2.68 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10778120656942512		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.10778120656942512 | validation: 0.1855705746835155]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484177299610794		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.1484177299610794 | validation: 0.27623773864747375]
	TIME [epoch: 2.68 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15996946248735808		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.15996946248735808 | validation: 0.16155185286164342]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11309947863475533		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.11309947863475533 | validation: 0.17924570611609708]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900119389954945		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.0900119389954945 | validation: 0.16223094242563915]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08556427322289696		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08556427322289696 | validation: 0.18165775110817395]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256254957931511		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.08256254957931511 | validation: 0.17030009763669757]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227324976321846		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.09227324976321846 | validation: 0.22192792809886425]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584011095258124		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.10584011095258124 | validation: 0.17731567602916573]
	TIME [epoch: 2.68 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12460779996852742		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.12460779996852742 | validation: 0.23304041127463498]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414162212172133		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.1414162212172133 | validation: 0.16515430155868357]
	TIME [epoch: 2.67 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285854552920714		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.1285854552920714 | validation: 0.20107615123409472]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09665376449639125		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.09665376449639125 | validation: 0.16236813086533775]
	TIME [epoch: 2.67 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08987859254773498		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.08987859254773498 | validation: 0.22068154375712742]
	TIME [epoch: 2.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10869159524657616		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.10869159524657616 | validation: 0.17583596068641094]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1337438645851748		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.1337438645851748 | validation: 0.1912095087527451]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469211231813251		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.10469211231813251 | validation: 0.16107533041604505]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141310336669296		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.09141310336669296 | validation: 0.17933498690059133]
	TIME [epoch: 2.68 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08881041995845916		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.08881041995845916 | validation: 0.17574667937097535]
	TIME [epoch: 2.67 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031977028750206		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.1031977028750206 | validation: 0.2271135205211784]
	TIME [epoch: 2.68 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117087036699877		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.117087036699877 | validation: 0.17686714640865261]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1395001991167736		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.1395001991167736 | validation: 0.19247859460067196]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10332497329029408		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.10332497329029408 | validation: 0.16200891297613562]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559356338012859		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.08559356338012859 | validation: 0.1644420107994723]
	TIME [epoch: 2.68 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08050971334847006		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.08050971334847006 | validation: 0.16516030718501204]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825601193566836		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.0825601193566836 | validation: 0.18818299823884221]
	TIME [epoch: 2.68 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885684865695935		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.0885684865695935 | validation: 0.16190940439255158]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11375685606601571		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.11375685606601571 | validation: 0.24509825420943568]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13503441777120273		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.13503441777120273 | validation: 0.18723611315753386]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589641114071548		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.1589641114071548 | validation: 0.2015831425857834]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09806548760399114		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.09806548760399114 | validation: 0.16195581967391426]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08519982346261744		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08519982346261744 | validation: 0.18602840741261661]
	TIME [epoch: 2.68 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555785295369332		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.08555785295369332 | validation: 0.16741479037920134]
	TIME [epoch: 2.68 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788256887089105		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08788256887089105 | validation: 0.21029511415507401]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10262176539648495		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.10262176539648495 | validation: 0.17436020524783213]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13064287305282762		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.13064287305282762 | validation: 0.20957351445401598]
	TIME [epoch: 2.67 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11222092026488145		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.11222092026488145 | validation: 0.16065008822774426]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_809.pth
	Model improved!!!
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09496309947264951		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09496309947264951 | validation: 0.19686412331632272]
	TIME [epoch: 2.68 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990029969944191		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.08990029969944191 | validation: 0.15582215361329432]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909705816877894		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.08909705816877894 | validation: 0.20944518556131247]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601875742062575		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.09601875742062575 | validation: 0.167846598017292]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474404051631998		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.11474404051631998 | validation: 0.22547492347086592]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10881955136530777		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.10881955136530777 | validation: 0.15025025239932832]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10247519072704175		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.10247519072704175 | validation: 0.1998202833227022]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880322347215962		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.08880322347215962 | validation: 0.16280608033819566]
	TIME [epoch: 2.69 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930373485229318		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.0930373485229318 | validation: 0.22706722981453487]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12104582123208309		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.12104582123208309 | validation: 0.1583959674134076]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13257524171838556		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.13257524171838556 | validation: 0.1882222092097622]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205096407483307		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.09205096407483307 | validation: 0.1575875319729228]
	TIME [epoch: 2.68 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08044319390879107		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.08044319390879107 | validation: 0.15672255505328883]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07896906327062267		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.07896906327062267 | validation: 0.18490210886008301]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193503358501569		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.08193503358501569 | validation: 0.15759832538762672]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08533139809403471		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.08533139809403471 | validation: 0.2071392378811778]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785530925809446		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.09785530925809446 | validation: 0.16552920230517007]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12147589786763296		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.12147589786763296 | validation: 0.22447195111931012]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1249896698053433		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.1249896698053433 | validation: 0.14697378667780425]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09705978854484343		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.09705978854484343 | validation: 0.18371719735618242]
	TIME [epoch: 2.67 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314986016708868		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.08314986016708868 | validation: 0.14929913548912163]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362410180412623		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.08362410180412623 | validation: 0.18530642884278725]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08373343385229466		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.08373343385229466 | validation: 0.1611793474449481]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09475234857542894		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.09475234857542894 | validation: 0.2159086070711645]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11053911523053649		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.11053911523053649 | validation: 0.15379717228287695]
	TIME [epoch: 2.67 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11924280202881982		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.11924280202881982 | validation: 0.18401761604790964]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09363756964635321		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.09363756964635321 | validation: 0.14599712575911356]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840229419127067		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.0840229419127067 | validation: 0.1891689864873537]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09201717061335332		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.09201717061335332 | validation: 0.16910468467947443]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996414984180757		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.09996414984180757 | validation: 0.19222547347306385]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09716357010173571		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.09716357010173571 | validation: 0.14246092546773903]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_840.pth
	Model improved!!!
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10430087637489957		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10430087637489957 | validation: 0.20783035440183514]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10781119329218758		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.10781119329218758 | validation: 0.15250490970975467]
	TIME [epoch: 2.67 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10121510919462569		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.10121510919462569 | validation: 0.19198325560006366]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444585525349237		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.08444585525349237 | validation: 0.15307623222274602]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08111598652219186		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.08111598652219186 | validation: 0.17806154482523673]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397059573577863		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.08397059573577863 | validation: 0.15316033376969052]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537287226823406		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.08537287226823406 | validation: 0.21360903342643828]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11106108279021959		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.11106108279021959 | validation: 0.15812251309432895]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12677799615570395		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.12677799615570395 | validation: 0.17454681396857052]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09208525377532642		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.09208525377532642 | validation: 0.15552380497675164]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07763653999339869		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.07763653999339869 | validation: 0.15769757290499664]
	TIME [epoch: 2.67 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07424060616313455		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.07424060616313455 | validation: 0.15323458492828282]
	TIME [epoch: 2.67 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0713274759695872		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.0713274759695872 | validation: 0.14522728883242678]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0756066284802424		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.0756066284802424 | validation: 0.15538421909082112]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671854083416817		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.07671854083416817 | validation: 0.13989109505581623]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07789315627769107		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.07789315627769107 | validation: 0.19843635376326035]
	TIME [epoch: 2.67 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10065496584647735		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.10065496584647735 | validation: 0.2188459883760793]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16614455207592557		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.16614455207592557 | validation: 0.18442217433344574]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09565389718929351		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.09565389718929351 | validation: 0.1618872544907337]
	TIME [epoch: 2.67 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295004266977469		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.07295004266977469 | validation: 0.15718955641993504]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07327336624886871		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.07327336624886871 | validation: 0.16549995681910468]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477497576974124		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07477497576974124 | validation: 0.1428078576765595]
	TIME [epoch: 2.67 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08298123529519633		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.08298123529519633 | validation: 0.19717422887641226]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10074754136781483		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.10074754136781483 | validation: 0.17573292546971192]
	TIME [epoch: 2.67 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12699103116099997		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.12699103116099997 | validation: 0.1934974354712875]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09635645286166972		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.09635645286166972 | validation: 0.1426598727943161]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08082584222053457		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.08082584222053457 | validation: 0.17441049092651648]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609905527976255		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.07609905527976255 | validation: 0.1477522898751252]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08578912885201649		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.08578912885201649 | validation: 0.19674057610040713]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09706257338699406		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.09706257338699406 | validation: 0.14445574019718652]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1104213137004084		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.1104213137004084 | validation: 0.1680811151780352]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08472766238158887		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.08472766238158887 | validation: 0.14371884611363114]
	TIME [epoch: 2.67 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354097512939167		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.07354097512939167 | validation: 0.1554407169748481]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697065256452633		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0697065256452633 | validation: 0.14068142038375464]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248543867457259		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.07248543867457259 | validation: 0.1945680702666231]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08512439938051833		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.08512439938051833 | validation: 0.157301868086781]
	TIME [epoch: 2.67 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09739558586586534		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.09739558586586534 | validation: 0.19330848929700933]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11195837588565528		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.11195837588565528 | validation: 0.13977594471111363]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10420497435831613		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.10420497435831613 | validation: 0.19745080375338797]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976033712129029		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.08976033712129029 | validation: 0.13759215580263678]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08024846372679238		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.08024846372679238 | validation: 0.16688705783737334]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822670022075413		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.07822670022075413 | validation: 0.1457339562528712]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08777467596237629		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.08777467596237629 | validation: 0.1895358676635102]
	TIME [epoch: 2.67 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09842987932677492		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.09842987932677492 | validation: 0.14422193718300674]
	TIME [epoch: 2.67 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09051346672098803		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.09051346672098803 | validation: 0.17249914911710862]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07756635089762318		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.07756635089762318 | validation: 0.14572863356722152]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172620963572865		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.07172620963572865 | validation: 0.16183437180494684]
	TIME [epoch: 2.67 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439621327298147		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.07439621327298147 | validation: 0.13680398545835915]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_888.pth
	Model improved!!!
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812439529198707		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.07812439529198707 | validation: 0.19393441674575426]
	TIME [epoch: 2.67 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129306863308388		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.09129306863308388 | validation: 0.14308057746226815]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786569179898681		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.09786569179898681 | validation: 0.1922266606991031]
	TIME [epoch: 2.67 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229987998428883		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.10229987998428883 | validation: 0.13033554527373353]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743655934371239		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08743655934371239 | validation: 0.150638272016655]
	TIME [epoch: 2.67 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070409957290546		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.070409957290546 | validation: 0.144058861051772]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686875286098637		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.0686875286098637 | validation: 0.15710442580331604]
	TIME [epoch: 2.67 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067628372559177		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.067628372559177 | validation: 0.14193552424552497]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06682848855164433		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.06682848855164433 | validation: 0.1663872794198395]
	TIME [epoch: 2.67 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090293530320663		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.07090293530320663 | validation: 0.14367742699014485]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07659324214848527		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.07659324214848527 | validation: 0.18368802502442808]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806724703428533		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.0806724703428533 | validation: 0.14935687566989855]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09138558153114008		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.09138558153114008 | validation: 0.20103026711289115]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11972518830452518		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.11972518830452518 | validation: 0.1451770761744464]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259046752648633		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.10259046752648633 | validation: 0.1662227007532759]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352896034639649		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.07352896034639649 | validation: 0.13810717627111826]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06972687279803426		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.06972687279803426 | validation: 0.13503119356138799]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874068824584895		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06874068824584895 | validation: 0.17695495423364638]
	TIME [epoch: 2.67 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188058120556235		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.08188058120556235 | validation: 0.15007231793093911]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10483704381249664		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.10483704381249664 | validation: 0.18337573143503164]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09800105642460008		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.09800105642460008 | validation: 0.1335505274330986]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08197600227877315		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.08197600227877315 | validation: 0.18531526308668536]
	TIME [epoch: 2.67 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08803073206362601		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.08803073206362601 | validation: 0.13378672260843935]
	TIME [epoch: 2.67 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810508243583395		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0810508243583395 | validation: 0.1664394373124698]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343857606898878		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.07343857606898878 | validation: 0.1354497303253598]
	TIME [epoch: 2.67 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248126935728127		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.07248126935728127 | validation: 0.1538857232660536]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07055138747231393		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.07055138747231393 | validation: 0.13349441807690354]
	TIME [epoch: 2.67 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753109330590129		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.0753109330590129 | validation: 0.18020937087231004]
	TIME [epoch: 2.67 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835643293826952		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.0835643293826952 | validation: 0.13804759013049153]
	TIME [epoch: 2.67 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185387697568236		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.09185387697568236 | validation: 0.16730496842135734]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07277593507538557		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.07277593507538557 | validation: 0.1398605937428195]
	TIME [epoch: 2.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06602082365515174		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06602082365515174 | validation: 0.14164261474605086]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06794138842158567		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.06794138842158567 | validation: 0.160166368384639]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06775449019827796		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.06775449019827796 | validation: 0.13533571759575533]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06961538783564857		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06961538783564857 | validation: 0.16605388081692843]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07278688857019035		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.07278688857019035 | validation: 0.13292947389385532]
	TIME [epoch: 2.67 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807169343868247		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.0807169343868247 | validation: 0.1959750220533126]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990171513281794		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.09990171513281794 | validation: 0.14124233789037677]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261079808499583		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.11261079808499583 | validation: 0.17500898335117177]
	TIME [epoch: 2.67 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574713432368615		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.07574713432368615 | validation: 0.13387298302986705]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06578671243024145		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.06578671243024145 | validation: 0.1457078912312295]
	TIME [epoch: 2.67 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442134978472659		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.06442134978472659 | validation: 0.13193799392325753]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06716270164413508		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.06716270164413508 | validation: 0.1468357495459188]
	TIME [epoch: 2.68 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670846827939837		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.06670846827939837 | validation: 0.13536300893372247]
	TIME [epoch: 2.67 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653776945092298		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.06653776945092298 | validation: 0.1470627484547937]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719142992177594		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.06719142992177594 | validation: 0.13496727714626897]
	TIME [epoch: 2.67 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07367049746690593		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.07367049746690593 | validation: 0.19993928794290716]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108902390033259		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.10108902390033259 | validation: 0.13796864348604346]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684134538716417		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.11684134538716417 | validation: 0.15194356148620858]
	TIME [epoch: 2.67 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341623020388002		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.07341623020388002 | validation: 0.13860729321965362]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728453484482538		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06728453484482538 | validation: 0.12447670717606836]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06352872541681112		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.06352872541681112 | validation: 0.14876771464326422]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06964586293175291		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.06964586293175291 | validation: 0.13804036600073652]
	TIME [epoch: 2.67 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976436210733977		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07976436210733977 | validation: 0.18929904054853242]
	TIME [epoch: 2.67 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725477894899676		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.08725477894899676 | validation: 0.13226521496922003]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829872289904164		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.08829872289904164 | validation: 0.16281156268623534]
	TIME [epoch: 2.67 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07854880685211973		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.07854880685211973 | validation: 0.12476062265333646]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063561623524216		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.07063561623524216 | validation: 0.1559582569176293]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703514227609113		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.0703514227609113 | validation: 0.13469303525790985]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06923886446386832		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.06923886446386832 | validation: 0.15899372344097384]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07408886609860994		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.07408886609860994 | validation: 0.12564933193384203]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550123627003792		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.08550123627003792 | validation: 0.17500353734859575]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0779269282588603		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.0779269282588603 | validation: 0.11868895351308217]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182223277063393		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.07182223277063393 | validation: 0.17111035498814822]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718921798756942		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.0718921798756942 | validation: 0.13232290174112674]
	TIME [epoch: 2.67 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07097589283841609		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07097589283841609 | validation: 0.16010466814188587]
	TIME [epoch: 2.66 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978058768602584		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.06978058768602584 | validation: 0.12253807212605844]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06787113683049466		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.06787113683049466 | validation: 0.15458179863418264]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608816308306979		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.06608816308306979 | validation: 0.12820963449664444]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694007118186279		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.06694007118186279 | validation: 0.1681070687877827]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07475890518632873		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.07475890518632873 | validation: 0.13260790151699495]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08085915941038938		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.08085915941038938 | validation: 0.15880202552214115]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330667372513947		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.08330667372513947 | validation: 0.12598442343857244]
	TIME [epoch: 2.66 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08045766240423274		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.08045766240423274 | validation: 0.15457193967327276]
	TIME [epoch: 2.66 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806228048956948		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.06806228048956948 | validation: 0.12735814946656415]
	TIME [epoch: 2.67 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551805433523264		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.06551805433523264 | validation: 0.13704708312576827]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115694837367522		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.06115694837367522 | validation: 0.138425550377768]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086202154065143		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.06086202154065143 | validation: 0.12007505632492893]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940947771340168		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.06940947771340168 | validation: 0.17413517894110053]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532866928233954		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.07532866928233954 | validation: 0.13283470445343246]
	TIME [epoch: 2.66 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09836564018221333		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.09836564018221333 | validation: 0.17983897512328728]
	TIME [epoch: 2.67 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160241396829413		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.09160241396829413 | validation: 0.13736046617500594]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07199595222261895		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.07199595222261895 | validation: 0.1508843376539678]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06671311634124638		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.06671311634124638 | validation: 0.12481679032587621]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322121684527444		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.06322121684527444 | validation: 0.12920341560123677]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06202500761989373		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.06202500761989373 | validation: 0.1263192196654619]
	TIME [epoch: 2.67 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006084851010065		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.06006084851010065 | validation: 0.13528043631367112]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958552763039306		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.05958552763039306 | validation: 0.13863335292049644]
	TIME [epoch: 2.66 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056975563584465		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.06056975563584465 | validation: 0.1282599604945238]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06305546734487542		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.06305546734487542 | validation: 0.15212714386835688]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332123364674827		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.06332123364674827 | validation: 0.13404982435149432]
	TIME [epoch: 2.67 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07416815265645152		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.07416815265645152 | validation: 0.1963958928900988]
	TIME [epoch: 2.67 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11107542407202331		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.11107542407202331 | validation: 0.13412617348679065]
	TIME [epoch: 2.67 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10782428555636658		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.10782428555636658 | validation: 0.14067971743994487]
	TIME [epoch: 2.67 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360683794510796		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.06360683794510796 | validation: 0.13633463032281168]
	TIME [epoch: 2.67 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061132139355833115		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.061132139355833115 | validation: 0.11938030250902049]
	TIME [epoch: 2.67 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05983638841646774		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.05983638841646774 | validation: 0.13411161175288075]
	TIME [epoch: 2.67 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546584034406132		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.06546584034406132 | validation: 0.12072838652520708]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498617157983726		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.06498617157983726 | validation: 0.16199700564505548]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795331674915467		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06795331674915467 | validation: 0.12244655928008706]
	TIME [epoch: 2.67 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041430934794753		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.07041430934794753 | validation: 0.15226920915364345]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539838101640832		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06539838101640832 | validation: 0.11945092051192692]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688335131002977		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0688335131002977 | validation: 0.1566129915750224]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642666894841124		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.0642666894841124 | validation: 0.12248450596730023]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06686291247769743		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.06686291247769743 | validation: 0.1716240628698687]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034470129141015		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.08034470129141015 | validation: 0.12255093680047827]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08127621160057384		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.08127621160057384 | validation: 0.14900225979464674]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557154108146981		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06557154108146981 | validation: 0.1264960873307994]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0614672939840689		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.0614672939840689 | validation: 0.12983726713374533]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059750834688633056		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.059750834688633056 | validation: 0.1230141847077856]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716481404667602		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.05716481404667602 | validation: 0.13099767880406302]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05877084284644086		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05877084284644086 | validation: 0.11818882814479051]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900734696087105		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.05900734696087105 | validation: 0.16350804783262446]
	TIME [epoch: 185 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06812324528402364		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06812324528402364 | validation: 0.1284929281197747]
	TIME [epoch: 5.72 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743079958750073		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.08743079958750073 | validation: 0.15509853753872757]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08241694287211015		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.08241694287211015 | validation: 0.11742006179603232]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1004.pth
	Model improved!!!
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06659920341335684		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.06659920341335684 | validation: 0.14627212659885047]
	TIME [epoch: 5.71 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386512839795203		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06386512839795203 | validation: 0.12304383232232073]
	TIME [epoch: 5.71 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06014937262007566		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.06014937262007566 | validation: 0.1305876844659264]
	TIME [epoch: 5.72 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0609956746165004		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.0609956746165004 | validation: 0.11736332129210876]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06351040643939176		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.06351040643939176 | validation: 0.14863317614529326]
	TIME [epoch: 5.72 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433087273382224		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06433087273382224 | validation: 0.12222586817180821]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06558132113689481		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.06558132113689481 | validation: 0.14840499845860253]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645281696180477		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.0645281696180477 | validation: 0.11230781633671803]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908775382400834		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.06908775382400834 | validation: 0.16559187758926655]
	TIME [epoch: 5.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930207109562358		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.07930207109562358 | validation: 0.12072774465552039]
	TIME [epoch: 5.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818508314049523		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.0818508314049523 | validation: 0.15368955639308002]
	TIME [epoch: 5.71 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557037253652516		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06557037253652516 | validation: 0.11833475057810793]
	TIME [epoch: 5.71 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806506257994115		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.05806506257994115 | validation: 0.1255598525073363]
	TIME [epoch: 5.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594525114369801		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.0594525114369801 | validation: 0.12451420058925322]
	TIME [epoch: 5.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057945655658206426		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.057945655658206426 | validation: 0.14245309816017682]
	TIME [epoch: 5.71 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058115534374694744		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.058115534374694744 | validation: 0.12155768462158034]
	TIME [epoch: 5.72 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05951665337445006		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.05951665337445006 | validation: 0.14405662649058978]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06212286684206728		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.06212286684206728 | validation: 0.1199550551237325]
	TIME [epoch: 5.71 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653806670419323		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.0653806670419323 | validation: 0.1559172995156438]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624724572898291		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.06624724572898291 | validation: 0.11599351374676879]
	TIME [epoch: 5.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688015089173351		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.0688015089173351 | validation: 0.14098351332592143]
	TIME [epoch: 5.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06717801853907354		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.06717801853907354 | validation: 0.12287960734094448]
	TIME [epoch: 5.71 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06893701515716005		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.06893701515716005 | validation: 0.14009841304440418]
	TIME [epoch: 5.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06543280560960757		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.06543280560960757 | validation: 0.11518285001713045]
	TIME [epoch: 5.71 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263192720893053		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.06263192720893053 | validation: 0.13522243365519895]
	TIME [epoch: 5.71 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065136733549487		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.06065136733549487 | validation: 0.12298567940612658]
	TIME [epoch: 5.71 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739453878626456		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.05739453878626456 | validation: 0.13014926592065873]
	TIME [epoch: 5.71 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056789890212463466		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.056789890212463466 | validation: 0.12373991498855602]
	TIME [epoch: 5.71 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056275346410320226		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.056275346410320226 | validation: 0.13640908196663815]
	TIME [epoch: 5.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057182562904228594		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.057182562904228594 | validation: 0.12078090315122839]
	TIME [epoch: 5.71 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06108200202579779		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.06108200202579779 | validation: 0.1451118200127003]
	TIME [epoch: 5.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682170152627922		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.0682170152627922 | validation: 0.12762820564901212]
	TIME [epoch: 5.71 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09724554331937789		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.09724554331937789 | validation: 0.17034144787634883]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574516859195607		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.07574516859195607 | validation: 0.1216234150837736]
	TIME [epoch: 5.71 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138108256128064		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.06138108256128064 | validation: 0.13350813367469555]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05689615359210676		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.05689615359210676 | validation: 0.1253941065692298]
	TIME [epoch: 5.71 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05536958218546867		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.05536958218546867 | validation: 0.12710187031607484]
	TIME [epoch: 5.72 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0568776938588878		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.0568776938588878 | validation: 0.1201960752347482]
	TIME [epoch: 5.71 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06014690192591697		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.06014690192591697 | validation: 0.13581768707266165]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096721095170295		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.06096721095170295 | validation: 0.11659851834489512]
	TIME [epoch: 5.71 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918036622902966		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.06918036622902966 | validation: 0.17302540132861507]
	TIME [epoch: 5.71 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07635239722925376		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.07635239722925376 | validation: 0.11658837224636552]
	TIME [epoch: 5.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07111226685470687		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.07111226685470687 | validation: 0.12569397102627722]
	TIME [epoch: 5.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060919747397163757		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.060919747397163757 | validation: 0.11939875345060341]
	TIME [epoch: 5.71 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058668779174812065		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.058668779174812065 | validation: 0.1302730568585395]
	TIME [epoch: 5.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05373336338154371		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05373336338154371 | validation: 0.12040770017873062]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660743630965378		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.05660743630965378 | validation: 0.12632717107547695]
	TIME [epoch: 5.74 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05734777220054234		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.05734777220054234 | validation: 0.12686927299845882]
	TIME [epoch: 5.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05501790616802762		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.05501790616802762 | validation: 0.11948314862350627]
	TIME [epoch: 5.72 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054454555535098204		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.054454555535098204 | validation: 0.12686880293568778]
	TIME [epoch: 5.72 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051938233102743155		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.051938233102743155 | validation: 0.11616856469575343]
	TIME [epoch: 5.72 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0564529161424155		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.0564529161424155 | validation: 0.1401622315491792]
	TIME [epoch: 5.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698146273957728		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06698146273957728 | validation: 0.1261617246355082]
	TIME [epoch: 5.72 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09404811570839343		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.09404811570839343 | validation: 0.1705647689788009]
	TIME [epoch: 5.72 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07816685232123054		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.07816685232123054 | validation: 0.11686565506707972]
	TIME [epoch: 5.72 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058084114223040315		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.058084114223040315 | validation: 0.12103883795226929]
	TIME [epoch: 5.72 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0545654433164785		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.0545654433164785 | validation: 0.1261949046406705]
	TIME [epoch: 5.73 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527274046443387		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.05527274046443387 | validation: 0.11773910132127437]
	TIME [epoch: 5.72 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05613127896311662		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.05613127896311662 | validation: 0.1172935888144878]
	TIME [epoch: 5.72 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05421657020665314		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.05421657020665314 | validation: 0.1266145210406746]
	TIME [epoch: 5.72 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05406238733750494		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.05406238733750494 | validation: 0.1258785484616125]
	TIME [epoch: 5.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343531833509542		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.05343531833509542 | validation: 0.10684341720420046]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1066.pth
	Model improved!!!
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06022935007947707		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.06022935007947707 | validation: 0.1607370048754997]
	TIME [epoch: 5.73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07509164708352152		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.07509164708352152 | validation: 0.11398161362958702]
	TIME [epoch: 5.72 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08809588168798142		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.08809588168798142 | validation: 0.14178108526403682]
	TIME [epoch: 5.72 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061730999407054725		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.061730999407054725 | validation: 0.11296240294601917]
	TIME [epoch: 5.71 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435900557710796		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.05435900557710796 | validation: 0.1150084503423967]
	TIME [epoch: 5.72 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055847300435971955		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.055847300435971955 | validation: 0.11802215863699964]
	TIME [epoch: 5.72 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352031029514604		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.05352031029514604 | validation: 0.12113171521488648]
	TIME [epoch: 5.72 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05556036107173866		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05556036107173866 | validation: 0.11188169811537874]
	TIME [epoch: 5.72 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05458329307906551		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.05458329307906551 | validation: 0.12741748287711938]
	TIME [epoch: 5.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544945989370361		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.05544945989370361 | validation: 0.1168940298262215]
	TIME [epoch: 5.72 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055910917369084726		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.055910917369084726 | validation: 0.13150748060849735]
	TIME [epoch: 5.72 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06090317046875105		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06090317046875105 | validation: 0.11277421139064976]
	TIME [epoch: 5.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047100764724622		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.07047100764724622 | validation: 0.14371143804227163]
	TIME [epoch: 5.71 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511378998684153		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.06511378998684153 | validation: 0.10941779362781023]
	TIME [epoch: 5.72 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05820494675072859		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.05820494675072859 | validation: 0.14183428976474596]
	TIME [epoch: 5.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057434474247603326		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.057434474247603326 | validation: 0.1136800737427274]
	TIME [epoch: 5.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05559507490075262		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05559507490075262 | validation: 0.13682607240757702]
	TIME [epoch: 5.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053373761142242966		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.053373761142242966 | validation: 0.11569639737214912]
	TIME [epoch: 5.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747801370970339		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.05747801370970339 | validation: 0.1261136125890899]
	TIME [epoch: 5.72 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05708865296413381		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.05708865296413381 | validation: 0.11804485013648423]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455432109152101		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.05455432109152101 | validation: 0.14093390234052552]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454272633956551		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.06454272633956551 | validation: 0.10993864196624314]
	TIME [epoch: 5.71 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671511787763859		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.07671511787763859 | validation: 0.14273467446089735]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461159358841131		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.06461159358841131 | validation: 0.11616326172001507]
	TIME [epoch: 5.71 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055644753646832364		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.055644753646832364 | validation: 0.13514961643255705]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05580386068816735		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.05580386068816735 | validation: 0.11840365247785521]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649023027626763		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.05649023027626763 | validation: 0.12534794860316087]
	TIME [epoch: 5.72 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05775321953978292		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.05775321953978292 | validation: 0.10714823531277805]
	TIME [epoch: 5.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05427277312184612		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.05427277312184612 | validation: 0.12304808789552109]
	TIME [epoch: 5.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05159623043479434		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.05159623043479434 | validation: 0.11058543036744801]
	TIME [epoch: 5.71 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05351257540045275		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.05351257540045275 | validation: 0.12007971162704356]
	TIME [epoch: 5.72 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053095502928332934		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.053095502928332934 | validation: 0.11096797993349078]
	TIME [epoch: 5.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055355836832352366		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.055355836832352366 | validation: 0.13032850199346555]
	TIME [epoch: 5.72 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543996852013899		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.0543996852013899 | validation: 0.1081588699921897]
	TIME [epoch: 5.71 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05972426148870682		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.05972426148870682 | validation: 0.14156640072311474]
	TIME [epoch: 5.72 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802983988890655		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.06802983988890655 | validation: 0.11539403316160818]
	TIME [epoch: 5.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649212583283042		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.0649212583283042 | validation: 0.12018676135321199]
	TIME [epoch: 5.72 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05841370255822838		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.05841370255822838 | validation: 0.11403663974490004]
	TIME [epoch: 5.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242680333522838		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.05242680333522838 | validation: 0.10826216174066175]
	TIME [epoch: 5.72 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05313717283983985		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.05313717283983985 | validation: 0.13257846338028298]
	TIME [epoch: 5.71 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05763077272871761		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.05763077272871761 | validation: 0.10425592138678152]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1107.pth
	Model improved!!!
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06268881919830988		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.06268881919830988 | validation: 0.14605368175977856]
	TIME [epoch: 5.72 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06287528297076414		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.06287528297076414 | validation: 0.11240162230388667]
	TIME [epoch: 5.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057949075211419156		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.057949075211419156 | validation: 0.11696438471416558]
	TIME [epoch: 5.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049952417495276825		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.049952417495276825 | validation: 0.10924050898201199]
	TIME [epoch: 5.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052565276683186626		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.052565276683186626 | validation: 0.11785812851601539]
	TIME [epoch: 5.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052753664121800194		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.052753664121800194 | validation: 0.1226288253514984]
	TIME [epoch: 5.72 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149472894345654		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.05149472894345654 | validation: 0.10459755473074117]
	TIME [epoch: 5.71 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051045346126918495		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.051045346126918495 | validation: 0.10530199565727219]
	TIME [epoch: 5.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051689261175572175		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.051689261175572175 | validation: 0.11381837782482768]
	TIME [epoch: 5.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532490459667394		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.0532490459667394 | validation: 0.12444396903457929]
	TIME [epoch: 5.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161137973939742		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.05161137973939742 | validation: 0.11300768173822502]
	TIME [epoch: 5.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017815927415922		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.05017815927415922 | validation: 0.11051047048201508]
	TIME [epoch: 5.71 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054107222574481105		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.054107222574481105 | validation: 0.13649827368782597]
	TIME [epoch: 5.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06007884124931315		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.06007884124931315 | validation: 0.11823097453813128]
	TIME [epoch: 5.71 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105598819823528		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.08105598819823528 | validation: 0.1486729487715471]
	TIME [epoch: 5.71 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272890957596185		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.06272890957596185 | validation: 0.1046954056482024]
	TIME [epoch: 5.71 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05671130360825906		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.05671130360825906 | validation: 0.11184014393429059]
	TIME [epoch: 5.72 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051733529566716355		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.051733529566716355 | validation: 0.12134390402336509]
	TIME [epoch: 5.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051059629302324425		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.051059629302324425 | validation: 0.11437802038707034]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303212815299955		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.05303212815299955 | validation: 0.14387948012797275]
	TIME [epoch: 5.71 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638633666114146		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0638633666114146 | validation: 0.10287174275554399]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06035183477383928		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.06035183477383928 | validation: 0.12293055970772063]
	TIME [epoch: 5.74 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05335336030762653		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05335336030762653 | validation: 0.10873601936711652]
	TIME [epoch: 5.74 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0499267119781859		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.0499267119781859 | validation: 0.11479722870306606]
	TIME [epoch: 5.74 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04981401035552173		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.04981401035552173 | validation: 0.12003891304552411]
	TIME [epoch: 5.73 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05223917145980865		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.05223917145980865 | validation: 0.10440888485862176]
	TIME [epoch: 5.74 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392573537276659		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.05392573537276659 | validation: 0.10859689552384964]
	TIME [epoch: 5.75 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921778271883394		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.04921778271883394 | validation: 0.12010939755366233]
	TIME [epoch: 5.74 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049837867530356565		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.049837867530356565 | validation: 0.11025504534047569]
	TIME [epoch: 5.73 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05008759196965853		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.05008759196965853 | validation: 0.1050184337937292]
	TIME [epoch: 5.74 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052011191764694646		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.052011191764694646 | validation: 0.12644813679739889]
	TIME [epoch: 5.74 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901763857230403		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.05901763857230403 | validation: 0.11032308720925027]
	TIME [epoch: 5.74 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359087392574126		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.07359087392574126 | validation: 0.1276666867436157]
	TIME [epoch: 5.74 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061232547278336345		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.061232547278336345 | validation: 0.10124569379774784]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05092532461799987		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.05092532461799987 | validation: 0.11022909069795445]
	TIME [epoch: 5.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05009094254590357		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.05009094254590357 | validation: 0.12185686542985952]
	TIME [epoch: 5.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289941877471421		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.05289941877471421 | validation: 0.10118389022538961]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1144.pth
	Model improved!!!
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05421769164102087		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.05421769164102087 | validation: 0.14992562881702468]
	TIME [epoch: 5.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07114427727897242		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.07114427727897242 | validation: 0.10454282344405873]
	TIME [epoch: 5.72 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054437315645325904		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.054437315645325904 | validation: 0.10032845120279138]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1147.pth
	Model improved!!!
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830844048353773		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04830844048353773 | validation: 0.10455410817950846]
	TIME [epoch: 5.73 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511277062428227		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.0511277062428227 | validation: 0.11436969686954016]
	TIME [epoch: 5.73 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010489442878262		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05010489442878262 | validation: 0.12638573850524493]
	TIME [epoch: 5.74 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05254113655720856		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.05254113655720856 | validation: 0.10797373786967734]
	TIME [epoch: 5.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554292866289459		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.05554292866289459 | validation: 0.12108766670739045]
	TIME [epoch: 5.72 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231875991166221		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.05231875991166221 | validation: 0.100839712657385]
	TIME [epoch: 5.71 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955349729286985		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.04955349729286985 | validation: 0.10804781171955713]
	TIME [epoch: 5.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049190035517642325		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.049190035517642325 | validation: 0.10344367949469234]
	TIME [epoch: 5.71 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05169245784049129		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.05169245784049129 | validation: 0.11709972078231355]
	TIME [epoch: 5.71 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050577056122150583		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.050577056122150583 | validation: 0.10043572919705486]
	TIME [epoch: 5.72 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255406699983443		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.05255406699983443 | validation: 0.12739558280314925]
	TIME [epoch: 5.73 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463245053136177		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.05463245053136177 | validation: 0.10628361140172433]
	TIME [epoch: 5.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055278254136324846		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.055278254136324846 | validation: 0.11266127305787116]
	TIME [epoch: 5.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052754319332301305		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.052754319332301305 | validation: 0.10249088356017302]
	TIME [epoch: 5.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079869778342488		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05079869778342488 | validation: 0.11522542626926598]
	TIME [epoch: 5.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901476881358978		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.04901476881358978 | validation: 0.10135849430249891]
	TIME [epoch: 5.71 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052060254871537225		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.052060254871537225 | validation: 0.125740532692124]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05402137578755381		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.05402137578755381 | validation: 0.1066801055860056]
	TIME [epoch: 5.72 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328536681160252		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.06328536681160252 | validation: 0.1368754808402187]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05918858560046863		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.05918858560046863 | validation: 0.10041791954813567]
	TIME [epoch: 5.72 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05124072575461349		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.05124072575461349 | validation: 0.11927957693495972]
	TIME [epoch: 5.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04874722648167991		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.04874722648167991 | validation: 0.0955177273460357]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1169.pth
	Model improved!!!
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136300681835582		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.05136300681835582 | validation: 0.10935113077721281]
	TIME [epoch: 5.73 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048912219690534435		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.048912219690534435 | validation: 0.10154714224412459]
	TIME [epoch: 5.73 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0491175323509268		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.0491175323509268 | validation: 0.10935964341642862]
	TIME [epoch: 5.74 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04852137316935668		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.04852137316935668 | validation: 0.10520524146195145]
	TIME [epoch: 5.73 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04968613102312885		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04968613102312885 | validation: 0.11727296372321468]
	TIME [epoch: 5.74 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858153545603352		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.04858153545603352 | validation: 0.10741087008523645]
	TIME [epoch: 5.74 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781526651888218		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.04781526651888218 | validation: 0.10397149398594721]
	TIME [epoch: 5.73 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493351769821		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.0493351769821 | validation: 0.10728703254159384]
	TIME [epoch: 5.73 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0498347703458052		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.0498347703458052 | validation: 0.12576162793150997]
	TIME [epoch: 5.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053638788579959566		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.053638788579959566 | validation: 0.11028881028479509]
	TIME [epoch: 5.72 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06499654022053084		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.06499654022053084 | validation: 0.1391673543008218]
	TIME [epoch: 5.73 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06785883223711232		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.06785883223711232 | validation: 0.10189324966872751]
	TIME [epoch: 5.73 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05588409118317595		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.05588409118317595 | validation: 0.10838952695025116]
	TIME [epoch: 5.74 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814083876665766		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.04814083876665766 | validation: 0.10520975907928841]
	TIME [epoch: 5.73 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064042883812473		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.05064042883812473 | validation: 0.09484079949541212]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1184.pth
	Model improved!!!
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04984339147647246		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.04984339147647246 | validation: 0.1258842347672568]
	TIME [epoch: 5.72 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507763966063281		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.0507763966063281 | validation: 0.10323741804958493]
	TIME [epoch: 5.72 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013890738375389		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.05013890738375389 | validation: 0.11053764580715596]
	TIME [epoch: 5.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048874237548458724		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.048874237548458724 | validation: 0.10070854081918255]
	TIME [epoch: 5.72 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049850207161747456		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.049850207161747456 | validation: 0.11352384791834794]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820560470572806		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.04820560470572806 | validation: 0.09780379958893562]
	TIME [epoch: 5.72 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04983527293722258		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.04983527293722258 | validation: 0.12680104266490266]
	TIME [epoch: 5.72 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05042498077396232		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.05042498077396232 | validation: 0.10866258739681683]
	TIME [epoch: 5.72 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04958376730780794		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.04958376730780794 | validation: 0.1207068307877059]
	TIME [epoch: 5.72 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357356509730362		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.05357356509730362 | validation: 0.09467256739923963]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1194.pth
	Model improved!!!
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05364007819055783		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.05364007819055783 | validation: 0.11754594584230796]
	TIME [epoch: 5.74 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053260966223135475		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.053260966223135475 | validation: 0.10082997744339971]
	TIME [epoch: 5.76 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05367571545887602		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.05367571545887602 | validation: 0.13044173765864006]
	TIME [epoch: 5.74 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04924789787325848		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.04924789787325848 | validation: 0.10859952796505529]
	TIME [epoch: 5.74 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049237088026229364		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.049237088026229364 | validation: 0.1016706948265322]
	TIME [epoch: 5.74 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963042819401587		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.04963042819401587 | validation: 0.1118660237184737]
	TIME [epoch: 5.75 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047866402089935464		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.047866402089935464 | validation: 0.09974287163986591]
	TIME [epoch: 5.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828327396437526		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.04828327396437526 | validation: 0.11718463614718595]
	TIME [epoch: 5.71 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05067355610643234		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.05067355610643234 | validation: 0.09527076731525846]
	TIME [epoch: 5.75 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051010746411293086		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.051010746411293086 | validation: 0.10824151030553593]
	TIME [epoch: 5.75 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068312950163796		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.05068312950163796 | validation: 0.0985227694979585]
	TIME [epoch: 5.75 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952311950397878		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.04952311950397878 | validation: 0.11397128322141481]
	TIME [epoch: 5.75 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049357206158224545		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.049357206158224545 | validation: 0.10322817236559999]
	TIME [epoch: 5.75 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020341234549048		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.05020341234549048 | validation: 0.10546001686335403]
	TIME [epoch: 5.74 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654715025976354		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.04654715025976354 | validation: 0.10722721628324261]
	TIME [epoch: 5.75 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641246868896344		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.04641246868896344 | validation: 0.1076643562557418]
	TIME [epoch: 5.74 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025882437971237		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.05025882437971237 | validation: 0.10054840360403233]
	TIME [epoch: 5.75 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046510304174938295		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.046510304174938295 | validation: 0.09782232040960735]
	TIME [epoch: 5.75 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035038677841417		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.05035038677841417 | validation: 0.1251229502339998]
	TIME [epoch: 5.75 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057677207343341426		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.057677207343341426 | validation: 0.10288420772726972]
	TIME [epoch: 5.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913496905748517		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.06913496905748517 | validation: 0.11143927627950086]
	TIME [epoch: 5.74 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938717488475042		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.04938717488475042 | validation: 0.10391912110414231]
	TIME [epoch: 5.75 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662401015412881		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.04662401015412881 | validation: 0.10329261853301493]
	TIME [epoch: 5.74 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049929482197515127		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.049929482197515127 | validation: 0.12508335051935565]
	TIME [epoch: 5.74 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206116874073749		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.05206116874073749 | validation: 0.09945655660518582]
	TIME [epoch: 5.74 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04761841646015102		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.04761841646015102 | validation: 0.10377103798895977]
	TIME [epoch: 5.74 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046302026123670396		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.046302026123670396 | validation: 0.10803759800270224]
	TIME [epoch: 5.74 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025666984766621		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.05025666984766621 | validation: 0.10755930864040947]
	TIME [epoch: 5.75 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564853644114278		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.04564853644114278 | validation: 0.1046591418397594]
	TIME [epoch: 5.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0459108651862436		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.0459108651862436 | validation: 0.10009910802602036]
	TIME [epoch: 5.75 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793072148088801		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.04793072148088801 | validation: 0.10295662074466705]
	TIME [epoch: 5.74 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050320369528897756		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.050320369528897756 | validation: 0.12531207416057807]
	TIME [epoch: 5.75 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744577214974236		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.05744577214974236 | validation: 0.1040743278335758]
	TIME [epoch: 5.74 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06544220521766063		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.06544220521766063 | validation: 0.11417620232685964]
	TIME [epoch: 5.74 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04753197642062871		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.04753197642062871 | validation: 0.09944695291272367]
	TIME [epoch: 5.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04521976340795387		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.04521976340795387 | validation: 0.10286031819016764]
	TIME [epoch: 5.74 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04865772668712242		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.04865772668712242 | validation: 0.11715761267664399]
	TIME [epoch: 5.74 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04782986652740464		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.04782986652740464 | validation: 0.10336716161643095]
	TIME [epoch: 5.75 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751380312292236		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.04751380312292236 | validation: 0.10608631899565331]
	TIME [epoch: 5.74 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04646657487800498		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.04646657487800498 | validation: 0.11161888521808794]
	TIME [epoch: 5.74 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04683432102545999		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.04683432102545999 | validation: 0.09573108151481953]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047846545410573434		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.047846545410573434 | validation: 0.11560081712932786]
	TIME [epoch: 5.74 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046292011765131764		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.046292011765131764 | validation: 0.09828468488048883]
	TIME [epoch: 5.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768832904673255		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.04768832904673255 | validation: 0.11445444985233494]
	TIME [epoch: 5.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0451009428499178		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.0451009428499178 | validation: 0.0987898057174019]
	TIME [epoch: 5.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04993823045695993		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.04993823045695993 | validation: 0.12260826183374225]
	TIME [epoch: 5.75 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149341785330961		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.05149341785330961 | validation: 0.08818565687118778]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1241.pth
	Model improved!!!
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692923713072931		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.05692923713072931 | validation: 0.10859815407542124]
	TIME [epoch: 5.73 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710450448336666		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.04710450448336666 | validation: 0.10296124780174094]
	TIME [epoch: 5.71 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045994803384096644		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.045994803384096644 | validation: 0.09961822140457581]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046634331775075824		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.046634331775075824 | validation: 0.11128895063366367]
	TIME [epoch: 5.73 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046457847031393947		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.046457847031393947 | validation: 0.09401479236119911]
	TIME [epoch: 5.74 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049849704850511954		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.049849704850511954 | validation: 0.1058641520307089]
	TIME [epoch: 5.74 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04854838848459607		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.04854838848459607 | validation: 0.09742371770509674]
	TIME [epoch: 5.75 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04916633817219946		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.04916633817219946 | validation: 0.11202133618480857]
	TIME [epoch: 5.74 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048783659333048686		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.048783659333048686 | validation: 0.09632477539196876]
	TIME [epoch: 5.74 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04908232180373797		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.04908232180373797 | validation: 0.10363119946938598]
	TIME [epoch: 5.75 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04528909033353358		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.04528909033353358 | validation: 0.11005289791802367]
	TIME [epoch: 5.74 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696633505854003		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.04696633505854003 | validation: 0.098243854594278]
	TIME [epoch: 5.75 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045797075992194014		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.045797075992194014 | validation: 0.1006287893673747]
	TIME [epoch: 5.74 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04819931261766529		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.04819931261766529 | validation: 0.10439291469975234]
	TIME [epoch: 5.74 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781465185057927		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.04781465185057927 | validation: 0.10555850008008466]
	TIME [epoch: 5.74 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04494769856441778		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.04494769856441778 | validation: 0.11683583977469499]
	TIME [epoch: 5.74 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05144369104994955		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05144369104994955 | validation: 0.09467684075846061]
	TIME [epoch: 5.75 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955153905208452		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.05955153905208452 | validation: 0.11422140065409048]
	TIME [epoch: 5.74 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631826290858814		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.04631826290858814 | validation: 0.10661453416135061]
	TIME [epoch: 5.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046537711528203664		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.046537711528203664 | validation: 0.09520056327455959]
	TIME [epoch: 5.74 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809563281567645		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.04809563281567645 | validation: 0.12238923369802504]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167552581261177		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.05167552581261177 | validation: 0.10125418285868276]
	TIME [epoch: 5.73 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048407279726564764		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.048407279726564764 | validation: 0.10251549521900286]
	TIME [epoch: 5.71 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045994785219918594		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.045994785219918594 | validation: 0.11326773822545055]
	TIME [epoch: 5.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047659082000131046		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.047659082000131046 | validation: 0.09945677153700161]
	TIME [epoch: 5.72 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569126368161703		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.04569126368161703 | validation: 0.10564438874273381]
	TIME [epoch: 5.72 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04440848843300693		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.04440848843300693 | validation: 0.09987178983901068]
	TIME [epoch: 5.73 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04546051685995489		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.04546051685995489 | validation: 0.10607807246616166]
	TIME [epoch: 5.72 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04517650780401731		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.04517650780401731 | validation: 0.0961585127715968]
	TIME [epoch: 5.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04699986141666289		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.04699986141666289 | validation: 0.09748272453268891]
	TIME [epoch: 5.71 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04792598957092844		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.04792598957092844 | validation: 0.10895357183028674]
	TIME [epoch: 5.72 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04618656340652638		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.04618656340652638 | validation: 0.11155070684899666]
	TIME [epoch: 5.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481106119256674		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.0481106119256674 | validation: 0.0946317789573271]
	TIME [epoch: 5.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118304572227318		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.05118304572227318 | validation: 0.12489551454304995]
	TIME [epoch: 5.72 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04942831063516643		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.04942831063516643 | validation: 0.10655518284552863]
	TIME [epoch: 5.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04718429788228992		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.04718429788228992 | validation: 0.10364146933207952]
	TIME [epoch: 5.72 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04452291393645062		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.04452291393645062 | validation: 0.10834240427064593]
	TIME [epoch: 5.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0466633550530155		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.0466633550530155 | validation: 0.09510469864671678]
	TIME [epoch: 5.72 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045756590047678625		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.045756590047678625 | validation: 0.10493365372730051]
	TIME [epoch: 5.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04550426580177101		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.04550426580177101 | validation: 0.10634769121819387]
	TIME [epoch: 5.71 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04415715455189718		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.04415715455189718 | validation: 0.10109529358179255]
	TIME [epoch: 5.72 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04848982666054314		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.04848982666054314 | validation: 0.1164352341127823]
	TIME [epoch: 5.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955128351024596		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.04955128351024596 | validation: 0.10160640198271312]
	TIME [epoch: 5.73 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05212297786525264		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.05212297786525264 | validation: 0.12094351631671421]
	TIME [epoch: 5.72 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049842774981581		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.049842774981581 | validation: 0.1014247147442359]
	TIME [epoch: 5.72 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044962148914175666		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.044962148914175666 | validation: 0.10106298890690389]
	TIME [epoch: 5.73 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04518829943368429		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.04518829943368429 | validation: 0.10359356391342707]
	TIME [epoch: 5.72 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045889228642237406		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.045889228642237406 | validation: 0.09342872290073972]
	TIME [epoch: 5.72 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046574108197199565		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.046574108197199565 | validation: 0.11966812778852912]
	TIME [epoch: 5.72 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04553103719090709		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.04553103719090709 | validation: 0.09111077356633994]
	TIME [epoch: 5.71 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739226995966202		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.04739226995966202 | validation: 0.10085982768879485]
	TIME [epoch: 5.72 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04574031204019204		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.04574031204019204 | validation: 0.1020367189507085]
	TIME [epoch: 5.71 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527239432204578		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.04527239432204578 | validation: 0.09921121946634681]
	TIME [epoch: 5.72 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046617672843010334		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.046617672843010334 | validation: 0.09414124116447352]
	TIME [epoch: 5.71 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04386337668361005		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04386337668361005 | validation: 0.10710143809590639]
	TIME [epoch: 5.72 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045858257012277547		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.045858257012277547 | validation: 0.10366814449130413]
	TIME [epoch: 5.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045393322122043625		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.045393322122043625 | validation: 0.11523088285164532]
	TIME [epoch: 5.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04824163249340438		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.04824163249340438 | validation: 0.09623594124366042]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051368081974694564		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.051368081974694564 | validation: 0.11164135262388175]
	TIME [epoch: 5.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049405570248239876		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.049405570248239876 | validation: 0.10337731886690243]
	TIME [epoch: 5.74 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04412860932539729		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.04412860932539729 | validation: 0.10363428142898745]
	TIME [epoch: 5.74 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426078409010792		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.04426078409010792 | validation: 0.10170883784893565]
	TIME [epoch: 5.74 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384529926017906		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.04384529926017906 | validation: 0.09928713636057562]
	TIME [epoch: 5.74 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666746148001209		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.04666746148001209 | validation: 0.09450011169032341]
	TIME [epoch: 5.74 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04517232634864043		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.04517232634864043 | validation: 0.09215243799337336]
	TIME [epoch: 5.73 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04680927123977007		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.04680927123977007 | validation: 0.11054165731821303]
	TIME [epoch: 5.73 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04667912621402609		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.04667912621402609 | validation: 0.08893083307827623]
	TIME [epoch: 5.74 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04742983591953667		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.04742983591953667 | validation: 0.1047077856476411]
	TIME [epoch: 5.74 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558317530691845		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.04558317530691845 | validation: 0.09571821638861291]
	TIME [epoch: 5.74 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04337998172534644		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.04337998172534644 | validation: 0.10142406183567948]
	TIME [epoch: 5.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044471507438884464		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.044471507438884464 | validation: 0.09830996993973079]
	TIME [epoch: 5.71 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045620358717107144		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.045620358717107144 | validation: 0.09528870745450728]
	TIME [epoch: 5.71 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0431824239760007		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.0431824239760007 | validation: 0.1096791820753697]
	TIME [epoch: 5.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045285390650139104		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.045285390650139104 | validation: 0.09658934340615327]
	TIME [epoch: 5.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858692729059661		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.04858692729059661 | validation: 0.11351419084925414]
	TIME [epoch: 5.71 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047615749979768925		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.047615749979768925 | validation: 0.09004355196116638]
	TIME [epoch: 5.71 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048526831164290594		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.048526831164290594 | validation: 0.11208494370440354]
	TIME [epoch: 5.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045108994275595615		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.045108994275595615 | validation: 0.10361282970863153]
	TIME [epoch: 5.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046545849496370646		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.046545849496370646 | validation: 0.08403916084329183]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1320.pth
	Model improved!!!
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818479929029673		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.04818479929029673 | validation: 0.10750581639858386]
	TIME [epoch: 5.73 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045769462270265716		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.045769462270265716 | validation: 0.10137703680951865]
	TIME [epoch: 5.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04417000242741358		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.04417000242741358 | validation: 0.09647852093506537]
	TIME [epoch: 5.73 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04503821095767388		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.04503821095767388 | validation: 0.10622208999221663]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04431734048518024		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.04431734048518024 | validation: 0.09312429850062633]
	TIME [epoch: 5.73 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346037110504103		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.04346037110504103 | validation: 0.10613520438498765]
	TIME [epoch: 5.72 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04718568028275124		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.04718568028275124 | validation: 0.08722184950834375]
	TIME [epoch: 5.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04927865753828829		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.04927865753828829 | validation: 0.11031821872994536]
	TIME [epoch: 5.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046099052605924576		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.046099052605924576 | validation: 0.09730129236571738]
	TIME [epoch: 5.72 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045663396256085155		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.045663396256085155 | validation: 0.09864135743143829]
	TIME [epoch: 5.73 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044360956859422596		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.044360956859422596 | validation: 0.10333476643453006]
	TIME [epoch: 5.73 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562446933433161		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.04562446933433161 | validation: 0.09839655151292918]
	TIME [epoch: 5.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862358034883444		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.04862358034883444 | validation: 0.09469513863929085]
	TIME [epoch: 5.73 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0435866988829535		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.0435866988829535 | validation: 0.09604737942779418]
	TIME [epoch: 5.72 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532298603443224		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.04532298603443224 | validation: 0.10267741789293808]
	TIME [epoch: 5.72 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043440295226624896		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.043440295226624896 | validation: 0.10793161322771572]
	TIME [epoch: 5.73 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551541051907137		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.04551541051907137 | validation: 0.08508803786070057]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04644051900749456		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.04644051900749456 | validation: 0.10014769595710511]
	TIME [epoch: 5.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04164772104782763		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.04164772104782763 | validation: 0.09603396699714209]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041788968048451294		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.041788968048451294 | validation: 0.08983417275649498]
	TIME [epoch: 5.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513910557096461		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.04513910557096461 | validation: 0.09320892252602907]
	TIME [epoch: 5.73 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525712764809063		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.04525712764809063 | validation: 0.09042588734107075]
	TIME [epoch: 5.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04429217843640133		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.04429217843640133 | validation: 0.09831081461487046]
	TIME [epoch: 5.73 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288404452792641		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.04288404452792641 | validation: 0.09920244964532599]
	TIME [epoch: 5.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04414757632061005		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.04414757632061005 | validation: 0.0902567101224331]
	TIME [epoch: 5.73 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04744653305172854		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.04744653305172854 | validation: 0.10945032796791852]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050268201424919		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.050268201424919 | validation: 0.0922375264086559]
	TIME [epoch: 5.75 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691120490770863		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.04691120490770863 | validation: 0.10393895525868313]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04410598838018874		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.04410598838018874 | validation: 0.102775592373093]
	TIME [epoch: 5.73 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044228274495878274		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.044228274495878274 | validation: 0.09599233063597479]
	TIME [epoch: 5.72 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588015947179581		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.04588015947179581 | validation: 0.09158237189078917]
	TIME [epoch: 5.74 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04230965269616281		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.04230965269616281 | validation: 0.09523932956630306]
	TIME [epoch: 5.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043503773740859016		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.043503773740859016 | validation: 0.09590506841372445]
	TIME [epoch: 5.73 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344176581388522		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.04344176581388522 | validation: 0.09461912459241585]
	TIME [epoch: 5.72 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04349355273864305		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.04349355273864305 | validation: 0.1021793331413548]
	TIME [epoch: 5.72 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04323806533230128		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.04323806533230128 | validation: 0.0946706217266398]
	TIME [epoch: 5.71 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04295404100573525		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.04295404100573525 | validation: 0.09016819229872747]
	TIME [epoch: 5.72 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044909643173787386		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.044909643173787386 | validation: 0.11321373114011027]
	TIME [epoch: 5.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045557890372001336		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.045557890372001336 | validation: 0.0906735430116692]
	TIME [epoch: 5.71 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05004886992887094		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.05004886992887094 | validation: 0.10970533937036149]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530213456530046		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.04530213456530046 | validation: 0.09205773427741151]
	TIME [epoch: 5.72 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301508420141208		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.04301508420141208 | validation: 0.09210947284601297]
	TIME [epoch: 5.72 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04449799684071883		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.04449799684071883 | validation: 0.10063267625952169]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043194085204460365		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.043194085204460365 | validation: 0.10205177519055009]
	TIME [epoch: 5.71 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045484425952496396		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.045484425952496396 | validation: 0.09313127142963767]
	TIME [epoch: 5.72 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276912007565879		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.04276912007565879 | validation: 0.10032842757023785]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044222597317485283		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.044222597317485283 | validation: 0.09157975851831382]
	TIME [epoch: 5.72 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04394445610699841		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.04394445610699841 | validation: 0.10058425450077563]
	TIME [epoch: 5.75 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041748277915047906		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.041748277915047906 | validation: 0.09605426553303549]
	TIME [epoch: 5.75 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04165483875002043		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.04165483875002043 | validation: 0.09346282997734956]
	TIME [epoch: 5.74 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04201367669552633		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.04201367669552633 | validation: 0.10295711241777372]
	TIME [epoch: 5.74 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0448520507950552		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.0448520507950552 | validation: 0.09567517536505898]
	TIME [epoch: 5.75 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046344099180033224		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.046344099180033224 | validation: 0.10845121954825224]
	TIME [epoch: 5.75 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674034766901741		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.04674034766901741 | validation: 0.09809697383415078]
	TIME [epoch: 5.75 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380591616041452		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.04380591616041452 | validation: 0.09617533659792112]
	TIME [epoch: 5.75 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384315832652686		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.04384315832652686 | validation: 0.09940720091671072]
	TIME [epoch: 5.74 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044880117941939225		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.044880117941939225 | validation: 0.0929555563721589]
	TIME [epoch: 5.75 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04165842120382992		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.04165842120382992 | validation: 0.09731264160884658]
	TIME [epoch: 5.71 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043792563773128526		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.043792563773128526 | validation: 0.09660548550360781]
	TIME [epoch: 5.71 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041686817749446575		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.041686817749446575 | validation: 0.09185487828583397]
	TIME [epoch: 5.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04182301798148425		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.04182301798148425 | validation: 0.10741346552446066]
	TIME [epoch: 5.71 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04217422944399037		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.04217422944399037 | validation: 0.09496238682967124]
	TIME [epoch: 5.71 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04252322036230523		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.04252322036230523 | validation: 0.09623780906108909]
	TIME [epoch: 5.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04422476289509149		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.04422476289509149 | validation: 0.08320386418491625]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1384.pth
	Model improved!!!
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729513945684428		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.04729513945684428 | validation: 0.10065028699922257]
	TIME [epoch: 5.75 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04775282602336261		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.04775282602336261 | validation: 0.08862144580704866]
	TIME [epoch: 5.74 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04270784508469973		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.04270784508469973 | validation: 0.09076679514259196]
	TIME [epoch: 5.75 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041507188593054994		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.041507188593054994 | validation: 0.09869966864969265]
	TIME [epoch: 5.76 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344602822615306		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.04344602822615306 | validation: 0.09460029908708985]
	TIME [epoch: 5.72 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04283980817827729		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.04283980817827729 | validation: 0.10119059428399245]
	TIME [epoch: 5.72 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046619608036537044		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.046619608036537044 | validation: 0.08921739897085587]
	TIME [epoch: 5.72 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628004014461515		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.04628004014461515 | validation: 0.0935275940347468]
	TIME [epoch: 5.72 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04220134231260467		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.04220134231260467 | validation: 0.10526716941728559]
	TIME [epoch: 5.73 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0428441194367662		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.0428441194367662 | validation: 0.0901050043870622]
	TIME [epoch: 5.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044925412373683936		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.044925412373683936 | validation: 0.10012589008049805]
	TIME [epoch: 5.73 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162283121149196		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.04162283121149196 | validation: 0.09853238358637947]
	TIME [epoch: 5.73 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103697321617151		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.04103697321617151 | validation: 0.09648913836818618]
	TIME [epoch: 5.74 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04133113694223086		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.04133113694223086 | validation: 0.0888969660298223]
	TIME [epoch: 5.75 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042405965471754306		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.042405965471754306 | validation: 0.0941984700583196]
	TIME [epoch: 5.75 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040058233560272154		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.040058233560272154 | validation: 0.09467147639134534]
	TIME [epoch: 5.75 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04138260417682437		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.04138260417682437 | validation: 0.09187171327091405]
	TIME [epoch: 5.85 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146931524396539		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.04146931524396539 | validation: 0.09280889683212372]
	TIME [epoch: 5.76 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419938182359099		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.0419938182359099 | validation: 0.09425751056304549]
	TIME [epoch: 5.76 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04199856156104019		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.04199856156104019 | validation: 0.09191620231302883]
	TIME [epoch: 5.75 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041852955144691756		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.041852955144691756 | validation: 0.10068807670763724]
	TIME [epoch: 5.76 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04243616288542653		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.04243616288542653 | validation: 0.11178782186637798]
	TIME [epoch: 5.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443144498122093		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.04443144498122093 | validation: 0.08054430331271387]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1407.pth
	Model improved!!!
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047815731704162645		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.047815731704162645 | validation: 0.09552862767381605]
	TIME [epoch: 5.74 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043550710108386424		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.043550710108386424 | validation: 0.10098736407464289]
	TIME [epoch: 5.74 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279245716911155		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.04279245716911155 | validation: 0.09387264290454493]
	TIME [epoch: 5.73 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04302821931824318		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.04302821931824318 | validation: 0.09936322603659836]
	TIME [epoch: 5.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04211527817113104		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.04211527817113104 | validation: 0.09663753129810665]
	TIME [epoch: 5.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214944922293113		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.04214944922293113 | validation: 0.09367866795051638]
	TIME [epoch: 5.73 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043080979784175286		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.043080979784175286 | validation: 0.10150115974372385]
	TIME [epoch: 5.71 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041118421691339335		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.041118421691339335 | validation: 0.08887070618605472]
	TIME [epoch: 5.71 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04386223018403375		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.04386223018403375 | validation: 0.09024607743287538]
	TIME [epoch: 5.7 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041436490957538714		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.041436490957538714 | validation: 0.09229928445941298]
	TIME [epoch: 5.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041605678758722335		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.041605678758722335 | validation: 0.09027084179971885]
	TIME [epoch: 5.72 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04333528857404174		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.04333528857404174 | validation: 0.08914595908972239]
	TIME [epoch: 5.71 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04404447691617671		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.04404447691617671 | validation: 0.09650893614197525]
	TIME [epoch: 5.71 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267547117258782		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.04267547117258782 | validation: 0.09000092298649894]
	TIME [epoch: 5.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04225835337199566		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.04225835337199566 | validation: 0.09791969553159165]
	TIME [epoch: 5.71 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04017156426020619		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.04017156426020619 | validation: 0.09647146507739134]
	TIME [epoch: 5.71 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04257764958637021		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.04257764958637021 | validation: 0.09466236582117021]
	TIME [epoch: 5.71 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129835883391089		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.04129835883391089 | validation: 0.09188080723554151]
	TIME [epoch: 5.7 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04298623418760817		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.04298623418760817 | validation: 0.09453400906352918]
	TIME [epoch: 5.7 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042639293211175114		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.042639293211175114 | validation: 0.09372739896834373]
	TIME [epoch: 5.7 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043090300168096174		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.043090300168096174 | validation: 0.10105295591801972]
	TIME [epoch: 5.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04334614959557544		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.04334614959557544 | validation: 0.09792438970725592]
	TIME [epoch: 5.71 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04468759469403464		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.04468759469403464 | validation: 0.09336283716476372]
	TIME [epoch: 5.7 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197073630302678		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.04197073630302678 | validation: 0.10346776540104055]
	TIME [epoch: 5.7 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282965295049726		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.04282965295049726 | validation: 0.09024492646260969]
	TIME [epoch: 5.7 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04331994913664474		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.04331994913664474 | validation: 0.10080331656188722]
	TIME [epoch: 5.7 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265070927047155		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.04265070927047155 | validation: 0.09430084947725736]
	TIME [epoch: 5.7 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042423021316910996		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.042423021316910996 | validation: 0.0934689635031612]
	TIME [epoch: 5.7 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106130256957708		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.04106130256957708 | validation: 0.08989074167805806]
	TIME [epoch: 5.7 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040638620760813995		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.040638620760813995 | validation: 0.08588135964098492]
	TIME [epoch: 5.7 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04220092286944713		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.04220092286944713 | validation: 0.09685430657472972]
	TIME [epoch: 5.7 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267786807284063		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.04267786807284063 | validation: 0.08918600603918891]
	TIME [epoch: 5.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286817298279967		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.04286817298279967 | validation: 0.09667337570793888]
	TIME [epoch: 5.7 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0397231431310508		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.0397231431310508 | validation: 0.09346463003027464]
	TIME [epoch: 5.7 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041319664433028404		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.041319664433028404 | validation: 0.08744781693541706]
	TIME [epoch: 5.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142316734164158		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.04142316734164158 | validation: 0.0849725328691685]
	TIME [epoch: 5.7 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188274926717651		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.04188274926717651 | validation: 0.09805205369509733]
	TIME [epoch: 5.7 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041702750751972495		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.041702750751972495 | validation: 0.09127784632672438]
	TIME [epoch: 5.7 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04183711812576091		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.04183711812576091 | validation: 0.09068363257911058]
	TIME [epoch: 5.7 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0409769784429565		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.0409769784429565 | validation: 0.09609944156454009]
	TIME [epoch: 5.7 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066867632684075		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.04066867632684075 | validation: 0.08762468322773903]
	TIME [epoch: 5.7 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202615732171765		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.04202615732171765 | validation: 0.09592397265569336]
	TIME [epoch: 5.7 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214515950890837		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.04214515950890837 | validation: 0.09459259402852362]
	TIME [epoch: 5.7 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042761217168615584		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.042761217168615584 | validation: 0.09491849065979803]
	TIME [epoch: 5.7 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087761828450887		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.04087761828450887 | validation: 0.10013004106208245]
	TIME [epoch: 5.7 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04220439468371653		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.04220439468371653 | validation: 0.08917571378300772]
	TIME [epoch: 5.7 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04100930435477722		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.04100930435477722 | validation: 0.09035412222252737]
	TIME [epoch: 5.71 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443885448442588		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.04443885448442588 | validation: 0.10457176722822657]
	TIME [epoch: 5.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04204331463703504		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.04204331463703504 | validation: 0.09599622318450071]
	TIME [epoch: 5.7 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040565032083093755		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.040565032083093755 | validation: 0.09210326987190065]
	TIME [epoch: 5.7 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103638458596136		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.04103638458596136 | validation: 0.09618537007881396]
	TIME [epoch: 5.7 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041073878831597244		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.041073878831597244 | validation: 0.08921272708378158]
	TIME [epoch: 5.7 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04128103277360357		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.04128103277360357 | validation: 0.09721808828523971]
	TIME [epoch: 5.7 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041789921905595113		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.041789921905595113 | validation: 0.09955000454634202]
	TIME [epoch: 5.7 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041328448251097036		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.041328448251097036 | validation: 0.08823024811828631]
	TIME [epoch: 5.7 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04241143704103245		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.04241143704103245 | validation: 0.0900622776481606]
	TIME [epoch: 5.7 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041202972454107985		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.041202972454107985 | validation: 0.10311384655770697]
	TIME [epoch: 5.7 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040024519881427224		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.040024519881427224 | validation: 0.08600327825785004]
	TIME [epoch: 5.7 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045588827809512934		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.045588827809512934 | validation: 0.08876954022551949]
	TIME [epoch: 5.7 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04239264446449068		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.04239264446449068 | validation: 0.08981302578372348]
	TIME [epoch: 5.7 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040992512554332616		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.040992512554332616 | validation: 0.09082277664772272]
	TIME [epoch: 5.7 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162542446154759		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.04162542446154759 | validation: 0.09971315030118548]
	TIME [epoch: 5.7 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04224500748649856		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.04224500748649856 | validation: 0.09630235317139628]
	TIME [epoch: 5.7 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04029764558958936		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.04029764558958936 | validation: 0.09164628003460505]
	TIME [epoch: 5.71 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04119242955869865		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.04119242955869865 | validation: 0.08654253804132295]
	TIME [epoch: 5.7 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042835728116683715		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.042835728116683715 | validation: 0.09626142853675634]
	TIME [epoch: 5.7 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04226395129290189		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.04226395129290189 | validation: 0.09503702694892699]
	TIME [epoch: 5.7 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04359892537602278		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.04359892537602278 | validation: 0.10261128729540135]
	TIME [epoch: 5.7 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259549588803731		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.04259549588803731 | validation: 0.0864210000735634]
	TIME [epoch: 5.71 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043137105893134374		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.043137105893134374 | validation: 0.09091256356244698]
	TIME [epoch: 5.7 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103120257723628		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.04103120257723628 | validation: 0.08685309474046697]
	TIME [epoch: 5.7 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040065057343189735		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.040065057343189735 | validation: 0.09002542948224343]
	TIME [epoch: 5.7 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04259759504621464		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.04259759504621464 | validation: 0.09134187996080037]
	TIME [epoch: 5.7 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404578927748149		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.0404578927748149 | validation: 0.08325658629173116]
	TIME [epoch: 5.7 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039487136490900174		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.039487136490900174 | validation: 0.08932027114378087]
	TIME [epoch: 5.7 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093398992309835		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.04093398992309835 | validation: 0.09632840312706428]
	TIME [epoch: 5.71 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04017439469015046		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.04017439469015046 | validation: 0.08775964625986149]
	TIME [epoch: 5.7 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041179116290114635		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.041179116290114635 | validation: 0.0962581746934413]
	TIME [epoch: 5.7 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041599317046247		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.041599317046247 | validation: 0.08488868271370788]
	TIME [epoch: 5.7 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461572754750483		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.04461572754750483 | validation: 0.10014902836761458]
	TIME [epoch: 5.7 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04256472563019015		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.04256472563019015 | validation: 0.08644950072173839]
	TIME [epoch: 5.7 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04039566366176926		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.04039566366176926 | validation: 0.08871382474434342]
	TIME [epoch: 5.7 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04081935923104504		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.04081935923104504 | validation: 0.09282493709862362]
	TIME [epoch: 5.7 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038074186316937424		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.038074186316937424 | validation: 0.0977128870262799]
	TIME [epoch: 5.7 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04164267990871649		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.04164267990871649 | validation: 0.09478088597631595]
	TIME [epoch: 5.7 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040160157052880265		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.040160157052880265 | validation: 0.09349907296834525]
	TIME [epoch: 5.69 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264912811979151		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.04264912811979151 | validation: 0.09557664519434104]
	TIME [epoch: 5.69 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04112044759452262		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.04112044759452262 | validation: 0.08316218484084346]
	TIME [epoch: 5.7 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106198516624875		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.04106198516624875 | validation: 0.09045041966213885]
	TIME [epoch: 5.7 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046970232929296		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.04046970232929296 | validation: 0.08223537661885265]
	TIME [epoch: 5.71 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04082857936449857		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.04082857936449857 | validation: 0.08680241636371473]
	TIME [epoch: 5.7 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120227928662324		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.04120227928662324 | validation: 0.08888252085747618]
	TIME [epoch: 5.7 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073844976252721		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.04073844976252721 | validation: 0.08729052264982515]
	TIME [epoch: 5.7 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040906386032778234		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.040906386032778234 | validation: 0.10012530520819624]
	TIME [epoch: 5.69 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042252397508560635		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.042252397508560635 | validation: 0.07921494937114566]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1502.pth
	Model improved!!!
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041185129718454956		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.041185129718454956 | validation: 0.09931919624318214]
	TIME [epoch: 5.7 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142784921187913		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.04142784921187913 | validation: 0.08899564885578823]
	TIME [epoch: 5.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188099957107444		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.04188099957107444 | validation: 0.08928251119780585]
	TIME [epoch: 5.7 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04040595556575898		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.04040595556575898 | validation: 0.08894775082182717]
	TIME [epoch: 5.7 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007830907418014		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.04007830907418014 | validation: 0.08662970415354414]
	TIME [epoch: 5.71 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04104571509739482		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.04104571509739482 | validation: 0.0896959341011803]
	TIME [epoch: 5.7 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060336253824924		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.04060336253824924 | validation: 0.09071661180901038]
	TIME [epoch: 5.7 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039492536460018636		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.039492536460018636 | validation: 0.08440220013032076]
	TIME [epoch: 5.7 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039931356200477046		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.039931356200477046 | validation: 0.09662404540850307]
	TIME [epoch: 5.7 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041783319648104		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.04041783319648104 | validation: 0.08730808483490468]
	TIME [epoch: 5.7 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236219565993785		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.04236219565993785 | validation: 0.09101645020229883]
	TIME [epoch: 5.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395824931594628		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.0395824931594628 | validation: 0.08648045818556746]
	TIME [epoch: 5.69 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040214507694292745		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.040214507694292745 | validation: 0.09069860807340752]
	TIME [epoch: 5.7 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04032554247002647		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.04032554247002647 | validation: 0.09421000911636998]
	TIME [epoch: 5.7 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042172493357799885		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.042172493357799885 | validation: 0.0873026345221235]
	TIME [epoch: 5.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0409613702043454		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.0409613702043454 | validation: 0.09593267867954035]
	TIME [epoch: 5.69 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04065946444257531		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.04065946444257531 | validation: 0.10254399504445387]
	TIME [epoch: 5.7 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898253644173021		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.03898253644173021 | validation: 0.08563246453430229]
	TIME [epoch: 5.7 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066876455948927		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.04066876455948927 | validation: 0.09168769789994476]
	TIME [epoch: 5.7 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04014979852660658		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.04014979852660658 | validation: 0.09127905399618287]
	TIME [epoch: 5.69 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04165182817625892		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.04165182817625892 | validation: 0.08739362306187193]
	TIME [epoch: 5.71 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0418763945508884		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.0418763945508884 | validation: 0.08845462643996219]
	TIME [epoch: 5.7 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023469165977816		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.04023469165977816 | validation: 0.08660214652516485]
	TIME [epoch: 5.7 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957241594840638		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.03957241594840638 | validation: 0.08979379315456221]
	TIME [epoch: 5.69 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914678823506181		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.03914678823506181 | validation: 0.09113101362753767]
	TIME [epoch: 5.7 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030666002071625		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.04030666002071625 | validation: 0.07956329606218927]
	TIME [epoch: 5.7 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03999609488520119		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.03999609488520119 | validation: 0.07834653284914278]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1529.pth
	Model improved!!!
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039930184142483796		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.039930184142483796 | validation: 0.0986911096050251]
	TIME [epoch: 5.7 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039785197153668066		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.039785197153668066 | validation: 0.08305044978320976]
	TIME [epoch: 5.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041066782440848465		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.041066782440848465 | validation: 0.0924813332755595]
	TIME [epoch: 5.7 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813285998801499		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.03813285998801499 | validation: 0.09149592008242173]
	TIME [epoch: 5.71 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03919673869544689		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.03919673869544689 | validation: 0.08469581579449419]
	TIME [epoch: 5.7 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041214782572958665		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.041214782572958665 | validation: 0.09697976618812507]
	TIME [epoch: 5.7 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040009807499643724		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.040009807499643724 | validation: 0.09330408059945663]
	TIME [epoch: 5.69 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03921871218755687		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.03921871218755687 | validation: 0.08966805878805464]
	TIME [epoch: 5.7 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126219929311068		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.04126219929311068 | validation: 0.08992724561413509]
	TIME [epoch: 5.7 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041543699273046485		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.041543699273046485 | validation: 0.0851150031626984]
	TIME [epoch: 5.7 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147243544406317		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.04147243544406317 | validation: 0.08591312335841483]
	TIME [epoch: 5.69 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993226420450931		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.03993226420450931 | validation: 0.0885945467314499]
	TIME [epoch: 5.7 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040498822922287625		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.040498822922287625 | validation: 0.08986558353541586]
	TIME [epoch: 5.69 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03971754461137083		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.03971754461137083 | validation: 0.08749725186199406]
	TIME [epoch: 5.7 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03746939233010694		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.03746939233010694 | validation: 0.08725366298142452]
	TIME [epoch: 5.69 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040929597281440924		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.040929597281440924 | validation: 0.09121060187729951]
	TIME [epoch: 5.7 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03999084512366753		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.03999084512366753 | validation: 0.09647082026594028]
	TIME [epoch: 5.69 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087513556708082		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.04087513556708082 | validation: 0.08937725530009716]
	TIME [epoch: 5.7 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039820414724489626		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.039820414724489626 | validation: 0.08872331475005911]
	TIME [epoch: 5.69 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04227612585708016		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.04227612585708016 | validation: 0.07893283584306618]
	TIME [epoch: 5.71 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303414683780519		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.04303414683780519 | validation: 0.09272705673235904]
	TIME [epoch: 5.69 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411290001578269		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.0411290001578269 | validation: 0.09443271664751966]
	TIME [epoch: 5.7 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389573412759506		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.0389573412759506 | validation: 0.09214877221868106]
	TIME [epoch: 5.7 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04001458860046732		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.04001458860046732 | validation: 0.08751168985930642]
	TIME [epoch: 5.71 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04175674922561503		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.04175674922561503 | validation: 0.09891200316284555]
	TIME [epoch: 5.7 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040102910672195284		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.040102910672195284 | validation: 0.09422300311662424]
	TIME [epoch: 5.7 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852500022621433		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.03852500022621433 | validation: 0.09267212698026893]
	TIME [epoch: 5.7 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039809334789752245		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.039809334789752245 | validation: 0.08908048725986235]
	TIME [epoch: 5.7 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041488695616713755		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.041488695616713755 | validation: 0.08173168139301967]
	TIME [epoch: 5.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073917084842754		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.04073917084842754 | validation: 0.08313986037799628]
	TIME [epoch: 5.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040093992765838		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.040093992765838 | validation: 0.08485332888306592]
	TIME [epoch: 5.7 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03999116156148702		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.03999116156148702 | validation: 0.09598877951903537]
	TIME [epoch: 5.71 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038811568287416824		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.038811568287416824 | validation: 0.08031644958173537]
	TIME [epoch: 5.7 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400399043397177		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.0400399043397177 | validation: 0.08264882172289986]
	TIME [epoch: 5.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066761377347893		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.04066761377347893 | validation: 0.09556953539449264]
	TIME [epoch: 5.7 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993444817027825		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.03993444817027825 | validation: 0.08439513341363833]
	TIME [epoch: 5.69 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04203737031372833		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.04203737031372833 | validation: 0.08965643601424403]
	TIME [epoch: 5.69 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04016794288361965		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.04016794288361965 | validation: 0.0900231799472826]
	TIME [epoch: 5.7 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04072148986196664		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.04072148986196664 | validation: 0.08593522125415043]
	TIME [epoch: 5.69 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040622309982352285		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.040622309982352285 | validation: 0.08122524241610393]
	TIME [epoch: 5.7 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04006983951360269		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.04006983951360269 | validation: 0.09533946683773509]
	TIME [epoch: 5.7 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040612652434136164		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.040612652434136164 | validation: 0.0896100318817627]
	TIME [epoch: 5.7 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04180504918216523		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.04180504918216523 | validation: 0.09046616869654965]
	TIME [epoch: 5.7 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038840182129586696		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.038840182129586696 | validation: 0.09325364335595913]
	TIME [epoch: 5.69 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03999303073206117		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.03999303073206117 | validation: 0.08765305969327396]
	TIME [epoch: 5.7 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037742717833933855		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.037742717833933855 | validation: 0.09247493310157169]
	TIME [epoch: 5.7 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039437747896584906		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.039437747896584906 | validation: 0.08749637975649263]
	TIME [epoch: 5.7 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038531607197431414		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.038531607197431414 | validation: 0.08936197194239073]
	TIME [epoch: 5.69 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04031019271339163		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.04031019271339163 | validation: 0.07925135511398017]
	TIME [epoch: 5.7 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0410414217170269		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.0410414217170269 | validation: 0.0983660834597872]
	TIME [epoch: 5.7 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04070446613481283		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.04070446613481283 | validation: 0.08511078553452078]
	TIME [epoch: 5.7 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039677912301564165		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.039677912301564165 | validation: 0.08429825044304573]
	TIME [epoch: 5.69 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973167812874941		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.03973167812874941 | validation: 0.09548367545943526]
	TIME [epoch: 5.7 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0409327957218146		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.0409327957218146 | validation: 0.08328352578791012]
	TIME [epoch: 5.69 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868586167420825		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.03868586167420825 | validation: 0.09451886718172628]
	TIME [epoch: 5.72 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103344814728809		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.04103344814728809 | validation: 0.08457281596953568]
	TIME [epoch: 5.71 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04072570372203727		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.04072570372203727 | validation: 0.09039750853285983]
	TIME [epoch: 5.71 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03831567831459708		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.03831567831459708 | validation: 0.09357427614911315]
	TIME [epoch: 5.71 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038495635679221055		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.038495635679221055 | validation: 0.08459687609017442]
	TIME [epoch: 5.72 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040575937482730355		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.040575937482730355 | validation: 0.08441238277675536]
	TIME [epoch: 5.71 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038390980656129996		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.038390980656129996 | validation: 0.08888636563522495]
	TIME [epoch: 5.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039178826054889856		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.039178826054889856 | validation: 0.08403115036217772]
	TIME [epoch: 5.71 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04088063235110747		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.04088063235110747 | validation: 0.08486950388069775]
	TIME [epoch: 5.71 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040732681651440936		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.040732681651440936 | validation: 0.08314842938968932]
	TIME [epoch: 5.71 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040022206268030845		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.040022206268030845 | validation: 0.08350713972449082]
	TIME [epoch: 5.71 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388838329986983		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.0388838329986983 | validation: 0.09725405682688801]
	TIME [epoch: 5.71 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384640574052429		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.0384640574052429 | validation: 0.08355101562581108]
	TIME [epoch: 5.72 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030111804188522		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.04030111804188522 | validation: 0.08158251201733954]
	TIME [epoch: 5.71 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03794011938971034		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.03794011938971034 | validation: 0.0900981192386705]
	TIME [epoch: 5.71 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179894960795147		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.04179894960795147 | validation: 0.07772045827228433]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1599.pth
	Model improved!!!
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04004174202357501		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.04004174202357501 | validation: 0.08117743628146983]
	TIME [epoch: 5.71 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03970543655509128		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.03970543655509128 | validation: 0.09740669966014831]
	TIME [epoch: 5.71 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039376928203005486		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.039376928203005486 | validation: 0.08663848749696512]
	TIME [epoch: 5.71 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03937778730271182		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.03937778730271182 | validation: 0.08641192130645096]
	TIME [epoch: 5.71 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03956250069577345		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.03956250069577345 | validation: 0.08794875383774595]
	TIME [epoch: 5.7 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942718298551939		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.03942718298551939 | validation: 0.08739255206523512]
	TIME [epoch: 5.71 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03785427857422796		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.03785427857422796 | validation: 0.08753014825651208]
	TIME [epoch: 5.72 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03931108845078256		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.03931108845078256 | validation: 0.09044631801783844]
	TIME [epoch: 5.71 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038721817878886185		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.038721817878886185 | validation: 0.0897043062973326]
	TIME [epoch: 5.71 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106851203418516		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.04106851203418516 | validation: 0.08713084876462354]
	TIME [epoch: 5.71 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916778463320643		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.03916778463320643 | validation: 0.08614177435709164]
	TIME [epoch: 5.72 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038833680865545686		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.038833680865545686 | validation: 0.08964606565399078]
	TIME [epoch: 5.71 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927859457677983		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.03927859457677983 | validation: 0.0829327019253812]
	TIME [epoch: 5.71 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039103756553650115		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.039103756553650115 | validation: 0.09287374955002241]
	TIME [epoch: 5.71 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03698533484244279		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.03698533484244279 | validation: 0.09516328573887355]
	TIME [epoch: 5.71 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039506972467573225		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.039506972467573225 | validation: 0.08258841164725883]
	TIME [epoch: 5.7 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038502741077591544		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.038502741077591544 | validation: 0.0964605060552568]
	TIME [epoch: 5.71 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03912108285424337		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.03912108285424337 | validation: 0.08672822103563528]
	TIME [epoch: 5.7 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03774092407938215		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.03774092407938215 | validation: 0.08434279467779743]
	TIME [epoch: 5.71 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037303874798680456		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.037303874798680456 | validation: 0.08750704384595703]
	TIME [epoch: 5.7 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885171561577983		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.03885171561577983 | validation: 0.08909824329358568]
	TIME [epoch: 5.71 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945007788992525		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.03945007788992525 | validation: 0.08669521483465624]
	TIME [epoch: 5.71 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039607001297008344		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.039607001297008344 | validation: 0.08631474555589193]
	TIME [epoch: 5.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942317558041812		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.03942317558041812 | validation: 0.07934736070882215]
	TIME [epoch: 5.7 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039931755815756365		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.039931755815756365 | validation: 0.0806915372518769]
	TIME [epoch: 5.7 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023477849945972		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.04023477849945972 | validation: 0.09148818464910446]
	TIME [epoch: 5.7 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378289287449816		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.0378289287449816 | validation: 0.09023361813674753]
	TIME [epoch: 5.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03894685555254232		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.03894685555254232 | validation: 0.0952793441652436]
	TIME [epoch: 5.71 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401689193289498		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.0401689193289498 | validation: 0.08817705735685874]
	TIME [epoch: 5.7 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03841134188643248		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.03841134188643248 | validation: 0.09005374575997782]
	TIME [epoch: 5.7 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03908855990265351		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.03908855990265351 | validation: 0.08999218211639909]
	TIME [epoch: 5.7 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03823227339382811		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.03823227339382811 | validation: 0.09289739835963177]
	TIME [epoch: 5.7 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03716828176513711		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.03716828176513711 | validation: 0.08431797232579906]
	TIME [epoch: 5.71 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04034817178314411		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.04034817178314411 | validation: 0.08583550114453838]
	TIME [epoch: 5.7 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886902426176342		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.03886902426176342 | validation: 0.0821954216622886]
	TIME [epoch: 5.7 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888205982161679		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.03888205982161679 | validation: 0.0892393232885852]
	TIME [epoch: 5.71 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03870098333972976		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.03870098333972976 | validation: 0.08578873031301675]
	TIME [epoch: 5.7 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03720974440875115		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.03720974440875115 | validation: 0.09284758449636321]
	TIME [epoch: 5.71 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897821726995967		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.03897821726995967 | validation: 0.09322172546576743]
	TIME [epoch: 5.7 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906657671388194		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.03906657671388194 | validation: 0.08548505754626062]
	TIME [epoch: 5.7 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0407009857347634		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.0407009857347634 | validation: 0.08554126182822303]
	TIME [epoch: 5.7 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039313721299311426		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.039313721299311426 | validation: 0.08509085342500365]
	TIME [epoch: 5.71 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042374345794621994		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.042374345794621994 | validation: 0.09751789671195833]
	TIME [epoch: 5.71 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040347794855645644		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.040347794855645644 | validation: 0.09000712408695932]
	TIME [epoch: 5.71 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039470913351484484		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.039470913351484484 | validation: 0.09065015023197838]
	TIME [epoch: 5.7 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038888284638967444		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.038888284638967444 | validation: 0.0816156403916937]
	TIME [epoch: 5.71 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392365479926002		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.0392365479926002 | validation: 0.08497769590938009]
	TIME [epoch: 5.7 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768046741818408		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.03768046741818408 | validation: 0.07997966158966714]
	TIME [epoch: 5.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03960601322893944		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.03960601322893944 | validation: 0.08536207943239722]
	TIME [epoch: 5.7 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04028071428586373		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.04028071428586373 | validation: 0.08535193410654701]
	TIME [epoch: 5.7 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03907270079766922		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.03907270079766922 | validation: 0.09122635558484371]
	TIME [epoch: 5.7 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037754737749902886		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.037754737749902886 | validation: 0.08703464073772145]
	TIME [epoch: 5.7 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038812740029519864		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.038812740029519864 | validation: 0.09335380282219766]
	TIME [epoch: 5.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927824166180204		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.03927824166180204 | validation: 0.08779571765464174]
	TIME [epoch: 5.71 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386340982336972		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.0386340982336972 | validation: 0.08198744398633931]
	TIME [epoch: 5.71 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687682928729735		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.03687682928729735 | validation: 0.09372610321675992]
	TIME [epoch: 5.7 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03919045829444869		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.03919045829444869 | validation: 0.0902207399850234]
	TIME [epoch: 5.7 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967287433766872		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.03967287433766872 | validation: 0.08256958227125476]
	TIME [epoch: 5.7 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804096994749055		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.03804096994749055 | validation: 0.0882635352984627]
	TIME [epoch: 5.71 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03714370162991455		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.03714370162991455 | validation: 0.08628358929705866]
	TIME [epoch: 5.7 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039914651805858875		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.039914651805858875 | validation: 0.09121422841246742]
	TIME [epoch: 5.71 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693023924664504		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.03693023924664504 | validation: 0.09214458054092095]
	TIME [epoch: 5.71 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038538702257817574		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.038538702257817574 | validation: 0.08527082832080218]
	TIME [epoch: 5.71 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04011319827044263		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.04011319827044263 | validation: 0.08396054772428693]
	TIME [epoch: 5.72 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038255817628955795		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.038255817628955795 | validation: 0.08503320249523771]
	TIME [epoch: 5.71 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03872176836748708		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.03872176836748708 | validation: 0.08408607538609947]
	TIME [epoch: 5.71 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03787011687246071		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.03787011687246071 | validation: 0.09234873379256443]
	TIME [epoch: 5.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914022292837383		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.03914022292837383 | validation: 0.07838062223679815]
	TIME [epoch: 5.71 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038798724903841515		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.038798724903841515 | validation: 0.08767614717662378]
	TIME [epoch: 5.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040155723176212736		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.040155723176212736 | validation: 0.09294413778571375]
	TIME [epoch: 5.71 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850890908404382		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.03850890908404382 | validation: 0.08487136018890719]
	TIME [epoch: 5.7 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03766624233143933		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.03766624233143933 | validation: 0.08152117492625068]
	TIME [epoch: 5.7 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038441213369237386		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.038441213369237386 | validation: 0.08649510367465756]
	TIME [epoch: 5.7 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03846421291738987		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.03846421291738987 | validation: 0.0843741809621075]
	TIME [epoch: 5.7 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037161206421277994		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.037161206421277994 | validation: 0.079788862074141]
	TIME [epoch: 5.71 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038864259092531626		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.038864259092531626 | validation: 0.08796940169307313]
	TIME [epoch: 5.7 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039518593596368015		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.039518593596368015 | validation: 0.08650088122687555]
	TIME [epoch: 5.71 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03917818766465953		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.03917818766465953 | validation: 0.08169969582744246]
	TIME [epoch: 5.71 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601409369822467		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.03601409369822467 | validation: 0.08272317270866336]
	TIME [epoch: 5.7 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038977068511369194		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.038977068511369194 | validation: 0.0866695928175284]
	TIME [epoch: 5.71 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03827128639153803		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.03827128639153803 | validation: 0.08031090506919131]
	TIME [epoch: 5.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790030290135232		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.03790030290135232 | validation: 0.0841988283651294]
	TIME [epoch: 5.7 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037511431847247145		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.037511431847247145 | validation: 0.09952006655432061]
	TIME [epoch: 5.7 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036015583134130995		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.036015583134130995 | validation: 0.08942601543599837]
	TIME [epoch: 5.7 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039029092236668346		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.039029092236668346 | validation: 0.08589710191420669]
	TIME [epoch: 5.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748973817403188		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.03748973817403188 | validation: 0.08570774822119637]
	TIME [epoch: 5.71 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03739308841535045		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.03739308841535045 | validation: 0.08237897004480352]
	TIME [epoch: 5.71 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04031214446892828		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.04031214446892828 | validation: 0.09329143694616365]
	TIME [epoch: 5.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836910946567799		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.03836910946567799 | validation: 0.08708545566139074]
	TIME [epoch: 5.71 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03832506906952051		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.03832506906952051 | validation: 0.08511502099789622]
	TIME [epoch: 5.71 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860983384065413		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.03860983384065413 | validation: 0.08803258197507005]
	TIME [epoch: 5.7 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886842345104766		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.03886842345104766 | validation: 0.08431646599180848]
	TIME [epoch: 5.71 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791759864716159		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.03791759864716159 | validation: 0.08263719199197239]
	TIME [epoch: 5.7 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0379396291404251		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.0379396291404251 | validation: 0.08793181325745399]
	TIME [epoch: 5.71 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909852389291784		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.03909852389291784 | validation: 0.08844603700480802]
	TIME [epoch: 5.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03817382705548221		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.03817382705548221 | validation: 0.08138041421636448]
	TIME [epoch: 5.7 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038492084267508664		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.038492084267508664 | validation: 0.08623003399748334]
	TIME [epoch: 5.7 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0377156706792462		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.0377156706792462 | validation: 0.09283450914397626]
	TIME [epoch: 5.71 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03839369519716767		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.03839369519716767 | validation: 0.0921200801260781]
	TIME [epoch: 5.7 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039206155623578265		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.039206155623578265 | validation: 0.08998052376635786]
	TIME [epoch: 5.71 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389635856493214		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.0389635856493214 | validation: 0.08705094056828447]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_6_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v1_6_v_mmd4_1700.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6742.066 seconds.
