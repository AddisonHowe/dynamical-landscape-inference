Args:
Namespace(name='model_phi1_4a_distortion_v1_3_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_3/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1564259370

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.549488282216559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.549488282216559 | validation: 6.249912285515717]
	TIME [epoch: 162 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.907971622291566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.907971622291566 | validation: 6.497423854999233]
	TIME [epoch: 0.75 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.556825161253538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.556825161253538 | validation: 6.39424862972274]
	TIME [epoch: 0.686 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.189820042713863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.189820042713863 | validation: 5.996985147248317]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.581025632750023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.581025632750023 | validation: 5.896999544537779]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5912152697191075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5912152697191075 | validation: 5.824647532450254]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.524712629635097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.524712629635097 | validation: 6.102553184648432]
	TIME [epoch: 0.69 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.502527098781398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.502527098781398 | validation: 5.963153314618654]
	TIME [epoch: 0.688 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2849535117636774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2849535117636774 | validation: 5.733271002260368]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.23398891095451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.23398891095451 | validation: 5.771099325081909]
	TIME [epoch: 0.687 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.064924022576967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.064924022576967 | validation: 5.5979310433548735]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.894880899814416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.894880899814416 | validation: 5.521381444045763]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7081356711244062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7081356711244062 | validation: 5.678632988683496]
	TIME [epoch: 0.69 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.768496515050474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.768496515050474 | validation: 5.982896279596911]
	TIME [epoch: 0.688 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.819194026567986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.819194026567986 | validation: 5.731707165976562]
	TIME [epoch: 0.689 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.365119406222657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.365119406222657 | validation: 5.613388616479032]
	TIME [epoch: 0.693 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8328545430421537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8328545430421537 | validation: 5.348322362246153]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3022934170945737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3022934170945737 | validation: 5.220516199040644]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3761311940106475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3761311940106475 | validation: 5.4581051794397135]
	TIME [epoch: 0.687 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.624730840839626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.624730840839626 | validation: 5.152101259493807]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2667246305146262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2667246305146262 | validation: 5.197160552494315]
	TIME [epoch: 0.692 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1303404000292403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1303404000292403 | validation: 5.059897794374846]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0674715764421787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0674715764421787 | validation: 5.1144076548669775]
	TIME [epoch: 0.685 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.14583712932864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.14583712932864 | validation: 5.197964491196141]
	TIME [epoch: 0.683 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.522577812290332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.522577812290332 | validation: 5.037764034407963]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1494479753708906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1494479753708906 | validation: 4.9343861201704655]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.955677945575494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.955677945575494 | validation: 4.945225547871911]
	TIME [epoch: 0.691 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9349542377218105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9349542377218105 | validation: 4.884914969141211]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9981059399069783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9981059399069783 | validation: 4.9190881442943635]
	TIME [epoch: 0.687 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0908298023446834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0908298023446834 | validation: 4.934988787787492]
	TIME [epoch: 0.686 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1505821073737708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1505821073737708 | validation: 4.805556541447226]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9321115215143196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9321115215143196 | validation: 4.766744628867623]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.827465561200682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.827465561200682 | validation: 4.722227957361176]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7978275674063258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7978275674063258 | validation: 4.687452614932167]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.785492956691361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.785492956691361 | validation: 4.662283961011349]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7832630888497873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7832630888497873 | validation: 4.695557709935911]
	TIME [epoch: 0.687 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8439158901794634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8439158901794634 | validation: 4.671953168700337]
	TIME [epoch: 0.685 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.028458880064684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.028458880064684 | validation: 4.72220765824766]
	TIME [epoch: 0.686 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.941501558840411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.941501558840411 | validation: 4.5884888980102945]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.79602909788376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.79602909788376 | validation: 4.573909119308512]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7065776235806673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7065776235806673 | validation: 4.523545508450827]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6896896013690736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6896896013690736 | validation: 4.505144389137183]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.681511419310185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.681511419310185 | validation: 4.467769501876115]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6733756462100473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6733756462100473 | validation: 4.4828582835368795]
	TIME [epoch: 0.683 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.688451573475031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.688451573475031 | validation: 4.430479572118182]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.75485886249467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.75485886249467 | validation: 4.562388760614072]
	TIME [epoch: 0.686 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.870305617521797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.870305617521797 | validation: 4.379813229513952]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7484452080062804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7484452080062804 | validation: 4.4184294794917145]
	TIME [epoch: 0.685 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6594849489623686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6594849489623686 | validation: 4.342015162649424]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.595654592149713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.595654592149713 | validation: 4.33127690054652]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5846258159851345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5846258159851345 | validation: 4.299094696253323]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.577796941478257		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.577796941478257 | validation: 4.28348253995308]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.56582598379801		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.56582598379801 | validation: 4.223365795088552]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.562369845268146		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.562369845268146 | validation: 4.301637553102746]
	TIME [epoch: 0.683 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6132152277473004		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 2.6132152277473004 | validation: 4.194684019897709]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6948511832305413		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.6948511832305413 | validation: 4.326847155252123]
	TIME [epoch: 0.691 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6743685134948167		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.6743685134948167 | validation: 4.162246378050294]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5502788404518477		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.5502788404518477 | validation: 4.162887717164208]
	TIME [epoch: 0.683 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.509428259948381		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.509428259948381 | validation: 4.136999200445408]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4980176575825794		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.4980176575825794 | validation: 4.083989478940528]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.489609502058726		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.489609502058726 | validation: 4.108520299877438]
	TIME [epoch: 0.688 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.479254389637429		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.479254389637429 | validation: 4.031932178415224]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4903469683171853		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.4903469683171853 | validation: 4.115099618905982]
	TIME [epoch: 0.682 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.536409455522238		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.536409455522238 | validation: 4.030164109382524]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.64548070146845		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.64548070146845 | validation: 4.0754761577833705]
	TIME [epoch: 0.683 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.48510055042238		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.48510055042238 | validation: 3.977043950558045]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4341063607487383		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.4341063607487383 | validation: 3.992332502429355]
	TIME [epoch: 0.683 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.434304998237813		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.434304998237813 | validation: 3.950902193912727]
	TIME [epoch: 0.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4269724778197674		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.4269724778197674 | validation: 3.946959450134366]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4134623332777183		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.4134623332777183 | validation: 3.89819641708249]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.406869300031555		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.406869300031555 | validation: 3.928658167877856]
	TIME [epoch: 0.689 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.428583498679369		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.428583498679369 | validation: 3.81408600667106]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.453392647546163		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.453392647546163 | validation: 3.9837549304170006]
	TIME [epoch: 0.684 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.503366550216733		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.503366550216733 | validation: 3.7907135526750633]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.45354732385603		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.45354732385603 | validation: 3.894425487664847]
	TIME [epoch: 0.683 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.456523216167663		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.456523216167663 | validation: 3.802284841664164]
	TIME [epoch: 0.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4271615471361376		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.4271615471361376 | validation: 3.787209855327113]
	TIME [epoch: 0.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.350515060152735		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.350515060152735 | validation: 3.7588538330422794]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3682784707193347		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.3682784707193347 | validation: 3.7379373203151944]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.352343565065893		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.352343565065893 | validation: 3.7165252575079695]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.339781697690254		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.339781697690254 | validation: 3.672437957108224]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3384259817627764		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.3384259817627764 | validation: 3.700694662933627]
	TIME [epoch: 0.682 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.345075755456284		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.345075755456284 | validation: 3.602191922388453]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.347942913090144		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.347942913090144 | validation: 3.763627278493807]
	TIME [epoch: 0.686 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4350414884537357		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.4350414884537357 | validation: 3.670769096130167]
	TIME [epoch: 0.685 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6243906517509368		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.6243906517509368 | validation: 3.6113322211208185]
	TIME [epoch: 0.684 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.314736234276614		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.314736234276614 | validation: 3.731185748219948]
	TIME [epoch: 0.683 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.392289886661455		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.392289886661455 | validation: 3.5905000633149506]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3407887506682807		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.3407887506682807 | validation: 3.6014708707438263]
	TIME [epoch: 0.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.31714738090145		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.31714738090145 | validation: 3.6056797217823]
	TIME [epoch: 0.687 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.306456288687687		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.306456288687687 | validation: 3.540838357053957]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2952233542937517		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.2952233542937517 | validation: 3.5455072764917164]
	TIME [epoch: 0.69 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2852574421209115		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.2852574421209115 | validation: 3.520482730358286]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.28994660266221		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.28994660266221 | validation: 3.4667882380285153]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2761001501015596		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.2761001501015596 | validation: 3.4704214362521344]
	TIME [epoch: 0.684 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.275260382625648		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.275260382625648 | validation: 3.405099238821493]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2745179996303024		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.2745179996303024 | validation: 3.4815445750106835]
	TIME [epoch: 0.683 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2769213237899253		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.2769213237899253 | validation: 3.348587229331782]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.291809472830438		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.291809472830438 | validation: 3.525226779648014]
	TIME [epoch: 0.69 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.34600609426062		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.34600609426062 | validation: 3.3719914533447035]
	TIME [epoch: 0.682 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4733512979904124		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.4733512979904124 | validation: 3.4194184561390757]
	TIME [epoch: 0.687 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.272610305416251		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.272610305416251 | validation: 3.4920210453850817]
	TIME [epoch: 0.686 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3045544978072736		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.3045544978072736 | validation: 3.362622903836481]
	TIME [epoch: 0.685 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2793352105480102		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.2793352105480102 | validation: 3.333948033419953]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2579842269771087		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.2579842269771087 | validation: 3.3571566963820256]
	TIME [epoch: 0.686 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2469064100106086		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.2469064100106086 | validation: 3.2816891330027143]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2553974441052573		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.2553974441052573 | validation: 3.3113055784066776]
	TIME [epoch: 0.689 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.233870323944204		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.233870323944204 | validation: 3.2601857231956775]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.222962850325235		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.222962850325235 | validation: 3.2465306883316227]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2259824922944254		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.2259824922944254 | validation: 3.191874918921146]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2375833673919407		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.2375833673919407 | validation: 3.318168617850823]
	TIME [epoch: 0.688 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2936759073020143		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.2936759073020143 | validation: 3.1649312492052726]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.334248435398848		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.334248435398848 | validation: 3.273997652688378]
	TIME [epoch: 0.686 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2257396150559745		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.2257396150559745 | validation: 3.2262682087622836]
	TIME [epoch: 0.688 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2246981612325887		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.2246981612325887 | validation: 3.1331779256438566]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.208063961088926		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.208063961088926 | validation: 3.1233277272669815]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1979220633320176		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.1979220633320176 | validation: 3.0638002494598755]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.19850881984914		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.19850881984914 | validation: 3.0379151902848998]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.179563995718682		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.179563995718682 | validation: 3.0012802691932716]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.168483459981627		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.168483459981627 | validation: 3.0192997295177175]
	TIME [epoch: 0.69 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.176182905324125		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.176182905324125 | validation: 2.864539268367743]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.350415885419804		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.350415885419804 | validation: 3.422051910805658]
	TIME [epoch: 0.691 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.502367523465811		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.502367523465811 | validation: 3.0127968732612747]
	TIME [epoch: 0.688 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2209751499140284		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.2209751499140284 | validation: 3.0069698519659416]
	TIME [epoch: 0.688 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2365630828234675		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.2365630828234675 | validation: 3.0121960986234444]
	TIME [epoch: 0.689 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1695938647104316		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.1695938647104316 | validation: 2.9996695223208953]
	TIME [epoch: 0.689 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1686165222444695		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.1686165222444695 | validation: 2.918458607800182]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.146968655261142		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.146968655261142 | validation: 2.8446204161150117]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1374791189450058		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.1374791189450058 | validation: 2.852673099141631]
	TIME [epoch: 0.691 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1264852730616495		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.1264852730616495 | validation: 2.7422251355113776]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1036786253739086		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.1036786253739086 | validation: 2.7514385445300924]
	TIME [epoch: 0.691 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.087677496220032		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.087677496220032 | validation: 2.5718566135684555]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.100455897264451		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.100455897264451 | validation: 2.9187109287270516]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.218492633964497		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.218492633964497 | validation: 2.472996484095278]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2024533091007075		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.2024533091007075 | validation: 2.715617088917676]
	TIME [epoch: 0.685 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0777312947333737		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.0777312947333737 | validation: 2.5555881523479447]
	TIME [epoch: 0.683 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0104895188494885		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.0104895188494885 | validation: 2.2795643717259546]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9930443135933258		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.9930443135933258 | validation: 2.4632221409303487]
	TIME [epoch: 0.687 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.009899626583425		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.009899626583425 | validation: 2.329524463877295]
	TIME [epoch: 0.685 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1708420224720046		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.1708420224720046 | validation: 2.7400591426925462]
	TIME [epoch: 0.686 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.160256722634083		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.160256722634083 | validation: 2.218448316233143]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.889149185061089		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.889149185061089 | validation: 2.0579797707725587]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.981072684796625		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.981072684796625 | validation: 2.401554967347539]
	TIME [epoch: 0.684 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.985309830513171		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.985309830513171 | validation: 1.6518452446379948]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.812740456751397		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.812740456751397 | validation: 1.9213064614708033]
	TIME [epoch: 0.683 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8201535325632414		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.8201535325632414 | validation: 1.5789176559111873]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7504365821762649		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.7504365821762649 | validation: 1.5557941839691771]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7240026662336398		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.7240026662336398 | validation: 2.2960380525129422]
	TIME [epoch: 0.687 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0179744393328285		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 2.0179744393328285 | validation: 1.0513191361488265]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8845839895830168		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.8845839895830168 | validation: 1.6167635311301887]
	TIME [epoch: 0.689 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.626252191295457		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.626252191295457 | validation: 1.1947252849500918]
	TIME [epoch: 0.689 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5343438657434747		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.5343438657434747 | validation: 1.3578896734769341]
	TIME [epoch: 0.687 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.547537243699393		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.547537243699393 | validation: 1.1034765985200334]
	TIME [epoch: 0.687 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5700878772133882		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5700878772133882 | validation: 1.7393310480911481]
	TIME [epoch: 0.687 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6861106037056568		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.6861106037056568 | validation: 0.813905734983282]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5431430160594186		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.5431430160594186 | validation: 1.4961217414176016]
	TIME [epoch: 0.687 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5525156529383342		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.5525156529383342 | validation: 0.8181348832561819]
	TIME [epoch: 0.685 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4472499475568872		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.4472499475568872 | validation: 1.3270751412248387]
	TIME [epoch: 0.685 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4389999431070808		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.4389999431070808 | validation: 0.8872757005655008]
	TIME [epoch: 0.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3831457254734398		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.3831457254734398 | validation: 1.3627475489638217]
	TIME [epoch: 0.685 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4232396212427763		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.4232396212427763 | validation: 0.8887086508851697]
	TIME [epoch: 0.685 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3995484129470115		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3995484129470115 | validation: 1.479571586164283]
	TIME [epoch: 0.685 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.480448333779454		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.480448333779454 | validation: 0.6686235901403127]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4679214891194263		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.4679214891194263 | validation: 1.3662262171641968]
	TIME [epoch: 0.7 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4779805322799353		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.4779805322799353 | validation: 0.7028016009162136]
	TIME [epoch: 0.687 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2177643410608578		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.2177643410608578 | validation: 0.9396106232577273]
	TIME [epoch: 0.687 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2095676218394844		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.2095676218394844 | validation: 1.0120664174148288]
	TIME [epoch: 0.686 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2578181548876506		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.2578181548876506 | validation: 1.2422208406781587]
	TIME [epoch: 0.686 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2973668707873827		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.2973668707873827 | validation: 0.8522629096819494]
	TIME [epoch: 0.685 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3677371716918927		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.3677371716918927 | validation: 1.6578714048710932]
	TIME [epoch: 0.686 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6146315642597717		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.6146315642597717 | validation: 0.647033739701573]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1486953510470064		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.1486953510470064 | validation: 0.810527464711813]
	TIME [epoch: 0.688 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0919626284350208		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.0919626284350208 | validation: 0.7415917314400122]
	TIME [epoch: 0.686 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0913128797742422		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.0913128797742422 | validation: 1.278743381906755]
	TIME [epoch: 0.687 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2711774471416033		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.2711774471416033 | validation: 0.9065796523041293]
	TIME [epoch: 0.684 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4251923151607424		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4251923151607424 | validation: 1.4570125445647706]
	TIME [epoch: 0.684 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4615848066129533		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.4615848066129533 | validation: 0.6308750827660954]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0628278639374207		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.0628278639374207 | validation: 0.7656097519290651]
	TIME [epoch: 0.688 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.015213021985439		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.015213021985439 | validation: 0.8139561871564237]
	TIME [epoch: 0.686 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246807465498184		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.0246807465498184 | validation: 0.819684171300202]
	TIME [epoch: 0.686 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093978597512517		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.1093978597512517 | validation: 1.5509663312467832]
	TIME [epoch: 0.689 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3658706588476957		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3658706588476957 | validation: 0.6961603655112099]
	TIME [epoch: 0.686 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2897346811867658		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.2897346811867658 | validation: 1.2982398092180907]
	TIME [epoch: 0.684 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3650122820912731		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3650122820912731 | validation: 0.6049411044699003]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0092701278354763		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.0092701278354763 | validation: 0.7345873405048093]
	TIME [epoch: 0.684 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9772831963411437		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.9772831963411437 | validation: 0.743383119373791]
	TIME [epoch: 0.682 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9974419062607814		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.9974419062607814 | validation: 1.026381642614507]
	TIME [epoch: 0.682 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994960319551799		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.0994960319551799 | validation: 0.9831892905754119]
	TIME [epoch: 0.681 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2720722419556254		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.2720722419556254 | validation: 1.2837101389674979]
	TIME [epoch: 0.682 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2699264695405976		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.2699264695405976 | validation: 0.5510580555101788]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127464582829585		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.127464582829585 | validation: 1.0878530420866113]
	TIME [epoch: 0.692 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1421931441519637		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.1421931441519637 | validation: 0.6553722970657335]
	TIME [epoch: 0.688 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0725781073692788		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.0725781073692788 | validation: 1.152797425025775]
	TIME [epoch: 0.689 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1382171177364417		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.1382171177364417 | validation: 0.753457193694989]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0515095995912336		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.0515095995912336 | validation: 0.941603842538747]
	TIME [epoch: 0.688 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0132223952762833		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.0132223952762833 | validation: 0.6134726042643567]
	TIME [epoch: 0.686 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.039049059802227		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.039049059802227 | validation: 1.1489245688597505]
	TIME [epoch: 0.688 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155282610434188		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.155282610434188 | validation: 0.5635378998421375]
	TIME [epoch: 0.69 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0351952378142377		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.0351952378142377 | validation: 0.9720781585499378]
	TIME [epoch: 0.691 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043664330200118		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.043664330200118 | validation: 0.6020343939113999]
	TIME [epoch: 0.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031293071781904		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.031293071781904 | validation: 1.0280473100791994]
	TIME [epoch: 170 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.054407153962269		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.054407153962269 | validation: 0.632413529058207]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0287527972167083		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.0287527972167083 | validation: 1.0344930090230466]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.05244688021549		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.05244688021549 | validation: 0.6455476909787448]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9955745358279527		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.9955745358279527 | validation: 0.9579835347649457]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0074586199356546		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.0074586199356546 | validation: 0.5952288467319838]
	TIME [epoch: 1.34 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0380464481340896		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.0380464481340896 | validation: 1.0944871577967477]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1352282011889618		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.1352282011889618 | validation: 0.5593751785632037]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9677248098237786		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.9677248098237786 | validation: 0.8681802693420885]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9689222133740236		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.9689222133740236 | validation: 0.5559499038974292]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9826810495489332		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.9826810495489332 | validation: 0.9541910430543755]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0280076481611848		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.0280076481611848 | validation: 0.6670183904299938]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077759795713275		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.077759795713275 | validation: 1.158279356494687]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1174314061695063		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1174314061695063 | validation: 0.6914239818715793]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9402637701584361		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.9402637701584361 | validation: 0.7240495928277139]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010962988811164		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9010962988811164 | validation: 0.6501487458628651]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023753079368925		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9023753079368925 | validation: 0.8088887514255494]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277247469333288		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9277247469333288 | validation: 0.6364074079025517]
	TIME [epoch: 1.34 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029992874811571		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.029992874811571 | validation: 1.2197893398252047]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1930226066236649		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1930226066236649 | validation: 0.5441327033732332]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299503906623177		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.9299503906623177 | validation: 0.8955746375122584]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9490623817078842		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9490623817078842 | validation: 0.5016479518204872]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929355361103626		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.9929355361103626 | validation: 1.096991621895338]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0876823037311851		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.0876823037311851 | validation: 0.6012088815683923]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9373680703732608		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.9373680703732608 | validation: 0.8853573663130405]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9389348232011718		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9389348232011718 | validation: 0.6888603386666546]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9423874050726815		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.9423874050726815 | validation: 0.8702100745079143]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9452417667042923		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.9452417667042923 | validation: 0.5542145221376245]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9408494885476429		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.9408494885476429 | validation: 1.0301892009595475]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0372585687956641		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.0372585687956641 | validation: 0.5196186041661232]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.975350810887936		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.975350810887936 | validation: 0.9543827184162655]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0062037463734834		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.0062037463734834 | validation: 0.5352585645398874]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982616391073591		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8982616391073591 | validation: 0.800856673673713]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949275339226529		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.8949275339226529 | validation: 0.6215124236549607]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9100711493089364		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.9100711493089364 | validation: 0.9465773304159365]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9614463615633665		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9614463615633665 | validation: 0.6211087123974958]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9708907990840483		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.9708907990840483 | validation: 0.9633066088246743]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.971268195648546		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.971268195648546 | validation: 0.49721588534989575]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9406751483551892		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.9406751483551892 | validation: 0.927837140372557]
	TIME [epoch: 1.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9847047221019133		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.9847047221019133 | validation: 0.5208343005469133]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9192642964431026		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.9192642964431026 | validation: 0.8551594780078539]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8969332387870832		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.8969332387870832 | validation: 0.5917754318796374]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956981635986614		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.8956981635986614 | validation: 0.8924051417176152]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281631650176709		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.9281631650176709 | validation: 0.5939963907417678]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925516364680546		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.925516364680546 | validation: 0.9097178911792633]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9378105964990046		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9378105964990046 | validation: 0.4863405430821093]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9187306196531233		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.9187306196531233 | validation: 0.9640288911142894]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9893454875381406		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.9893454875381406 | validation: 0.4927312392030727]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.900351942731086		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.900351942731086 | validation: 0.8010066948688226]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862048684760486		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.8862048684760486 | validation: 0.5792584938080684]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504387043410045		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.8504387043410045 | validation: 0.8172361480739068]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851754080030244		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.8851754080030244 | validation: 0.6150031633816253]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236828208375031		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.9236828208375031 | validation: 0.9534470802914193]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9586243275803155		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9586243275803155 | validation: 0.5093007239542985]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971811690103314		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.8971811690103314 | validation: 0.8809352916224524]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9324989270825447		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.9324989270825447 | validation: 0.48841275846318166]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046160887848601		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.9046160887848601 | validation: 0.8644421213735376]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239477804836276		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.9239477804836276 | validation: 0.49449029775987985]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8787009997764387		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8787009997764387 | validation: 0.800526673651906]
	TIME [epoch: 1.34 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786414131032695		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.8786414131032695 | validation: 0.5997669439926087]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739054865097774		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.8739054865097774 | validation: 0.7446284638803364]
	TIME [epoch: 1.34 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8494631494658933		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.8494631494658933 | validation: 0.6083261892105994]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506642083432562		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.8506642083432562 | validation: 0.7741691342179536]
	TIME [epoch: 1.34 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586542933730632		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.8586542933730632 | validation: 0.5146296804901062]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8829476756962418		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.8829476756962418 | validation: 1.0385835033628863]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0044050433591085		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.0044050433591085 | validation: 0.46670226417665683]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225312949785377		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.9225312949785377 | validation: 0.8756655822463215]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245275243231835		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.9245275243231835 | validation: 0.49702531804011313]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536424253933623		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.8536424253933623 | validation: 0.8134797719161867]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609414142612051		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.8609414142612051 | validation: 0.5168090653722631]
	TIME [epoch: 1.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772839004306158		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.8772839004306158 | validation: 0.8950678516270587]
	TIME [epoch: 1.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033268738776531		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.9033268738776531 | validation: 0.5347430560428444]
	TIME [epoch: 1.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587161289732297		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.8587161289732297 | validation: 0.7717698509728633]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444840948235877		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8444840948235877 | validation: 0.5300645178495406]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297841222049379		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.8297841222049379 | validation: 0.7627559357597383]
	TIME [epoch: 1.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458397663817491		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.8458397663817491 | validation: 0.4654439864608871]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107084570606554		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.9107084570606554 | validation: 1.0112889425212215]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9958290147630643		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.9958290147630643 | validation: 0.5414327745282208]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283428148330608		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.8283428148330608 | validation: 0.7073220094018832]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810730973093359		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.810730973093359 | validation: 0.5805566535811509]
	TIME [epoch: 1.35 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017538141349787		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.8017538141349787 | validation: 0.6978434428659439]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180383781760736		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8180383781760736 | validation: 0.5351613658646793]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578397772579338		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.8578397772579338 | validation: 0.976075081106256]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9361510897587225		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.9361510897587225 | validation: 0.504960351120128]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957516873295185		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.8957516873295185 | validation: 0.8257095760543692]
	TIME [epoch: 1.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658682412814241		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8658682412814241 | validation: 0.4837791557860072]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8720799927802514		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.8720799927802514 | validation: 0.8942173559824573]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924128718592439		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.924128718592439 | validation: 0.4842892708475951]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848608723930411		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.848608723930411 | validation: 0.7536527744496199]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418605907986094		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8418605907986094 | validation: 0.5672378970476973]
	TIME [epoch: 1.35 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385596291666033		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.8385596291666033 | validation: 0.7038911651027153]
	TIME [epoch: 1.35 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243658453311463		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.8243658453311463 | validation: 0.5350541472760019]
	TIME [epoch: 1.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315347822495118		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.8315347822495118 | validation: 0.7728124692314428]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301080866459798		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.8301080866459798 | validation: 0.4886066330148728]
	TIME [epoch: 1.35 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475200934128466		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.8475200934128466 | validation: 0.9537634063804561]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9632472645397242		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.9632472645397242 | validation: 0.4900155509621813]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296754686431544		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.8296754686431544 | validation: 0.7501367409362016]
	TIME [epoch: 1.35 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252391508025372		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.8252391508025372 | validation: 0.5410775294649619]
	TIME [epoch: 1.35 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066357892989712		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.8066357892989712 | validation: 0.7263231846766022]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125099260857064		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.8125099260857064 | validation: 0.5313547743076704]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218310678834173		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.8218310678834173 | validation: 0.7851955344441942]
	TIME [epoch: 1.34 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839158409659717		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.839158409659717 | validation: 0.5020830002028402]
	TIME [epoch: 1.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518945381716223		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.8518945381716223 | validation: 0.9416875336429001]
	TIME [epoch: 1.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273383011175538		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.9273383011175538 | validation: 0.47648901854140946]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594145003568476		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.8594145003568476 | validation: 0.8092702534962737]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406942724619674		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8406942724619674 | validation: 0.5216576352717709]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033604945798299		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8033604945798299 | validation: 0.6563874181643694]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884719362118867		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.7884719362118867 | validation: 0.5337495713618092]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151374222948985		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.8151374222948985 | validation: 0.8811727334658563]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756159542876464		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.8756159542876464 | validation: 0.5057026475508517]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709950351183713		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.8709950351183713 | validation: 0.8155205563920124]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565579182970425		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.8565579182970425 | validation: 0.5081039424182797]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128864234145843		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.8128864234145843 | validation: 0.7707677341088006]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227440557300315		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.8227440557300315 | validation: 0.4757927079781092]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183268721362201		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.8183268721362201 | validation: 0.752849008413476]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306342051735369		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.8306342051735369 | validation: 0.5123209585801688]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321403168590038		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.8321403168590038 | validation: 0.8036243060073677]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833731304597486		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.833731304597486 | validation: 0.5067290927542815]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8005021387032073		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.8005021387032073 | validation: 0.6650737869022074]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955143099696342		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.7955143099696342 | validation: 0.5398888898875719]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756089278140874		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.7756089278140874 | validation: 0.6323314305980832]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7816829262124441		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.7816829262124441 | validation: 0.5123697094031405]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941442640152141		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.7941442640152141 | validation: 0.7876823850160681]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336679015814636		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.8336679015814636 | validation: 0.4676539028583926]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717224506085668		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.8717224506085668 | validation: 0.9801801788537534]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448225951354772		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.9448225951354772 | validation: 0.5166211509380196]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994027313062231		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.7994027313062231 | validation: 0.6199999341240501]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741259829218428		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7741259829218428 | validation: 0.646846131827333]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883039596724892		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.7883039596724892 | validation: 0.5610360004227681]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806722661799761		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.806722661799761 | validation: 0.7704066846717436]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194664973933056		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.8194664973933056 | validation: 0.47949344812033773]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231695887308849		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.8231695887308849 | validation: 0.846917090125764]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731404392855696		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.8731404392855696 | validation: 0.4455541773217784]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358711997608967		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.8358711997608967 | validation: 0.7370872948435148]
	TIME [epoch: 1.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114822068178685		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.8114822068178685 | validation: 0.501842931806806]
	TIME [epoch: 1.34 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789067798330703		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.789067798330703 | validation: 0.6894828237926466]
	TIME [epoch: 1.34 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885781330124626		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.7885781330124626 | validation: 0.5062052857929462]
	TIME [epoch: 1.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991704654204631		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.7991704654204631 | validation: 0.6995014477717917]
	TIME [epoch: 1.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929699826490685		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.7929699826490685 | validation: 0.5038920528030444]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937692353772833		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.7937692353772833 | validation: 0.7143863184982491]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061258254547601		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.8061258254547601 | validation: 0.4993977764037202]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927423322724326		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.7927423322724326 | validation: 0.7412332838103529]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015676525721739		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.8015676525721739 | validation: 0.4572890469928256]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006025241927179		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.8006025241927179 | validation: 0.8387636798954138]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565957395087016		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.8565957395087016 | validation: 0.4707731253642966]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8223502421558763		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.8223502421558763 | validation: 0.7107044983879809]
	TIME [epoch: 1.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876937012924825		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.7876937012924825 | validation: 0.5270691655526624]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671228753286897		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.7671228753286897 | validation: 0.6399661071021289]
	TIME [epoch: 1.34 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764916599078259		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.764916599078259 | validation: 0.5714436586823123]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731456947823577		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.7731456947823577 | validation: 0.5903236938189613]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7674505781298965		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.7674505781298965 | validation: 0.6224158871919488]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857225186933815		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.7857225186933815 | validation: 0.5830374159162636]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899683373172516		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.7899683373172516 | validation: 0.5698847736222089]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7816135539539195		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.7816135539539195 | validation: 0.6536220042459062]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640584411931204		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.7640584411931204 | validation: 0.48487080740260674]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974554691085243		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.7974554691085243 | validation: 0.9089696921502943]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992635195235996		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.8992635195235996 | validation: 0.42787547658650216]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8988767405575854		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.8988767405575854 | validation: 0.741425314645771]
	TIME [epoch: 1.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797135805001796		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.797135805001796 | validation: 0.5260549451407327]
	TIME [epoch: 1.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656195875232724		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.7656195875232724 | validation: 0.5460598553904601]
	TIME [epoch: 1.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548409463198544		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.7548409463198544 | validation: 0.6733615573255982]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620120013620754		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.7620120013620754 | validation: 0.4811405120304655]
	TIME [epoch: 1.33 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881025031213011		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.7881025031213011 | validation: 0.8538590248860003]
	TIME [epoch: 1.34 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475201220445441		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.8475201220445441 | validation: 0.44941267251647243]
	TIME [epoch: 1.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021216070476761		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.8021216070476761 | validation: 0.7164243539638714]
	TIME [epoch: 1.33 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868670788297177		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.7868670788297177 | validation: 0.4840233904839828]
	TIME [epoch: 1.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722924700122286		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.7722924700122286 | validation: 0.6092248656298777]
	TIME [epoch: 1.34 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619870518931657		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.7619870518931657 | validation: 0.5707865367750904]
	TIME [epoch: 1.33 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549485008489338		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.7549485008489338 | validation: 0.5050498141540211]
	TIME [epoch: 1.33 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715158246555778		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.7715158246555778 | validation: 0.6630241637169809]
	TIME [epoch: 1.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651916152001852		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.7651916152001852 | validation: 0.5111038060536747]
	TIME [epoch: 1.34 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717329852771379		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.7717329852771379 | validation: 0.6870198147405925]
	TIME [epoch: 1.33 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782123642660559		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.782123642660559 | validation: 0.49805016636427574]
	TIME [epoch: 1.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8118828389406159		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.8118828389406159 | validation: 0.8528361649164049]
	TIME [epoch: 1.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544364011528301		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.8544364011528301 | validation: 0.424818575966813]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434065286592087		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.8434065286592087 | validation: 0.7684581934657868]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8254059025301976		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.8254059025301976 | validation: 0.5074533515970575]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612967728480644		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.7612967728480644 | validation: 0.585580627673495]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7561961738179892		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.7561961738179892 | validation: 0.5855873732019244]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618120890739871		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.7618120890739871 | validation: 0.5278706920291137]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477230588869012		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.7477230588869012 | validation: 0.5541597593163122]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476808889311277		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.7476808889311277 | validation: 0.5758211116945913]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477750042157453		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.7477750042157453 | validation: 0.5505287024319385]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494646473649426		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.7494646473649426 | validation: 0.5786818852440012]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483582528962165		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.7483582528962165 | validation: 0.598203132053957]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549655680165499		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.7549655680165499 | validation: 0.4639126164194923]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895003403018749		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.7895003403018749 | validation: 0.9563498962301478]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9103465298017657		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9103465298017657 | validation: 0.4506345460783532]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804518895340212		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.7804518895340212 | validation: 0.56234682633776]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498456480717803		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.7498456480717803 | validation: 0.6103555940070575]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504369001149656		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.7504369001149656 | validation: 0.5274536711765635]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595443073487925		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.7595443073487925 | validation: 0.6087215956981727]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503792588076763		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.7503792588076763 | validation: 0.4697473597877008]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788855492699901		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.7788855492699901 | validation: 0.7460512479717708]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801328191438362		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.801328191438362 | validation: 0.4509668584514578]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720077726443404		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.7720077726443404 | validation: 0.7516230616697683]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8154113865851543		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.8154113865851543 | validation: 0.4336788583406086]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780083947459633		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.780083947459633 | validation: 0.6550466918448746]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487424439652726		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.7487424439652726 | validation: 0.49458669977582626]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602359825763726		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.7602359825763726 | validation: 0.6698743106125989]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817266013238426		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.7817266013238426 | validation: 0.43823449822781396]
	TIME [epoch: 1.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868462763295709		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.7868462763295709 | validation: 0.6554744775357089]
	TIME [epoch: 1.33 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540448646683886		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.7540448646683886 | validation: 0.4453431624584212]
	TIME [epoch: 1.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764704365339806		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.764704365339806 | validation: 0.6385755155200433]
	TIME [epoch: 1.34 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7500551007800837		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.7500551007800837 | validation: 0.48638236162597376]
	TIME [epoch: 1.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568941992412012		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.7568941992412012 | validation: 0.5460543017009366]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419685966928131		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.7419685966928131 | validation: 0.5057317101883997]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380950286960923		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.7380950286960923 | validation: 0.5943156557285069]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476366542433281		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.7476366542433281 | validation: 0.47940624453765407]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511509761405332		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.7511509761405332 | validation: 0.5907087731402125]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393091798160358		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.7393091798160358 | validation: 0.5375213940732108]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413112773770405		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.7413112773770405 | validation: 0.4687796714613212]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488072784411756		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.7488072784411756 | validation: 0.8581878564236072]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467611696648039		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.8467611696648039 | validation: 0.42044044556382903]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444491189216808		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8444491189216808 | validation: 0.6717401259533662]
	TIME [epoch: 1.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686607697628108		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.7686607697628108 | validation: 0.5218065703095777]
	TIME [epoch: 1.34 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305771470454265		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.7305771470454265 | validation: 0.46968401885575645]
	TIME [epoch: 1.34 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497049912981225		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.7497049912981225 | validation: 0.6727266496621171]
	TIME [epoch: 1.34 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775453747280388		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.775453747280388 | validation: 0.46388652658275575]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466748316491753		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.7466748316491753 | validation: 0.5960861831082278]
	TIME [epoch: 1.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7358173562100615		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.7358173562100615 | validation: 0.4959541735645065]
	TIME [epoch: 1.33 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424339411339287		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.7424339411339287 | validation: 0.6499863718812964]
	TIME [epoch: 1.33 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533057451348184		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.7533057451348184 | validation: 0.49253799662474385]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730922149144848		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.730922149144848 | validation: 0.6271077010817218]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371162448177707		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.7371162448177707 | validation: 0.47844609326920545]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333349556492642		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.7333349556492642 | validation: 0.5628775260478815]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305336537337773		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.7305336537337773 | validation: 0.5237075284668418]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275977720565515		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.7275977720565515 | validation: 0.5244829963890848]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348505823735241		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.7348505823735241 | validation: 0.5573335787192069]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308335524908679		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.7308335524908679 | validation: 0.5278210240072168]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260753157933257		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.7260753157933257 | validation: 0.5947334388058716]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253465029225938		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.7253465029225938 | validation: 0.4892273044239696]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327632831864364		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.7327632831864364 | validation: 0.754326116548133]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021099639169508		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.8021099639169508 | validation: 0.41323065808098347]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015366594102179		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.8015366594102179 | validation: 0.7302294251619645]
	TIME [epoch: 1.34 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767841590622057		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.7767841590622057 | validation: 0.46041746221189994]
	TIME [epoch: 1.33 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299391373806796		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.7299391373806796 | validation: 0.4906255310891499]
	TIME [epoch: 1.33 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232342278104426		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.7232342278104426 | validation: 0.5595453444447843]
	TIME [epoch: 1.33 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233445313903422		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.7233445313903422 | validation: 0.43022363000657154]
	TIME [epoch: 1.33 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606927026074812		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.7606927026074812 | validation: 0.7252446825656591]
	TIME [epoch: 1.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013987486688358		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.8013987486688358 | validation: 0.4781010542094981]
	TIME [epoch: 1.33 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268163284661331		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.7268163284661331 | validation: 0.4808119060862276]
	TIME [epoch: 1.33 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7261649367823797		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.7261649367823797 | validation: 0.5896405052207685]
	TIME [epoch: 1.33 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7230976401378277		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.7230976401378277 | validation: 0.5099819739907664]
	TIME [epoch: 1.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212506521641749		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.7212506521641749 | validation: 0.5174345181657745]
	TIME [epoch: 1.33 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103282663783589		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.7103282663783589 | validation: 0.5075934601520972]
	TIME [epoch: 1.34 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192322346739585		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.7192322346739585 | validation: 0.6035854302857382]
	TIME [epoch: 1.33 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312746422916689		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.7312746422916689 | validation: 0.3970801838175039]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776394797712696		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.7776394797712696 | validation: 0.7063972157116164]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652851048265976		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.7652851048265976 | validation: 0.49796546131948]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115979090328466		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.7115979090328466 | validation: 0.45891778833477104]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238064121193005		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.7238064121193005 | validation: 0.5936316302427265]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278422094009704		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.7278422094009704 | validation: 0.5056730117157487]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239401430655041		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.7239401430655041 | validation: 0.5628139915514149]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228672392956688		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.7228672392956688 | validation: 0.42116950585277463]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7330437923005354		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.7330437923005354 | validation: 0.6232288331135692]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398704359506673		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.7398704359506673 | validation: 0.44625173763891574]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154683074609061		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.7154683074609061 | validation: 0.5205923200987729]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7039428959390176		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.7039428959390176 | validation: 0.5393369555759385]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079502510633825		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.7079502510633825 | validation: 0.5486191603902707]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711101213490505		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.711101213490505 | validation: 0.4486069208023349]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718301715454679		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.718301715454679 | validation: 0.6170168939906783]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359767345526957		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.7359767345526957 | validation: 0.46923626058910684]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153457338258038		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.7153457338258038 | validation: 0.6338760594033203]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398308078359237		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.7398308078359237 | validation: 0.41986063480568814]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557231416569925		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.7557231416569925 | validation: 0.6114024001523076]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198522236937344		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.7198522236937344 | validation: 0.4250013158063185]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232851839416194		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.7232851839416194 | validation: 0.5437056251024945]
	TIME [epoch: 1.35 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093406908767301		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.7093406908767301 | validation: 0.44090517898178316]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112943130948857		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.7112943130948857 | validation: 0.5305586050123766]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016397432997615		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.7016397432997615 | validation: 0.46222166436383505]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001432987716949		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.7001432987716949 | validation: 0.47534504892416934]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984422148042825		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.6984422148042825 | validation: 0.5963564736235054]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7172846597947959		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.7172846597947959 | validation: 0.38873037594635473]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652310166589135		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.7652310166589135 | validation: 0.7040818629467318]
	TIME [epoch: 1.34 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703657058617727		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.7703657058617727 | validation: 0.5061764807194095]
	TIME [epoch: 1.34 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923038050711565		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.6923038050711565 | validation: 0.402102775990328]
	TIME [epoch: 1.34 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289233161203993		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.7289233161203993 | validation: 0.5671673962146827]
	TIME [epoch: 1.33 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7089901598696976		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.7089901598696976 | validation: 0.49102141217469825]
	TIME [epoch: 1.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931890292229508		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.6931890292229508 | validation: 0.4207319063063901]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076310125287734		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.7076310125287734 | validation: 0.5577192093040164]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079889753261464		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.7079889753261464 | validation: 0.4827306034894364]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690030771310509		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.690030771310509 | validation: 0.4478736430679078]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008548659112784		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.7008548659112784 | validation: 0.5336983838770711]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6941236153592513		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.6941236153592513 | validation: 0.44158042752624377]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004560087490473		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.7004560087490473 | validation: 0.5938257830801262]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712544293470869		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.712544293470869 | validation: 0.39945073530093506]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7508511229836122		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.7508511229836122 | validation: 0.5280693030636617]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699280187218821		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.699280187218821 | validation: 0.560835108589621]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211076196566575		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.7211076196566575 | validation: 0.39741990338427347]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292792046111677		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.7292792046111677 | validation: 0.5531608727758405]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922301821538074		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.6922301821538074 | validation: 0.5203098957210248]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955745067959538		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.6955745067959538 | validation: 0.4194791415735452]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088065918836713		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.7088065918836713 | validation: 0.6265013082220746]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7234877263906799		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.7234877263906799 | validation: 0.45348189592787524]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915015231352318		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.6915015231352318 | validation: 0.4430470394987648]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.689656162479937		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.689656162479937 | validation: 0.5971329923002221]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7122013713485267		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.7122013713485267 | validation: 0.4284380230151184]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927759938992231		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.6927759938992231 | validation: 0.5465307228273739]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893345000286288		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.6893345000286288 | validation: 0.47325652148627917]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782615199624346		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.6782615199624346 | validation: 0.3834352386572894]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092644192114179		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.7092644192114179 | validation: 0.5800308730019303]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998524489386989		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.6998524489386989 | validation: 0.48405506210649324]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785963622153383		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.6785963622153383 | validation: 0.42689447089858135]
	TIME [epoch: 2.66 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864232115317797		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.6864232115317797 | validation: 0.4969565828620821]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816403050600883		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.6816403050600883 | validation: 0.4859575356777618]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6845542439384645		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.6845542439384645 | validation: 0.45670030343868206]
	TIME [epoch: 2.66 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870875779890677		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.6870875779890677 | validation: 0.5744217040117641]
	TIME [epoch: 2.66 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69754876012748		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.69754876012748 | validation: 0.4388723327615587]
	TIME [epoch: 2.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697220628857842		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.697220628857842 | validation: 0.6032557463299641]
	TIME [epoch: 2.66 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7071942507184684		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.7071942507184684 | validation: 0.4097848739538737]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961054270835288		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.6961054270835288 | validation: 0.46640186833609737]
	TIME [epoch: 2.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682405088583464		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.682405088583464 | validation: 0.5714030557356903]
	TIME [epoch: 2.66 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908730313739072		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.6908730313739072 | validation: 0.39458682117008426]
	TIME [epoch: 2.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014992890108768		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.7014992890108768 | validation: 0.5439850655505023]
	TIME [epoch: 2.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870725481188674		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.6870725481188674 | validation: 0.418429643059427]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827386109379413		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.6827386109379413 | validation: 0.4607157272414192]
	TIME [epoch: 2.66 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677808155831729		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.6677808155831729 | validation: 0.5571477241388533]
	TIME [epoch: 2.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883789740240955		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.6883789740240955 | validation: 0.4586408439479218]
	TIME [epoch: 2.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711206577764854		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.6711206577764854 | validation: 0.4341356150082793]
	TIME [epoch: 2.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682393737431687		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.682393737431687 | validation: 0.5041268722447667]
	TIME [epoch: 2.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747588990191388		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.6747588990191388 | validation: 0.5411971503775101]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893954846008441		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.6893954846008441 | validation: 0.37181714316912373]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365706297082396		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.7365706297082396 | validation: 0.5036708585907451]
	TIME [epoch: 2.66 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710029891651432		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.6710029891651432 | validation: 0.4310897989910456]
	TIME [epoch: 2.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712950173635984		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.6712950173635984 | validation: 0.41936087194432387]
	TIME [epoch: 2.66 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800207400205198		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.6800207400205198 | validation: 0.5736893592480057]
	TIME [epoch: 2.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933133428515833		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.6933133428515833 | validation: 0.40874175704473353]
	TIME [epoch: 2.66 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826472255177803		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.6826472255177803 | validation: 0.5358264555716625]
	TIME [epoch: 2.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746198059740425		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.6746198059740425 | validation: 0.4216752394816502]
	TIME [epoch: 2.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897259979668516		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.6897259979668516 | validation: 0.5026664014325596]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690292240085127		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.6690292240085127 | validation: 0.48577584387182454]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6672324933335129		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.6672324933335129 | validation: 0.4352550062747944]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706974061030448		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.6706974061030448 | validation: 0.4347240707701862]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771756484309336		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.6771756484309336 | validation: 0.4312220931678339]
	TIME [epoch: 2.66 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616238905739604		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.6616238905739604 | validation: 0.6295630344257762]
	TIME [epoch: 2.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091130024599851		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.7091130024599851 | validation: 0.4253276624264965]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603285725530332		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.6603285725530332 | validation: 0.4469140346994319]
	TIME [epoch: 2.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662097125844987		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.662097125844987 | validation: 0.4937452604951107]
	TIME [epoch: 2.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595953414564855		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.6595953414564855 | validation: 0.42817661383317784]
	TIME [epoch: 2.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639446342451217		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.6639446342451217 | validation: 0.5190562711279144]
	TIME [epoch: 2.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753062299894075		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.6753062299894075 | validation: 0.4741444848571602]
	TIME [epoch: 2.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6577948318432926		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.6577948318432926 | validation: 0.4633352936791165]
	TIME [epoch: 2.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603210783189647		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.6603210783189647 | validation: 0.4347237044615832]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671476253271424		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.6671476253271424 | validation: 0.5235619953772666]
	TIME [epoch: 2.66 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6661014653619407		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.6661014653619407 | validation: 0.4276807261423905]
	TIME [epoch: 2.66 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643262742346389		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.6643262742346389 | validation: 0.46749620599000496]
	TIME [epoch: 2.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647052141041851		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.647052141041851 | validation: 0.43058704223376143]
	TIME [epoch: 2.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607914723685298		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.6607914723685298 | validation: 0.622235134305868]
	TIME [epoch: 2.66 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153541391105899		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.7153541391105899 | validation: 0.4340884385429258]
	TIME [epoch: 2.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527341909018084		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.6527341909018084 | validation: 0.438487506456261]
	TIME [epoch: 2.66 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6576993338542587		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.6576993338542587 | validation: 0.4315236252172716]
	TIME [epoch: 2.66 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546846942268263		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.6546846942268263 | validation: 0.4058500003976157]
	TIME [epoch: 2.66 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606516309976419		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.6606516309976419 | validation: 0.5367192726180987]
	TIME [epoch: 2.66 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764193930818954		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.6764193930818954 | validation: 0.4694961056430127]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652944231452554		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.652944231452554 | validation: 0.46364852162822656]
	TIME [epoch: 2.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541796480596981		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.6541796480596981 | validation: 0.48367369027615076]
	TIME [epoch: 2.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650147498284153		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.650147498284153 | validation: 0.4317054236694654]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564490598451108		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.6564490598451108 | validation: 0.6114755219448211]
	TIME [epoch: 2.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097784326441405		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.7097784326441405 | validation: 0.40179172072331676]
	TIME [epoch: 2.66 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743179585910757		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.6743179585910757 | validation: 0.44668203580330534]
	TIME [epoch: 2.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514144901131479		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.6514144901131479 | validation: 0.4920370049088194]
	TIME [epoch: 2.66 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567011776061403		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.6567011776061403 | validation: 0.43343698970564754]
	TIME [epoch: 2.66 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6424097868844554		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.6424097868844554 | validation: 0.5220225334840811]
	TIME [epoch: 2.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551945777355078		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.6551945777355078 | validation: 0.41435632195402566]
	TIME [epoch: 2.66 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613874338188744		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.6613874338188744 | validation: 0.488363696415259]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475301894331144		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.6475301894331144 | validation: 0.5233955345054855]
	TIME [epoch: 2.66 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546180067799852		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.6546180067799852 | validation: 0.3664429604147259]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781378713524552		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.6781378713524552 | validation: 0.5338954229262147]
	TIME [epoch: 2.66 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664646615152162		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.664646615152162 | validation: 0.44203626767691856]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368214425838233		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.6368214425838233 | validation: 0.46228032970283667]
	TIME [epoch: 2.66 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417361763485445		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.6417361763485445 | validation: 0.3998644880763072]
	TIME [epoch: 2.66 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591189212045844		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.6591189212045844 | validation: 0.5107488177353434]
	TIME [epoch: 2.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432546713019514		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.6432546713019514 | validation: 0.4091873887667379]
	TIME [epoch: 2.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449870686598554		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.6449870686598554 | validation: 0.4538461107071878]
	TIME [epoch: 2.66 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407499481988943		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.6407499481988943 | validation: 0.47987339387908057]
	TIME [epoch: 2.66 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427464321460096		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.6427464321460096 | validation: 0.4416949447763687]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640379260042414		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.640379260042414 | validation: 0.45863765289557756]
	TIME [epoch: 2.66 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453133635484188		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.6453133635484188 | validation: 0.451850457596964]
	TIME [epoch: 2.66 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383136027856408		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.6383136027856408 | validation: 0.4402671586096908]
	TIME [epoch: 2.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398380949890218		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.6398380949890218 | validation: 0.4352146637556432]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346144846222684		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.6346144846222684 | validation: 0.4056176672137835]
	TIME [epoch: 2.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420270583127823		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.6420270583127823 | validation: 0.5001014063792192]
	TIME [epoch: 2.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434949330299901		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.6434949330299901 | validation: 0.36437534521997983]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755882507398434		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.6755882507398434 | validation: 0.5032369839502465]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391182344287101		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.6391182344287101 | validation: 0.44681988786879795]
	TIME [epoch: 2.66 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354530116871931		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.6354530116871931 | validation: 0.40591374248399464]
	TIME [epoch: 2.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386490561355086		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.6386490561355086 | validation: 0.4280590311620758]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319970943358167		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.6319970943358167 | validation: 0.7051072439019593]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611328428385946		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.7611328428385946 | validation: 0.4655086250905366]
	TIME [epoch: 2.66 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365233499625004		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.6365233499625004 | validation: 0.36175268191275256]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708960760221935		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.6708960760221935 | validation: 0.4671463440680683]
	TIME [epoch: 2.66 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6313732389382739		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.6313732389382739 | validation: 0.6091386597212956]
	TIME [epoch: 2.66 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6907756131277663		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.6907756131277663 | validation: 0.44714082930120225]
	TIME [epoch: 2.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235258266851781		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.6235258266851781 | validation: 0.3852226349824137]
	TIME [epoch: 2.66 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502925322714698		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.6502925322714698 | validation: 0.43291047246637526]
	TIME [epoch: 2.66 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194367348818759		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.6194367348818759 | validation: 0.4708215575147723]
	TIME [epoch: 2.66 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270596215903482		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.6270596215903482 | validation: 0.4880265438076921]
	TIME [epoch: 2.66 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278315320525707		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.6278315320525707 | validation: 0.4197902111711354]
	TIME [epoch: 2.66 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321754991109918		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.6321754991109918 | validation: 0.4329234033756775]
	TIME [epoch: 2.66 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181637361080289		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.6181637361080289 | validation: 0.4211974961489737]
	TIME [epoch: 2.66 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413752262533039		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.6413752262533039 | validation: 0.4435188690540793]
	TIME [epoch: 2.66 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311923934059128		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.6311923934059128 | validation: 0.4216734297353404]
	TIME [epoch: 2.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6183787014944852		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.6183787014944852 | validation: 0.4103881405326313]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6171903869090807		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.6171903869090807 | validation: 0.48219075220618235]
	TIME [epoch: 2.66 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206110771709097		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.6206110771709097 | validation: 0.41902880573994433]
	TIME [epoch: 2.66 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6091304084456776		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.6091304084456776 | validation: 0.41836969547369024]
	TIME [epoch: 2.66 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6054065744154564		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.6054065744154564 | validation: 0.3990220043053574]
	TIME [epoch: 2.66 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843460776611196		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.5843460776611196 | validation: 0.33683152758132107]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5698120618058354		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.5698120618058354 | validation: 0.4773262297290172]
	TIME [epoch: 2.67 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783060830226149		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.6783060830226149 | validation: 0.394891565247663]
	TIME [epoch: 2.66 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.599193650700911		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.599193650700911 | validation: 0.3375634178813541]
	TIME [epoch: 2.66 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592722589729406		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.592722589729406 | validation: 0.39963521013665537]
	TIME [epoch: 2.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5607792708846046		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.5607792708846046 | validation: 0.4618018374379462]
	TIME [epoch: 2.66 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042133088028028		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.6042133088028028 | validation: 0.37068192645995723]
	TIME [epoch: 2.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6345166953942777		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.6345166953942777 | validation: 0.2880866107418062]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574185454848238		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.5574185454848238 | validation: 0.44209501136785134]
	TIME [epoch: 2.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887075396441683		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.5887075396441683 | validation: 0.38600964954585293]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5771739855968173		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.5771739855968173 | validation: 0.3073502129816053]
	TIME [epoch: 2.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5406540230012729		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.5406540230012729 | validation: 0.3307668509123239]
	TIME [epoch: 2.66 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5584879035912763		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.5584879035912763 | validation: 0.3171802204877954]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631846265519824		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.5631846265519824 | validation: 0.2929385452751723]
	TIME [epoch: 2.66 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5355353169659981		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.5355353169659981 | validation: 0.3379021958885279]
	TIME [epoch: 2.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5237463438173819		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.5237463438173819 | validation: 0.3215980858245192]
	TIME [epoch: 2.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5310070648167173		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.5310070648167173 | validation: 0.2768075151411232]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191920430042137		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.5191920430042137 | validation: 0.2808713268120485]
	TIME [epoch: 2.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5260273780783102		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.5260273780783102 | validation: 0.4107059755107729]
	TIME [epoch: 2.66 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5294329393483155		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.5294329393483155 | validation: 0.29845917536093314]
	TIME [epoch: 2.66 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506302629338319		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.5506302629338319 | validation: 0.3698428339811764]
	TIME [epoch: 2.65 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5285372178915856		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.5285372178915856 | validation: 0.32175826465663265]
	TIME [epoch: 2.66 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5170818651882423		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.5170818651882423 | validation: 0.27911758218351357]
	TIME [epoch: 2.66 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49275437292852786		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.49275437292852786 | validation: 0.32151483321064395]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48539497994851877		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.48539497994851877 | validation: 0.2640372266343428]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4911464450988226		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.4911464450988226 | validation: 0.3475341137921873]
	TIME [epoch: 2.66 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4957605122950623		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.4957605122950623 | validation: 0.3064623559183988]
	TIME [epoch: 2.66 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5358351114986705		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.5358351114986705 | validation: 0.4505221396640141]
	TIME [epoch: 2.66 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5507227072361		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.5507227072361 | validation: 0.3104710508671413]
	TIME [epoch: 2.65 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165670191170405		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.5165670191170405 | validation: 0.2547654560923278]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46972743286066715		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.46972743286066715 | validation: 0.34796982787022646]
	TIME [epoch: 2.66 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4716156322696516		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.4716156322696516 | validation: 0.281783562521539]
	TIME [epoch: 2.66 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46729447803677643		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.46729447803677643 | validation: 0.29364031961575615]
	TIME [epoch: 2.66 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4535268837903241		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.4535268837903241 | validation: 0.2512998327618974]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4580422918178138		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.4580422918178138 | validation: 0.31288438583329187]
	TIME [epoch: 2.67 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4526252207446692		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.4526252207446692 | validation: 0.2531746731303298]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4689184512740529		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.4689184512740529 | validation: 0.2963396459860826]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4705199498547999		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.4705199498547999 | validation: 0.273217031402314]
	TIME [epoch: 2.67 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709439763790428		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.4709439763790428 | validation: 0.3586381305342987]
	TIME [epoch: 2.66 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4514492053802928		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.4514492053802928 | validation: 0.24228835341249516]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4521902018331024		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.4521902018331024 | validation: 0.29707141651446306]
	TIME [epoch: 2.66 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4212109830553257		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.4212109830553257 | validation: 0.28004386472484033]
	TIME [epoch: 2.66 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.414014543795068		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.414014543795068 | validation: 0.257696019465462]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39728452277416654		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.39728452277416654 | validation: 0.20930324856320248]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40773847395703117		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.40773847395703117 | validation: 0.33269185903716153]
	TIME [epoch: 2.67 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41789819910127274		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.41789819910127274 | validation: 0.2285123228395649]
	TIME [epoch: 2.66 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285326548540775		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.4285326548540775 | validation: 0.3771737137265476]
	TIME [epoch: 2.65 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005911236862235		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.5005911236862235 | validation: 0.38105932620982524]
	TIME [epoch: 2.65 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5902480225695093		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.5902480225695093 | validation: 0.1995650345293023]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40400494785162805		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.40400494785162805 | validation: 0.42326466814210684]
	TIME [epoch: 2.65 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856661400114281		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.4856661400114281 | validation: 0.28812620227951585]
	TIME [epoch: 2.64 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.469548263902642		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.469548263902642 | validation: 0.1994412004333744]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38592373795666773		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.38592373795666773 | validation: 0.34670427093568257]
	TIME [epoch: 2.66 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43014902147339173		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.43014902147339173 | validation: 0.23722617425308581]
	TIME [epoch: 2.66 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4314949573534818		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.4314949573534818 | validation: 0.21063363366547383]
	TIME [epoch: 2.66 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3770614394578939		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.3770614394578939 | validation: 0.30294539179630653]
	TIME [epoch: 2.67 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38658344442981774		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.38658344442981774 | validation: 0.22706912724367445]
	TIME [epoch: 2.66 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39728287980063054		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.39728287980063054 | validation: 0.2313021929101261]
	TIME [epoch: 2.66 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37283563581946694		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.37283563581946694 | validation: 0.2328973288407238]
	TIME [epoch: 2.66 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3627523888608108		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.3627523888608108 | validation: 0.22437192905157952]
	TIME [epoch: 2.66 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594623076157859		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.3594623076157859 | validation: 0.2060317722578836]
	TIME [epoch: 2.66 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35813786252601665		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.35813786252601665 | validation: 0.26879399513803115]
	TIME [epoch: 2.64 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.356965506628969		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.356965506628969 | validation: 0.2176610748489345]
	TIME [epoch: 2.64 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.351466070008362		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.351466070008362 | validation: 0.24703855624225168]
	TIME [epoch: 2.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594955123209242		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.3594955123209242 | validation: 0.24409485666083205]
	TIME [epoch: 2.64 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41362034786373314		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.41362034786373314 | validation: 0.31349669405976016]
	TIME [epoch: 2.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43743497278299076		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.43743497278299076 | validation: 0.24278139486554573]
	TIME [epoch: 2.65 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44096050648894525		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.44096050648894525 | validation: 0.2021514725847391]
	TIME [epoch: 2.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33857864345062455		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.33857864345062455 | validation: 0.26526764164145605]
	TIME [epoch: 2.64 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35988500826593134		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.35988500826593134 | validation: 0.21615502085298424]
	TIME [epoch: 2.64 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3868367317609526		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.3868367317609526 | validation: 0.23785237894388436]
	TIME [epoch: 2.65 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34367432901119155		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.34367432901119155 | validation: 0.20982883979660621]
	TIME [epoch: 2.64 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33050908914946886		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.33050908914946886 | validation: 0.19612782780877508]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227508463602171		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.3227508463602171 | validation: 0.18634372153156567]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204551089681616		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.3204551089681616 | validation: 0.23984340187265582]
	TIME [epoch: 2.66 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31523919476489537		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.31523919476489537 | validation: 0.1924268025798213]
	TIME [epoch: 2.66 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162152602975195		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.3162152602975195 | validation: 0.22574689622421548]
	TIME [epoch: 2.66 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33436253523062204		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.33436253523062204 | validation: 0.25659412066544846]
	TIME [epoch: 2.67 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41199482285479777		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.41199482285479777 | validation: 0.2788194743229458]
	TIME [epoch: 2.66 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3880748832733146		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.3880748832733146 | validation: 0.21387537111785818]
	TIME [epoch: 2.65 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3654481000214952		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.3654481000214952 | validation: 0.18212527046400992]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31157120999972654		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.31157120999972654 | validation: 0.22933829344268003]
	TIME [epoch: 2.68 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30672235778398316		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.30672235778398316 | validation: 0.18056246944228518]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023084912847428		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.3023084912847428 | validation: 0.23241699392383997]
	TIME [epoch: 2.64 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3250887152771874		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.3250887152771874 | validation: 0.21781636574757549]
	TIME [epoch: 2.65 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36663298055695676		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.36663298055695676 | validation: 0.23555830312332443]
	TIME [epoch: 2.64 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34033509703513853		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.34033509703513853 | validation: 0.18395083519731512]
	TIME [epoch: 2.65 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297819576184446		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.3297819576184446 | validation: 0.20680521056172285]
	TIME [epoch: 2.64 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29872008000149736		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.29872008000149736 | validation: 0.18812724558115335]
	TIME [epoch: 2.66 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2884487037922056		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.2884487037922056 | validation: 0.19124143441506783]
	TIME [epoch: 2.64 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881923744497469		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.2881923744497469 | validation: 0.18990604205555628]
	TIME [epoch: 2.65 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938863342298499		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.2938863342298499 | validation: 0.21345311986204654]
	TIME [epoch: 2.64 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3020956820926002		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.3020956820926002 | validation: 0.21547388770468834]
	TIME [epoch: 2.64 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37998874239304337		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.37998874239304337 | validation: 0.22644902887791762]
	TIME [epoch: 2.65 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335843505818242		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.335843505818242 | validation: 0.1735978434856793]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_703.pth
	Model improved!!!
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31039791780005993		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.31039791780005993 | validation: 0.18381015950285662]
	TIME [epoch: 2.66 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832286849703067		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.2832286849703067 | validation: 0.17817592181124353]
	TIME [epoch: 2.66 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2753765948465103		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.2753765948465103 | validation: 0.1788656413027812]
	TIME [epoch: 2.64 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.269263625156384		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.269263625156384 | validation: 0.16209566634445272]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26906534001248245		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.26906534001248245 | validation: 0.20335774948218388]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27597434208572114		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.27597434208572114 | validation: 0.1775634151942728]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317313395241107		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.317313395241107 | validation: 0.2638188451717786]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35972255707233375		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.35972255707233375 | validation: 0.23633711584068756]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3951354857397448		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.3951354857397448 | validation: 0.14926005396696324]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725506694200658		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.2725506694200658 | validation: 0.24083872494962746]
	TIME [epoch: 2.66 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29139050528867794		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.29139050528867794 | validation: 0.17676645846795336]
	TIME [epoch: 2.66 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3118504182197343		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.3118504182197343 | validation: 0.17315592137464808]
	TIME [epoch: 2.66 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.281734762605997		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.281734762605997 | validation: 0.16642309798121593]
	TIME [epoch: 2.66 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2623566621464355		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.2623566621464355 | validation: 0.17082186382481335]
	TIME [epoch: 2.66 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26105880264679804		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.26105880264679804 | validation: 0.15953317493112948]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634625699018233		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.2634625699018233 | validation: 0.17227222698440148]
	TIME [epoch: 2.66 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639561464740892		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.2639561464740892 | validation: 0.15955668567527875]
	TIME [epoch: 2.66 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2796384118322597		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.2796384118322597 | validation: 0.20186982118463193]
	TIME [epoch: 2.66 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29578685689752643		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.29578685689752643 | validation: 0.1832893281692264]
	TIME [epoch: 2.66 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163315170010853		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.3163315170010853 | validation: 0.17492613208377886]
	TIME [epoch: 2.66 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26995586435982766		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.26995586435982766 | validation: 0.1472674923140677]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516306341506144		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.2516306341506144 | validation: 0.17644637127827523]
	TIME [epoch: 2.66 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24790410742891417		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.24790410742891417 | validation: 0.1433805405164204]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_726.pth
	Model improved!!!
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24710067158448584		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.24710067158448584 | validation: 0.18152038234471832]
	TIME [epoch: 2.66 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25624688418094826		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.25624688418094826 | validation: 0.18480877668684426]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298106957138586		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.298106957138586 | validation: 0.2149630501216087]
	TIME [epoch: 2.68 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28361225338204427		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.28361225338204427 | validation: 0.16344483447320968]
	TIME [epoch: 2.66 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29853463481972736		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.29853463481972736 | validation: 0.15328060164005458]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25219361532409645		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.25219361532409645 | validation: 0.16466605902975817]
	TIME [epoch: 2.66 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23661480283707156		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.23661480283707156 | validation: 0.15723674487185735]
	TIME [epoch: 2.66 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23018834147036005		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.23018834147036005 | validation: 0.13775690427509055]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22352196706354563		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.22352196706354563 | validation: 0.16604053936888383]
	TIME [epoch: 2.66 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22869115545361735		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.22869115545361735 | validation: 0.1355040982213839]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23667544072307473		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.23667544072307473 | validation: 0.20559762156019978]
	TIME [epoch: 2.66 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740417707340502		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.2740417707340502 | validation: 0.23483670217434743]
	TIME [epoch: 2.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3581854873975838		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.3581854873975838 | validation: 0.1422145704861269]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618568481414308		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.2618568481414308 | validation: 0.15760447653404214]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2339719775410392		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.2339719775410392 | validation: 0.14266181060692615]
	TIME [epoch: 2.66 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2188775668331447		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.2188775668331447 | validation: 0.13358002951909098]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22368544748137273		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.22368544748137273 | validation: 0.1387310684332917]
	TIME [epoch: 2.66 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21840416893409156		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.21840416893409156 | validation: 0.14454673925119246]
	TIME [epoch: 2.66 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206222320602608		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.2206222320602608 | validation: 0.15625586126940688]
	TIME [epoch: 2.66 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2723959591461938		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.2723959591461938 | validation: 0.21583927725499097]
	TIME [epoch: 2.66 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3283145632048483		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.3283145632048483 | validation: 0.17142531538011632]
	TIME [epoch: 2.66 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2986917275137394		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.2986917275137394 | validation: 0.13813113704393776]
	TIME [epoch: 2.66 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21602600041517261		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.21602600041517261 | validation: 0.15650298730505927]
	TIME [epoch: 2.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22623271843011303		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.22623271843011303 | validation: 0.14426920387109235]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2510436028867045		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.2510436028867045 | validation: 0.16095194975313293]
	TIME [epoch: 2.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23601500976569031		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.23601500976569031 | validation: 0.13662883464210243]
	TIME [epoch: 2.66 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23088736501173712		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.23088736501173712 | validation: 0.1428640375474064]
	TIME [epoch: 2.66 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22432989842132828		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.22432989842132828 | validation: 0.13741044732610389]
	TIME [epoch: 2.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181250269975614		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.2181250269975614 | validation: 0.13337994951464296]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21365663757904244		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.21365663757904244 | validation: 0.13584093861501237]
	TIME [epoch: 2.66 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21949160720370806		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.21949160720370806 | validation: 0.15700676079675482]
	TIME [epoch: 2.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2207058992429855		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.2207058992429855 | validation: 0.15957477505034579]
	TIME [epoch: 2.66 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566211123514578		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.2566211123514578 | validation: 0.17159752346362878]
	TIME [epoch: 2.66 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.256513868597139		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.256513868597139 | validation: 0.1388376924920894]
	TIME [epoch: 2.67 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2415595415621854		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.2415595415621854 | validation: 0.14487719955021602]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21428077730032932		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.21428077730032932 | validation: 0.14000999228933705]
	TIME [epoch: 2.66 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20901403649451897		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.20901403649451897 | validation: 0.12397111262396543]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20342618776074387		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.20342618776074387 | validation: 0.1488068151079962]
	TIME [epoch: 2.66 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031912167772284		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.2031912167772284 | validation: 0.11897032715719535]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20404194667702857		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.20404194667702857 | validation: 0.13403219080300235]
	TIME [epoch: 2.66 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20563130462352638		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.20563130462352638 | validation: 0.15754907364753246]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2287040650657361		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.2287040650657361 | validation: 0.1650379799151689]
	TIME [epoch: 2.66 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28305969066636766		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.28305969066636766 | validation: 0.15183427513459652]
	TIME [epoch: 2.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22516924748240033		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.22516924748240033 | validation: 0.1253659484911179]
	TIME [epoch: 2.66 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20923841253553677		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.20923841253553677 | validation: 0.128342950338685]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018197874370579		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.2018197874370579 | validation: 0.12682657279903442]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20143977945465932		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.20143977945465932 | validation: 0.14061578732146152]
	TIME [epoch: 2.66 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1969237079424925		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.1969237079424925 | validation: 0.14027922631441517]
	TIME [epoch: 2.66 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208169134018024		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.2208169134018024 | validation: 0.15777245229686743]
	TIME [epoch: 2.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24561759702484837		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.24561759702484837 | validation: 0.16435867474516777]
	TIME [epoch: 2.66 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2488936380813506		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.2488936380813506 | validation: 0.1344182270269234]
	TIME [epoch: 2.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20324174276420168		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.20324174276420168 | validation: 0.1118225591651429]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844871415277537		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.1844871415277537 | validation: 0.13458112863044724]
	TIME [epoch: 2.66 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1848608867757285		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.1848608867757285 | validation: 0.10888802548500005]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18536473850649696		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.18536473850649696 | validation: 0.13136404654554856]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18214608622824613		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.18214608622824613 | validation: 0.10378085388477579]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_782.pth
	Model improved!!!
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18710310680882417		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.18710310680882417 | validation: 0.14642063015531567]
	TIME [epoch: 2.66 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20078470627483228		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.20078470627483228 | validation: 0.1658854369424726]
	TIME [epoch: 2.66 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423068161116459		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.2423068161116459 | validation: 0.15561068633514932]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25123699147601214		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.25123699147601214 | validation: 0.14321500952487273]
	TIME [epoch: 2.66 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.236435491095067		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.236435491095067 | validation: 0.11978088330444137]
	TIME [epoch: 2.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19072103984837693		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.19072103984837693 | validation: 0.11754931200875551]
	TIME [epoch: 2.66 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18009836915419436		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.18009836915419436 | validation: 0.11488466985936335]
	TIME [epoch: 2.66 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17396368830726133		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.17396368830726133 | validation: 0.12539082156582468]
	TIME [epoch: 2.66 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18528887431541954		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.18528887431541954 | validation: 0.13050727428803044]
	TIME [epoch: 2.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19963166839808894		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.19963166839808894 | validation: 0.13178076678445896]
	TIME [epoch: 2.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2233670170688667		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.2233670170688667 | validation: 0.14685059052623864]
	TIME [epoch: 2.66 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365940182407583		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.2365940182407583 | validation: 0.14007674031507653]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19402971447813627		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.19402971447813627 | validation: 0.11499495562940948]
	TIME [epoch: 2.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18212238265359884		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.18212238265359884 | validation: 0.11281263881071367]
	TIME [epoch: 2.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17381732357567173		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.17381732357567173 | validation: 0.12309525394572261]
	TIME [epoch: 2.66 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17892650802847648		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.17892650802847648 | validation: 0.11496224158938571]
	TIME [epoch: 2.66 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185383534145511		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.185383534145511 | validation: 0.1259094179282978]
	TIME [epoch: 2.66 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19691476947982836		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.19691476947982836 | validation: 0.1348352964606899]
	TIME [epoch: 2.66 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2085190839096697		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.2085190839096697 | validation: 0.1312796679236796]
	TIME [epoch: 2.66 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21540311628452244		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.21540311628452244 | validation: 0.13031780907928292]
	TIME [epoch: 2.66 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896468106259509		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.1896468106259509 | validation: 0.11198026973381672]
	TIME [epoch: 2.66 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18450148946223993		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.18450148946223993 | validation: 0.13005374434602332]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16964586469752782		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.16964586469752782 | validation: 0.11928485954200854]
	TIME [epoch: 2.66 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170743360780765		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.170743360780765 | validation: 0.11460718570028866]
	TIME [epoch: 2.66 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16836285076232255		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.16836285076232255 | validation: 0.11290628538100077]
	TIME [epoch: 2.66 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1774768462726654		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.1774768462726654 | validation: 0.11986639882779326]
	TIME [epoch: 2.66 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19463748294933694		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.19463748294933694 | validation: 0.16472109968131243]
	TIME [epoch: 2.66 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23308934966158887		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.23308934966158887 | validation: 0.13506773789632362]
	TIME [epoch: 2.66 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001295540244194		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.2001295540244194 | validation: 0.1061636994225653]
	TIME [epoch: 2.66 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17512716238355153		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.17512716238355153 | validation: 0.10443166619538642]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16380342692617983		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.16380342692617983 | validation: 0.10859582325295825]
	TIME [epoch: 2.66 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617665929834823		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.1617665929834823 | validation: 0.1083634406258164]
	TIME [epoch: 2.66 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614440045817493		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.1614440045817493 | validation: 0.11489972341871094]
	TIME [epoch: 2.66 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702965521581288		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.1702965521581288 | validation: 0.12296115833453168]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18995280477918874		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.18995280477918874 | validation: 0.15134601419690785]
	TIME [epoch: 2.66 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22914364972790238		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.22914364972790238 | validation: 0.1091487911374069]
	TIME [epoch: 2.66 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17896570202131784		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.17896570202131784 | validation: 0.11585015828119537]
	TIME [epoch: 2.66 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16202869018850133		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.16202869018850133 | validation: 0.10315898474511549]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16168512577773828		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.16168512577773828 | validation: 0.11657146746927148]
	TIME [epoch: 2.66 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698690701234476		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.1698690701234476 | validation: 0.10986853218617416]
	TIME [epoch: 2.66 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17729255469502833		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.17729255469502833 | validation: 0.12431915500447942]
	TIME [epoch: 2.66 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19178198287633652		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.19178198287633652 | validation: 0.13129641461756353]
	TIME [epoch: 2.66 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18568867884787693		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.18568867884787693 | validation: 0.11508442884410677]
	TIME [epoch: 2.66 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18052214327164923		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.18052214327164923 | validation: 0.12427324578913349]
	TIME [epoch: 2.66 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677959459255162		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.1677959459255162 | validation: 0.10719351016153256]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17224378983586994		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.17224378983586994 | validation: 0.12730074506612155]
	TIME [epoch: 2.66 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17081588392418176		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.17081588392418176 | validation: 0.1161555723270499]
	TIME [epoch: 2.66 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17028544465188872		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.17028544465188872 | validation: 0.10743900130525658]
	TIME [epoch: 2.66 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16880302763033353		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.16880302763033353 | validation: 0.1228807105218178]
	TIME [epoch: 2.66 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1749359853232646		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.1749359853232646 | validation: 0.11008691807131427]
	TIME [epoch: 2.66 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17649827234289425		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.17649827234289425 | validation: 0.11595306980009089]
	TIME [epoch: 2.66 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16897469374552335		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.16897469374552335 | validation: 0.09914296124398048]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16530946234818458		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.16530946234818458 | validation: 0.11971630575124362]
	TIME [epoch: 2.66 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622951584830124		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.1622951584830124 | validation: 0.09863886367695057]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16395915675353223		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.16395915675353223 | validation: 0.10679217790812463]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16324232084605642		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.16324232084605642 | validation: 0.11339527126073712]
	TIME [epoch: 2.66 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16504055734081152		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.16504055734081152 | validation: 0.10792491759989449]
	TIME [epoch: 2.66 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1736830344633495		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.1736830344633495 | validation: 0.12634314883549577]
	TIME [epoch: 2.66 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781179357155363		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.1781179357155363 | validation: 0.11645177309553001]
	TIME [epoch: 2.66 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17456041882395532		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.17456041882395532 | validation: 0.1069472643450754]
	TIME [epoch: 2.66 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15512798505437353		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.15512798505437353 | validation: 0.09883837010230295]
	TIME [epoch: 2.66 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15062121750426816		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.15062121750426816 | validation: 0.09764996966505013]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14953263046468804		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.14953263046468804 | validation: 0.11424221184488514]
	TIME [epoch: 2.66 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15110131643626798		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.15110131643626798 | validation: 0.10041438654693711]
	TIME [epoch: 2.66 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15540789507510158		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.15540789507510158 | validation: 0.11538443524676145]
	TIME [epoch: 2.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16636024081637182		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.16636024081637182 | validation: 0.11331044157042869]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1721687729832606		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.1721687729832606 | validation: 0.1215469953117643]
	TIME [epoch: 2.66 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19207315620800447		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.19207315620800447 | validation: 0.11481578110779958]
	TIME [epoch: 2.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16139502174587697		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.16139502174587697 | validation: 0.10245409699640434]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14579563567957832		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.14579563567957832 | validation: 0.08995628766559903]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14876528934826577		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.14876528934826577 | validation: 0.10164162993093728]
	TIME [epoch: 2.64 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14495708702979607		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.14495708702979607 | validation: 0.08692026815486022]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1408941686988722		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.1408941686988722 | validation: 0.09735140630031468]
	TIME [epoch: 2.65 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14045139484710653		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.14045139484710653 | validation: 0.10218887317793697]
	TIME [epoch: 2.65 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13940502195557464		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.13940502195557464 | validation: 0.09086070282557238]
	TIME [epoch: 2.65 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14243112735126315		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.14243112735126315 | validation: 0.11507679849566417]
	TIME [epoch: 2.65 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14778981325753726		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.14778981325753726 | validation: 0.11151645217008994]
	TIME [epoch: 2.66 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17839694369369574		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.17839694369369574 | validation: 0.12743991644201635]
	TIME [epoch: 2.65 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23030731906422053		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.23030731906422053 | validation: 0.11532686707769338]
	TIME [epoch: 2.65 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1754832887529296		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.1754832887529296 | validation: 0.09219632235822794]
	TIME [epoch: 2.66 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139426058894986		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.139426058894986 | validation: 0.09204111244866076]
	TIME [epoch: 2.65 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407377482288784		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.1407377482288784 | validation: 0.09634080503385233]
	TIME [epoch: 2.64 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13933822944350285		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.13933822944350285 | validation: 0.09622803892029268]
	TIME [epoch: 2.64 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1516222382890778		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.1516222382890778 | validation: 0.11193324139732104]
	TIME [epoch: 2.64 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17102293416238376		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.17102293416238376 | validation: 0.11529422149299284]
	TIME [epoch: 2.64 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702348871986079		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.1702348871986079 | validation: 0.10100682325299425]
	TIME [epoch: 2.64 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15618334858460034		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.15618334858460034 | validation: 0.10274466866372119]
	TIME [epoch: 2.64 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14055731367834648		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.14055731367834648 | validation: 0.09937625686742854]
	TIME [epoch: 2.65 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14184696561996785		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.14184696561996785 | validation: 0.0940726623939786]
	TIME [epoch: 2.64 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13857177851292438		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.13857177851292438 | validation: 0.09390272093600523]
	TIME [epoch: 2.64 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13726616614386186		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.13726616614386186 | validation: 0.09645033619922966]
	TIME [epoch: 2.64 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13910391280697362		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.13910391280697362 | validation: 0.10550180653420914]
	TIME [epoch: 2.64 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15428940427068744		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.15428940427068744 | validation: 0.1135087933351632]
	TIME [epoch: 2.64 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1723497199827346		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.1723497199827346 | validation: 0.12020516538993485]
	TIME [epoch: 2.64 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16369704215972292		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.16369704215972292 | validation: 0.09718058190024273]
	TIME [epoch: 2.64 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522141635399016		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.1522141635399016 | validation: 0.09663869187414752]
	TIME [epoch: 2.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14405291783273724		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.14405291783273724 | validation: 0.09050236857501659]
	TIME [epoch: 2.64 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13570439589838312		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.13570439589838312 | validation: 0.09145973263453662]
	TIME [epoch: 2.64 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13329002486053887		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.13329002486053887 | validation: 0.09308608617567585]
	TIME [epoch: 2.66 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13486569460809944		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.13486569460809944 | validation: 0.09489638913135306]
	TIME [epoch: 2.64 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697933523857606		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.14697933523857606 | validation: 0.12085134419063258]
	TIME [epoch: 2.64 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651650310142351		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.1651650310142351 | validation: 0.10863742413001348]
	TIME [epoch: 2.65 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16605491426975294		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.16605491426975294 | validation: 0.08514823652965338]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_885.pth
	Model improved!!!
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14173743871948336		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.14173743871948336 | validation: 0.10642141213484263]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376259158446154		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.1376259158446154 | validation: 0.08398903488490007]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13364132319402733		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.13364132319402733 | validation: 0.09191183006530956]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12861032750442797		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.12861032750442797 | validation: 0.0878817200442259]
	TIME [epoch: 2.66 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13093359051186315		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.13093359051186315 | validation: 0.1052529042523083]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14549930057820784		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.14549930057820784 | validation: 0.10345762728774854]
	TIME [epoch: 2.66 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16662534083102032		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.16662534083102032 | validation: 0.11054416714566953]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.171359275523192		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.171359275523192 | validation: 0.09210986529011947]
	TIME [epoch: 2.66 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416977711650911		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.1416977711650911 | validation: 0.08689054983999564]
	TIME [epoch: 2.66 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13248699155774024		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.13248699155774024 | validation: 0.08706106039066858]
	TIME [epoch: 2.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12848425497887497		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.12848425497887497 | validation: 0.09053002394222197]
	TIME [epoch: 2.66 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652886721919238		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.12652886721919238 | validation: 0.07990183493450172]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12409424631365931		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.12409424631365931 | validation: 0.09504888018645266]
	TIME [epoch: 2.66 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12676393691814808		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.12676393691814808 | validation: 0.09925174986497086]
	TIME [epoch: 2.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409920310014425		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.1409920310014425 | validation: 0.11229156106138645]
	TIME [epoch: 2.65 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15822264574826317		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.15822264574826317 | validation: 0.10518441888815203]
	TIME [epoch: 2.65 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15702957086057626		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.15702957086057626 | validation: 0.1010805007822556]
	TIME [epoch: 2.65 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14681662877962964		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.14681662877962964 | validation: 0.08871097700922337]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13718843964212513		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.13718843964212513 | validation: 0.09474942494042576]
	TIME [epoch: 2.66 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287931356279914		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.1287931356279914 | validation: 0.09114211315927412]
	TIME [epoch: 2.66 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12125868737506296		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.12125868737506296 | validation: 0.08712454959458245]
	TIME [epoch: 2.65 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318170027626345		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.12318170027626345 | validation: 0.091460751335383]
	TIME [epoch: 2.65 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12551006210147986		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.12551006210147986 | validation: 0.08724777506358328]
	TIME [epoch: 2.65 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12211491069602795		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.12211491069602795 | validation: 0.09054863953988414]
	TIME [epoch: 2.65 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1252549297957671		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.1252549297957671 | validation: 0.09147014906520268]
	TIME [epoch: 2.65 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13358669954764035		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.13358669954764035 | validation: 0.11175441788327976]
	TIME [epoch: 2.65 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15698240558621124		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.15698240558621124 | validation: 0.1321677320357275]
	TIME [epoch: 2.65 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689741156719758		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.1689741156719758 | validation: 0.08937335520699971]
	TIME [epoch: 2.65 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13807006807015662		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.13807006807015662 | validation: 0.09145775053595054]
	TIME [epoch: 2.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11900452683613216		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.11900452683613216 | validation: 0.08874765985409122]
	TIME [epoch: 2.65 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11832996267248728		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.11832996267248728 | validation: 0.08443869625561473]
	TIME [epoch: 2.65 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11572827525424256		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.11572827525424256 | validation: 0.08346708552494635]
	TIME [epoch: 2.65 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11609029658061969		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.11609029658061969 | validation: 0.08533552708961811]
	TIME [epoch: 2.65 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12051206710799227		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.12051206710799227 | validation: 0.08738831437317515]
	TIME [epoch: 2.65 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11659720608288732		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.11659720608288732 | validation: 0.09149619922787529]
	TIME [epoch: 2.65 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11602688813702913		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.11602688813702913 | validation: 0.08985181738681136]
	TIME [epoch: 2.65 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12004806921152814		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.12004806921152814 | validation: 0.09167849024314897]
	TIME [epoch: 2.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13110391766821342		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.13110391766821342 | validation: 0.10414716341396504]
	TIME [epoch: 2.65 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18244035799950864		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.18244035799950864 | validation: 0.13917953387131596]
	TIME [epoch: 2.65 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17552199029788934		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.17552199029788934 | validation: 0.09205043888288506]
	TIME [epoch: 2.66 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12466706364757742		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.12466706364757742 | validation: 0.0776302488530257]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11743503297428438		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.11743503297428438 | validation: 0.08952699814051601]
	TIME [epoch: 2.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13140321623455484		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.13140321623455484 | validation: 0.09676372734173327]
	TIME [epoch: 2.65 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14458382677382386		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.14458382677382386 | validation: 0.09541437631178129]
	TIME [epoch: 2.65 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13259618841368295		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.13259618841368295 | validation: 0.08732652901475974]
	TIME [epoch: 2.65 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1221659798211115		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.1221659798211115 | validation: 0.09000523063583409]
	TIME [epoch: 2.65 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11438005852406889		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.11438005852406889 | validation: 0.08669721565941735]
	TIME [epoch: 2.65 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11397140606610441		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.11397140606610441 | validation: 0.07564741199863509]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.112195271285463		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.112195271285463 | validation: 0.09196736441446407]
	TIME [epoch: 2.65 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11643679118772667		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.11643679118772667 | validation: 0.08102220147356237]
	TIME [epoch: 2.65 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11296684574167806		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.11296684574167806 | validation: 0.09026141011367904]
	TIME [epoch: 2.66 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11547387495956925		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.11547387495956925 | validation: 0.07951841738363881]
	TIME [epoch: 2.65 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11387932142290669		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.11387932142290669 | validation: 0.09036577701852203]
	TIME [epoch: 2.65 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313045384893315		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.1313045384893315 | validation: 0.12718893699767536]
	TIME [epoch: 2.65 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734537269970966		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.1734537269970966 | validation: 0.09258839862176707]
	TIME [epoch: 2.65 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14588614208531436		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.14588614208531436 | validation: 0.08631105802123035]
	TIME [epoch: 2.65 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11889097432076896		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.11889097432076896 | validation: 0.08768015742938766]
	TIME [epoch: 2.65 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11241926132457998		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.11241926132457998 | validation: 0.08567093752775691]
	TIME [epoch: 2.65 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11681715716199768		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.11681715716199768 | validation: 0.09144951429612211]
	TIME [epoch: 2.65 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272839635019887		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.12272839635019887 | validation: 0.08369428622852437]
	TIME [epoch: 2.65 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12947376559879362		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.12947376559879362 | validation: 0.09774683800661718]
	TIME [epoch: 2.66 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13272740594634658		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.13272740594634658 | validation: 0.08460287323363469]
	TIME [epoch: 2.66 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052184878045523		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.12052184878045523 | validation: 0.08218842171841463]
	TIME [epoch: 2.65 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11828366616637861		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.11828366616637861 | validation: 0.07837548779150116]
	TIME [epoch: 2.66 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11395268219003934		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.11395268219003934 | validation: 0.09001532269040945]
	TIME [epoch: 2.66 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061233661438437		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.11061233661438437 | validation: 0.08390627966355038]
	TIME [epoch: 2.65 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11473984522356334		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.11473984522356334 | validation: 0.08715989508572407]
	TIME [epoch: 2.65 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12060471047075577		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.12060471047075577 | validation: 0.0948601868058109]
	TIME [epoch: 2.65 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13481733902922396		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.13481733902922396 | validation: 0.10251826425281306]
	TIME [epoch: 2.65 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13682949617118337		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.13682949617118337 | validation: 0.0827215351050493]
	TIME [epoch: 2.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12460369375180001		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.12460369375180001 | validation: 0.08571925986374103]
	TIME [epoch: 2.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11562363926796014		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.11562363926796014 | validation: 0.08054494858537342]
	TIME [epoch: 2.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1091600031922249		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.1091600031922249 | validation: 0.07399819803112277]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_958.pth
	Model improved!!!
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11230732035401075		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.11230732035401075 | validation: 0.07821262900341053]
	TIME [epoch: 2.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910177054081292		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.10910177054081292 | validation: 0.08650833424788451]
	TIME [epoch: 2.65 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647025707113617		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.10647025707113617 | validation: 0.07402951094601401]
	TIME [epoch: 2.65 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10544854657859631		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.10544854657859631 | validation: 0.08408635330202656]
	TIME [epoch: 2.65 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10536910854951564		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.10536910854951564 | validation: 0.08543373448312476]
	TIME [epoch: 2.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10977407673763462		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.10977407673763462 | validation: 0.08594679835783106]
	TIME [epoch: 2.65 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12168172185623957		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.12168172185623957 | validation: 0.1143960748879577]
	TIME [epoch: 2.65 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15231584560826686		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.15231584560826686 | validation: 0.08447610460980465]
	TIME [epoch: 2.65 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14536972780067994		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.14536972780067994 | validation: 0.08375663521887701]
	TIME [epoch: 2.65 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11573977471299038		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.11573977471299038 | validation: 0.08111137658586175]
	TIME [epoch: 2.65 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11086639069615037		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.11086639069615037 | validation: 0.07657812619005316]
	TIME [epoch: 2.66 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10950757856395182		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.10950757856395182 | validation: 0.08150875981671557]
	TIME [epoch: 2.65 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10683347791983777		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.10683347791983777 | validation: 0.08152142815332065]
	TIME [epoch: 2.65 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10225562487431238		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.10225562487431238 | validation: 0.08021612670137103]
	TIME [epoch: 2.65 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10802511988313972		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.10802511988313972 | validation: 0.09024498433725303]
	TIME [epoch: 2.65 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11304501522817395		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.11304501522817395 | validation: 0.08381443728222343]
	TIME [epoch: 2.65 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12704043063011972		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.12704043063011972 | validation: 0.11406125013463425]
	TIME [epoch: 2.65 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1438029006367023		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.1438029006367023 | validation: 0.08159392094421708]
	TIME [epoch: 2.65 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12136286366716836		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.12136286366716836 | validation: 0.07632237421284545]
	TIME [epoch: 2.65 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647206677096932		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.10647206677096932 | validation: 0.08524587954451324]
	TIME [epoch: 2.65 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064571824094373		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.1064571824094373 | validation: 0.08825202014837741]
	TIME [epoch: 2.65 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967912705695046		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.10967912705695046 | validation: 0.07592666989665194]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10407185750145899		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.10407185750145899 | validation: 0.07708431312290291]
	TIME [epoch: 2.65 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10360053526776464		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.10360053526776464 | validation: 0.07949924996198944]
	TIME [epoch: 2.65 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10276897954085447		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.10276897954085447 | validation: 0.07471141659626027]
	TIME [epoch: 2.65 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10455845569667503		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.10455845569667503 | validation: 0.07601979222174535]
	TIME [epoch: 2.65 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10178154409737172		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.10178154409737172 | validation: 0.08814911248052322]
	TIME [epoch: 2.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183529351792939		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.10183529351792939 | validation: 0.08755551878563228]
	TIME [epoch: 2.65 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10293113374108159		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.10293113374108159 | validation: 0.08224384149390207]
	TIME [epoch: 2.65 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10107521617098629		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.10107521617098629 | validation: 0.07580266658139453]
	TIME [epoch: 2.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072089482884773		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.1072089482884773 | validation: 0.10162447741378021]
	TIME [epoch: 2.65 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13018597853200456		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.13018597853200456 | validation: 0.10900881161644037]
	TIME [epoch: 2.65 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18770703378749418		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.18770703378749418 | validation: 0.08925373444926399]
	TIME [epoch: 2.66 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10853306703272943		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.10853306703272943 | validation: 0.081956911351808]
	TIME [epoch: 2.65 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10423170997465452		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.10423170997465452 | validation: 0.07889272891618723]
	TIME [epoch: 2.65 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11804263581628054		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.11804263581628054 | validation: 0.0936700538797612]
	TIME [epoch: 2.65 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11800567570423001		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.11800567570423001 | validation: 0.07882897125772009]
	TIME [epoch: 2.65 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10339926800442611		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.10339926800442611 | validation: 0.08024460579570687]
	TIME [epoch: 2.65 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10065342505998516		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.10065342505998516 | validation: 0.08294634538392243]
	TIME [epoch: 2.65 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10249662719684294		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.10249662719684294 | validation: 0.08195724993132174]
	TIME [epoch: 2.65 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10609307989791628		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.10609307989791628 | validation: 0.08715125098400156]
	TIME [epoch: 2.65 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11189146409089563		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.11189146409089563 | validation: 0.08259013667132112]
	TIME [epoch: 2.65 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748907131744656		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.11748907131744656 | validation: 0.08542801977800162]
	TIME [epoch: 176 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10940662309006982		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.10940662309006982 | validation: 0.0751157859513345]
	TIME [epoch: 5.69 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256555873208939		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.10256555873208939 | validation: 0.08490805777171756]
	TIME [epoch: 5.69 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09716916468153879		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.09716916468153879 | validation: 0.07985900096520002]
	TIME [epoch: 5.69 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09988702931554168		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.09988702931554168 | validation: 0.0787725526951198]
	TIME [epoch: 5.68 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927084794430836		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.09927084794430836 | validation: 0.08605720101656214]
	TIME [epoch: 5.69 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222614268004114		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.10222614268004114 | validation: 0.08210582413315402]
	TIME [epoch: 5.68 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10253240888289025		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.10253240888289025 | validation: 0.10265120325732427]
	TIME [epoch: 5.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11558406501604936		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.11558406501604936 | validation: 0.08342514018607404]
	TIME [epoch: 5.68 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12998197784182225		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.12998197784182225 | validation: 0.10499308053497308]
	TIME [epoch: 5.69 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052144642296454		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.12052144642296454 | validation: 0.07843673793098518]
	TIME [epoch: 5.67 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092225439118863		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.10092225439118863 | validation: 0.07602905223846251]
	TIME [epoch: 5.69 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0952723261569249		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.0952723261569249 | validation: 0.08189687043527483]
	TIME [epoch: 5.68 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10309753845925501		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.10309753845925501 | validation: 0.07809273984840145]
	TIME [epoch: 5.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687025759205394		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.10687025759205394 | validation: 0.09259957862227783]
	TIME [epoch: 5.68 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470396623228347		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.10470396623228347 | validation: 0.07993217278450189]
	TIME [epoch: 5.68 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702873606987556		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.10702873606987556 | validation: 0.09810719627640886]
	TIME [epoch: 5.67 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1152874953340717		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.1152874953340717 | validation: 0.07514215918141137]
	TIME [epoch: 5.68 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10712056954356552		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.10712056954356552 | validation: 0.0872914958674284]
	TIME [epoch: 5.68 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10171965590967577		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.10171965590967577 | validation: 0.07223315746271201]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09706901951909602		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.09706901951909602 | validation: 0.08081133547123318]
	TIME [epoch: 5.68 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09691830294768174		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.09691830294768174 | validation: 0.0816786384002553]
	TIME [epoch: 5.68 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09592359571808101		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.09592359571808101 | validation: 0.08218955873789124]
	TIME [epoch: 5.68 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09759270617857745		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.09759270617857745 | validation: 0.08480692553224538]
	TIME [epoch: 5.69 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994724439338998		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.0994724439338998 | validation: 0.07533015943414277]
	TIME [epoch: 5.68 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11153426771293991		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.11153426771293991 | validation: 0.09904522261592506]
	TIME [epoch: 5.68 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12393223013106759		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.12393223013106759 | validation: 0.08551734573464356]
	TIME [epoch: 5.68 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11339083459497888		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.11339083459497888 | validation: 0.08447911327279459]
	TIME [epoch: 5.68 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10111196065985123		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.10111196065985123 | validation: 0.08812142873193668]
	TIME [epoch: 5.68 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09412119459948262		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.09412119459948262 | validation: 0.07611281831159139]
	TIME [epoch: 5.68 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942998841862746		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.0942998841862746 | validation: 0.07757024291617078]
	TIME [epoch: 5.68 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690365368266278		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.09690365368266278 | validation: 0.07994510687609725]
	TIME [epoch: 5.68 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10381278607609701		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.10381278607609701 | validation: 0.09303337950770041]
	TIME [epoch: 5.67 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1081505402805734		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.1081505402805734 | validation: 0.080659426400712]
	TIME [epoch: 5.68 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10390375779577493		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.10390375779577493 | validation: 0.0887588271289493]
	TIME [epoch: 5.69 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953127610424385		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.09953127610424385 | validation: 0.08061081608867297]
	TIME [epoch: 5.68 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996246893484184		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.09996246893484184 | validation: 0.08688754389581702]
	TIME [epoch: 5.68 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09439594841537417		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.09439594841537417 | validation: 0.0769359875249723]
	TIME [epoch: 5.68 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983026111844493		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.0983026111844493 | validation: 0.07661768616372826]
	TIME [epoch: 5.68 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09360388815483733		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.09360388815483733 | validation: 0.08615738947668669]
	TIME [epoch: 5.69 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270720176879949		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.09270720176879949 | validation: 0.07510785493827925]
	TIME [epoch: 5.68 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386521554423684		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.09386521554423684 | validation: 0.08346821176699673]
	TIME [epoch: 5.68 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10195902891206826		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.10195902891206826 | validation: 0.08003493257258576]
	TIME [epoch: 5.68 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11814326751837666		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.11814326751837666 | validation: 0.09835484664788528]
	TIME [epoch: 5.68 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601004035960784		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.11601004035960784 | validation: 0.08086559193932784]
	TIME [epoch: 5.69 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09815747642186662		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.09815747642186662 | validation: 0.08055534982395282]
	TIME [epoch: 5.68 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947506794306086		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.0947506794306086 | validation: 0.07552633404880511]
	TIME [epoch: 5.68 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141349367902223		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.09141349367902223 | validation: 0.07578456955934804]
	TIME [epoch: 5.68 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09086261791310435		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.09086261791310435 | validation: 0.07414096981571126]
	TIME [epoch: 5.67 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09154683307600504		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.09154683307600504 | validation: 0.073901333775586]
	TIME [epoch: 5.69 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126474244108955		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.09126474244108955 | validation: 0.07895864111760449]
	TIME [epoch: 5.67 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09294019452378485		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.09294019452378485 | validation: 0.08266867932518664]
	TIME [epoch: 5.68 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530922612899867		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.09530922612899867 | validation: 0.07380445287922696]
	TIME [epoch: 5.68 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09830022756629422		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.09830022756629422 | validation: 0.10632977957208062]
	TIME [epoch: 5.68 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12555685580757953		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.12555685580757953 | validation: 0.08374016331892073]
	TIME [epoch: 5.68 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12421479710937558		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.12421479710937558 | validation: 0.08472231123802398]
	TIME [epoch: 5.69 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09617481039304686		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.09617481039304686 | validation: 0.07937972140065308]
	TIME [epoch: 5.67 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09277639682894882		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.09277639682894882 | validation: 0.07174158104257393]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708996692580811		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.09708996692580811 | validation: 0.08655720386064808]
	TIME [epoch: 5.67 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09526795336880314		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.09526795336880314 | validation: 0.07352452208004949]
	TIME [epoch: 5.68 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654175804615565		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.09654175804615565 | validation: 0.08105930850083348]
	TIME [epoch: 5.69 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160087052023111		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.09160087052023111 | validation: 0.07643082435233356]
	TIME [epoch: 5.68 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09013491170070762		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.09013491170070762 | validation: 0.07678591650799989]
	TIME [epoch: 5.68 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09117811859410097		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.09117811859410097 | validation: 0.07571763471888387]
	TIME [epoch: 5.68 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016991873586036		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.09016991873586036 | validation: 0.06989132457580524]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1065.pth
	Model improved!!!
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513602084467782		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.09513602084467782 | validation: 0.0774387607418085]
	TIME [epoch: 5.69 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849271010618715		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.09849271010618715 | validation: 0.10075595630408932]
	TIME [epoch: 5.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11074757066547967		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.11074757066547967 | validation: 0.0725640929860293]
	TIME [epoch: 5.68 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11171453212630209		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.11171453212630209 | validation: 0.0794213215946163]
	TIME [epoch: 5.68 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09370949886113654		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.09370949886113654 | validation: 0.07542433377511427]
	TIME [epoch: 5.68 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09014515588047509		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.09014515588047509 | validation: 0.07591770700652685]
	TIME [epoch: 5.69 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817844476878431		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.08817844476878431 | validation: 0.08068736449957487]
	TIME [epoch: 5.69 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913837157739928		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.0913837157739928 | validation: 0.07373139017558442]
	TIME [epoch: 5.69 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08915136507423531		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.08915136507423531 | validation: 0.07951257301192159]
	TIME [epoch: 5.68 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09114959937373737		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.09114959937373737 | validation: 0.08104375529772287]
	TIME [epoch: 5.68 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975424381345253		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.08975424381345253 | validation: 0.07546474260531721]
	TIME [epoch: 5.69 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959368583165793		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.08959368583165793 | validation: 0.0710095401305557]
	TIME [epoch: 5.68 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813289218314228		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.08813289218314228 | validation: 0.08117058388540524]
	TIME [epoch: 5.67 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09166262102763095		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.09166262102763095 | validation: 0.07234365797788565]
	TIME [epoch: 5.68 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09963204772485665		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.09963204772485665 | validation: 0.1004622829231962]
	TIME [epoch: 5.68 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871257540638044		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.11871257540638044 | validation: 0.07160673986283977]
	TIME [epoch: 5.68 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633039452570532		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.10633039452570532 | validation: 0.07742151818677295]
	TIME [epoch: 5.69 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381993637905994		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.09381993637905994 | validation: 0.07491572678515111]
	TIME [epoch: 5.69 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08964566606090486		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.08964566606090486 | validation: 0.07462706463160589]
	TIME [epoch: 5.68 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09290094448134167		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.09290094448134167 | validation: 0.08307448637428227]
	TIME [epoch: 5.69 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09436515471241512		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.09436515471241512 | validation: 0.07642982434328266]
	TIME [epoch: 5.68 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09201367079432003		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.09201367079432003 | validation: 0.08006279309674302]
	TIME [epoch: 5.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663235230233571		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.08663235230233571 | validation: 0.07725584159614378]
	TIME [epoch: 5.68 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08557174057076938		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.08557174057076938 | validation: 0.07037979547469109]
	TIME [epoch: 5.68 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882481160594953		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0882481160594953 | validation: 0.07376807630515501]
	TIME [epoch: 5.68 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660820972507569		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.08660820972507569 | validation: 0.06997691497386163]
	TIME [epoch: 5.68 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959132855895244		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.08959132855895244 | validation: 0.08842138660677698]
	TIME [epoch: 5.69 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09416082125614182		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.09416082125614182 | validation: 0.08775279209928388]
	TIME [epoch: 5.68 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10591773640982907		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.10591773640982907 | validation: 0.09543933799071784]
	TIME [epoch: 5.68 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000814709023402		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.1000814709023402 | validation: 0.07293112230131636]
	TIME [epoch: 5.68 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09208363675614077		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.09208363675614077 | validation: 0.0723293266885473]
	TIME [epoch: 5.68 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666524154535254		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.08666524154535254 | validation: 0.0828087468551536]
	TIME [epoch: 5.69 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826045945905704		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.08826045945905704 | validation: 0.07474634676480368]
	TIME [epoch: 5.68 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263633337648056		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.09263633337648056 | validation: 0.07732931155940263]
	TIME [epoch: 5.68 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09384346838778701		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.09384346838778701 | validation: 0.08352136628820524]
	TIME [epoch: 5.68 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09702827164117502		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.09702827164117502 | validation: 0.0889107029039689]
	TIME [epoch: 5.68 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921292129226849		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.0921292129226849 | validation: 0.06984310469396843]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09198570957816983		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.09198570957816983 | validation: 0.07705121766975469]
	TIME [epoch: 5.68 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08770983369239431		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.08770983369239431 | validation: 0.07572218716933571]
	TIME [epoch: 5.68 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362409566159074		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.08362409566159074 | validation: 0.07675002823610705]
	TIME [epoch: 5.68 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714616142434632		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.08714616142434632 | validation: 0.08548441825562972]
	TIME [epoch: 5.67 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08334499493160959		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.08334499493160959 | validation: 0.07714630277011071]
	TIME [epoch: 5.69 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08703601438652989		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.08703601438652989 | validation: 0.07562323381353006]
	TIME [epoch: 5.68 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09373479259143296		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.09373479259143296 | validation: 0.08278307710708174]
	TIME [epoch: 5.68 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655165991670361		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.10655165991670361 | validation: 0.09664016867156257]
	TIME [epoch: 5.68 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086478710191297		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.1086478710191297 | validation: 0.07612475085134661]
	TIME [epoch: 5.68 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140867245115686		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.09140867245115686 | validation: 0.075086125065284]
	TIME [epoch: 5.68 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08479190525563932		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.08479190525563932 | validation: 0.0846309688325721]
	TIME [epoch: 5.69 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09047282556667531		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.09047282556667531 | validation: 0.06867238672186388]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09064185560568523		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.09064185560568523 | validation: 0.08040868455867747]
	TIME [epoch: 5.68 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471699802697255		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.08471699802697255 | validation: 0.07556231386520208]
	TIME [epoch: 5.68 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843886681061262		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.0843886681061262 | validation: 0.06701372526429601]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1117.pth
	Model improved!!!
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08596140994046707		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.08596140994046707 | validation: 0.08241562052718585]
	TIME [epoch: 5.69 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850514267315867		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.0850514267315867 | validation: 0.07145390324084468]
	TIME [epoch: 5.68 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852919177288055		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.0852919177288055 | validation: 0.07537606825331924]
	TIME [epoch: 5.68 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08690448647485809		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.08690448647485809 | validation: 0.08065637887991156]
	TIME [epoch: 5.67 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08717975101322586		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.08717975101322586 | validation: 0.07318082740421221]
	TIME [epoch: 5.68 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09210452857344693		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.09210452857344693 | validation: 0.09565876539035913]
	TIME [epoch: 5.69 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405538607846704		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.09405538607846704 | validation: 0.07476724035750026]
	TIME [epoch: 5.67 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09833595867635678		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.09833595867635678 | validation: 0.08549461163654658]
	TIME [epoch: 5.67 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09351399435348551		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.09351399435348551 | validation: 0.0769557688771376]
	TIME [epoch: 5.67 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895793791729734		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.0895793791729734 | validation: 0.07686437406871954]
	TIME [epoch: 5.67 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848238856236982		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.0848238856236982 | validation: 0.07884426673274893]
	TIME [epoch: 5.69 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08539366842590713		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.08539366842590713 | validation: 0.07081706560157809]
	TIME [epoch: 5.67 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08423197184443307		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.08423197184443307 | validation: 0.07702953184530305]
	TIME [epoch: 5.68 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08877645530737574		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.08877645530737574 | validation: 0.0740370396865031]
	TIME [epoch: 5.67 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893484245515943		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.0893484245515943 | validation: 0.0785644949108312]
	TIME [epoch: 5.67 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599221120734711		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.08599221120734711 | validation: 0.07029309026860246]
	TIME [epoch: 5.69 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08656603028930653		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.08656603028930653 | validation: 0.07149904996234027]
	TIME [epoch: 5.67 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08273658046397159		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.08273658046397159 | validation: 0.07215982518627916]
	TIME [epoch: 5.68 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08417471448101031		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.08417471448101031 | validation: 0.07565865202774463]
	TIME [epoch: 5.67 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545929928424371		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.08545929928424371 | validation: 0.07469936196897044]
	TIME [epoch: 5.68 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08409241517066907		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.08409241517066907 | validation: 0.07353161518820962]
	TIME [epoch: 5.68 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08585934235132996		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.08585934235132996 | validation: 0.07334352623246886]
	TIME [epoch: 5.69 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091628692139648		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.091628692139648 | validation: 0.0899666976742508]
	TIME [epoch: 5.68 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327245295179594		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.09327245295179594 | validation: 0.07092765407780725]
	TIME [epoch: 5.68 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09131272303462648		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.09131272303462648 | validation: 0.08054137615308143]
	TIME [epoch: 5.68 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694004215486104		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.08694004215486104 | validation: 0.07285326491342768]
	TIME [epoch: 5.68 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08254301597278478		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.08254301597278478 | validation: 0.07804551791397457]
	TIME [epoch: 5.69 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.082888693229966		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.082888693229966 | validation: 0.07369935719995514]
	TIME [epoch: 5.69 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285783862704689		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.08285783862704689 | validation: 0.07559517115465463]
	TIME [epoch: 5.68 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526939377289847		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.08526939377289847 | validation: 0.0803498411842209]
	TIME [epoch: 5.68 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811128064527196		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.0811128064527196 | validation: 0.06769234045535392]
	TIME [epoch: 5.68 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328375021368142		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.08328375021368142 | validation: 0.07144432630269008]
	TIME [epoch: 5.68 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08207801527303872		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.08207801527303872 | validation: 0.08807401805090104]
	TIME [epoch: 5.68 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893395263208858		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.0893395263208858 | validation: 0.07308985336870398]
	TIME [epoch: 5.67 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050414322018522		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.10050414322018522 | validation: 0.08537776583460122]
	TIME [epoch: 5.68 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09424998087901015		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.09424998087901015 | validation: 0.07642151222436937]
	TIME [epoch: 5.67 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811490008794874		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.0811490008794874 | validation: 0.07550897053364793]
	TIME [epoch: 5.69 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08366954185974372		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.08366954185974372 | validation: 0.08100553914151151]
	TIME [epoch: 5.68 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08279261703393408		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.08279261703393408 | validation: 0.07780766322356181]
	TIME [epoch: 5.68 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345903857434764		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.08345903857434764 | validation: 0.07876622950188962]
	TIME [epoch: 5.67 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08222045106201914		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.08222045106201914 | validation: 0.07773365300173435]
	TIME [epoch: 5.68 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807722361477217		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.0807722361477217 | validation: 0.0801078916542308]
	TIME [epoch: 5.68 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046851281151533		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.08046851281151533 | validation: 0.07847777385704297]
	TIME [epoch: 5.68 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08022202090180698		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.08022202090180698 | validation: 0.07060155867503091]
	TIME [epoch: 5.68 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08489528953403179		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.08489528953403179 | validation: 0.08291370342377624]
	TIME [epoch: 5.68 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256678272828989		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.08256678272828989 | validation: 0.07575058193266122]
	TIME [epoch: 5.68 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08823390215171623		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.08823390215171623 | validation: 0.0804991060080526]
	TIME [epoch: 5.67 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08941401375625112		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.08941401375625112 | validation: 0.07681926859367594]
	TIME [epoch: 5.69 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016044547959971		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.09016044547959971 | validation: 0.08214465529288238]
	TIME [epoch: 5.67 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08563017148255067		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.08563017148255067 | validation: 0.07676164258665719]
	TIME [epoch: 5.68 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08281785352542934		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.08281785352542934 | validation: 0.07417129626944]
	TIME [epoch: 5.67 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08279378957958895		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.08279378957958895 | validation: 0.07657981306606504]
	TIME [epoch: 5.68 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07851600726657194		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.07851600726657194 | validation: 0.0770314283763419]
	TIME [epoch: 5.68 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08000286862048003		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.08000286862048003 | validation: 0.08029385320144959]
	TIME [epoch: 5.68 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165819146782985		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.08165819146782985 | validation: 0.07016001694527452]
	TIME [epoch: 5.68 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08261199904721135		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.08261199904721135 | validation: 0.07979423262131273]
	TIME [epoch: 5.68 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885743340457917		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.0885743340457917 | validation: 0.07783208181713792]
	TIME [epoch: 5.68 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892813685546008		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.0892813685546008 | validation: 0.08218004847106376]
	TIME [epoch: 5.69 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08803182806152349		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.08803182806152349 | validation: 0.07111080427511894]
	TIME [epoch: 5.68 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08303523754333496		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.08303523754333496 | validation: 0.07110789248826033]
	TIME [epoch: 5.67 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818617285699988		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.0818617285699988 | validation: 0.07760271799675791]
	TIME [epoch: 5.68 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07898998459763441		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.07898998459763441 | validation: 0.08179520737366686]
	TIME [epoch: 5.68 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147428224383696		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.08147428224383696 | validation: 0.07904125735840438]
	TIME [epoch: 5.68 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002391688850762		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.08002391688850762 | validation: 0.06906646621527728]
	TIME [epoch: 5.69 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083355540500455		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.08083355540500455 | validation: 0.08811080153130949]
	TIME [epoch: 5.68 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08212553815186352		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.08212553815186352 | validation: 0.07471015155904621]
	TIME [epoch: 5.68 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0799405942705893		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.0799405942705893 | validation: 0.08180511158533031]
	TIME [epoch: 5.68 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08512423141545741		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.08512423141545741 | validation: 0.06993755317079811]
	TIME [epoch: 5.67 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08189080001482422		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.08189080001482422 | validation: 0.07926598928389153]
	TIME [epoch: 5.69 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961093940546832		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.07961093940546832 | validation: 0.07628187654923009]
	TIME [epoch: 5.67 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07784709034527644		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.07784709034527644 | validation: 0.0778728008502598]
	TIME [epoch: 5.68 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07709577695773895		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.07709577695773895 | validation: 0.07655967079264234]
	TIME [epoch: 5.67 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787128985440287		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.0787128985440287 | validation: 0.06687731984241939]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1190.pth
	Model improved!!!
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07892819910787671		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.07892819910787671 | validation: 0.07213924712037605]
	TIME [epoch: 5.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07907464029973545		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.07907464029973545 | validation: 0.07329316787906436]
	TIME [epoch: 5.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943798858038169		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.07943798858038169 | validation: 0.08708602728595849]
	TIME [epoch: 5.69 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0786332126777412		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.0786332126777412 | validation: 0.07621395872318469]
	TIME [epoch: 5.69 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08202679075282748		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.08202679075282748 | validation: 0.07904362092774556]
	TIME [epoch: 5.68 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323755218387191		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.08323755218387191 | validation: 0.08199390045247265]
	TIME [epoch: 5.69 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09825625850095619		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.09825625850095619 | validation: 0.09145842833671086]
	TIME [epoch: 5.68 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09495672321658444		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.09495672321658444 | validation: 0.07286926632964676]
	TIME [epoch: 5.69 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07903052085954552		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.07903052085954552 | validation: 0.08376218867919075]
	TIME [epoch: 5.69 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07824239031901238		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.07824239031901238 | validation: 0.07439639057050745]
	TIME [epoch: 5.65 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0790289259288917		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.0790289259288917 | validation: 0.07442250935077864]
	TIME [epoch: 5.69 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07952440054058178		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.07952440054058178 | validation: 0.07098645439044873]
	TIME [epoch: 5.68 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821490630334077		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.07821490630334077 | validation: 0.07181191662736033]
	TIME [epoch: 5.69 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08195741962668311		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.08195741962668311 | validation: 0.07288111267942068]
	TIME [epoch: 5.68 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976637733463943		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.07976637733463943 | validation: 0.07270364554676594]
	TIME [epoch: 5.68 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07626245606518985		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.07626245606518985 | validation: 0.07032454288411367]
	TIME [epoch: 5.68 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904005947554564		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.07904005947554564 | validation: 0.07973306086391169]
	TIME [epoch: 5.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07968340160469882		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.07968340160469882 | validation: 0.08035941653801842]
	TIME [epoch: 5.69 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07668506055901234		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.07668506055901234 | validation: 0.07912845067570723]
	TIME [epoch: 5.69 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07865467529588675		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.07865467529588675 | validation: 0.07519681929904401]
	TIME [epoch: 5.69 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08187719314865956		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.08187719314865956 | validation: 0.07242859326633153]
	TIME [epoch: 5.68 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727097933596504		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.07727097933596504 | validation: 0.06945189368763753]
	TIME [epoch: 5.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620264748106218		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.08620264748106218 | validation: 0.0880319539331022]
	TIME [epoch: 5.69 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09318518023564824		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.09318518023564824 | validation: 0.0725998679267485]
	TIME [epoch: 5.68 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08108885440114846		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.08108885440114846 | validation: 0.06726936633770345]
	TIME [epoch: 5.68 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07633438736319545		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.07633438736319545 | validation: 0.0796127175252163]
	TIME [epoch: 5.69 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922288473711118		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.07922288473711118 | validation: 0.07316701942922824]
	TIME [epoch: 5.69 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961936840431336		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.07961936840431336 | validation: 0.07939236386633607]
	TIME [epoch: 5.69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07926475310169831		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.07926475310169831 | validation: 0.0725585361128343]
	TIME [epoch: 5.69 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720117570889161		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.07720117570889161 | validation: 0.07640576683904071]
	TIME [epoch: 5.68 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439463789715653		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.07439463789715653 | validation: 0.07649866164859032]
	TIME [epoch: 5.68 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718200384271719		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.07718200384271719 | validation: 0.07580500928278083]
	TIME [epoch: 5.7 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07783030289630345		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.07783030289630345 | validation: 0.07204052667138576]
	TIME [epoch: 5.69 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07843395314681036		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.07843395314681036 | validation: 0.07949884173434875]
	TIME [epoch: 5.68 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07590406879694954		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.07590406879694954 | validation: 0.07773842746679956]
	TIME [epoch: 5.69 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810789187558002		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.07810789187558002 | validation: 0.07174492482646874]
	TIME [epoch: 5.69 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07675855593880906		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.07675855593880906 | validation: 0.07208112612649503]
	TIME [epoch: 5.69 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787254856412571		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0787254856412571 | validation: 0.07643672129107593]
	TIME [epoch: 5.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07583665718438547		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.07583665718438547 | validation: 0.06838754490796582]
	TIME [epoch: 5.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07636998702350584		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.07636998702350584 | validation: 0.07893936485891077]
	TIME [epoch: 5.69 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779566504251922		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.07779566504251922 | validation: 0.06431789347982608]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1231.pth
	Model improved!!!
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063440036060765		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.08063440036060765 | validation: 0.09054883380272276]
	TIME [epoch: 5.68 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09757207785267706		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.09757207785267706 | validation: 0.06500013007850305]
	TIME [epoch: 5.69 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359167841557437		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.08359167841557437 | validation: 0.0720862781307826]
	TIME [epoch: 5.67 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678513873019176		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.07678513873019176 | validation: 0.08225347528218144]
	TIME [epoch: 5.68 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07781743505174889		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.07781743505174889 | validation: 0.0707288223085678]
	TIME [epoch: 5.67 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08302980957444638		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.08302980957444638 | validation: 0.07126894445547487]
	TIME [epoch: 5.68 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07469955651192567		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.07469955651192567 | validation: 0.07177722972333492]
	TIME [epoch: 5.69 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07514840322237257		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.07514840322237257 | validation: 0.07347583440339871]
	TIME [epoch: 5.68 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661132920046529		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.07661132920046529 | validation: 0.07251300233464929]
	TIME [epoch: 5.68 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07588743799574048		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.07588743799574048 | validation: 0.07660191633989266]
	TIME [epoch: 5.68 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555909356679026		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.07555909356679026 | validation: 0.0681151975201844]
	TIME [epoch: 5.67 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07635504620273768		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.07635504620273768 | validation: 0.07324450277172982]
	TIME [epoch: 5.69 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07401115554562772		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.07401115554562772 | validation: 0.06910464092490486]
	TIME [epoch: 5.68 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07701697139794508		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.07701697139794508 | validation: 0.07984389392911648]
	TIME [epoch: 5.68 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272922827568992		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.07272922827568992 | validation: 0.0720834342025388]
	TIME [epoch: 5.68 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388971049397905		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.07388971049397905 | validation: 0.06830700709586214]
	TIME [epoch: 5.68 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337682294144178		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.07337682294144178 | validation: 0.07339129665558146]
	TIME [epoch: 5.68 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0755073122164958		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.0755073122164958 | validation: 0.0891220794382385]
	TIME [epoch: 5.69 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661880573521489		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.07661880573521489 | validation: 0.07285221192159884]
	TIME [epoch: 5.68 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832511102657669		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.0832511102657669 | validation: 0.08167532200859996]
	TIME [epoch: 5.68 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057738896355236		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.08057738896355236 | validation: 0.07800075403343165]
	TIME [epoch: 5.67 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776483862649651		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.0776483862649651 | validation: 0.07782313809406277]
	TIME [epoch: 5.67 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07885089145709202		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.07885089145709202 | validation: 0.07623066594929051]
	TIME [epoch: 5.69 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447584370784767		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.07447584370784767 | validation: 0.0727968714415784]
	TIME [epoch: 5.67 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480613070613192		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.07480613070613192 | validation: 0.07963892753443094]
	TIME [epoch: 5.68 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07514917196957038		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.07514917196957038 | validation: 0.07468759111630993]
	TIME [epoch: 5.68 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0805130789296825		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.0805130789296825 | validation: 0.08139630252359223]
	TIME [epoch: 5.68 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07676309015410712		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.07676309015410712 | validation: 0.0711526275588203]
	TIME [epoch: 5.68 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07643190798283803		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.07643190798283803 | validation: 0.08691827152826381]
	TIME [epoch: 5.68 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07690774162088862		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.07690774162088862 | validation: 0.07621631815930295]
	TIME [epoch: 5.68 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08020909283802037		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.08020909283802037 | validation: 0.0878683477101289]
	TIME [epoch: 5.67 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07860577586591065		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.07860577586591065 | validation: 0.07136454270668972]
	TIME [epoch: 5.68 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794508065011854		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.0794508065011854 | validation: 0.06879263888909302]
	TIME [epoch: 5.69 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07978653140808646		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.07978653140808646 | validation: 0.07741493658235907]
	TIME [epoch: 5.68 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771589442034534		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.0771589442034534 | validation: 0.07414394794555824]
	TIME [epoch: 5.68 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07611820478888333		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.07611820478888333 | validation: 0.06765271707560207]
	TIME [epoch: 5.68 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282779997837706		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.07282779997837706 | validation: 0.07593456514478779]
	TIME [epoch: 5.68 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07589409669948351		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.07589409669948351 | validation: 0.07536363200592244]
	TIME [epoch: 5.67 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07391126454951735		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.07391126454951735 | validation: 0.07884130875609345]
	TIME [epoch: 5.69 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07960113823778842		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.07960113823778842 | validation: 0.08432932168257659]
	TIME [epoch: 5.68 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07750542332823933		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.07750542332823933 | validation: 0.06858867611786848]
	TIME [epoch: 5.68 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241883284004248		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.07241883284004248 | validation: 0.07524846042336003]
	TIME [epoch: 5.68 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07608684044307673		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.07608684044307673 | validation: 0.0734218587626269]
	TIME [epoch: 5.67 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07281148406664421		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.07281148406664421 | validation: 0.07211365194391323]
	TIME [epoch: 5.69 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07614912040961297		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.07614912040961297 | validation: 0.07555305857305353]
	TIME [epoch: 5.67 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07438560054016523		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.07438560054016523 | validation: 0.07555442478250261]
	TIME [epoch: 5.67 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513480001596819		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.07513480001596819 | validation: 0.0704488649110017]
	TIME [epoch: 5.67 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272288721658661		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.07272288721658661 | validation: 0.07262001291559243]
	TIME [epoch: 5.68 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369398060097938		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.07369398060097938 | validation: 0.0838585522929596]
	TIME [epoch: 5.68 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07563434083279078		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.07563434083279078 | validation: 0.07632094152578434]
	TIME [epoch: 5.68 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08419602543220034		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.08419602543220034 | validation: 0.08124218468952873]
	TIME [epoch: 5.68 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835409027172602		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.07835409027172602 | validation: 0.0719462376839388]
	TIME [epoch: 5.67 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07633066627433366		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.07633066627433366 | validation: 0.07560536070300669]
	TIME [epoch: 5.68 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07401755434348042		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.07401755434348042 | validation: 0.08221063440642182]
	TIME [epoch: 5.69 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794107370192049		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.0794107370192049 | validation: 0.0736713605033696]
	TIME [epoch: 5.67 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07685832922828563		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.07685832922828563 | validation: 0.08168946808081223]
	TIME [epoch: 5.67 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076397135210157		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.076397135210157 | validation: 0.07128733889060737]
	TIME [epoch: 5.67 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723671336723034		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.0723671336723034 | validation: 0.06575850228204069]
	TIME [epoch: 5.67 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07285354009995558		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.07285354009995558 | validation: 0.07565232390288179]
	TIME [epoch: 5.68 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07414189608435579		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.07414189608435579 | validation: 0.07316205546288312]
	TIME [epoch: 5.69 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07726464181703738		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.07726464181703738 | validation: 0.07725917213358928]
	TIME [epoch: 5.68 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07686095744913725		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.07686095744913725 | validation: 0.07190892928577296]
	TIME [epoch: 5.68 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728051715943757		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.0728051715943757 | validation: 0.0700120960315201]
	TIME [epoch: 5.67 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07284824831123611		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.07284824831123611 | validation: 0.07860944195217578]
	TIME [epoch: 5.67 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07442751976832679		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.07442751976832679 | validation: 0.06795409557671078]
	TIME [epoch: 5.68 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640399519676451		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.07640399519676451 | validation: 0.07794422658438668]
	TIME [epoch: 5.67 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07423987361737912		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.07423987361737912 | validation: 0.0733905755369596]
	TIME [epoch: 5.67 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07798107461973716		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.07798107461973716 | validation: 0.07290523413182418]
	TIME [epoch: 5.67 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601397049384803		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.07601397049384803 | validation: 0.07044697629392942]
	TIME [epoch: 5.68 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705944246287852		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.07705944246287852 | validation: 0.07402634187758882]
	TIME [epoch: 5.68 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0775155398731406		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.0775155398731406 | validation: 0.08327234621746482]
	TIME [epoch: 5.67 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07408924125168023		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.07408924125168023 | validation: 0.07606757079337918]
	TIME [epoch: 5.68 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07222599681215046		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.07222599681215046 | validation: 0.07882491055871015]
	TIME [epoch: 5.67 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531003591846844		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.07531003591846844 | validation: 0.0656066118567791]
	TIME [epoch: 5.67 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657701787335204		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.07657701787335204 | validation: 0.0698516536044055]
	TIME [epoch: 5.69 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07481637106756535		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.07481637106756535 | validation: 0.07330896326805884]
	TIME [epoch: 5.68 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07688014157530781		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.07688014157530781 | validation: 0.0762685645281729]
	TIME [epoch: 5.68 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497708462794425		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.07497708462794425 | validation: 0.06917586109510271]
	TIME [epoch: 5.68 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340887588945198		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.07340887588945198 | validation: 0.0790721322391331]
	TIME [epoch: 5.67 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534795947144773		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.07534795947144773 | validation: 0.07605176408139852]
	TIME [epoch: 5.69 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07356445981882309		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.07356445981882309 | validation: 0.08294957080414822]
	TIME [epoch: 5.68 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07284956333186374		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.07284956333186374 | validation: 0.0785175770446236]
	TIME [epoch: 5.67 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369502297689427		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.07369502297689427 | validation: 0.07753930207370287]
	TIME [epoch: 5.67 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387938443285465		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.07387938443285465 | validation: 0.08424791755453012]
	TIME [epoch: 5.67 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725639254954152		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.07725639254954152 | validation: 0.07943705584533411]
	TIME [epoch: 5.67 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718246152039092		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.07718246152039092 | validation: 0.0799690451834909]
	TIME [epoch: 5.68 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07529799409873522		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.07529799409873522 | validation: 0.07870799774800663]
	TIME [epoch: 5.67 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07385142198645664		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.07385142198645664 | validation: 0.07017026656285306]
	TIME [epoch: 5.68 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513477310773707		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.07513477310773707 | validation: 0.08206950219012522]
	TIME [epoch: 5.67 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07387923666792709		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.07387923666792709 | validation: 0.07140008031641536]
	TIME [epoch: 5.67 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259452096846687		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.07259452096846687 | validation: 0.07837244684479648]
	TIME [epoch: 5.68 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07012874729985275		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.07012874729985275 | validation: 0.07161217747175573]
	TIME [epoch: 5.68 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07378426996208144		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.07378426996208144 | validation: 0.08183008523831285]
	TIME [epoch: 5.67 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448462665678386		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.07448462665678386 | validation: 0.07903605265258917]
	TIME [epoch: 5.68 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349519254079284		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.07349519254079284 | validation: 0.07989155890229488]
	TIME [epoch: 5.67 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543744887958552		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.07543744887958552 | validation: 0.07467356160137921]
	TIME [epoch: 5.69 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419105204164335		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.07419105204164335 | validation: 0.07842096978419294]
	TIME [epoch: 5.68 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07389082290794853		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.07389082290794853 | validation: 0.07000715733897556]
	TIME [epoch: 5.68 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07284422863696068		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.07284422863696068 | validation: 0.0752801948914873]
	TIME [epoch: 5.67 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263910710314439		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.07263910710314439 | validation: 0.07590565569529119]
	TIME [epoch: 5.67 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07301380577535124		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.07301380577535124 | validation: 0.06942489056138888]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_3_v_mmd1_20250430_112623/states/model_phi1_4a_distortion_v1_3_v_mmd1_1332.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4554.873 seconds.
