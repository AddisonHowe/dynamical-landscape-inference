Args:
Namespace(name='model_phi1_4a_distortion_v2_7_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_7/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_7/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.024629543, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1897853692

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.428792086420044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.428792086420044 | validation: 3.4808272900467103]
	TIME [epoch: 161 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6873203919344237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6873203919344237 | validation: 6.26237750472864]
	TIME [epoch: 0.771 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.264314024512021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.264314024512021 | validation: 4.918764374876258]
	TIME [epoch: 0.695 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.465039056257337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.465039056257337 | validation: 3.6433627114213993]
	TIME [epoch: 0.692 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7162804911790466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7162804911790466 | validation: 3.5609222614009797]
	TIME [epoch: 0.69 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.773484418411846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.773484418411846 | validation: 3.349577616756934]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.508201681546112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.508201681546112 | validation: 3.2061942091468767]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.157955880098253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157955880098253 | validation: 2.8552366881916744]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.904335776229091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.904335776229091 | validation: 2.223922003549094]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8628396885075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8628396885075 | validation: 2.357191443867008]
	TIME [epoch: 0.692 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6944091586704078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6944091586704078 | validation: 2.403519441863665]
	TIME [epoch: 0.692 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.668487856018889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.668487856018889 | validation: 2.1337890492465275]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.639272297464009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.639272297464009 | validation: 2.0949239107305737]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.627068373644764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.627068373644764 | validation: 2.1955917838234984]
	TIME [epoch: 0.693 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.627642797822359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.627642797822359 | validation: 2.23028601199105]
	TIME [epoch: 0.689 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.588851208667528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.588851208667528 | validation: 2.095391760534549]
	TIME [epoch: 0.694 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.582205520901375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.582205520901375 | validation: 2.304101018875432]
	TIME [epoch: 0.691 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5463283877271343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5463283877271343 | validation: 2.105222494740212]
	TIME [epoch: 0.692 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.535775112245372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.535775112245372 | validation: 2.2537493328929368]
	TIME [epoch: 0.693 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4884696517253646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4884696517253646 | validation: 2.088859092525293]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.470937342806036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.470937342806036 | validation: 2.438774915591373]
	TIME [epoch: 0.694 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.441062326834662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.441062326834662 | validation: 1.8855527846106654]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.559926538367108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.559926538367108 | validation: 2.6873447051914754]
	TIME [epoch: 0.694 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.541302777911286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.541302777911286 | validation: 2.184471424294185]
	TIME [epoch: 0.696 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.324586812090019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.324586812090019 | validation: 1.8853265559629593]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.382183790115215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.382183790115215 | validation: 2.500403041431904]
	TIME [epoch: 0.69 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.380850078026145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.380850078026145 | validation: 2.1365905826090628]
	TIME [epoch: 0.694 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2222505433039563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2222505433039563 | validation: 1.9077383089483115]
	TIME [epoch: 0.694 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2451211981253896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2451211981253896 | validation: 2.440236154721591]
	TIME [epoch: 0.695 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.299971527240708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.299971527240708 | validation: 1.9317604499578416]
	TIME [epoch: 0.695 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.229703703857881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.229703703857881 | validation: 3.244831142146386]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5912499154665625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5912499154665625 | validation: 1.7610572041632708]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.24988433655971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.24988433655971 | validation: 2.119361462648882]
	TIME [epoch: 0.689 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1415764112012967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1415764112012967 | validation: 2.2103363351405565]
	TIME [epoch: 0.691 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0783526300477186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0783526300477186 | validation: 1.735315735140112]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.986096154499219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.986096154499219 | validation: 2.152356934750243]
	TIME [epoch: 0.691 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0111920110975086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0111920110975086 | validation: 1.7392336564549495]
	TIME [epoch: 0.69 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3179894084680317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3179894084680317 | validation: 2.786083294104975]
	TIME [epoch: 0.688 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.358014832919507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.358014832919507 | validation: 2.152155931331653]
	TIME [epoch: 0.688 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.035886198157547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.035886198157547 | validation: 1.831035755214626]
	TIME [epoch: 0.691 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0457186496695683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0457186496695683 | validation: 1.8866741381035308]
	TIME [epoch: 0.691 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9139324999594323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9139324999594323 | validation: 2.0282848222444616]
	TIME [epoch: 0.693 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9040352839535069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9040352839535069 | validation: 1.7243916319221988]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.923390654337942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.923390654337942 | validation: 2.1884094066447943]
	TIME [epoch: 0.692 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.943435299613864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.943435299613864 | validation: 1.730693183537466]
	TIME [epoch: 0.691 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9408050049992278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9408050049992278 | validation: 2.0764994191897457]
	TIME [epoch: 0.691 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.87159014693025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.87159014693025 | validation: 1.8037964041914938]
	TIME [epoch: 0.691 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8266946919137559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8266946919137559 | validation: 1.8905016848876646]
	TIME [epoch: 0.695 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8139946328998722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8139946328998722 | validation: 1.7142656549136703]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.836984519054334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.836984519054334 | validation: 2.133637861460405]
	TIME [epoch: 0.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9441870258201177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9441870258201177 | validation: 1.6763957833945562]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0655367788443164		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.0655367788443164 | validation: 2.365549710730279]
	TIME [epoch: 0.697 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9860967872375233		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.9860967872375233 | validation: 1.9341170343304463]
	TIME [epoch: 0.695 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8424279674166724		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.8424279674166724 | validation: 1.737733679048211]
	TIME [epoch: 0.693 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.896033781965342		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.896033781965342 | validation: 2.3929422329768024]
	TIME [epoch: 0.689 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0538597627960336		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.0538597627960336 | validation: 1.7057608965815092]
	TIME [epoch: 0.687 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9500394214291037		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.9500394214291037 | validation: 1.871341549840825]
	TIME [epoch: 0.687 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8465207056783937		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.8465207056783937 | validation: 1.9935074399629635]
	TIME [epoch: 0.688 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.841808768862776		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.841808768862776 | validation: 1.6839776756198752]
	TIME [epoch: 0.693 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8412383475523126		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.8412383475523126 | validation: 1.9214321226399713]
	TIME [epoch: 0.692 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8044374039416249		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.8044374039416249 | validation: 1.738782205720031]
	TIME [epoch: 0.692 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7739669878690394		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.7739669878690394 | validation: 1.767195583029421]
	TIME [epoch: 0.69 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.759531603786217		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.759531603786217 | validation: 1.7733433443328837]
	TIME [epoch: 0.688 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7504929159591462		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.7504929159591462 | validation: 1.7513292778932823]
	TIME [epoch: 0.687 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7426308580937155		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.7426308580937155 | validation: 1.8042900416201804]
	TIME [epoch: 0.687 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7425878220708395		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.7425878220708395 | validation: 1.7015979129897751]
	TIME [epoch: 0.693 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7980648325374993		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.7980648325374993 | validation: 2.336731540342789]
	TIME [epoch: 0.691 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9833831879899522		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.9833831879899522 | validation: 1.7185367222355659]
	TIME [epoch: 0.691 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0590708359909553		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.0590708359909553 | validation: 1.9538102159686943]
	TIME [epoch: 0.689 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8518110115457092		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.8518110115457092 | validation: 1.9203141360817222]
	TIME [epoch: 0.689 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.816578777469633		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.816578777469633 | validation: 1.6831275513083441]
	TIME [epoch: 0.688 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0734088848414487		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.0734088848414487 | validation: 2.1434216198848945]
	TIME [epoch: 0.687 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9120735578420658		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.9120735578420658 | validation: 2.042585847408057]
	TIME [epoch: 0.69 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8226345065844853		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.8226345065844853 | validation: 1.6914893814483598]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7770888121777677		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.7770888121777677 | validation: 1.8414376182260384]
	TIME [epoch: 0.694 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7924329091966724		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7924329091966724 | validation: 1.7005707779932067]
	TIME [epoch: 0.691 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7560610535636643		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.7560610535636643 | validation: 1.835022644540561]
	TIME [epoch: 0.689 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7320584258129537		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7320584258129537 | validation: 1.751436371797526]
	TIME [epoch: 0.687 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7248090417932043		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.7248090417932043 | validation: 1.6810602056211543]
	TIME [epoch: 0.687 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.716261331980428		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.716261331980428 | validation: 1.7920714612110704]
	TIME [epoch: 0.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.719259147677567		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.719259147677567 | validation: 1.6242802200092274]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7357199566479558		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7357199566479558 | validation: 1.8989277058834535]
	TIME [epoch: 0.692 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.790356577559539		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.790356577559539 | validation: 1.517888439602453]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8674489346717058		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.8674489346717058 | validation: 1.8359111705401236]
	TIME [epoch: 0.691 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.755613819229626		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.755613819229626 | validation: 1.8366743155887528]
	TIME [epoch: 0.69 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.753157519292466		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.753157519292466 | validation: 1.8255632598173819]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.026057525078156		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.026057525078156 | validation: 2.079554239175792]
	TIME [epoch: 0.691 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8344374873294091		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8344374873294091 | validation: 1.955607820863726]
	TIME [epoch: 0.691 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7671845338236176		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.7671845338236176 | validation: 1.6387406809121339]
	TIME [epoch: 0.692 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7712469087298865		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.7712469087298865 | validation: 1.814305090762025]
	TIME [epoch: 0.687 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6979560514989078		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.6979560514989078 | validation: 1.7253328970599655]
	TIME [epoch: 0.686 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7115549002533477		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.7115549002533477 | validation: 1.8406623644440068]
	TIME [epoch: 0.686 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7460587063729542		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.7460587063729542 | validation: 1.622318131208563]
	TIME [epoch: 0.692 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7153594230195597		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.7153594230195597 | validation: 1.8870883458905234]
	TIME [epoch: 0.691 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.700811153822354		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.700811153822354 | validation: 1.5898410638447278]
	TIME [epoch: 0.691 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6787388990985195		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6787388990985195 | validation: 1.7370400889030497]
	TIME [epoch: 0.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6675715812715237		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6675715812715237 | validation: 1.5534601201190688]
	TIME [epoch: 0.689 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7154848713924002		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.7154848713924002 | validation: 1.7673105074650661]
	TIME [epoch: 0.687 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6820605355928717		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.6820605355928717 | validation: 1.5233208286956945]
	TIME [epoch: 0.686 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7064615986163478		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.7064615986163478 | validation: 1.7794551767024729]
	TIME [epoch: 0.687 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6707013599639782		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6707013599639782 | validation: 1.5628164294663733]
	TIME [epoch: 0.693 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6385616762817337		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6385616762817337 | validation: 1.7739077693593075]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7861001061893467		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.7861001061893467 | validation: 2.4632647297703487]
	TIME [epoch: 0.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.262694647910941		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.262694647910941 | validation: 1.6372282582092261]
	TIME [epoch: 0.688 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6615279881172997		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.6615279881172997 | validation: 1.7013102225497656]
	TIME [epoch: 0.687 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8928161654711009		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.8928161654711009 | validation: 1.6997558752350654]
	TIME [epoch: 0.686 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.652284695512372		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.652284695512372 | validation: 1.6442464326315012]
	TIME [epoch: 0.688 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.675985568939605		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.675985568939605 | validation: 1.6081985218253463]
	TIME [epoch: 0.691 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.612001478878874		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.612001478878874 | validation: 1.5612354343886885]
	TIME [epoch: 0.691 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.609084124995631		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.609084124995631 | validation: 1.5706825898776864]
	TIME [epoch: 0.691 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5898988789413866		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.5898988789413866 | validation: 1.5657587170702518]
	TIME [epoch: 0.69 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5778064954790103		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5778064954790103 | validation: 1.5678383146718844]
	TIME [epoch: 0.689 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.56393708070248		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.56393708070248 | validation: 1.5015420858530923]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.571769302011665		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.571769302011665 | validation: 1.6626420007406495]
	TIME [epoch: 0.691 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6587305184713677		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.6587305184713677 | validation: 1.5334586652011215]
	TIME [epoch: 0.692 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.858097115997893		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.858097115997893 | validation: 1.767038542774055]
	TIME [epoch: 0.692 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6898128915620685		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.6898128915620685 | validation: 1.6932765436687813]
	TIME [epoch: 0.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7335539348228899		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.7335539348228899 | validation: 1.4721521846296128]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.700910208806999		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.700910208806999 | validation: 1.7849658489856333]
	TIME [epoch: 0.694 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6795469619135817		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.6795469619135817 | validation: 1.6276189610492684]
	TIME [epoch: 0.692 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.588485836165205		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.588485836165205 | validation: 1.455217951061405]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5648795124855455		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5648795124855455 | validation: 1.4976194993198537]
	TIME [epoch: 0.693 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5221827802555339		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5221827802555339 | validation: 1.5508352484903332]
	TIME [epoch: 0.691 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.518098448133558		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.518098448133558 | validation: 1.420585062537986]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5085110115500897		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.5085110115500897 | validation: 1.5427518319665514]
	TIME [epoch: 0.695 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5242678622256158		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.5242678622256158 | validation: 1.5861390104596227]
	TIME [epoch: 0.693 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.770836532774199		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.770836532774199 | validation: 1.94967550111666]
	TIME [epoch: 0.697 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8841605201066842		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.8841605201066842 | validation: 1.4612885418696635]
	TIME [epoch: 0.693 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4962007006300277		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.4962007006300277 | validation: 1.5804779321558475]
	TIME [epoch: 0.692 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.715525440137524		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.715525440137524 | validation: 1.519202695079052]
	TIME [epoch: 0.688 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5357879131236885		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.5357879131236885 | validation: 1.475401998198576]
	TIME [epoch: 0.689 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5043115637316242		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.5043115637316242 | validation: 1.3819023521723865]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5031785835355413		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.5031785835355413 | validation: 1.446898473857006]
	TIME [epoch: 0.692 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4802530068853994		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.4802530068853994 | validation: 1.4198051807601875]
	TIME [epoch: 0.69 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4699348149866285		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.4699348149866285 | validation: 1.372035260496086]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.466823412570002		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.466823412570002 | validation: 1.4317841177342256]
	TIME [epoch: 0.691 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4811779885281267		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.4811779885281267 | validation: 1.4363928702659419]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4993775533741502		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.4993775533741502 | validation: 1.4424733418561873]
	TIME [epoch: 0.69 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5260804184625436		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.5260804184625436 | validation: 1.453086020505589]
	TIME [epoch: 0.692 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4840768293761237		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.4840768293761237 | validation: 1.2968820882593612]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4979507926260158		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.4979507926260158 | validation: 1.5999235748176952]
	TIME [epoch: 0.691 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5651199519120866		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5651199519120866 | validation: 1.2794860426257386]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.475211769745837		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.475211769745837 | validation: 1.395224085642643]
	TIME [epoch: 0.691 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.426940809820544		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.426940809820544 | validation: 1.3170784504121964]
	TIME [epoch: 0.693 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4004948660139371		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.4004948660139371 | validation: 1.3510944468730561]
	TIME [epoch: 0.692 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4143825124840215		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.4143825124840215 | validation: 1.38939615793669]
	TIME [epoch: 0.688 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.447452710097899		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.447452710097899 | validation: 1.5185018276982039]
	TIME [epoch: 0.687 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.594768434788926		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.594768434788926 | validation: 2.0118176840708544]
	TIME [epoch: 0.686 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8564626585500537		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.8564626585500537 | validation: 1.2867587173191315]
	TIME [epoch: 0.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4373627559275872		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.4373627559275872 | validation: 1.4550823893767242]
	TIME [epoch: 0.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.486887076331538		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.486887076331538 | validation: 1.4488381985131167]
	TIME [epoch: 0.691 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4814513639812106		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.4814513639812106 | validation: 1.3170277945730444]
	TIME [epoch: 0.69 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409036633102591		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.409036633102591 | validation: 1.4342538887086744]
	TIME [epoch: 0.688 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4135562444816703		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.4135562444816703 | validation: 1.3774000493510856]
	TIME [epoch: 0.686 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4277121566637443		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.4277121566637443 | validation: 1.3541601273311867]
	TIME [epoch: 0.687 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3763095647598007		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3763095647598007 | validation: 1.3322331193799273]
	TIME [epoch: 0.69 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3685391287657727		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.3685391287657727 | validation: 1.3433037986869447]
	TIME [epoch: 0.691 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.366959332121378		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.366959332121378 | validation: 1.312014139860849]
	TIME [epoch: 0.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3952897872873318		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.3952897872873318 | validation: 1.699406628819665]
	TIME [epoch: 0.688 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.635454757452919		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.635454757452919 | validation: 1.3452462007297097]
	TIME [epoch: 0.687 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5468246684667963		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5468246684667963 | validation: 1.4138172263232458]
	TIME [epoch: 0.687 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4348112588331379		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.4348112588331379 | validation: 1.2547152160860484]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3557973623529274		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3557973623529274 | validation: 1.3027030676945146]
	TIME [epoch: 0.692 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.375442081938397		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.375442081938397 | validation: 1.3661964520484808]
	TIME [epoch: 0.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4041411339501741		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.4041411339501741 | validation: 1.296458266615912]
	TIME [epoch: 0.687 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.372272713682262		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.372272713682262 | validation: 1.3000126087651844]
	TIME [epoch: 0.688 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3688486127314912		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.3688486127314912 | validation: 1.3095042972781972]
	TIME [epoch: 0.688 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36668963738366		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.36668963738366 | validation: 1.3014179422634276]
	TIME [epoch: 0.691 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3759900892826156		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.3759900892826156 | validation: 1.3087003737194038]
	TIME [epoch: 0.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388508161387561		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.388508161387561 | validation: 1.3001947622916064]
	TIME [epoch: 0.689 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3606857156435614		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3606857156435614 | validation: 1.215183876596827]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356660486017393		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.356660486017393 | validation: 1.3441491545913238]
	TIME [epoch: 0.691 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.36376145562557		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.36376145562557 | validation: 1.2285911749647154]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3874782777958854		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.3874782777958854 | validation: 1.3167271020042317]
	TIME [epoch: 0.692 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3312827978414197		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3312827978414197 | validation: 1.409590333553486]
	TIME [epoch: 0.691 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4032883458578256		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4032883458578256 | validation: 1.51278790483624]
	TIME [epoch: 0.689 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7312105930089192		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.7312105930089192 | validation: 1.4946739175684447]
	TIME [epoch: 0.686 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4678403436161802		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.4678403436161802 | validation: 1.278324528662761]
	TIME [epoch: 0.686 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3060103353016193		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.3060103353016193 | validation: 1.2294085251726454]
	TIME [epoch: 0.689 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3822751764405377		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3822751764405377 | validation: 1.3432034188243793]
	TIME [epoch: 0.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3533498899961292		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3533498899961292 | validation: 1.2712166050506255]
	TIME [epoch: 0.692 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3031465480266495		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3031465480266495 | validation: 1.1890321103863617]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3051690041912485		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3051690041912485 | validation: 1.259790787456209]
	TIME [epoch: 0.692 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3119539894450793		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3119539894450793 | validation: 1.1901783589641464]
	TIME [epoch: 0.692 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.299849391965596		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.299849391965596 | validation: 1.2383449261301953]
	TIME [epoch: 0.695 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.301578811189477		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.301578811189477 | validation: 1.2322214468006427]
	TIME [epoch: 0.691 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3305568858649297		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.3305568858649297 | validation: 1.2678522816645152]
	TIME [epoch: 0.693 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3350229618688105		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3350229618688105 | validation: 1.2177174082007403]
	TIME [epoch: 0.692 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3335041981646896		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.3335041981646896 | validation: 1.3353152670187063]
	TIME [epoch: 0.693 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3491110092612109		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3491110092612109 | validation: 1.2807423397174897]
	TIME [epoch: 0.691 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3942469739374213		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.3942469739374213 | validation: 1.4262512335231063]
	TIME [epoch: 0.692 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4351227195595462		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.4351227195595462 | validation: 1.2138540960618858]
	TIME [epoch: 0.691 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2740739758691837		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.2740739758691837 | validation: 1.1317743379449972]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2853461858331106		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.2853461858331106 | validation: 1.2738095227869424]
	TIME [epoch: 0.691 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2927147027870622		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.2927147027870622 | validation: 1.1477696136041353]
	TIME [epoch: 0.69 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2793429988904412		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2793429988904412 | validation: 1.2851308390133556]
	TIME [epoch: 0.691 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2946682498562985		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.2946682498562985 | validation: 1.1670089633989489]
	TIME [epoch: 0.694 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29783553858444		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.29783553858444 | validation: 1.2567765724051547]
	TIME [epoch: 0.69 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3214958801008347		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.3214958801008347 | validation: 1.2248554487916472]
	TIME [epoch: 0.69 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3058166472532242		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.3058166472532242 | validation: 1.2159588426243109]
	TIME [epoch: 0.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2990812634915188		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2990812634915188 | validation: 1.21184524839659]
	TIME [epoch: 171 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2767621741024082		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.2767621741024082 | validation: 1.2154233540409665]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2895879670819101		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.2895879670819101 | validation: 1.2144881606152083]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.274000981699043		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.274000981699043 | validation: 1.1525161816998755]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.25894263809036		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.25894263809036 | validation: 1.2123791390953247]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2501842159667893		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.2501842159667893 | validation: 1.0873987501800506]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2630891009021499		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.2630891009021499 | validation: 1.253926748671216]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2693108346182629		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.2693108346182629 | validation: 1.0807294138776542]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3230656970234316		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.3230656970234316 | validation: 1.4352309134829038]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.420439160787385		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.420439160787385 | validation: 1.1511330447716153]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2485191753998848		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.2485191753998848 | validation: 1.0990344641691443]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.211304074639177		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.211304074639177 | validation: 1.108969526955033]
	TIME [epoch: 1.35 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1971530330074265		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.1971530330074265 | validation: 1.081993157382916]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1985226547249945		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.1985226547249945 | validation: 1.1379718001505548]
	TIME [epoch: 1.35 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1930118693936738		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.1930118693936738 | validation: 1.1182962991196377]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2238659402988643		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.2238659402988643 | validation: 1.3312025043418574]
	TIME [epoch: 1.35 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3766243694838485		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.3766243694838485 | validation: 1.1052136927907215]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2919707369223188		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2919707369223188 | validation: 1.194368643456374]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2278082316797674		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.2278082316797674 | validation: 1.043207089629469]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1857406520988811		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1857406520988811 | validation: 1.1310124981890999]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1666749488601473		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1666749488601473 | validation: 1.0361960229141451]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.168616035767911		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.168616035767911 | validation: 1.1414982451519364]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1741948756724094		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.1741948756724094 | validation: 1.0748157237383127]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009597710066497		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2009597710066497 | validation: 1.131300532195201]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.223951440718734		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.223951440718734 | validation: 1.255254437127392]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2834579220758324		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.2834579220758324 | validation: 0.9849782794815038]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3601319026692869		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.3601319026692869 | validation: 1.303569560646426]
	TIME [epoch: 1.35 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2570158900852266		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.2570158900852266 | validation: 1.0419579426745547]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1448941754729771		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1448941754729771 | validation: 0.9631755651138093]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150262991714879		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.150262991714879 | validation: 1.0947645124068057]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1463774756539624		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1463774756539624 | validation: 0.9885013011954046]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313926948789637		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.1313926948789637 | validation: 1.0496249315632447]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1236986091027705		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.1236986091027705 | validation: 1.0565495832908425]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1477337709833		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1477337709833 | validation: 1.1098442960934023]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.267612758632025		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.267612758632025 | validation: 1.2189542866350747]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2103525267314128		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2103525267314128 | validation: 1.0524920674569103]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575273596641502		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1575273596641502 | validation: 0.9759679842498814]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1801163538513462		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1801163538513462 | validation: 1.318341776674576]
	TIME [epoch: 1.34 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3710661395286519		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.3710661395286519 | validation: 0.9260472147475926]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.190514741079414		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.190514741079414 | validation: 0.9880094356274611]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1157905932219963		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.1157905932219963 | validation: 1.037082635320569]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1129069843172938		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1129069843172938 | validation: 0.9314702646425861]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1347330348970825		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1347330348970825 | validation: 1.0568394246751953]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1220574764527864		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1220574764527864 | validation: 0.9130042384688866]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1237138300453013		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1237138300453013 | validation: 1.117333948459066]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1457244873086163		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.1457244873086163 | validation: 0.9638102776412212]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228733141802429		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.228733141802429 | validation: 1.287611146382957]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3135097007123855		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.3135097007123855 | validation: 0.979696030363165]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0872787377714626		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.0872787377714626 | validation: 0.9305991497708703]
	TIME [epoch: 1.35 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091569528809163		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.091569528809163 | validation: 1.074973577186429]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1164099007579997		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1164099007579997 | validation: 0.9081592768379303]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099276460344506		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.099276460344506 | validation: 1.0852708662516162]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1202388408068173		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.1202388408068173 | validation: 0.9061129569980658]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151654091502928		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.151654091502928 | validation: 1.1008306436973054]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1795130726999181		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.1795130726999181 | validation: 1.0066460310926284]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1273651814423626		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.1273651814423626 | validation: 0.9764505273347132]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1177021268718048		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1177021268718048 | validation: 1.0471960425279014]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1018106336353557		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.1018106336353557 | validation: 0.9644912851630041]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1331343453872957		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.1331343453872957 | validation: 1.1129040209714578]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1538412531178435		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.1538412531178435 | validation: 0.9359333755273425]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121051591136664		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.121051591136664 | validation: 1.0289076408674949]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0806625057161925		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.0806625057161925 | validation: 0.8574230468208219]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.074008591338705		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.074008591338705 | validation: 1.0484920575510255]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1117478829510654		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.1117478829510654 | validation: 0.9052711333598247]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2247631930067728		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.2247631930067728 | validation: 1.2628539216475272]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2601004385885102		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.2601004385885102 | validation: 0.9003262876946334]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.071070124807247		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.071070124807247 | validation: 0.9369785678884643]
	TIME [epoch: 1.35 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079863288352835		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.079863288352835 | validation: 1.0123724477643499]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0986192392226943		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.0986192392226943 | validation: 0.9663899846937785]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0915354878336834		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.0915354878336834 | validation: 0.9613749046759319]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0990366413525647		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.0990366413525647 | validation: 0.9935826116203954]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0750352240694703		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0750352240694703 | validation: 0.925993853674099]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0709044335020752		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.0709044335020752 | validation: 0.9619903066972532]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0756999171641979		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.0756999171641979 | validation: 0.9820078159332843]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087641192387586		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.087641192387586 | validation: 0.9041254576254723]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0942897731105987		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.0942897731105987 | validation: 1.0948975369960332]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1544576210106636		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.1544576210106636 | validation: 0.8171854255638795]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1480376629871551		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.1480376629871551 | validation: 1.1870042432450034]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15289632602022		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.15289632602022 | validation: 0.8481507087433817]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0814925274216205		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.0814925274216205 | validation: 0.9752926630224121]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0534407818357578		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.0534407818357578 | validation: 0.9698550530088724]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0856804460551943		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.0856804460551943 | validation: 0.9669115999258987]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0760534064801623		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0760534064801623 | validation: 0.9795715419878279]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0714643592966742		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0714643592966742 | validation: 0.9877155459062031]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067337700543218		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.067337700543218 | validation: 0.9613381810741095]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.07773924138768		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.07773924138768 | validation: 0.9323905599222817]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0775705166786356		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.0775705166786356 | validation: 1.0389807362106154]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1029491632657242		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.1029491632657242 | validation: 0.8257966329463996]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1383614854542399		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.1383614854542399 | validation: 1.1247882367300175]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1230265924098881		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.1230265924098881 | validation: 0.8321229545899977]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0650444827456038		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.0650444827456038 | validation: 0.972672317816426]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0382222582255032		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0382222582255032 | validation: 0.8639019904525768]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031345455205659		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.031345455205659 | validation: 0.9184144102043317]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0308333630186615		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0308333630186615 | validation: 0.9095441658664036]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040212100569717		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.040212100569717 | validation: 0.9848040850750011]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785136620811187		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0785136620811187 | validation: 0.9860530839488982]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1274281791592426		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.1274281791592426 | validation: 1.0437105482190807]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.077605271602613		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.077605271602613 | validation: 0.9205573636300006]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.078168597082557		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.078168597082557 | validation: 0.9045365102241292]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0753726916008497		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.0753726916008497 | validation: 1.0930980319176953]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375667089439803		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.1375667089439803 | validation: 0.7686867254457942]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14924914372982		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.14924914372982 | validation: 1.0403987671830586]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0656636342141486		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.0656636342141486 | validation: 0.885250375035703]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022742938605456		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.022742938605456 | validation: 0.8649298811108754]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192236437421285		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.0192236437421285 | validation: 0.9601504484714106]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0198423842518123		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.0198423842518123 | validation: 0.8556974689053028]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0224444126102932		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.0224444126102932 | validation: 0.9683962814867899]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0358312709399777		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.0358312709399777 | validation: 0.8576996310834005]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.095656316204094		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.095656316204094 | validation: 1.1440787809752861]
	TIME [epoch: 1.36 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15340159635304		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.15340159635304 | validation: 0.8030454383755856]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0638742382881685		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0638742382881685 | validation: 1.0259798852000823]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0313596794845783		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.0313596794845783 | validation: 0.8460257428190096]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0180053613265616		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.0180053613265616 | validation: 0.9141892315710174]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0090653246649328		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.0090653246649328 | validation: 0.8946643346672727]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0149893066941231		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.0149893066941231 | validation: 0.8903773158280673]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126901842037241		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.0126901842037241 | validation: 1.0404137583172854]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0429076247490263		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.0429076247490263 | validation: 0.9106760425673919]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1696621309664537		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.1696621309664537 | validation: 1.0365771822190402]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0543970465058778		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.0543970465058778 | validation: 0.8342034528503386]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0313658121887606		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.0313658121887606 | validation: 1.0471181477061682]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105234796896321		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.105234796896321 | validation: 0.9151332442541297]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137563786501392		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.1137563786501392 | validation: 1.0221378674172243]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.062732117479975		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.062732117479975 | validation: 0.8103216658696587]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0158210257958091		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0158210257958091 | validation: 0.9655906033469741]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016616763639179		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.016616763639179 | validation: 0.8207427634334789]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019879239372829		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.019879239372829 | validation: 0.9809401326208452]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014785857059253		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.014785857059253 | validation: 0.8278839841422807]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0095880960713084		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.0095880960713084 | validation: 1.0078816590309525]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0413832604262852		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.0413832604262852 | validation: 0.8230765362016816]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0679037020898157		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.0679037020898157 | validation: 1.0221911968255046]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586856828634714		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.0586856828634714 | validation: 0.7895262030131799]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0674472365713732		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.0674472365713732 | validation: 1.0926717383626763]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0912186327705655		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.0912186327705655 | validation: 0.8363410016422833]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0438658905366365		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.0438658905366365 | validation: 0.9525148690774341]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0065374268678675		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.0065374268678675 | validation: 0.8541068454898847]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940293113869584		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.9940293113869584 | validation: 0.9391692883186687]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9991566987831243		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.9991566987831243 | validation: 0.8349803149628592]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0018259527185394		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.0018259527185394 | validation: 0.9848079546420057]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0259088544181258		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.0259088544181258 | validation: 0.7582002213951786]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0738636942771178		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.0738636942771178 | validation: 1.054002885404259]
	TIME [epoch: 1.35 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0560734893158905		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.0560734893158905 | validation: 0.78000947612682]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0334712491478435		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0334712491478435 | validation: 1.0449422456078459]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0420413471833063		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.0420413471833063 | validation: 0.9232783366218684]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0481713660919025		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.0481713660919025 | validation: 0.9127720304836523]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0301153586026799		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.0301153586026799 | validation: 0.9095139071959567]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9852353655515166		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.9852353655515166 | validation: 0.8978964587487814]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9874096400642384		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.9874096400642384 | validation: 0.8177182204422828]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9787118171607645		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9787118171607645 | validation: 0.9075759404170516]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835475824089477		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9835475824089477 | validation: 0.7943657263717281]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862685018649753		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9862685018649753 | validation: 1.039038103743623]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0293293622461135		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.0293293622461135 | validation: 0.7396827604248097]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134836616168087		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.134836616168087 | validation: 1.114875417084828]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0846651676175898		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0846651676175898 | validation: 0.8335397627728096]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190428584809874		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.0190428584809874 | validation: 0.9233342836571901]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0113995971745124		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.0113995971745124 | validation: 0.8983826492268883]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0026057520688982		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.0026057520688982 | validation: 0.881814171815559]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779195285408503		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.9779195285408503 | validation: 0.8753989739710418]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9709747097778507		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.9709747097778507 | validation: 0.8364811501969103]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668329869999249		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.9668329869999249 | validation: 0.8930328696305179]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723501739394524		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.9723501739394524 | validation: 0.8180182879215409]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.982489809871635		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.982489809871635 | validation: 1.0060828161071347]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0196301610196463		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.0196301610196463 | validation: 0.717404524762689]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.097318467581509		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.097318467581509 | validation: 1.151674138791919]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.09863659276633		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.09863659276633 | validation: 0.8263249062537716]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.003787646946322		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.003787646946322 | validation: 0.8820301614470556]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980599299463012		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.9980599299463012 | validation: 0.9416338062203483]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9964164006956984		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.9964164006956984 | validation: 0.851389080171631]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9839986951669637		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9839986951669637 | validation: 0.9084805084957296]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662103047896139		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.9662103047896139 | validation: 0.8749272773029784]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9572498809220672		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9572498809220672 | validation: 0.8480988999799988]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9612702248175466		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.9612702248175466 | validation: 0.9283671023024248]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9615659745130138		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.9615659745130138 | validation: 0.8016189312695214]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000526844852169		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.000526844852169 | validation: 1.0870141371644062]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0730597560594297		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.0730597560594297 | validation: 0.7877971821203678]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.064734314518065		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.064734314518065 | validation: 1.026760661579934]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0269309333507985		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.0269309333507985 | validation: 0.8020264200632504]
	TIME [epoch: 1.35 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9870680915546405		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.9870680915546405 | validation: 0.9520131722041256]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9810350260294093		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.9810350260294093 | validation: 0.8308957515907042]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9801219634117686		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.9801219634117686 | validation: 0.9187411788430926]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9695485909638072		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.9695485909638072 | validation: 0.8042947881836473]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9599175759276184		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9599175759276184 | validation: 0.9017195620739454]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480308838308935		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9480308838308935 | validation: 0.7959645383286083]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494692437291612		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.9494692437291612 | validation: 0.9540802085839344]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597694713341433		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.9597694713341433 | validation: 0.7554056222370126]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802480191742441		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9802480191742441 | validation: 1.0821956341192944]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.033963895933838		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.033963895933838 | validation: 0.7038687702638803]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0588327735243164		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.0588327735243164 | validation: 1.0597283846083296]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0275618601581593		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.0275618601581593 | validation: 0.8360191913067953]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9829964975662228		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.9829964975662228 | validation: 0.9053500686777906]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722795687093784		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9722795687093784 | validation: 0.856178141690858]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507629726513023		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.9507629726513023 | validation: 0.8702931602490471]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397601038715423		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.9397601038715423 | validation: 0.8626145219134692]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281380777715463		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.9281380777715463 | validation: 0.8309265958109049]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9351721235686139		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.9351721235686139 | validation: 0.9248110199583565]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447771610221528		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.9447771610221528 | validation: 0.7460774837103937]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9892799781613101		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.9892799781613101 | validation: 1.1677544238156774]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.085889891428562		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.085889891428562 | validation: 0.7071450803154362]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.023605204845635		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.023605204845635 | validation: 0.9683439597696165]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569446514754478		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9569446514754478 | validation: 0.8056194402017649]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9263049921872135		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9263049921872135 | validation: 0.8681589498966515]
	TIME [epoch: 1.35 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262655212156524		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.9262655212156524 | validation: 0.8195938439832476]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281979322497995		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9281979322497995 | validation: 0.9197759845232923]
	TIME [epoch: 1.36 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290919517663356		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.9290919517663356 | validation: 0.7229871795058802]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9729678425539788		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.9729678425539788 | validation: 1.07129799733932]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0438068349085068		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.0438068349085068 | validation: 0.7800086889178397]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936259453973105		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.9936259453973105 | validation: 0.8523972112571067]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228687149939734		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.9228687149939734 | validation: 0.9009671042335375]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245684090149727		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.9245684090149727 | validation: 0.7506285565590695]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.943206973517523		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.943206973517523 | validation: 1.0356624186131616]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9903411510091491		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.9903411510091491 | validation: 0.7655408942953872]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0328487901167285		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.0328487901167285 | validation: 1.0267789833252605]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9690381963967162		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.9690381963967162 | validation: 0.7735213048882257]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.921690107672857		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.921690107672857 | validation: 0.8745825363939226]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919400147379774		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.919400147379774 | validation: 0.8158566680939301]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.909994913077731		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.909994913077731 | validation: 0.8579681811037324]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9083122608899895		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.9083122608899895 | validation: 0.8705089069880754]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9330669315941986		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.9330669315941986 | validation: 0.8763636544573417]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9607965888399914		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.9607965888399914 | validation: 0.8487997347961269]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9625406958710165		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.9625406958710165 | validation: 0.9562843296420926]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9399985227986202		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.9399985227986202 | validation: 0.7185302945438449]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643734769493827		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.9643734769493827 | validation: 1.1287707671540768]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0318956033638593		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.0318956033638593 | validation: 0.7221264699217065]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.974511539318038		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.974511539318038 | validation: 0.910534447685196]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248509747961382		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.9248509747961382 | validation: 0.7930644380794339]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025082423407864		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.9025082423407864 | validation: 0.8415285499992775]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980789500519339		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.8980789500519339 | validation: 0.8593201305241682]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8955857439513107		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.8955857439513107 | validation: 0.7857615998092878]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072625892434587		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9072625892434587 | validation: 0.8729535385525298]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8921133614148811		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.8921133614148811 | validation: 0.725548552572295]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9390472259962183		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.9390472259962183 | validation: 1.0571378166500243]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9977615155550286		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.9977615155550286 | validation: 0.7099281997696721]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603765329553136		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.9603765329553136 | validation: 0.9428026778033202]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066988330680326		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.9066988330680326 | validation: 0.749422276918382]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9233589106866765		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.9233589106866765 | validation: 1.0553600468848232]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778194120239272		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.9778194120239272 | validation: 0.7801638523721596]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682874304939365		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9682874304939365 | validation: 0.9100096783286887]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.91478527893267		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.91478527893267 | validation: 0.8290513218834772]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848197654388108		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.8848197654388108 | validation: 0.7484247370801485]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878323637316153		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.8878323637316153 | validation: 0.9382423246823359]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933568966373304		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8933568966373304 | validation: 0.7046298197232131]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216566119350817		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.9216566119350817 | validation: 1.0406464519848442]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9711216430003653		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.9711216430003653 | validation: 0.7157187312677407]
	TIME [epoch: 1.35 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9241916528873141		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.9241916528873141 | validation: 0.8886342404944]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853391596269728		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.8853391596269728 | validation: 0.7607336368779074]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876998672164933		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.876998672164933 | validation: 0.9108666789514592]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854340356815569		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.8854340356815569 | validation: 0.6957963164202434]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9063408936686347		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.9063408936686347 | validation: 1.0646364208731516]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9558434494841554		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.9558434494841554 | validation: 0.6940753608673491]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9547007620327941		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.9547007620327941 | validation: 0.9637428182525145]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9250273563327384		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.9250273563327384 | validation: 0.8488860080408664]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183428206978588		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.9183428206978588 | validation: 0.845256236152871]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9373831741957724		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.9373831741957724 | validation: 0.9253906569344171]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9203411451023402		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9203411451023402 | validation: 0.7439334841583624]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880778915024781		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.8880778915024781 | validation: 0.8872534873671489]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778375242179223		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.8778375242179223 | validation: 0.7248734233839853]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865592545841557		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.865592545841557 | validation: 0.9045087099643276]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836001874516657		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.8836001874516657 | validation: 0.6821226386445467]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9109725433603808		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.9109725433603808 | validation: 1.0499774142234661]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422950302205803		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.9422950302205803 | validation: 0.7075810528539503]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888823454451524		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.8888823454451524 | validation: 0.9087984575839315]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651527279483738		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.8651527279483738 | validation: 0.7486234561592461]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581280176612188		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.8581280176612188 | validation: 0.8764512802300216]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620340559650714		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.8620340559650714 | validation: 0.7570740550985907]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722790702876253		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.8722790702876253 | validation: 0.9629822578351741]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092751866544512		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.9092751866544512 | validation: 0.8085890648235087]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9338639569309242		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.9338639569309242 | validation: 0.8783090337205087]
	TIME [epoch: 1.35 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9350335354962251		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.9350335354962251 | validation: 0.9166899291916352]
	TIME [epoch: 1.35 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9019683508397843		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.9019683508397843 | validation: 0.6747563544073782]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9079698699060588		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.9079698699060588 | validation: 1.013582572832299]
	TIME [epoch: 1.35 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.91991047609352		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.91991047609352 | validation: 0.6926863967023514]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8790830366115513		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.8790830366115513 | validation: 0.9068485521621075]
	TIME [epoch: 1.35 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564083688590575		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.8564083688590575 | validation: 0.7662946325251225]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8414654899129783		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.8414654899129783 | validation: 0.8130220772154341]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8397946483666908		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.8397946483666908 | validation: 0.7807621526559919]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843291236721459		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.843291236721459 | validation: 0.8254617167003648]
	TIME [epoch: 1.35 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8545132925814889		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.8545132925814889 | validation: 0.8689724049031933]
	TIME [epoch: 1.35 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865764567569788		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.8865764567569788 | validation: 0.9190647486025818]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311873671903834		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.9311873671903834 | validation: 0.8213774762465539]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756810203607346		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.8756810203607346 | validation: 0.8215458616405957]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8351312812540508		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.8351312812540508 | validation: 0.7663384026369117]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235942971043021		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.8235942971043021 | validation: 0.7958869311290644]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222575584276715		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.8222575584276715 | validation: 0.7570592503362544]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8116941280061045		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.8116941280061045 | validation: 0.8633607937944959]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192607571224313		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.8192607571224313 | validation: 0.6536150719376446]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833161349668385		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.8833161349668385 | validation: 1.2519365817507864]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1017318110185808		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1017318110185808 | validation: 0.6875253955174868]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489807217750004		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.8489807217750004 | validation: 0.7983620241316292]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052121561628603		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.8052121561628603 | validation: 0.7999901630992059]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097608866164611		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.8097608866164611 | validation: 0.7209476763089989]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185802250088616		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.8185802250088616 | validation: 0.9612463367826234]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632905364362591		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.8632905364362591 | validation: 0.6981427345798872]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519073522728994		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.9519073522728994 | validation: 0.9866918441900213]
	TIME [epoch: 1.35 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8954194986154289		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.8954194986154289 | validation: 0.8239881344150803]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574582537898294		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.8574582537898294 | validation: 0.6688149604463183]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191165542798244		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.9191165542798244 | validation: 0.9724427399588997]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800954532472846		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.8800954532472846 | validation: 0.6819122104546896]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292837051928893		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.8292837051928893 | validation: 0.8633891878243434]
	TIME [epoch: 1.35 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220911932621793		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.8220911932621793 | validation: 0.6986681992248263]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8263661829654985		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.8263661829654985 | validation: 0.8809835824301078]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250032548125121		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.8250032548125121 | validation: 0.7074605141313288]
	TIME [epoch: 1.35 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224895425668715		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.8224895425668715 | validation: 0.8945343720976248]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277330751419265		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.8277330751419265 | validation: 0.7349208360001109]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357771694789937		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.8357771694789937 | validation: 0.8755451805833137]
	TIME [epoch: 2.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8405918342673898		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.8405918342673898 | validation: 0.8129660167937574]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531299218762033		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.8531299218762033 | validation: 0.7700125512200144]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614549148460026		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.8614549148460026 | validation: 0.8941619357258983]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543447133912397		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.8543447133912397 | validation: 0.6353601673659786]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8455849350016433		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.8455849350016433 | validation: 0.9522238857428267]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84536498875354		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.84536498875354 | validation: 0.6679131828599068]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168759527487798		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.8168759527487798 | validation: 0.8676203147498152]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045889505077243		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.8045889505077243 | validation: 0.7021329181833921]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047725205096157		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.8047725205096157 | validation: 0.8757786524364521]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062924770550238		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.8062924770550238 | validation: 0.6645169237242822]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820344127135014		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.820344127135014 | validation: 0.917628734585438]
	TIME [epoch: 2.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381406384250556		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.8381406384250556 | validation: 0.6410199219073276]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290771706611823		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8290771706611823 | validation: 0.8878349872117394]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234221981326715		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8234221981326715 | validation: 0.6776481307688514]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462657188785572		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8462657188785572 | validation: 0.8910964332389238]
	TIME [epoch: 2.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401407768247592		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.8401407768247592 | validation: 0.7804318260973888]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189942225272097		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.8189942225272097 | validation: 0.7578365419512816]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057225299270465		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.8057225299270465 | validation: 0.8635452260695353]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8129264709863117		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.8129264709863117 | validation: 0.7288819302087948]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134892106350643		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.8134892106350643 | validation: 0.8521346000780675]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802347242140368		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.802347242140368 | validation: 0.7008027880541241]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960121149401408		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.7960121149401408 | validation: 0.834611093058415]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892662340727802		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.7892662340727802 | validation: 0.6907641395721249]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7945710543723437		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.7945710543723437 | validation: 0.8991280314307666]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030796235389579		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.8030796235389579 | validation: 0.6072258194185022]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299519755366066		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.8299519755366066 | validation: 0.9364555759896593]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413020779434206		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.8413020779434206 | validation: 0.6511156527156063]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475657022269754		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.8475657022269754 | validation: 0.8707707134964157]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161086084597199		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.8161086084597199 | validation: 0.7650711576486506]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771392626662983		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.7771392626662983 | validation: 0.7218405056440956]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809139990549521		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.7809139990549521 | validation: 0.8442989502432638]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788205474216059		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.7788205474216059 | validation: 0.6987197193881731]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856453042508261		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.7856453042508261 | validation: 0.8520208737306476]
	TIME [epoch: 2.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7845757622386142		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.7845757622386142 | validation: 0.7318715477008771]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877156652901824		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.7877156652901824 | validation: 0.8224594475988527]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912283104658573		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.7912283104658573 | validation: 0.7760885742331602]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794765232311222		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.794765232311222 | validation: 0.7410324437198735]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867569810722737		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.7867569810722737 | validation: 0.8152632482718467]
	TIME [epoch: 2.66 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813378757763902		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.7813378757763902 | validation: 0.6542759359183362]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817556162389709		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.7817556162389709 | validation: 0.9411750447486974]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152672034413087		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8152672034413087 | validation: 0.5911691558469673]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8282583591330629		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.8282583591330629 | validation: 0.8583123124585722]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691938902758615		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.7691938902758615 | validation: 0.6641556788239402]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760151689406528		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.760151689406528 | validation: 0.8276818365772913]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576561116367191		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.7576561116367191 | validation: 0.6302465419744732]
	TIME [epoch: 2.67 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597801460103523		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.7597801460103523 | validation: 0.89451930007592]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780257148221746		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.7780257148221746 | validation: 0.6497180869148684]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712745692257991		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.7712745692257991 | validation: 0.820963662791765]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642907761454352		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.7642907761454352 | validation: 0.71039495270706]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688472289139614		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.7688472289139614 | validation: 0.8199321393863022]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174964426272873		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.8174964426272873 | validation: 0.8542574417278939]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416529459057154		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.8416529459057154 | validation: 0.6540339916666756]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692190171509387		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.7692190171509387 | validation: 0.8275183366414658]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551197831489102		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.7551197831489102 | validation: 0.599669671233439]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814005191684421		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.7814005191684421 | validation: 0.8985267988105753]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715641793831886		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.7715641793831886 | validation: 0.6151772882652973]
	TIME [epoch: 2.67 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719483260524856		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.7719483260524856 | validation: 0.8637112911228524]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630198909873369		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.7630198909873369 | validation: 0.6303570659713859]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571678808159683		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.7571678808159683 | validation: 0.814273278724534]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460677522664263		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.7460677522664263 | validation: 0.62262422054192]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455182519634701		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.7455182519634701 | validation: 0.8157249693683739]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552501999856598		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.7552501999856598 | validation: 0.6626782838883659]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617969764022697		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.7617969764022697 | validation: 0.8018244542263683]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586053193816276		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.7586053193816276 | validation: 0.7129843084928753]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756848759507271		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.756848759507271 | validation: 0.7518111969936343]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636816741867568		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.7636816741867568 | validation: 0.8520643708357692]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840701880634157		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.7840701880634157 | validation: 0.6630497063623761]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664637504213676		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.7664637504213676 | validation: 0.8056150759901876]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389697730028699		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.7389697730028699 | validation: 0.6354499144270668]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226762575645248		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.7226762575645248 | validation: 0.8285226555487303]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732935344610355		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.732935344610355 | validation: 0.5831594648160693]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513588974977634		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.7513588974977634 | validation: 0.8905882116821607]
	TIME [epoch: 2.69 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697430261188904		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.7697430261188904 | validation: 0.5925393261824582]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573572937778005		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.7573572937778005 | validation: 0.8278237449696004]
	TIME [epoch: 2.69 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276334606935697		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.7276334606935697 | validation: 0.6137478349642097]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200529022189687		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.7200529022189687 | validation: 0.8138386052084811]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729023582938087		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.729023582938087 | validation: 0.5954602435625341]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312911126106301		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.7312911126106301 | validation: 0.8388641900242366]
	TIME [epoch: 2.69 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372618984042046		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.7372618984042046 | validation: 0.6180583663312658]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265871422966467		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.7265871422966467 | validation: 0.8139395453488257]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236063541588171		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.7236063541588171 | validation: 0.5912659040437076]
	TIME [epoch: 2.67 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311541233555604		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.7311541233555604 | validation: 0.8258016462296887]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262730506445669		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.7262730506445669 | validation: 0.6118585904817171]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156278358782732		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.7156278358782732 | validation: 0.7928028443188427]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202249492333681		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.7202249492333681 | validation: 0.700301867299002]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746708037485592		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.746708037485592 | validation: 0.7800747931078337]
	TIME [epoch: 2.67 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266321562348111		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8266321562348111 | validation: 0.8981658457471025]
	TIME [epoch: 2.66 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734952846214154		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.7734952846214154 | validation: 0.6298411089515774]
	TIME [epoch: 2.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045738279065753		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.7045738279065753 | validation: 0.764579851101924]
	TIME [epoch: 2.66 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886416241969013		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.6886416241969013 | validation: 0.6077567254912861]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69680111174097		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.69680111174097 | validation: 0.8265514219960437]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184911165360301		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.7184911165360301 | validation: 0.5795263466853545]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728942931409481		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.728942931409481 | validation: 0.8112205000534148]
	TIME [epoch: 2.69 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112646354236216		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.7112646354236216 | validation: 0.5959667635217787]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932893663925997		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.6932893663925997 | validation: 0.7774570296386639]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912007749149685		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.6912007749149685 | validation: 0.5726870028282803]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058557334820773		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.7058557334820773 | validation: 0.8404951143397584]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192807812292228		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.7192807812292228 | validation: 0.5904045063670827]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698770613894668		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.698770613894668 | validation: 0.8041404234762943]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895857576601562		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.6895857576601562 | validation: 0.5832535999663944]
	TIME [epoch: 2.67 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854959385636492		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.6854959385636492 | validation: 0.8003366670522104]
	TIME [epoch: 2.67 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6892376850890716		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.6892376850890716 | validation: 0.56576443192691]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955125038210457		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.6955125038210457 | validation: 0.8221734935123703]
	TIME [epoch: 2.68 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061252706344854		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.7061252706344854 | validation: 0.5643819150010148]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961618420840289		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.6961618420840289 | validation: 0.8049101244962394]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6891301830770425		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.6891301830770425 | validation: 0.5784465524658651]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834745671691911		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.6834745671691911 | validation: 0.7881394620211103]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868704136028335		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.6868704136028335 | validation: 0.5654909113106225]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7067995312631724		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.7067995312631724 | validation: 0.8388008088127145]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869004724236325		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.7869004724236325 | validation: 0.8560671065909123]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021376101190709		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.8021376101190709 | validation: 0.6123521233064544]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7000317315550089		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.7000317315550089 | validation: 0.7457870175691842]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549816095618703		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.6549816095618703 | validation: 0.5970735621384565]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600739583273356		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.6600739583273356 | validation: 0.7384528765626057]
	TIME [epoch: 2.67 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680738181691152		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.6680738181691152 | validation: 0.592479643223231]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788765100646142		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.6788765100646142 | validation: 0.768450793787955]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686743832484766		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.686743832484766 | validation: 0.5538897252144007]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865622399543464		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.6865622399543464 | validation: 0.7933516734092086]
	TIME [epoch: 2.67 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665924777571145		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.665924777571145 | validation: 0.5696538488231638]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678595977251393		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.6678595977251393 | validation: 0.7770286229006036]
	TIME [epoch: 2.67 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754758576955499		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.6754758576955499 | validation: 0.630036335464048]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6841630561853649		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.6841630561853649 | validation: 0.7465089679213421]
	TIME [epoch: 2.67 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954010814068086		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.6954010814068086 | validation: 0.7337619892822351]
	TIME [epoch: 2.67 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167772566391335		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.7167772566391335 | validation: 0.6642162405124744]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772981253959693		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.6772981253959693 | validation: 0.6852470718868791]
	TIME [epoch: 2.67 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432797488578393		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.6432797488578393 | validation: 0.6114313589320396]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638819098719969		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.638819098719969 | validation: 0.7579334387656396]
	TIME [epoch: 2.67 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570236697742516		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.6570236697742516 | validation: 0.5267688058789364]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962894516692555		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.6962894516692555 | validation: 0.8136682457842332]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882075514691295		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.6882075514691295 | validation: 0.5693428629753653]
	TIME [epoch: 2.67 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423296145554103		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.6423296145554103 | validation: 0.7470338750678519]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441395684131076		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.6441395684131076 | validation: 0.5423253757294665]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499757621264297		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.6499757621264297 | validation: 0.7802384982427747]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585593901349617		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.6585593901349617 | validation: 0.5520345753351678]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652317792663929		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.652317792663929 | validation: 0.7681269162837417]
	TIME [epoch: 2.68 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492898356814046		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.6492898356814046 | validation: 0.5590631534923013]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509331845259676		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.6509331845259676 | validation: 0.7455457122217046]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554097268844368		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.6554097268844368 | validation: 0.5868124404743245]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815074529315034		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.6815074529315034 | validation: 0.7124559285837733]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829763441335978		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.6829763441335978 | validation: 0.7868177341423682]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052707553102028		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.7052707553102028 | validation: 0.6050817092541001]
	TIME [epoch: 2.68 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6784385141876229		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.6784385141876229 | validation: 0.7066262357407039]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255147781751942		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.6255147781751942 | validation: 0.578242815103755]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100285205728959		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.6100285205728959 | validation: 0.7249637435835613]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265727145810687		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.6265727145810687 | validation: 0.5119417994815935]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603917175953753		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.6603917175953753 | validation: 0.7783465437374741]
	TIME [epoch: 2.67 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584161398836177		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.6584161398836177 | validation: 0.5655964331708349]
	TIME [epoch: 2.67 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6281991534252943		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.6281991534252943 | validation: 0.7004822608601815]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148881558428205		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.6148881558428205 | validation: 0.5723511873194761]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6172978735441879		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.6172978735441879 | validation: 0.7029293287024772]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194889540498386		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.6194889540498386 | validation: 0.5463428761602839]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415302919543242		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.6415302919543242 | validation: 0.750428427264081]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343567387493251		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.6343567387493251 | validation: 0.526092368391652]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232371043912991		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.6232371043912991 | validation: 0.7432164001340489]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.624847492502491		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.624847492502491 | validation: 0.5489726534796623]
	TIME [epoch: 2.68 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229408796451833		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.6229408796451833 | validation: 0.7833985345131305]
	TIME [epoch: 2.68 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524198772558246		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.6524198772558246 | validation: 0.5974994380265213]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417112974536286		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.6417112974536286 | validation: 0.7010363920226222]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587721518041656		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.6587721518041656 | validation: 0.7031923877224643]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6807793942749283		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.6807793942749283 | validation: 0.6438562817451466]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615288231575764		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.615288231575764 | validation: 0.6297206785031779]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913145399841153		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.5913145399841153 | validation: 0.6362840933719616]
	TIME [epoch: 2.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5801859782395278		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.5801859782395278 | validation: 0.6178381281417926]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908672989447736		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.5908672989447736 | validation: 0.6703675120932151]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6173880153818871		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.6173880153818871 | validation: 0.6105054129691392]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6308495541203106		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.6308495541203106 | validation: 0.7136077173269445]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262828978969437		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.6262828978969437 | validation: 0.5627197632976445]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5958916935228442		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.5958916935228442 | validation: 0.7284573504054728]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018911219634098		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.6018911219634098 | validation: 0.48935698218365264]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530791545600175		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.6530791545600175 | validation: 0.7714551575647609]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532461198886824		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.6532461198886824 | validation: 0.5414072692404627]
	TIME [epoch: 2.67 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988204444189974		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.5988204444189974 | validation: 0.6654189044676956]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5884480632124967		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.5884480632124967 | validation: 0.5328876059820191]
	TIME [epoch: 2.67 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5968136507922093		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.5968136507922093 | validation: 0.734772469421423]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6142006794799473		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.6142006794799473 | validation: 0.5036042059651346]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6071654013034242		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.6071654013034242 | validation: 0.7020787654738003]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589572661831365		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.589572661831365 | validation: 0.518134589202253]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5934934754636282		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.5934934754636282 | validation: 0.7156320019709534]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5986391978411227		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.5986391978411227 | validation: 0.5178349391339618]
	TIME [epoch: 2.68 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908043907206626		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.5908043907206626 | validation: 0.7135409914239912]
	TIME [epoch: 2.68 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5980405062150013		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.5980405062150013 | validation: 0.5397419794484887]
	TIME [epoch: 2.68 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5866897092187161		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.5866897092187161 | validation: 0.679783865015594]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5892246031307079		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.5892246031307079 | validation: 0.5968412370722634]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6199707621456687		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.6199707621456687 | validation: 0.6730737940495205]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703093587729017		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.6703093587729017 | validation: 0.668076888151746]
	TIME [epoch: 2.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029429823362892		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.6029429823362892 | validation: 0.5429726808478366]
	TIME [epoch: 2.68 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5610234388914905		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.5610234388914905 | validation: 0.6487266896746783]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658113446608021		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.5658113446608021 | validation: 0.48973146714784205]
	TIME [epoch: 2.67 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908373705420579		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.5908373705420579 | validation: 0.7453353658845394]
	TIME [epoch: 2.68 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613634072047082		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.613634072047082 | validation: 0.4999975408239163]
	TIME [epoch: 2.67 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5732416598341201		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.5732416598341201 | validation: 0.6635923673115507]
	TIME [epoch: 2.67 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568116083892605		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.568116083892605 | validation: 0.5046719136015044]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654624148008671		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.5654624148008671 | validation: 0.7048429299574894]
	TIME [epoch: 2.67 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925323934535502		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.5925323934535502 | validation: 0.49411473711452225]
	TIME [epoch: 2.67 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.584121523847672		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.584121523847672 | validation: 0.6723190394795]
	TIME [epoch: 2.67 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651412926157803		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.5651412926157803 | validation: 0.5111723205425517]
	TIME [epoch: 2.67 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5672774618547266		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.5672774618547266 | validation: 0.6791152935624889]
	TIME [epoch: 2.67 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5738666883242134		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.5738666883242134 | validation: 0.495087402852295]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568715126620821		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.568715126620821 | validation: 0.6626340415141119]
	TIME [epoch: 2.66 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5604296221838924		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.5604296221838924 | validation: 0.4991564041849388]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5652408888678313		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.5652408888678313 | validation: 0.6786425781435744]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725234657587844		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.5725234657587844 | validation: 0.48793217945578116]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5674476707191819		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.5674476707191819 | validation: 0.6776450968946723]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566133501845637		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.5566133501845637 | validation: 0.49256742449677615]
	TIME [epoch: 2.68 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5565161340043082		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.5565161340043082 | validation: 0.7081865920575515]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5616078881173141		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.5616078881173141 | validation: 0.5010805106559392]
	TIME [epoch: 2.68 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935834513262976		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.5935834513262976 | validation: 0.7323090195060569]
	TIME [epoch: 2.68 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062560213396109		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.6062560213396109 | validation: 0.65742658500761]
	TIME [epoch: 2.68 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366968613920454		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.6366968613920454 | validation: 0.5776092623876247]
	TIME [epoch: 2.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5734341520031102		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.5734341520031102 | validation: 0.5847611659911408]
	TIME [epoch: 2.68 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5300316684555915		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.5300316684555915 | validation: 0.5622532053674814]
	TIME [epoch: 2.68 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5233689762130636		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.5233689762130636 | validation: 0.5948146093048492]
	TIME [epoch: 2.68 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5275366470131737		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.5275366470131737 | validation: 0.5340775316495178]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480361111039687		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.5480361111039687 | validation: 0.6493404279849249]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5604068368309263		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.5604068368309263 | validation: 0.5001081061970228]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5457209522179559		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.5457209522179559 | validation: 0.670432507369212]
	TIME [epoch: 2.68 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496327646885353		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.5496327646885353 | validation: 0.45669619579274523]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707143987174743		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.5707143987174743 | validation: 0.7029140742299002]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5645300944208451		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.5645300944208451 | validation: 0.49142750889185854]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.529125515919297		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.529125515919297 | validation: 0.6348855080742517]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298194904005152		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.5298194904005152 | validation: 0.4751800772485683]
	TIME [epoch: 2.66 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5446222058336662		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.5446222058336662 | validation: 0.6746312172668127]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5474358922953656		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.5474358922953656 | validation: 0.47909895655070245]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5358415850790776		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.5358415850790776 | validation: 0.6462276127664346]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.535713170331047		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.535713170331047 | validation: 0.4842070778980283]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5438161546877742		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.5438161546877742 | validation: 0.6420199524410046]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5457781962742344		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.5457781962742344 | validation: 0.4969148629162361]
	TIME [epoch: 2.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440370556207664		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.5440370556207664 | validation: 0.6100368321731384]
	TIME [epoch: 2.68 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5423750683314623		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.5423750683314623 | validation: 0.6284516393835244]
	TIME [epoch: 2.68 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668982941352038		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.5668982941352038 | validation: 0.5621524030416033]
	TIME [epoch: 2.68 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5808964123914799		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.5808964123914799 | validation: 0.626001006329509]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5284178398632292		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.5284178398632292 | validation: 0.521333736580571]
	TIME [epoch: 2.66 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005497976290519		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.5005497976290519 | validation: 0.5916826730834404]
	TIME [epoch: 2.66 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5016612440724021		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.5016612440724021 | validation: 0.47520598329075947]
	TIME [epoch: 2.66 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168676941361204		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.5168676941361204 | validation: 0.6993640947887408]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5655719562146981		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.5655719562146981 | validation: 0.46795102771207164]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281807028634041		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.5281807028634041 | validation: 0.6076664934156911]
	TIME [epoch: 2.66 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.508611434828663		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.508611434828663 | validation: 0.4631229492722441]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5152465891686651		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.5152465891686651 | validation: 0.6636673196466332]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5303349940025599		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.5303349940025599 | validation: 0.4584907319040994]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187237658931065		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.5187237658931065 | validation: 0.6342589385402191]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076473788734125		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.5076473788734125 | validation: 0.4552808946062988]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.505151347379754		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.505151347379754 | validation: 0.6371299304161616]
	TIME [epoch: 2.67 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5156353240051393		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.5156353240051393 | validation: 0.4627803239948377]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5091194691672792		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.5091194691672792 | validation: 0.6318916540170403]
	TIME [epoch: 2.66 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5058092507138727		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.5058092507138727 | validation: 0.4609862488673055]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211626641523212		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.5211626641523212 | validation: 0.6327308953052402]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276169780037699		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.5276169780037699 | validation: 0.5145928594491239]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5448991655626427		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.5448991655626427 | validation: 0.5965630671850599]
	TIME [epoch: 2.66 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553158138262634		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.553158138262634 | validation: 0.6196576353569858]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258109807804636		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.5258109807804636 | validation: 0.5014062576288375]
	TIME [epoch: 2.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960768900444546		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.4960768900444546 | validation: 0.5870936977144874]
	TIME [epoch: 2.66 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48314025536340693		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.48314025536340693 | validation: 0.4632138727677287]
	TIME [epoch: 2.66 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4922649622168365		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.4922649622168365 | validation: 0.642480643274636]
	TIME [epoch: 2.67 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217168526238501		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.5217168526238501 | validation: 0.4500782613017087]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5054864777204349		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.5054864777204349 | validation: 0.6083773272691229]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49493567259669663		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.49493567259669663 | validation: 0.4662107999118272]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49630159064338886		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.49630159064338886 | validation: 0.608113539656983]
	TIME [epoch: 3.55 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49937796089490244		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.49937796089490244 | validation: 0.443623037036687]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4962472409961906		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.4962472409961906 | validation: 0.6136206351697502]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49540634740548917		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.49540634740548917 | validation: 0.44462026172256625]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877140249265118		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.4877140249265118 | validation: 0.6196333695085756]
	TIME [epoch: 2.67 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49348232334318354		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.49348232334318354 | validation: 0.44267806499952916]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_766.pth
	Model improved!!!
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.493519515186663		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.493519515186663 | validation: 0.6064547798273352]
	TIME [epoch: 2.67 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4878024280845796		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.4878024280845796 | validation: 0.4498494018012717]
	TIME [epoch: 2.67 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48381741834790876		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.48381741834790876 | validation: 0.6047053219304849]
	TIME [epoch: 2.67 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4833246811335454		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.4833246811335454 | validation: 0.4488058693281305]
	TIME [epoch: 2.67 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49086051516379714		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.49086051516379714 | validation: 0.6086888564774761]
	TIME [epoch: 2.67 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5143537268935188		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.5143537268935188 | validation: 0.5443947954726492]
	TIME [epoch: 2.67 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5310189329864107		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.5310189329864107 | validation: 0.5488775309090739]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534612243932693		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.534612243932693 | validation: 0.5799726316898621]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48120286220119		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.48120286220119 | validation: 0.4848247141207332]
	TIME [epoch: 2.67 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45307147434562767		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.45307147434562767 | validation: 0.5386542018837952]
	TIME [epoch: 2.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44696041325593855		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.44696041325593855 | validation: 0.43423023934674065]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47439331355707043		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.47439331355707043 | validation: 0.6507468630455928]
	TIME [epoch: 2.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132358721768285		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.5132358721768285 | validation: 0.42930506039532823]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4731184951444823		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.4731184951444823 | validation: 0.5780343582675138]
	TIME [epoch: 2.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4593671051186602		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.4593671051186602 | validation: 0.43851110721490116]
	TIME [epoch: 2.67 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4715332594572513		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.4715332594572513 | validation: 0.6031870210095032]
	TIME [epoch: 2.67 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4887027952248454		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.4887027952248454 | validation: 0.45305651133139]
	TIME [epoch: 2.66 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4674692338584828		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.4674692338584828 | validation: 0.5697354653334105]
	TIME [epoch: 2.67 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4664781764122332		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.4664781764122332 | validation: 0.44524500620338064]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4697661199777761		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.4697661199777761 | validation: 0.6024427516465432]
	TIME [epoch: 2.66 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47852772688754996		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.47852772688754996 | validation: 0.45166647366147655]
	TIME [epoch: 2.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656419563827084		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.4656419563827084 | validation: 0.5823451403257267]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4603855811029126		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.4603855811029126 | validation: 0.4324612719770616]
	TIME [epoch: 2.66 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4739193558312374		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.4739193558312374 | validation: 0.5910999943247596]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46749621108054507		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.46749621108054507 | validation: 0.4307117712352]
	TIME [epoch: 2.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46008551812240855		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.46008551812240855 | validation: 0.5852630044229532]
	TIME [epoch: 2.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46712541605996716		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.46712541605996716 | validation: 0.43075100576537767]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4642706382548812		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.4642706382548812 | validation: 0.6043616990541957]
	TIME [epoch: 2.67 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4681247092174662		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.4681247092174662 | validation: 0.4532029292100672]
	TIME [epoch: 2.67 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4703123278144552		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.4703123278144552 | validation: 0.5988985699154801]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48344381109510887		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.48344381109510887 | validation: 0.545263865877706]
	TIME [epoch: 2.67 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5092497294788356		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.5092497294788356 | validation: 0.49825259724373244]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49286907217642906		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.49286907217642906 | validation: 0.5376295061114232]
	TIME [epoch: 2.67 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4421651878143188		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.4421651878143188 | validation: 0.46358644221482237]
	TIME [epoch: 2.67 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43540802150823543		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.43540802150823543 | validation: 0.5825077546370506]
	TIME [epoch: 2.66 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45974863366854424		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.45974863366854424 | validation: 0.4141140367969085]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4679842881074152		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.4679842881074152 | validation: 0.5695343740262325]
	TIME [epoch: 2.67 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4480877455067763		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.4480877455067763 | validation: 0.43765478887523956]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44125733230262676		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.44125733230262676 | validation: 0.5573054602944572]
	TIME [epoch: 2.67 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45607072490930944		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.45607072490930944 | validation: 0.41797062577851263]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45673218455604697		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.45673218455604697 | validation: 0.5782892929196347]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45026370901880614		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.45026370901880614 | validation: 0.4347487673387786]
	TIME [epoch: 2.66 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4435147590534409		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.4435147590534409 | validation: 0.5667430672391102]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44219249112134135		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.44219249112134135 | validation: 0.4152491637987821]
	TIME [epoch: 2.66 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44777789643969346		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.44777789643969346 | validation: 0.5682456784001507]
	TIME [epoch: 2.66 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4429108957883597		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.4429108957883597 | validation: 0.42654316025668343]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43730567751780725		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.43730567751780725 | validation: 0.5705359645392017]
	TIME [epoch: 2.66 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44395387528411745		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.44395387528411745 | validation: 0.4197531449612877]
	TIME [epoch: 2.66 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4441042277388821		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.4441042277388821 | validation: 0.5562310532124627]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43524802718127364		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.43524802718127364 | validation: 0.43684421387376376]
	TIME [epoch: 2.66 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43686698173319727		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.43686698173319727 | validation: 0.563479847771786]
	TIME [epoch: 2.66 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44747329598894975		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.44747329598894975 | validation: 0.47059323992702995]
	TIME [epoch: 2.66 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.461229555212321		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.461229555212321 | validation: 0.5248287989678463]
	TIME [epoch: 2.66 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46829825774156464		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.46829825774156464 | validation: 0.5763951799992929]
	TIME [epoch: 2.66 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45480597114469784		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.45480597114469784 | validation: 0.4604598375005695]
	TIME [epoch: 2.67 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41942061597621944		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.41942061597621944 | validation: 0.5173339755647224]
	TIME [epoch: 2.67 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4104696193285732		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.4104696193285732 | validation: 0.42699672653252074]
	TIME [epoch: 2.67 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41892115757931225		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.41892115757931225 | validation: 0.5831731697173433]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4433457130564507		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.4433457130564507 | validation: 0.40066012641814464]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4437056993174845		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.4437056993174845 | validation: 0.5442762952854827]
	TIME [epoch: 2.67 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43172825793126834		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.43172825793126834 | validation: 0.4292249651111707]
	TIME [epoch: 2.67 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42317706910468716		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.42317706910468716 | validation: 0.5515359579273895]
	TIME [epoch: 2.67 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221462219555059		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.4221462219555059 | validation: 0.40625437063539105]
	TIME [epoch: 2.67 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43097215438489855		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.43097215438489855 | validation: 0.5533652551750944]
	TIME [epoch: 2.67 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.426794666330549		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.426794666330549 | validation: 0.40579714349982]
	TIME [epoch: 2.67 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41771792305771344		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.41771792305771344 | validation: 0.5595746796221176]
	TIME [epoch: 2.67 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4192259658875355		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.4192259658875355 | validation: 0.4074901109157299]
	TIME [epoch: 2.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42259919393246576		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.42259919393246576 | validation: 0.5446910928361094]
	TIME [epoch: 2.67 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42079150487643374		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.42079150487643374 | validation: 0.4291810169384327]
	TIME [epoch: 2.67 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42408985383072073		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.42408985383072073 | validation: 0.5392097609600672]
	TIME [epoch: 2.67 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42485589556335385		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.42485589556335385 | validation: 0.4594794806199916]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4265763958862834		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.4265763958862834 | validation: 0.5146275615582521]
	TIME [epoch: 2.67 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43601661291610483		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.43601661291610483 | validation: 0.5305426679521981]
	TIME [epoch: 2.67 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4380800602434829		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.4380800602434829 | validation: 0.45216998963867266]
	TIME [epoch: 2.67 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42218767605971996		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.42218767605971996 | validation: 0.5043381206665173]
	TIME [epoch: 2.67 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3981595047790245		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.3981595047790245 | validation: 0.45315610451213884]
	TIME [epoch: 2.66 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3894377512419624		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.3894377512419624 | validation: 0.4763690991577096]
	TIME [epoch: 2.67 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3889050849785812		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.3889050849785812 | validation: 0.42460830861101084]
	TIME [epoch: 2.66 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39598418358622256		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.39598418358622256 | validation: 0.5676041076432128]
	TIME [epoch: 2.67 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42285251873414276		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.42285251873414276 | validation: 0.39257486801813535]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4484395802579947		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.4484395802579947 | validation: 0.5341241376840667]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41348257778553804		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.41348257778553804 | validation: 0.41201622359204654]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41055534671101357		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.41055534671101357 | validation: 0.5463696287177575]
	TIME [epoch: 2.66 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4183173574567983		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.4183173574567983 | validation: 0.4207985677571551]
	TIME [epoch: 2.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4183108091661747		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.4183108091661747 | validation: 0.5270322360132512]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4051432155926391		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.4051432155926391 | validation: 0.3925440564013322]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059243127791771		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.4059243127791771 | validation: 0.5398016938031326]
	TIME [epoch: 2.67 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40637477964675783		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.40637477964675783 | validation: 0.40297846111155594]
	TIME [epoch: 2.67 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037808111802353		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.4037808111802353 | validation: 0.5325126988085276]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39480683315577153		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.39480683315577153 | validation: 0.3961924115175817]
	TIME [epoch: 2.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40228469513223525		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.40228469513223525 | validation: 0.5378075033948884]
	TIME [epoch: 2.67 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40247721541860826		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.40247721541860826 | validation: 0.4024570246590406]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40508334477943775		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.40508334477943775 | validation: 0.5361759055346178]
	TIME [epoch: 2.66 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4001398029566925		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.4001398029566925 | validation: 0.41789365230002673]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948802865549736		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.3948802865549736 | validation: 0.5062457765262653]
	TIME [epoch: 2.67 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3980735882701592		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.3980735882701592 | validation: 0.42953899004960566]
	TIME [epoch: 2.66 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4115660816680435		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.4115660816680435 | validation: 0.4887477189689851]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4045031558958473		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.4045031558958473 | validation: 0.5265147023491709]
	TIME [epoch: 2.66 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097138964718823		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.4097138964718823 | validation: 0.4524075668681611]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38743648238742423		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.38743648238742423 | validation: 0.4852924743519636]
	TIME [epoch: 2.66 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37754691013131053		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.37754691013131053 | validation: 0.4610197327239838]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3810076108248158		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.3810076108248158 | validation: 0.49421756407928763]
	TIME [epoch: 2.66 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37929412112137445		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.37929412112137445 | validation: 0.4789874262503143]
	TIME [epoch: 2.67 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38390546809069775		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.38390546809069775 | validation: 0.4442222960532831]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38963405906853665		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.38963405906853665 | validation: 0.5010270239919915]
	TIME [epoch: 2.67 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38821986651431517		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.38821986651431517 | validation: 0.3811981700119555]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_872.pth
	Model improved!!!
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4069318088861632		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.4069318088861632 | validation: 0.551501616986782]
	TIME [epoch: 2.67 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40147750957373646		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.40147750957373646 | validation: 0.4048851350753827]
	TIME [epoch: 2.66 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3837987569677245		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.3837987569677245 | validation: 0.5021587942061946]
	TIME [epoch: 2.67 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.373645335667305		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.373645335667305 | validation: 0.3942799011245337]
	TIME [epoch: 2.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3951918243084308		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.3951918243084308 | validation: 0.5178064514060913]
	TIME [epoch: 2.66 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39583783307869774		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.39583783307869774 | validation: 0.40031017492530235]
	TIME [epoch: 2.67 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3827154286282395		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.3827154286282395 | validation: 0.512482429553913]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812948371583032		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.3812948371583032 | validation: 0.38986463057285337]
	TIME [epoch: 2.67 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38410611992971516		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.38410611992971516 | validation: 0.5131891507378796]
	TIME [epoch: 2.67 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3863556069440831		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.3863556069440831 | validation: 0.39693212317093113]
	TIME [epoch: 2.66 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3761898525057881		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.3761898525057881 | validation: 0.5045319242229576]
	TIME [epoch: 2.66 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3715441258453299		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.3715441258453299 | validation: 0.391266078417359]
	TIME [epoch: 2.66 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380975479870567		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.380975479870567 | validation: 0.504607419374176]
	TIME [epoch: 2.67 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37802627710111025		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.37802627710111025 | validation: 0.39150131260614585]
	TIME [epoch: 2.67 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37519713498338897		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.37519713498338897 | validation: 0.5160860786340352]
	TIME [epoch: 2.66 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37825084547432214		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.37825084547432214 | validation: 0.38598441847899345]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3803774384862142		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.3803774384862142 | validation: 0.506945677786059]
	TIME [epoch: 2.66 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37332325777218606		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.37332325777218606 | validation: 0.39641485567184553]
	TIME [epoch: 2.66 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710514890667716		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.3710514890667716 | validation: 0.49905490806780883]
	TIME [epoch: 2.66 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3686188375436871		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.3686188375436871 | validation: 0.3987881765862536]
	TIME [epoch: 2.67 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3729080677147023		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.3729080677147023 | validation: 0.5144587855342538]
	TIME [epoch: 2.66 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37823892132858083		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.37823892132858083 | validation: 0.41282287148212726]
	TIME [epoch: 2.67 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38214105194702974		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.38214105194702974 | validation: 0.48231645834322434]
	TIME [epoch: 2.66 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3797754143986931		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.3797754143986931 | validation: 0.4838780467286006]
	TIME [epoch: 2.67 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864543604658086		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.3864543604658086 | validation: 0.44095611888444053]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3717952428945745		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.3717952428945745 | validation: 0.4759349545923714]
	TIME [epoch: 2.67 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35405193492499876		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.35405193492499876 | validation: 0.4237101331526356]
	TIME [epoch: 2.67 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.354286085346383		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.354286085346383 | validation: 0.48537501862319293]
	TIME [epoch: 2.67 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35238462014637734		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.35238462014637734 | validation: 0.3934756582578399]
	TIME [epoch: 2.67 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3660880876730292		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.3660880876730292 | validation: 0.5318413804101038]
	TIME [epoch: 2.67 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3869726929116868		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.3869726929116868 | validation: 0.39302196481356894]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.371508169644273		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.371508169644273 | validation: 0.49401403110974357]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36192442239983885		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.36192442239983885 | validation: 0.39261143192022296]
	TIME [epoch: 2.67 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587558919177471		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.3587558919177471 | validation: 0.49124611377505745]
	TIME [epoch: 2.67 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3650353433177385		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.3650353433177385 | validation: 0.3881487935579639]
	TIME [epoch: 2.67 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3593833368063181		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.3593833368063181 | validation: 0.5000418442789982]
	TIME [epoch: 2.67 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35424552539280835		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.35424552539280835 | validation: 0.38857871284451895]
	TIME [epoch: 2.66 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34852307117314935		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.34852307117314935 | validation: 0.4912844022717178]
	TIME [epoch: 2.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36265280152501317		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.36265280152501317 | validation: 0.39388119745583466]
	TIME [epoch: 2.66 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35857024430205825		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.35857024430205825 | validation: 0.4813296846022945]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35229292287481556		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.35229292287481556 | validation: 0.3975866179045454]
	TIME [epoch: 2.66 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35290107081868044		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.35290107081868044 | validation: 0.49076140194560103]
	TIME [epoch: 2.67 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35767131804449664		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.35767131804449664 | validation: 0.39091925182971826]
	TIME [epoch: 2.66 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36406168749725754		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.36406168749725754 | validation: 0.47953255751547347]
	TIME [epoch: 2.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34765787775770735		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.34765787775770735 | validation: 0.39131220082575857]
	TIME [epoch: 2.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35441737288072955		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.35441737288072955 | validation: 0.49697055902104464]
	TIME [epoch: 2.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35703125218434445		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.35703125218434445 | validation: 0.3841153865061201]
	TIME [epoch: 2.66 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35219652550285857		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.35219652550285857 | validation: 0.48673798541621927]
	TIME [epoch: 2.66 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463382122958312		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.3463382122958312 | validation: 0.38646141429567166]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3550704462932875		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.3550704462932875 | validation: 0.5052909647208355]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3493664815686428		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.3493664815686428 | validation: 0.40385977807556145]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3475916091320634		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.3475916091320634 | validation: 0.4774870962016806]
	TIME [epoch: 2.66 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3405942834033639		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.3405942834033639 | validation: 0.396707398696609]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33885108475269377		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.33885108475269377 | validation: 0.4635109347238358]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3382410237882859		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.3382410237882859 | validation: 0.444255948229247]
	TIME [epoch: 2.66 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34737612359713943		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.34737612359713943 | validation: 0.4459893238747218]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3799932866616987		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.3799932866616987 | validation: 0.48496852579540584]
	TIME [epoch: 2.66 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36065996371790787		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.36065996371790787 | validation: 0.4052970567553336]
	TIME [epoch: 2.66 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3368315573048124		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.3368315573048124 | validation: 0.48871351763315246]
	TIME [epoch: 2.67 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379118242612884		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.3379118242612884 | validation: 0.36636035339958956]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34919889975512225		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.34919889975512225 | validation: 0.47903972337003886]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34000766185331527		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.34000766185331527 | validation: 0.38706978974099693]
	TIME [epoch: 2.67 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33836491715782074		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.33836491715782074 | validation: 0.47987747232260997]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34360712134355015		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.34360712134355015 | validation: 0.3805994607414891]
	TIME [epoch: 2.67 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3400282576434253		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.3400282576434253 | validation: 0.46578293053865266]
	TIME [epoch: 2.66 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33477132918093544		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.33477132918093544 | validation: 0.38054976683966757]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33983112817815314		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.33983112817815314 | validation: 0.47951295300373414]
	TIME [epoch: 2.66 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34076378694492976		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.34076378694492976 | validation: 0.3823187809476454]
	TIME [epoch: 2.67 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3427442591369569		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.3427442591369569 | validation: 0.4755820022495647]
	TIME [epoch: 2.66 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33155048902335876		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.33155048902335876 | validation: 0.37531944838621084]
	TIME [epoch: 2.66 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354823097447021		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.3354823097447021 | validation: 0.47019780092241525]
	TIME [epoch: 2.66 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33392578104897086		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.33392578104897086 | validation: 0.37338425065539316]
	TIME [epoch: 2.66 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3322266186473998		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.3322266186473998 | validation: 0.48207093822246416]
	TIME [epoch: 2.66 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266475694597958		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.3266475694597958 | validation: 0.3728220971741113]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3373909005602389		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.3373909005602389 | validation: 0.46693072543314423]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32770436140604586		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.32770436140604586 | validation: 0.3699685407441677]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3242952365604488		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.3242952365604488 | validation: 0.47829487044262814]
	TIME [epoch: 2.66 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3312216451515838		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.3312216451515838 | validation: 0.3867263239676919]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33657417983282406		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.33657417983282406 | validation: 0.47102894788383864]
	TIME [epoch: 2.66 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3483232955438746		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.3483232955438746 | validation: 0.44509466515304913]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3496030312881021		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.3496030312881021 | validation: 0.4231026145080183]
	TIME [epoch: 2.66 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33255601513506355		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.33255601513506355 | validation: 0.43326980581074737]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155423165531087		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.3155423165531087 | validation: 0.410651754014327]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31046348470082513		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.31046348470082513 | validation: 0.41662591287743]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132697360738743		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.3132697360738743 | validation: 0.4515019226623639]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280523545371191		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.3280523545371191 | validation: 0.3723780002516006]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34361841082725375		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.34361841082725375 | validation: 0.486390832576277]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351113586970893		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.3351113586970893 | validation: 0.3779573500928424]
	TIME [epoch: 2.67 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3255459161622007		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.3255459161622007 | validation: 0.4613682040929879]
	TIME [epoch: 2.67 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3170057427765193		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.3170057427765193 | validation: 0.3679645568526373]
	TIME [epoch: 2.67 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3244377314525962		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.3244377314525962 | validation: 0.4648831441638372]
	TIME [epoch: 2.67 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32528436030008656		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.32528436030008656 | validation: 0.37858880648336773]
	TIME [epoch: 2.67 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3233478134729279		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.3233478134729279 | validation: 0.45955258973514507]
	TIME [epoch: 2.67 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32373513133109044		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.32373513133109044 | validation: 0.3784895724849977]
	TIME [epoch: 2.67 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31992978001336986		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.31992978001336986 | validation: 0.4774490664414562]
	TIME [epoch: 2.67 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32321683029862186		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.32321683029862186 | validation: 0.36497710406898737]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_968.pth
	Model improved!!!
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3166575542217819		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.3166575542217819 | validation: 0.4388656242134621]
	TIME [epoch: 2.69 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30970225141951824		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.30970225141951824 | validation: 0.37758288690937497]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32717118834371806		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.32717118834371806 | validation: 0.4658447090916269]
	TIME [epoch: 2.68 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32027660133422503		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.32027660133422503 | validation: 0.37474614004045254]
	TIME [epoch: 2.66 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31362606253481373		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.31362606253481373 | validation: 0.4601749790065617]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313315411056961		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.313315411056961 | validation: 0.3777419532446269]
	TIME [epoch: 2.66 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31446481534480014		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.31446481534480014 | validation: 0.4633883407730503]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3148672948367554		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.3148672948367554 | validation: 0.35978832117709914]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31575873021393397		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.31575873021393397 | validation: 0.46056399906300655]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30836036809044526		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.30836036809044526 | validation: 0.3671801814757557]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3109908100874895		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.3109908100874895 | validation: 0.463276080498247]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3111028701172466		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.3111028701172466 | validation: 0.3820265601565166]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3108368469002921		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.3108368469002921 | validation: 0.4665174590667608]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102996769652962		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.3102996769652962 | validation: 0.3749828533076531]
	TIME [epoch: 2.68 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31095926238554666		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.31095926238554666 | validation: 0.45065712292908877]
	TIME [epoch: 2.68 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3202675427266925		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.3202675427266925 | validation: 0.3968001586474388]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32824341856370404		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.32824341856370404 | validation: 0.4488353020744704]
	TIME [epoch: 2.68 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285169524186003		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.3285169524186003 | validation: 0.43854979529896776]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145301365563225		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.3145301365563225 | validation: 0.4021172245568032]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29910009636413926		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.29910009636413926 | validation: 0.40777764719678]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983862461133897		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.2983862461133897 | validation: 0.43215222949027976]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30047977960083744		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.30047977960083744 | validation: 0.3518122661841398]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3271853366575794		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.3271853366575794 | validation: 0.46848207833916267]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162573198023001		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.3162573198023001 | validation: 0.3757006882010044]
	TIME [epoch: 2.66 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2985038371268912		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.2985038371268912 | validation: 0.45185229238169344]
	TIME [epoch: 2.67 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29839966721891387		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.29839966721891387 | validation: 0.3645190279249551]
	TIME [epoch: 2.66 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3076076276969186		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.3076076276969186 | validation: 0.4560936593639882]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30602667345139756		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.30602667345139756 | validation: 0.3598991973711801]
	TIME [epoch: 2.66 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30246121822834116		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.30246121822834116 | validation: 0.4383546557657736]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30313312586132474		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.30313312586132474 | validation: 0.3788580854179478]
	TIME [epoch: 2.68 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30956015534110165		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.30956015534110165 | validation: 0.44358195137815803]
	TIME [epoch: 2.68 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2985649755049308		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.2985649755049308 | validation: 0.3804838308841321]
	TIME [epoch: 2.68 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.306964807904733		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.306964807904733 | validation: 0.4490221692676233]
	TIME [epoch: 178 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297920698120797		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.297920698120797 | validation: 0.3672433673794232]
	TIME [epoch: 5.74 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2993669536854763		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.2993669536854763 | validation: 0.4485826795288012]
	TIME [epoch: 5.73 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30025471621600575		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.30025471621600575 | validation: 0.3622957431440574]
	TIME [epoch: 5.73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975421242611289		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.2975421242611289 | validation: 0.45338112529567987]
	TIME [epoch: 5.74 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29752488463753424		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.29752488463753424 | validation: 0.37166126568625846]
	TIME [epoch: 5.73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297604409356195		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.297604409356195 | validation: 0.43998216499712745]
	TIME [epoch: 5.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2930355579373304		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.2930355579373304 | validation: 0.3758239880122725]
	TIME [epoch: 5.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978231172533411		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.2978231172533411 | validation: 0.43298371235978533]
	TIME [epoch: 5.73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29469797889577626		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.29469797889577626 | validation: 0.38431671235651765]
	TIME [epoch: 5.73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30346608636995875		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.30346608636995875 | validation: 0.4436824513995907]
	TIME [epoch: 5.73 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30982002299370137		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.30982002299370137 | validation: 0.41655388384759856]
	TIME [epoch: 5.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3030928923876636		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.3030928923876636 | validation: 0.39004219580723015]
	TIME [epoch: 5.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29776254156278315		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.29776254156278315 | validation: 0.41451950839318724]
	TIME [epoch: 5.73 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2841800042567633		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.2841800042567633 | validation: 0.417165150421332]
	TIME [epoch: 5.73 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2870234815655003		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.2870234815655003 | validation: 0.3859462370752851]
	TIME [epoch: 5.73 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29223713353731406		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.29223713353731406 | validation: 0.45727764226732764]
	TIME [epoch: 5.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298565812740378		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.298565812740378 | validation: 0.3580589782811302]
	TIME [epoch: 5.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30981948180342467		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.30981948180342467 | validation: 0.4457091509173159]
	TIME [epoch: 5.73 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893556111826753		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.2893556111826753 | validation: 0.3639714359528321]
	TIME [epoch: 5.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28632233285468717		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.28632233285468717 | validation: 0.44771339678438515]
	TIME [epoch: 5.73 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29203988471900133		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.29203988471900133 | validation: 0.3690789123695223]
	TIME [epoch: 5.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29468710144881916		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.29468710144881916 | validation: 0.44793503707273274]
	TIME [epoch: 5.74 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929490280730595		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.2929490280730595 | validation: 0.37545159400675443]
	TIME [epoch: 5.73 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2867779165862659		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.2867779165862659 | validation: 0.4341405877972708]
	TIME [epoch: 5.73 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29089600435794155		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.29089600435794155 | validation: 0.3740570213869422]
	TIME [epoch: 5.73 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2890924440936118		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.2890924440936118 | validation: 0.4497748362064933]
	TIME [epoch: 5.73 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2894887955607749		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.2894887955607749 | validation: 0.3617512098052882]
	TIME [epoch: 5.74 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28876511335990934		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.28876511335990934 | validation: 0.4330755212577671]
	TIME [epoch: 5.73 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278381976592767		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.278381976592767 | validation: 0.3786075235042218]
	TIME [epoch: 5.73 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27954936034060857		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.27954936034060857 | validation: 0.4419311303140779]
	TIME [epoch: 5.74 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28407060537548384		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.28407060537548384 | validation: 0.36933895746788103]
	TIME [epoch: 5.74 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28613493827073944		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.28613493827073944 | validation: 0.4295284392952973]
	TIME [epoch: 5.75 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880460697141019		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.2880460697141019 | validation: 0.37805286268642446]
	TIME [epoch: 5.74 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29249818963166474		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.29249818963166474 | validation: 0.4216536463431787]
	TIME [epoch: 5.73 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945995261250074		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.2945995261250074 | validation: 0.3941471208624201]
	TIME [epoch: 5.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28302840120420963		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.28302840120420963 | validation: 0.4087961058687423]
	TIME [epoch: 5.74 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2795122854944084		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.2795122854944084 | validation: 0.40911805918594796]
	TIME [epoch: 5.73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27802142106287947		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.27802142106287947 | validation: 0.40794326283685645]
	TIME [epoch: 5.74 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2786836686882686		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.2786836686882686 | validation: 0.4079234202839736]
	TIME [epoch: 5.73 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745424057145344		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.2745424057145344 | validation: 0.4029980340728405]
	TIME [epoch: 5.74 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2733436099614885		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.2733436099614885 | validation: 0.40119865598182713]
	TIME [epoch: 5.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2724830756834431		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.2724830756834431 | validation: 0.4291775490111258]
	TIME [epoch: 5.74 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2774429529363584		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.2774429529363584 | validation: 0.3679776037697012]
	TIME [epoch: 5.74 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886863065427756		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.2886863065427756 | validation: 0.4596586165654712]
	TIME [epoch: 5.74 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927187387511585		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.2927187387511585 | validation: 0.3475403126328255]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1046.pth
	Model improved!!!
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2795212625069029		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.2795212625069029 | validation: 0.42515834113709305]
	TIME [epoch: 5.75 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693810812246094		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.2693810812246094 | validation: 0.3723884238410693]
	TIME [epoch: 5.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2775427338936356		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.2775427338936356 | validation: 0.44076454853058744]
	TIME [epoch: 5.76 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2809439010513196		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.2809439010513196 | validation: 0.34751024155251004]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1050.pth
	Model improved!!!
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2839890060455219		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.2839890060455219 | validation: 0.4426652115750489]
	TIME [epoch: 5.74 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769657356346662		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.2769657356346662 | validation: 0.362298291674283]
	TIME [epoch: 5.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27389338963400434		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.27389338963400434 | validation: 0.4424859675360816]
	TIME [epoch: 5.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2779114806566235		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.2779114806566235 | validation: 0.36754716628739487]
	TIME [epoch: 5.74 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27683615857593513		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.27683615857593513 | validation: 0.43518696024312153]
	TIME [epoch: 5.74 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27780958894501295		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.27780958894501295 | validation: 0.3772599952030508]
	TIME [epoch: 5.73 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27916628230862633		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.27916628230862633 | validation: 0.4360841296141309]
	TIME [epoch: 5.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27747487902489554		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.27747487902489554 | validation: 0.36183528627472894]
	TIME [epoch: 5.74 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2772978520130288		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.2772978520130288 | validation: 0.4358396186611696]
	TIME [epoch: 5.74 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2801937617362643		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.2801937617362643 | validation: 0.3624153849952839]
	TIME [epoch: 5.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27268983595306945		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.27268983595306945 | validation: 0.42940968212646313]
	TIME [epoch: 5.73 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693685254981925		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.2693685254981925 | validation: 0.36747633302913796]
	TIME [epoch: 5.74 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751748071418069		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.2751748071418069 | validation: 0.43376298119878354]
	TIME [epoch: 5.74 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749697882759519		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.2749697882759519 | validation: 0.35919744071484294]
	TIME [epoch: 5.75 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743846337872714		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.2743846337872714 | validation: 0.4262325585714569]
	TIME [epoch: 5.74 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27141622249052855		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.27141622249052855 | validation: 0.3650917125629815]
	TIME [epoch: 5.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688562694405665		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.2688562694405665 | validation: 0.427993323262505]
	TIME [epoch: 5.73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26464228710901444		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.26464228710901444 | validation: 0.35538009754496436]
	TIME [epoch: 5.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667247591020105		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.2667247591020105 | validation: 0.42916027727570594]
	TIME [epoch: 5.74 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27011823710825783		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.27011823710825783 | validation: 0.3632415986483655]
	TIME [epoch: 5.74 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754007042140421		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.2754007042140421 | validation: 0.414816421866569]
	TIME [epoch: 5.74 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763952349921009		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.2763952349921009 | validation: 0.4059282213607003]
	TIME [epoch: 5.73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27396114933964083		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.27396114933964083 | validation: 0.40158579977835696]
	TIME [epoch: 5.74 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26225869402257873		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.26225869402257873 | validation: 0.40158896845496983]
	TIME [epoch: 5.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603212955511713		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.2603212955511713 | validation: 0.38479064091659865]
	TIME [epoch: 5.75 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2607682095558383		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.2607682095558383 | validation: 0.40848557243114814]
	TIME [epoch: 5.75 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2565465472923568		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.2565465472923568 | validation: 0.38290680368161656]
	TIME [epoch: 5.74 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2584279793548498		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.2584279793548498 | validation: 0.4295839245346086]
	TIME [epoch: 5.74 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26683658304551666		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.26683658304551666 | validation: 0.34495394492068476]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1079.pth
	Model improved!!!
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2771988738212264		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.2771988738212264 | validation: 0.4283566724206943]
	TIME [epoch: 5.74 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2653262288667549		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.2653262288667549 | validation: 0.3716581524029759]
	TIME [epoch: 5.73 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727885450877704		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.2727885450877704 | validation: 0.4234999181770504]
	TIME [epoch: 5.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262180612150508		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.262180612150508 | validation: 0.35752582617660383]
	TIME [epoch: 5.74 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702062118308576		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.2702062118308576 | validation: 0.4191260807023645]
	TIME [epoch: 5.73 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26236628694372494		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.26236628694372494 | validation: 0.3643352008577141]
	TIME [epoch: 5.74 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26550999492291955		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.26550999492291955 | validation: 0.41362364097407944]
	TIME [epoch: 5.73 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25760293709003085		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.25760293709003085 | validation: 0.3697494791646708]
	TIME [epoch: 5.74 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2640905493313671		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.2640905493313671 | validation: 0.4328353441106023]
	TIME [epoch: 5.73 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669384006475669		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.2669384006475669 | validation: 0.35008541030909585]
	TIME [epoch: 5.74 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26365341292903105		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.26365341292903105 | validation: 0.4150135061034268]
	TIME [epoch: 5.75 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553166373110791		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.2553166373110791 | validation: 0.3620734274253979]
	TIME [epoch: 5.75 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603093891316746		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.2603093891316746 | validation: 0.4162711198296732]
	TIME [epoch: 5.75 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628644576747118		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.2628644576747118 | validation: 0.3722419106605614]
	TIME [epoch: 5.75 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2627073448146452		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.2627073448146452 | validation: 0.4275698590261219]
	TIME [epoch: 5.75 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.261402686049116		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.261402686049116 | validation: 0.35286310346568117]
	TIME [epoch: 5.75 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667735414945065		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.2667735414945065 | validation: 0.412482803801089]
	TIME [epoch: 5.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25631164413425		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.25631164413425 | validation: 0.34233287163087117]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1097.pth
	Model improved!!!
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25515438797256546		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.25515438797256546 | validation: 0.4271063037683262]
	TIME [epoch: 5.73 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2608302271951421		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.2608302271951421 | validation: 0.3475332182967937]
	TIME [epoch: 5.73 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25885236089451413		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.25885236089451413 | validation: 0.4205047188091845]
	TIME [epoch: 5.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25664476814325166		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.25664476814325166 | validation: 0.36561152415569786]
	TIME [epoch: 5.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508339590610963		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.2508339590610963 | validation: 0.4067694421197168]
	TIME [epoch: 5.74 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25422184554582933		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.25422184554582933 | validation: 0.36502638297183815]
	TIME [epoch: 5.73 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26399544735121744		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.26399544735121744 | validation: 0.41573612212022604]
	TIME [epoch: 5.74 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2579178730066403		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.2579178730066403 | validation: 0.3707496539716564]
	TIME [epoch: 5.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564208440438466		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.2564208440438466 | validation: 0.40281598525614903]
	TIME [epoch: 5.74 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24854907177679644		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.24854907177679644 | validation: 0.37878940288877216]
	TIME [epoch: 5.73 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533011336770511		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.2533011336770511 | validation: 0.40161931466832823]
	TIME [epoch: 5.75 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553958598949652		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.2553958598949652 | validation: 0.36403270615563155]
	TIME [epoch: 5.74 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557951427755599		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.2557951427755599 | validation: 0.41542416050051173]
	TIME [epoch: 5.75 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2489853348511734		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.2489853348511734 | validation: 0.35927818796200434]
	TIME [epoch: 5.74 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2484422833962156		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.2484422833962156 | validation: 0.4279803025361066]
	TIME [epoch: 5.74 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557541216452516		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.2557541216452516 | validation: 0.3583690629229501]
	TIME [epoch: 5.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651039182489289		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.2651039182489289 | validation: 0.4218065921822568]
	TIME [epoch: 5.74 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25152647205720763		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.25152647205720763 | validation: 0.36746950721976834]
	TIME [epoch: 5.74 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24467105915617748		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.24467105915617748 | validation: 0.4117441934530942]
	TIME [epoch: 5.75 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.249938002175019		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.249938002175019 | validation: 0.3468877403434374]
	TIME [epoch: 5.74 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26299104450876337		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.26299104450876337 | validation: 0.4099078582785919]
	TIME [epoch: 5.74 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564663467536727		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.2564663467536727 | validation: 0.3638431069473588]
	TIME [epoch: 5.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2506516027815378		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.2506516027815378 | validation: 0.40591214248996704]
	TIME [epoch: 5.73 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24565243084316796		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.24565243084316796 | validation: 0.36720222172459843]
	TIME [epoch: 5.75 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2495992723659428		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.2495992723659428 | validation: 0.4286315406631662]
	TIME [epoch: 5.74 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2488934970737562		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.2488934970737562 | validation: 0.353846907702958]
	TIME [epoch: 5.75 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25205177464647827		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.25205177464647827 | validation: 0.406583453038681]
	TIME [epoch: 5.74 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25160430793791017		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.25160430793791017 | validation: 0.3555607508619792]
	TIME [epoch: 5.74 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24811452512584176		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.24811452512584176 | validation: 0.4067657932614064]
	TIME [epoch: 5.74 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2500433061590249		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.2500433061590249 | validation: 0.36213982044174386]
	TIME [epoch: 5.74 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24599539132626924		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.24599539132626924 | validation: 0.40323817564333003]
	TIME [epoch: 5.74 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531057260326896		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.2531057260326896 | validation: 0.3856003705333632]
	TIME [epoch: 5.74 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25613440713968677		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.25613440713968677 | validation: 0.405029722542809]
	TIME [epoch: 5.74 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2501229958941419		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.2501229958941419 | validation: 0.3679737696282123]
	TIME [epoch: 5.74 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2468470476309578		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.2468470476309578 | validation: 0.40809872752591575]
	TIME [epoch: 5.75 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24630839298550108		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.24630839298550108 | validation: 0.35461355200199174]
	TIME [epoch: 5.74 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24755317060402043		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.24755317060402043 | validation: 0.40278064699170835]
	TIME [epoch: 5.73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2470860268701371		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.2470860268701371 | validation: 0.361268332059235]
	TIME [epoch: 5.74 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24508533109908817		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.24508533109908817 | validation: 0.4217762872162017]
	TIME [epoch: 5.74 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2466169305202865		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.2466169305202865 | validation: 0.3467722790688151]
	TIME [epoch: 5.76 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24729099349037242		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.24729099349037242 | validation: 0.4148427418406043]
	TIME [epoch: 5.74 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24118422058570338		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.24118422058570338 | validation: 0.3666364911547918]
	TIME [epoch: 5.75 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24295800793216374		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.24295800793216374 | validation: 0.4029362737823868]
	TIME [epoch: 5.74 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24038660855364583		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.24038660855364583 | validation: 0.3682467010533863]
	TIME [epoch: 5.75 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24334005557202154		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.24334005557202154 | validation: 0.40614317593234966]
	TIME [epoch: 5.74 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24312814392606555		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.24312814392606555 | validation: 0.3568885973599243]
	TIME [epoch: 5.73 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24799469404435434		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.24799469404435434 | validation: 0.4144488684091685]
	TIME [epoch: 5.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2463208023049709		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.2463208023049709 | validation: 0.3608483137053884]
	TIME [epoch: 5.74 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24136687997814485		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.24136687997814485 | validation: 0.4066889289180587]
	TIME [epoch: 5.74 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23569254545649707		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.23569254545649707 | validation: 0.36639917183763426]
	TIME [epoch: 5.74 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2417218586705673		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.2417218586705673 | validation: 0.4141959012425021]
	TIME [epoch: 5.74 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24139279975799924		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.24139279975799924 | validation: 0.34784888679823706]
	TIME [epoch: 5.74 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24833094147277351		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.24833094147277351 | validation: 0.4007078059798758]
	TIME [epoch: 5.75 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25055789526600764		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.25055789526600764 | validation: 0.3595391033933986]
	TIME [epoch: 5.75 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24283283497301902		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.24283283497301902 | validation: 0.3966110769305416]
	TIME [epoch: 5.75 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23975141944644335		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.23975141944644335 | validation: 0.37059940395117696]
	TIME [epoch: 5.75 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23711773646011147		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.23711773646011147 | validation: 0.40441952973713075]
	TIME [epoch: 5.74 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23354960736927177		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.23354960736927177 | validation: 0.36256583424537975]
	TIME [epoch: 5.75 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23670470093533105		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.23670470093533105 | validation: 0.40006534379052683]
	TIME [epoch: 5.73 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23735936012976033		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.23735936012976033 | validation: 0.3438345576369297]
	TIME [epoch: 5.74 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24058670068680035		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.24058670068680035 | validation: 0.41520824876431234]
	TIME [epoch: 5.73 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24236022548702288		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.24236022548702288 | validation: 0.3612168265435674]
	TIME [epoch: 5.74 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23916017200073605		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.23916017200073605 | validation: 0.40826982125640277]
	TIME [epoch: 5.73 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23639466081040117		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.23639466081040117 | validation: 0.35697128935151395]
	TIME [epoch: 5.74 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2321494827176383		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.2321494827176383 | validation: 0.4028123928463881]
	TIME [epoch: 5.73 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23888998788854074		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.23888998788854074 | validation: 0.35708572831767604]
	TIME [epoch: 5.75 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24106556366241133		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.24106556366241133 | validation: 0.3962596667739233]
	TIME [epoch: 5.74 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23778882491175765		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.23778882491175765 | validation: 0.3836049451399511]
	TIME [epoch: 5.75 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23227042201180456		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.23227042201180456 | validation: 0.3809478339452709]
	TIME [epoch: 5.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23766389585462527		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.23766389585462527 | validation: 0.37674246130691]
	TIME [epoch: 5.75 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2359424522783956		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.2359424522783956 | validation: 0.39541918160472295]
	TIME [epoch: 5.74 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23316448835422754		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.23316448835422754 | validation: 0.3567768737409111]
	TIME [epoch: 5.74 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24035814197663752		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.24035814197663752 | validation: 0.4057310909288268]
	TIME [epoch: 5.73 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2402325033802682		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.2402325033802682 | validation: 0.34766433756109477]
	TIME [epoch: 5.74 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372494845085894		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.2372494845085894 | validation: 0.3973745238138424]
	TIME [epoch: 5.73 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23064688199329475		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.23064688199329475 | validation: 0.3554555406162434]
	TIME [epoch: 5.74 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23765489107104085		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.23765489107104085 | validation: 0.40409512895018707]
	TIME [epoch: 5.73 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23994941611758783		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.23994941611758783 | validation: 0.34969122023011484]
	TIME [epoch: 5.73 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23408298183207485		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.23408298183207485 | validation: 0.3907110034893633]
	TIME [epoch: 5.73 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23146524627944653		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.23146524627944653 | validation: 0.3594240155487852]
	TIME [epoch: 5.74 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23500822211354183		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.23500822211354183 | validation: 0.4117168752069336]
	TIME [epoch: 5.75 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23265349051161188		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.23265349051161188 | validation: 0.3574950677528791]
	TIME [epoch: 5.75 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2339119661764726		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.2339119661764726 | validation: 0.3976316820262433]
	TIME [epoch: 5.75 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23232582000651053		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.23232582000651053 | validation: 0.3552888660536486]
	TIME [epoch: 5.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23663366183542855		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.23663366183542855 | validation: 0.4028331363527617]
	TIME [epoch: 5.73 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22527256508930052		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.22527256508930052 | validation: 0.35990292394975043]
	TIME [epoch: 5.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2296649828410262		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.2296649828410262 | validation: 0.4027579563444696]
	TIME [epoch: 5.73 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23215014988069588		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.23215014988069588 | validation: 0.36997333781745073]
	TIME [epoch: 5.73 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23542923467545582		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.23542923467545582 | validation: 0.3962064870907982]
	TIME [epoch: 5.73 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22860504740034934		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.22860504740034934 | validation: 0.35362192037408413]
	TIME [epoch: 5.73 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2335690337083338		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.2335690337083338 | validation: 0.39449384073172733]
	TIME [epoch: 5.74 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2302186907488634		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.2302186907488634 | validation: 0.35099371469196855]
	TIME [epoch: 5.74 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2270742836260411		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.2270742836260411 | validation: 0.4089501637244312]
	TIME [epoch: 5.74 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305109361305349		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.2305109361305349 | validation: 0.3605804294490078]
	TIME [epoch: 5.75 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2275775198605603		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.2275775198605603 | validation: 0.388498334343559]
	TIME [epoch: 5.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22948175998429304		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.22948175998429304 | validation: 0.38879264311124373]
	TIME [epoch: 5.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23451008331959997		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.23451008331959997 | validation: 0.3670699940812335]
	TIME [epoch: 5.75 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23512459473562466		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.23512459473562466 | validation: 0.39762643060096137]
	TIME [epoch: 5.74 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2342695356586343		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.2342695356586343 | validation: 0.3524065420453927]
	TIME [epoch: 5.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332919878521552		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.2332919878521552 | validation: 0.3941566648172935]
	TIME [epoch: 5.74 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23084408688154198		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.23084408688154198 | validation: 0.363204824590035]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_7_v_mmd4_20250519_185624/states/model_phi1_4a_distortion_v2_7_v_mmd4_1198.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3774.250 seconds.
