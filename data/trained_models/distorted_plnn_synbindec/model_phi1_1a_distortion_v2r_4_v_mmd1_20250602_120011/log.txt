Args:
Namespace(name='model_phi1_1a_distortion_v2r_4_v_mmd1', outdir='out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_4/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v2r_4/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.028015178, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3254852618

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.738493559579679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.738493559579679 | validation: 4.3731034468553425]
	TIME [epoch: 418 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.670766055530686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.670766055530686 | validation: 3.8091971639991895]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.867743524417602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.867743524417602 | validation: 3.3137030161948005]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0607965541421054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0607965541421054 | validation: 2.4798994989278027]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.839740395888778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.839740395888778 | validation: 2.0358747423224086]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4173188114127813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4173188114127813 | validation: 1.8734450176550208]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.355327642827594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.355327642827594 | validation: 2.0006621678953813]
	TIME [epoch: 6 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.101189023180217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.101189023180217 | validation: 2.030420698753956]
	TIME [epoch: 6.01 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.159730043854684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.159730043854684 | validation: 2.106618442176091]
	TIME [epoch: 6 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.03000619059753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.03000619059753 | validation: 2.0289826623569205]
	TIME [epoch: 6 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.952040660303459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.952040660303459 | validation: 2.171756120506404]
	TIME [epoch: 6 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0388946158370906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0388946158370906 | validation: 1.955640907889616]
	TIME [epoch: 6.01 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9788599825440898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9788599825440898 | validation: 1.9280345053107335]
	TIME [epoch: 6 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8425621894261561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8425621894261561 | validation: 1.944300594211903]
	TIME [epoch: 5.99 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8661886488047599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8661886488047599 | validation: 1.9809490112538082]
	TIME [epoch: 6 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8311585244972477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8311585244972477 | validation: 1.9727333888440395]
	TIME [epoch: 6 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8737569139989412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8737569139989412 | validation: 1.913595535663258]
	TIME [epoch: 6 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8290650511423419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8290650511423419 | validation: 2.003191227359177]
	TIME [epoch: 6 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8069287233256284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8069287233256284 | validation: 1.7670358464368032]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8963597350970829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8963597350970829 | validation: 2.052562471784036]
	TIME [epoch: 6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9860079151067445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9860079151067445 | validation: 1.812799585900132]
	TIME [epoch: 5.99 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7705005150492092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7705005150492092 | validation: 1.7415979883934751]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.721513013856232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.721513013856232 | validation: 1.7147705567974714]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7668991617099066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7668991617099066 | validation: 1.7856501424171185]
	TIME [epoch: 5.99 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7515506539922872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7515506539922872 | validation: 1.7706577889041335]
	TIME [epoch: 6 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8135932255714102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8135932255714102 | validation: 1.6941739954199815]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.676636055647178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.676636055647178 | validation: 1.9477664126524177]
	TIME [epoch: 6 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7421465572697106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7421465572697106 | validation: 1.7785948153788511]
	TIME [epoch: 5.99 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.739899778369769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.739899778369769 | validation: 1.6438028939522633]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.746730365739456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.746730365739456 | validation: 1.741117521901213]
	TIME [epoch: 5.99 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7497776796877873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7497776796877873 | validation: 1.6330576740155804]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6995131591928565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6995131591928565 | validation: 1.6827739547632246]
	TIME [epoch: 6.04 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6645079601207255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6645079601207255 | validation: 1.735764151087205]
	TIME [epoch: 6.01 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6002101990009694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6002101990009694 | validation: 1.8047800263986584]
	TIME [epoch: 6 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6292011182729282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6292011182729282 | validation: 1.6105881075860915]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5626817384927691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5626817384927691 | validation: 1.5436818013178097]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.903573083661867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.903573083661867 | validation: 1.7503316762240786]
	TIME [epoch: 6 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5792609306227963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5792609306227963 | validation: 1.4310098992236981]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6453673452755737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6453673452755737 | validation: 1.5393833957136036]
	TIME [epoch: 6 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.455352988569436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.455352988569436 | validation: 1.4840646697869568]
	TIME [epoch: 6 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.377880225851656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.377880225851656 | validation: 1.9718033170841478]
	TIME [epoch: 6 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.525871461918657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.525871461918657 | validation: 1.5990128724061656]
	TIME [epoch: 6 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6275403778890007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6275403778890007 | validation: 1.5222620555338944]
	TIME [epoch: 6 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.478657575054495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.478657575054495 | validation: 1.3131843226902786]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2833761120347482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2833761120347482 | validation: 1.651879463265355]
	TIME [epoch: 6.02 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4041106971157427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4041106971157427 | validation: 1.4213443237959333]
	TIME [epoch: 6.04 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.442672334001619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.442672334001619 | validation: 1.11522877006169]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4387685165323434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4387685165323434 | validation: 1.1594250819280534]
	TIME [epoch: 6.01 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3526753828772213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3526753828772213 | validation: 1.262009775432188]
	TIME [epoch: 6.01 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3102208577608314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3102208577608314 | validation: 1.2428084057997026]
	TIME [epoch: 6 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2476272348745898		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.2476272348745898 | validation: 1.5206020698461562]
	TIME [epoch: 5.99 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4906495436638136		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.4906495436638136 | validation: 1.1251353479135089]
	TIME [epoch: 6 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.284334231119513		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.284334231119513 | validation: 1.0894606076802185]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0464064534634747		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.0464064534634747 | validation: 1.1892327745074702]
	TIME [epoch: 6.02 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.365306502026903		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.365306502026903 | validation: 1.4147493070870625]
	TIME [epoch: 5.99 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.424240733353628		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.424240733353628 | validation: 1.1168223829718542]
	TIME [epoch: 6.09 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0630583736662664		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.0630583736662664 | validation: 1.0636990717332733]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1686276701703044		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.1686276701703044 | validation: 1.3222802678209535]
	TIME [epoch: 6 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.210545364931388		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 1.210545364931388 | validation: 1.2090040630139196]
	TIME [epoch: 5.99 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0876869657406014		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.0876869657406014 | validation: 1.197700421103301]
	TIME [epoch: 6 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2388998643673428		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 1.2388998643673428 | validation: 1.022138444130404]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1563524639615774		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.1563524639615774 | validation: 1.229113515309961]
	TIME [epoch: 6.01 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1347706002536095		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.1347706002536095 | validation: 1.1106275388984765]
	TIME [epoch: 6.01 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2588332230131547		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 1.2588332230131547 | validation: 1.1857578468755157]
	TIME [epoch: 6.01 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2375431781113597		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 1.2375431781113597 | validation: 1.176790336339741]
	TIME [epoch: 6 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1841086126000673		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 1.1841086126000673 | validation: 1.0832572048621572]
	TIME [epoch: 6 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.425795531346826		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.425795531346826 | validation: 4.2053324090375925]
	TIME [epoch: 6 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7672933098580885		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.7672933098580885 | validation: 2.1525841302460504]
	TIME [epoch: 6 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.710208546618314		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 4.710208546618314 | validation: 7.710501157611333]
	TIME [epoch: 6.09 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.073940894495223		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 8.073940894495223 | validation: 8.047095114554502]
	TIME [epoch: 6 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.920032840004101		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 7.920032840004101 | validation: 8.302123050720205]
	TIME [epoch: 6.01 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.876779410756177		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 7.876779410756177 | validation: 8.378610053378935]
	TIME [epoch: 6.04 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.139881939471522		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 8.139881939471522 | validation: 7.677107980330369]
	TIME [epoch: 6 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.182242102571369		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 7.182242102571369 | validation: 7.906388620614003]
	TIME [epoch: 6 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.892400407485614		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 5.892400407485614 | validation: 3.125033464502943]
	TIME [epoch: 6 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4291045774572777		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 3.4291045774572777 | validation: 3.1733733546259018]
	TIME [epoch: 6 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2502265231150913		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 3.2502265231150913 | validation: 2.737109143707581]
	TIME [epoch: 6.01 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.870977590765546		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 2.870977590765546 | validation: 2.7118178923327916]
	TIME [epoch: 6.01 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.423924456581323		[learning rate: 0.0090317]
nan encountered in epoch 79 (validation loss).
	Learning Rate: 0.00903169
	LOSS [training: 3.423924456581323 | validation: nan]
	TIME [epoch: 6.01 sec]
EPOCH 80/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 6.18590402217813		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 6.18590402217813 | validation: 7.0280552720522795]
	TIME [epoch: 437 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.02062737936512		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 7.02062737936512 | validation: 6.960891278002455]
	TIME [epoch: 6.73 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.939375440093497		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 6.939375440093497 | validation: 6.580718623230307]
	TIME [epoch: 5.98 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.68050168191451		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 6.68050168191451 | validation: 6.278140805749384]
	TIME [epoch: 5.99 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.071798720248346		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 6.071798720248346 | validation: 5.929399859079858]
	TIME [epoch: 5.99 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.641726274415535		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 5.641726274415535 | validation: 5.338737150179522]
	TIME [epoch: 5.99 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.973929919251252		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.973929919251252 | validation: 3.1132080896600147]
	TIME [epoch: 6 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2489162941819023		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.2489162941819023 | validation: 3.9415704241383915]
	TIME [epoch: 5.99 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.008225208018654		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 4.008225208018654 | validation: 4.094523099967643]
	TIME [epoch: 5.99 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9470432003269167		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.9470432003269167 | validation: 3.1283366680642177]
	TIME [epoch: 5.99 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9880633739021327		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 2.9880633739021327 | validation: 5.4034622852795415]
	TIME [epoch: 5.98 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.489699006293977		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 3.489699006293977 | validation: 2.511630624256937]
	TIME [epoch: 6 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5723911725055197		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 2.5723911725055197 | validation: 2.467249784335542]
	TIME [epoch: 5.99 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3697915734515993		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 2.3697915734515993 | validation: 2.907626523340281]
	TIME [epoch: 5.98 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5706285701736373		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 2.5706285701736373 | validation: 3.200463632221787]
	TIME [epoch: 5.98 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6447722992990297		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 2.6447722992990297 | validation: 2.1149878926754866]
	TIME [epoch: 5.98 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1967471163946133		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 3.1967471163946133 | validation: 3.520765209734364]
	TIME [epoch: 5.98 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6519575967585833		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 2.6519575967585833 | validation: 2.2380361639297]
	TIME [epoch: 5.98 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9604017119177763		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 1.9604017119177763 | validation: 7.257474219361837]
	TIME [epoch: 5.97 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9948491131817567		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.9948491131817567 | validation: 2.4523109192349657]
	TIME [epoch: 5.98 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.379281757872243		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 2.379281757872243 | validation: 2.130039439802876]
	TIME [epoch: 5.98 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.124505704809528		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 2.124505704809528 | validation: 2.01183569229307]
	TIME [epoch: 5.99 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.230000679403293		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 2.230000679403293 | validation: 2.731961817795377]
	TIME [epoch: 5.98 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2838812729964246		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 2.2838812729964246 | validation: 2.3503145186513485]
	TIME [epoch: 5.98 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0802558024983817		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 2.0802558024983817 | validation: 1.6491602545510657]
	TIME [epoch: 5.98 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.720958347106956		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.720958347106956 | validation: 7.990999665881547]
	TIME [epoch: 6.33 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.696398588842586		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 7.696398588842586 | validation: 7.410514756198717]
	TIME [epoch: 5.98 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.566965353452493		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 5.566965353452493 | validation: 3.7731408732161364]
	TIME [epoch: 5.98 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0976754723821935		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.0976754723821935 | validation: 2.6515150024771046]
	TIME [epoch: 5.98 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2704310419981044		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 2.2704310419981044 | validation: 2.1959240144099454]
	TIME [epoch: 5.98 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0555262051559673		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 2.0555262051559673 | validation: 2.0940262260268567]
	TIME [epoch: 5.98 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23345868825781		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.23345868825781 | validation: 3.899407475046062]
	TIME [epoch: 5.99 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.526433329253121		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 3.526433329253121 | validation: 2.7914570937385164]
	TIME [epoch: 5.98 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3488438193606904		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 2.3488438193606904 | validation: 2.3875710280648796]
	TIME [epoch: 5.97 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.183120323569913		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 2.183120323569913 | validation: 2.7236660805753097]
	TIME [epoch: 5.97 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.343117895974646		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 2.343117895974646 | validation: 2.3408466455854744]
	TIME [epoch: 5.98 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1405869468248575		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 2.1405869468248575 | validation: 2.167912680122427]
	TIME [epoch: 5.98 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0014149245147816		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.0014149245147816 | validation: 4.602525187283331]
	TIME [epoch: 5.98 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.677269386303344		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 5.677269386303344 | validation: 5.55771182583406]
	TIME [epoch: 5.99 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.561120128232604		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 5.561120128232604 | validation: 8.103230542016242]
	TIME [epoch: 5.99 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.0444893912207		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 8.0444893912207 | validation: 6.9869109445796]
	TIME [epoch: 5.98 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.706524207319535		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 7.706524207319535 | validation: 5.924355575671193]
	TIME [epoch: 5.98 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.871043698936909		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 5.871043698936909 | validation: 5.367347441082466]
	TIME [epoch: 5.98 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.822321062355376		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 4.822321062355376 | validation: 3.7419452952299395]
	TIME [epoch: 5.98 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.797458226856434		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 5.797458226856434 | validation: 5.257820208398117]
	TIME [epoch: 5.99 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.269072199578694		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 5.269072199578694 | validation: 5.230788192269393]
	TIME [epoch: 5.99 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.417276259798961		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 5.417276259798961 | validation: 5.183231253324063]
	TIME [epoch: 5.99 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.895756281685042		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 4.895756281685042 | validation: 5.569773677944973]
	TIME [epoch: 5.99 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.525010623477911		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 5.525010623477911 | validation: 5.895293026254985]
	TIME [epoch: 5.98 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.97624886774016		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 6.97624886774016 | validation: 8.056377387588924]
	TIME [epoch: 5.98 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.2491819836325355		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 6.2491819836325355 | validation: 5.784555915146469]
	TIME [epoch: 5.98 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.583611313441628		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 5.583611313441628 | validation: 5.5321258125498565]
	TIME [epoch: 5.98 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.529963447587018		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 5.529963447587018 | validation: 5.302140481586459]
	TIME [epoch: 5.98 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.243169063279287		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 5.243169063279287 | validation: 4.371076709264139]
	TIME [epoch: 5.98 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.451082244491334		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 4.451082244491334 | validation: 4.699535394969983]
	TIME [epoch: 5.99 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.808177133296438		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 4.808177133296438 | validation: 4.582735737290413]
	TIME [epoch: 5.99 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.54031627694086		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 5.54031627694086 | validation: 5.3374011581780705]
	TIME [epoch: 5.98 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.0786154689513765		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 6.0786154689513765 | validation: 8.221776747925734]
	TIME [epoch: 5.98 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.155022819232723		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 7.155022819232723 | validation: 4.7297770510347155]
	TIME [epoch: 5.98 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.820118994772223		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 4.820118994772223 | validation: 4.630607322172398]
	TIME [epoch: 5.99 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.39376667065991		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 4.39376667065991 | validation: 3.9494933773039476]
	TIME [epoch: 5.99 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.645423855590144		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 4.645423855590144 | validation: 5.206391308613464]
	TIME [epoch: 5.98 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.305675464195694		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 5.305675464195694 | validation: 6.089854425421048]
	TIME [epoch: 5.98 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.486805726323547		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 5.486805726323547 | validation: 5.210228199312886]
	TIME [epoch: 6.25 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.064336263016087		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 5.064336263016087 | validation: 5.206318783163553]
	TIME [epoch: 5.99 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.205115119311328		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 5.205115119311328 | validation: 5.3180446148047285]
	TIME [epoch: 5.98 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.900268646235386		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 5.900268646235386 | validation: 5.473707058647934]
	TIME [epoch: 5.98 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.704115075671883		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 5.704115075671883 | validation: 5.306465993421464]
	TIME [epoch: 5.99 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.281663378363924		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 5.281663378363924 | validation: 5.296020785308968]
	TIME [epoch: 5.99 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.274961730053057		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 5.274961730053057 | validation: 5.267992936381161]
	TIME [epoch: 5.99 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1993307214294955		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 5.1993307214294955 | validation: 5.356501700933593]
	TIME [epoch: 5.99 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2298936411720724		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 5.2298936411720724 | validation: 5.138005359085118]
	TIME [epoch: 5.98 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.40894796185809		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 5.40894796185809 | validation: 5.740518268442212]
	TIME [epoch: 5.98 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.681260880786148		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 4.681260880786148 | validation: 4.288449122818093]
	TIME [epoch: 5.99 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.461653454415447		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 4.461653454415447 | validation: 5.206297443118203]
	TIME [epoch: 5.98 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.430109193101339		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 5.430109193101339 | validation: 5.074744351145213]
	TIME [epoch: 5.99 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.187509074114482		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 5.187509074114482 | validation: 5.8085811987645055]
	TIME [epoch: 5.98 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.482670300837515		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 6.482670300837515 | validation: 7.2943934771785575]
	TIME [epoch: 5.98 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.218179428649235		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 6.218179428649235 | validation: 5.427626291360157]
	TIME [epoch: 6.1 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.464272198536754		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 4.464272198536754 | validation: 4.797502772048226]
	TIME [epoch: 5.99 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.306669294454038		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 4.306669294454038 | validation: 4.178759487677597]
	TIME [epoch: 5.98 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.491879216810597		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 4.491879216810597 | validation: 5.066912320742466]
	TIME [epoch: 5.98 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.129010473834912		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 5.129010473834912 | validation: 5.362589893277542]
	TIME [epoch: 5.98 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.557415679753858		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 5.557415679753858 | validation: 5.84118346867467]
	TIME [epoch: 6.09 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.111069691398871		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 6.111069691398871 | validation: 5.427237620760476]
	TIME [epoch: 5.98 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.3967747686490135		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 5.3967747686490135 | validation: 5.259881938648977]
	TIME [epoch: 5.99 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.355504529577901		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 5.355504529577901 | validation: 5.073120808446198]
	TIME [epoch: 5.98 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.939885583741118		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 4.939885583741118 | validation: 4.947444737965439]
	TIME [epoch: 5.99 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.94022812326646		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 4.94022812326646 | validation: 4.979021965922462]
	TIME [epoch: 5.99 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917410412088563		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 4.917410412088563 | validation: 5.1512072451538184]
	TIME [epoch: 5.99 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.984579311615895		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 4.984579311615895 | validation: 5.1247112209563195]
	TIME [epoch: 5.99 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.28049022494522		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 5.28049022494522 | validation: 5.458237302333927]
	TIME [epoch: 5.99 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.468870644682583		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 5.468870644682583 | validation: 5.550020657138117]
	TIME [epoch: 6 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.40683728684886		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 5.40683728684886 | validation: 5.299894710555305]
	TIME [epoch: 6 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1170209289872925		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 5.1170209289872925 | validation: 4.767385556786655]
	TIME [epoch: 5.99 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.69381928268184		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 4.69381928268184 | validation: 4.865978258126111]
	TIME [epoch: 5.99 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.659431212648172		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 4.659431212648172 | validation: 4.754660570708127]
	TIME [epoch: 5.99 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.818987492011703		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 4.818987492011703 | validation: 4.964398529231811]
	TIME [epoch: 5.98 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063026635134171		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 5.063026635134171 | validation: 5.179437252231241]
	TIME [epoch: 5.99 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.042045393778411		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 5.042045393778411 | validation: 5.231377070354995]
	TIME [epoch: 5.99 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.286441352217352		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 5.286441352217352 | validation: 5.499399353777587]
	TIME [epoch: 5.99 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.427606421742723		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 5.427606421742723 | validation: 5.551623867057971]
	TIME [epoch: 6 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.314293650566782		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 5.314293650566782 | validation: 5.171930729914585]
	TIME [epoch: 6 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.937139674113327		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 4.937139674113327 | validation: 5.397544152282077]
	TIME [epoch: 5.99 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.875522998340912		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 6.875522998340912 | validation: 8.326284748399562]
	TIME [epoch: 5.99 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.28594207765275		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 8.28594207765275 | validation: 8.17998711450159]
	TIME [epoch: 5.98 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.184395226174829		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 8.184395226174829 | validation: 8.13556409567068]
	TIME [epoch: 6 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.119884050530366		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 8.119884050530366 | validation: 8.166649197850731]
	TIME [epoch: 5.99 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.004617872313558		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 8.004617872313558 | validation: 8.15166911498584]
	TIME [epoch: 5.99 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.046957712532798		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 8.046957712532798 | validation: 5.631828852479693]
	TIME [epoch: 5.99 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.348587163231821		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 5.348587163231821 | validation: 5.0664711590976275]
	TIME [epoch: 5.99 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.386532300709611		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 5.386532300709611 | validation: 4.941855814594385]
	TIME [epoch: 6 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.179337167920469		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 5.179337167920469 | validation: 5.735773782300985]
	TIME [epoch: 5.99 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9111620141052486		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 4.9111620141052486 | validation: 4.017745753563107]
	TIME [epoch: 5.98 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.491431621214934		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 3.491431621214934 | validation: 3.640667322120011]
	TIME [epoch: 5.99 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0603234556965426		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 3.0603234556965426 | validation: 3.144784945474284]
	TIME [epoch: 5.99 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.637811727045203		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 2.637811727045203 | validation: 2.95582658783492]
	TIME [epoch: 5.99 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5342614886214454		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 2.5342614886214454 | validation: 2.619731830354912]
	TIME [epoch: 5.99 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3321493546901757		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 2.3321493546901757 | validation: 2.313996432275493]
	TIME [epoch: 5.99 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2469776228578655		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 2.2469776228578655 | validation: 2.6868130519790006]
	TIME [epoch: 5.99 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5279601683422612		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 2.5279601683422612 | validation: 3.1878229481086127]
	TIME [epoch: 5.99 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0150768547221496		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 3.0150768547221496 | validation: 3.0797843092441792]
	TIME [epoch: 445 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5626183582272		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 2.5626183582272 | validation: 2.601885666411074]
	TIME [epoch: 11.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3326894349116145		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 2.3326894349116145 | validation: 2.425778776892276]
	TIME [epoch: 11.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.243909630269628		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 2.243909630269628 | validation: 2.327099155832331]
	TIME [epoch: 11.8 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.153620325194594		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 2.153620325194594 | validation: 2.257719635603026]
	TIME [epoch: 11.8 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.617853099369072		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 2.617853099369072 | validation: 2.800924299983959]
	TIME [epoch: 11.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.75905295304598		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 2.75905295304598 | validation: 2.219188750684366]
	TIME [epoch: 11.8 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0310193786205324		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 2.0310193786205324 | validation: 2.0065643455094833]
	TIME [epoch: 11.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8902272028643292		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 1.8902272028643292 | validation: 1.9247115023299488]
	TIME [epoch: 11.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8190424331746597		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 1.8190424331746597 | validation: 1.8361348594432485]
	TIME [epoch: 11.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7697759965302466		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 1.7697759965302466 | validation: 1.8585073819237348]
	TIME [epoch: 11.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.722671684281518		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 1.722671684281518 | validation: 1.8143232308599968]
	TIME [epoch: 11.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6979367508565502		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 1.6979367508565502 | validation: 1.7420556355374728]
	TIME [epoch: 11.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6568341793196069		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 1.6568341793196069 | validation: 1.7240519432342365]
	TIME [epoch: 11.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6337693794028034		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 1.6337693794028034 | validation: 1.57204439572513]
	TIME [epoch: 11.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5822958519596213		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 1.5822958519596213 | validation: 1.8985446644662896]
	TIME [epoch: 11.8 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.752635859006035		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 1.752635859006035 | validation: 1.5009404775293262]
	TIME [epoch: 11.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5090056897525084		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 1.5090056897525084 | validation: 1.477603005102002]
	TIME [epoch: 11.8 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.472224832640678		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 1.472224832640678 | validation: 1.4934440723650453]
	TIME [epoch: 11.8 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4509155305949266		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 1.4509155305949266 | validation: 1.578353566790768]
	TIME [epoch: 11.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4672995191035434		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 1.4672995191035434 | validation: 1.4849183351774193]
	TIME [epoch: 11.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3807776089574613		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 1.3807776089574613 | validation: 1.5395886314206337]
	TIME [epoch: 11.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3837834377955254		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 1.3837834377955254 | validation: 1.383725374369193]
	TIME [epoch: 11.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3361338892862493		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 1.3361338892862493 | validation: 1.7308681764563965]
	TIME [epoch: 11.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4062833245594348		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 1.4062833245594348 | validation: 1.3512912202775471]
	TIME [epoch: 11.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.357924545289239		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 1.357924545289239 | validation: 1.475095698572037]
	TIME [epoch: 11.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4907366700897517		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 1.4907366700897517 | validation: 1.450579398213155]
	TIME [epoch: 11.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.431700250911708		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 1.431700250911708 | validation: 1.2098119662068962]
	TIME [epoch: 11.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2578257102366095		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 1.2578257102366095 | validation: 1.1953764879977646]
	TIME [epoch: 11.8 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2003471279697848		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 1.2003471279697848 | validation: 1.1931006827268937]
	TIME [epoch: 11.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2850939994032267		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 1.2850939994032267 | validation: 1.1586453522813205]
	TIME [epoch: 11.8 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1854877823376617		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 1.1854877823376617 | validation: 1.5497171612642278]
	TIME [epoch: 12.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2645845888489686		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 1.2645845888489686 | validation: 1.1639655223135046]
	TIME [epoch: 11.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2381640862226404		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 1.2381640862226404 | validation: 1.2602729468597005]
	TIME [epoch: 11.8 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1762916720393157		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 1.1762916720393157 | validation: 1.412257214592323]
	TIME [epoch: 11.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2712706128806404		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 1.2712706128806404 | validation: 1.0965139368223493]
	TIME [epoch: 11.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1384998098818233		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 1.1384998098818233 | validation: 1.0966534535404837]
	TIME [epoch: 11.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0986688449564803		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 1.0986688449564803 | validation: 1.3598910298635611]
	TIME [epoch: 11.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.182610699156946		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 1.182610699156946 | validation: 1.0718787655940338]
	TIME [epoch: 11.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1319259969542197		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 1.1319259969542197 | validation: 1.2594539649128373]
	TIME [epoch: 11.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1549978714620472		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 1.1549978714620472 | validation: 1.0582631627273487]
	TIME [epoch: 11.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1431190697489102		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 1.1431190697489102 | validation: 1.190789991054916]
	TIME [epoch: 11.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0907587727475923		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 1.0907587727475923 | validation: 1.1367242927263934]
	TIME [epoch: 11.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0715121974102748		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 1.0715121974102748 | validation: 1.2877402235666984]
	TIME [epoch: 11.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0985562847081205		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 1.0985562847081205 | validation: 1.1300871570728348]
	TIME [epoch: 11.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0733263348193116		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 1.0733263348193116 | validation: 1.262141991961095]
	TIME [epoch: 11.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1627085314876857		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 1.1627085314876857 | validation: 0.9818931678516896]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0264454108790502		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 1.0264454108790502 | validation: 1.2863351825326168]
	TIME [epoch: 11.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0802446431431836		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 1.0802446431431836 | validation: 1.1503785494355678]
	TIME [epoch: 11.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0723933278577817		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 1.0723933278577817 | validation: 1.0852872273242664]
	TIME [epoch: 11.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0809161645724343		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 1.0809161645724343 | validation: 1.1055430603722436]
	TIME [epoch: 11.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0254070726649476		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 1.0254070726649476 | validation: 1.0678566358933157]
	TIME [epoch: 11.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9973618019959267		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.9973618019959267 | validation: 0.9462211757504653]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0295995204981168		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 1.0295995204981168 | validation: 0.9975417436238679]
	TIME [epoch: 11.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9844990371066568		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.9844990371066568 | validation: 0.9373464427462948]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9734837407812054		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.9734837407812054 | validation: 0.9563559249829772]
	TIME [epoch: 11.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9689744056599291		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.9689744056599291 | validation: 0.9856036112406079]
	TIME [epoch: 11.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9402264881193874		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.9402264881193874 | validation: 0.922742457820237]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9729360795675048		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.9729360795675048 | validation: 1.2663656163672747]
	TIME [epoch: 11.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.146093267970838		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 1.146093267970838 | validation: 1.1987526114177878]
	TIME [epoch: 11.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9883268042323067		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.9883268042323067 | validation: 1.1476662944952096]
	TIME [epoch: 11.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0112509728933412		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 1.0112509728933412 | validation: 1.143063496212775]
	TIME [epoch: 11.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0001067655566571		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 1.0001067655566571 | validation: 0.9914332625786992]
	TIME [epoch: 11.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9563814349142141		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.9563814349142141 | validation: 0.9955485956399197]
	TIME [epoch: 11.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9466394960670879		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.9466394960670879 | validation: 1.2847439736749506]
	TIME [epoch: 11.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1108705773688132		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 1.1108705773688132 | validation: 1.0138518507270948]
	TIME [epoch: 11.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0622537724596373		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 1.0622537724596373 | validation: 1.0364653594091853]
	TIME [epoch: 11.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9748220757969328		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.9748220757969328 | validation: 0.875014015822039]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.941505119410974		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.941505119410974 | validation: 0.9309559217974412]
	TIME [epoch: 11.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9323614030933782		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.9323614030933782 | validation: 0.9864660586466221]
	TIME [epoch: 11.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9488119687864349		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.9488119687864349 | validation: 1.0792994068406092]
	TIME [epoch: 11.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9491694319799735		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.9491694319799735 | validation: 1.0666398874917382]
	TIME [epoch: 11.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9291693838640719		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.9291693838640719 | validation: 1.0722507041965676]
	TIME [epoch: 11.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9418204056969455		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.9418204056969455 | validation: 1.0985551992340115]
	TIME [epoch: 11.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9592311507644553		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.9592311507644553 | validation: 1.0362982629030513]
	TIME [epoch: 11.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9132917603185258		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.9132917603185258 | validation: 1.0390384845559328]
	TIME [epoch: 11.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9063528854603808		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.9063528854603808 | validation: 1.0298725944815805]
	TIME [epoch: 11.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0077291000320712		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 1.0077291000320712 | validation: 0.971218180815878]
	TIME [epoch: 11.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9358705094505271		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.9358705094505271 | validation: 0.8884987765886854]
	TIME [epoch: 11.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9019559789271325		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.9019559789271325 | validation: 1.0108476424719475]
	TIME [epoch: 11.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9128859871566337		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.9128859871566337 | validation: 0.9335088133478981]
	TIME [epoch: 11.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9328291168136186		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.9328291168136186 | validation: 0.9327187498859837]
	TIME [epoch: 11.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.885606143177732		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.885606143177732 | validation: 1.0024929964604852]
	TIME [epoch: 11.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9018880076800456		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.9018880076800456 | validation: 0.9637919098192471]
	TIME [epoch: 11.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9503967185931019		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.9503967185931019 | validation: 0.868273166153274]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8804579751771968		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.8804579751771968 | validation: 0.9765558579464368]
	TIME [epoch: 11.8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9457256863924172		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.9457256863924172 | validation: 0.8848131423297945]
	TIME [epoch: 11.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9192291278878341		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.9192291278878341 | validation: 0.9774763256470143]
	TIME [epoch: 11.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9232584010082094		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.9232584010082094 | validation: 0.8703476477844443]
	TIME [epoch: 11.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9074333819047387		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.9074333819047387 | validation: 0.9455123815404419]
	TIME [epoch: 11.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9307653098568689		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.9307653098568689 | validation: 0.9584126351004425]
	TIME [epoch: 11.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8734549458976598		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.8734549458976598 | validation: 0.8640492810588657]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8856988642786816		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.8856988642786816 | validation: 0.8750224649862188]
	TIME [epoch: 11.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0307977185452863		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 1.0307977185452863 | validation: 0.9606489381849163]
	TIME [epoch: 11.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8855955833360325		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.8855955833360325 | validation: 0.9179012981028246]
	TIME [epoch: 11.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8891574163030358		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.8891574163030358 | validation: 0.8979918099778981]
	TIME [epoch: 11.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8696285000260101		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.8696285000260101 | validation: 0.8756159508517455]
	TIME [epoch: 11.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.854774231131998		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.854774231131998 | validation: 0.9303204488047019]
	TIME [epoch: 11.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8624826219060068		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.8624826219060068 | validation: 0.8235789528148814]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8733834112196059		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.8733834112196059 | validation: 1.365683053151185]
	TIME [epoch: 11.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0011297312845298		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 1.0011297312845298 | validation: 0.8640256141567958]
	TIME [epoch: 11.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8634619175106962		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.8634619175106962 | validation: 0.9110013122096636]
	TIME [epoch: 11.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8755542894363859		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.8755542894363859 | validation: 1.1299307145882413]
	TIME [epoch: 11.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9382411507524787		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.9382411507524787 | validation: 0.9018052241006962]
	TIME [epoch: 11.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8680214237796451		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.8680214237796451 | validation: 0.82111027645929]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8478957786086897		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.8478957786086897 | validation: 0.8839392110188784]
	TIME [epoch: 11.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8721573782942519		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.8721573782942519 | validation: 1.170410282389887]
	TIME [epoch: 11.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9290836579333619		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.9290836579333619 | validation: 1.1663765304864349]
	TIME [epoch: 11.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9450207509350952		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.9450207509350952 | validation: 0.936110183258958]
	TIME [epoch: 11.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8776682146573395		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.8776682146573395 | validation: 0.9000216545228228]
	TIME [epoch: 11.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8469986364632762		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.8469986364632762 | validation: 0.886452918571673]
	TIME [epoch: 11.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8707236778199352		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.8707236778199352 | validation: 0.8901655722681384]
	TIME [epoch: 11.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9429594069837387		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.9429594069837387 | validation: 0.859758710292463]
	TIME [epoch: 11.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8302021282662855		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.8302021282662855 | validation: 0.8820936340033705]
	TIME [epoch: 11.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8618750216910541		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.8618750216910541 | validation: 0.8948189290508166]
	TIME [epoch: 11.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8327403159120523		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.8327403159120523 | validation: 0.8533820109897131]
	TIME [epoch: 11.8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8610826879436164		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.8610826879436164 | validation: 0.9537419690984419]
	TIME [epoch: 11.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8595504459334541		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.8595504459334541 | validation: 0.9254601299985818]
	TIME [epoch: 11.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8900975717791		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.8900975717791 | validation: 0.8783913675295771]
	TIME [epoch: 11.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8486712772816385		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.8486712772816385 | validation: 0.8745310817140899]
	TIME [epoch: 11.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8224635479588014		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.8224635479588014 | validation: 0.8736149088342194]
	TIME [epoch: 11.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8489589569873882		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.8489589569873882 | validation: 0.8267476031217049]
	TIME [epoch: 11.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.82735499165502		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.82735499165502 | validation: 0.8494393601366586]
	TIME [epoch: 11.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8611576272017494		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.8611576272017494 | validation: 0.8237425247485546]
	TIME [epoch: 11.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8663699032346353		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.8663699032346353 | validation: 0.9974978558842845]
	TIME [epoch: 11.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8420235830616141		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.8420235830616141 | validation: 0.8345467420611721]
	TIME [epoch: 11.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8129458005001831		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.8129458005001831 | validation: 0.8633981985665482]
	TIME [epoch: 11.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8518735090003069		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.8518735090003069 | validation: 0.8085300376930182]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8072485146154291		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.8072485146154291 | validation: 0.9488062883051946]
	TIME [epoch: 11.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8669443796511049		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.8669443796511049 | validation: 0.8299525886822092]
	TIME [epoch: 11.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8229806011346072		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.8229806011346072 | validation: 0.8158171805046739]
	TIME [epoch: 11.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8318197943088584		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.8318197943088584 | validation: 0.8353090408802167]
	TIME [epoch: 11.8 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8229200463320178		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.8229200463320178 | validation: 0.8010064198116513]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.827635453081883		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.827635453081883 | validation: 0.8487716565191188]
	TIME [epoch: 11.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8185796620130872		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.8185796620130872 | validation: 0.8018599331758769]
	TIME [epoch: 11.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8607068658918865		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.8607068658918865 | validation: 0.8714094320874473]
	TIME [epoch: 11.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.820879208857829		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.820879208857829 | validation: 1.0101974252192707]
	TIME [epoch: 11.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8524236123007908		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.8524236123007908 | validation: 0.8415522792949004]
	TIME [epoch: 11.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8291355780909881		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.8291355780909881 | validation: 0.7930006629736575]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8025256023678257		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.8025256023678257 | validation: 0.9485533721384403]
	TIME [epoch: 11.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8269258748155408		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.8269258748155408 | validation: 0.8380726600080273]
	TIME [epoch: 11.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8150099910313975		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.8150099910313975 | validation: 0.8595443740713841]
	TIME [epoch: 11.8 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3400726740433966		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 1.3400726740433966 | validation: 1.0999675882753226]
	TIME [epoch: 11.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9428092298041411		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.9428092298041411 | validation: 0.9065952939422831]
	TIME [epoch: 11.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0627010770192107		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 1.0627010770192107 | validation: 1.4797753329308836]
	TIME [epoch: 11.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0552090224948354		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 1.0552090224948354 | validation: 1.0190448851121274]
	TIME [epoch: 11.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8494978677982274		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.8494978677982274 | validation: 0.7829311855793689]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8290761074121086		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.8290761074121086 | validation: 0.7595750738256086]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8203209471080989		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.8203209471080989 | validation: 0.756816530851449]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8376336116879797		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.8376336116879797 | validation: 0.7788576659623263]
	TIME [epoch: 11.8 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7991083303148923		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.7991083303148923 | validation: 0.8105342208388098]
	TIME [epoch: 11.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8177483398586529		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.8177483398586529 | validation: 0.8085718941954488]
	TIME [epoch: 11.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7977198457282813		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.7977198457282813 | validation: 0.8194603107555176]
	TIME [epoch: 11.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7853448843374737		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.7853448843374737 | validation: 0.8208013481570454]
	TIME [epoch: 11.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8199867352691734		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.8199867352691734 | validation: 0.8002239920791758]
	TIME [epoch: 11.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7912959181288597		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.7912959181288597 | validation: 0.7925448365851839]
	TIME [epoch: 11.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8038705844619617		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.8038705844619617 | validation: 0.7953639152197387]
	TIME [epoch: 11.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7942042539506742		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.7942042539506742 | validation: 0.8405019471276067]
	TIME [epoch: 11.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8424158552287924		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.8424158552287924 | validation: 0.8545045918161172]
	TIME [epoch: 11.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.812523443036734		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.812523443036734 | validation: 0.8484641800246251]
	TIME [epoch: 11.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7896853018715152		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.7896853018715152 | validation: 0.8538517298494483]
	TIME [epoch: 11.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8306294704433733		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.8306294704433733 | validation: 0.823134660819923]
	TIME [epoch: 11.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.776079505383633		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.776079505383633 | validation: 0.7408967300348076]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7931740218958367		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.7931740218958367 | validation: 0.7977943452405403]
	TIME [epoch: 11.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7920812405095727		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.7920812405095727 | validation: 0.790084909897369]
	TIME [epoch: 11.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7854494511163894		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.7854494511163894 | validation: 0.8161932926223759]
	TIME [epoch: 11.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7978057246047384		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.7978057246047384 | validation: 1.0993970760317657]
	TIME [epoch: 11.8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8964764729378613		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.8964764729378613 | validation: 0.9871236857034613]
	TIME [epoch: 11.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8162725756673532		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.8162725756673532 | validation: 0.7621279538853881]
	TIME [epoch: 11.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7805894603261169		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.7805894603261169 | validation: 0.7707355252245227]
	TIME [epoch: 11.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7606281335407297		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.7606281335407297 | validation: 0.7591253227386533]
	TIME [epoch: 11.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8311693883240199		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.8311693883240199 | validation: 0.7455803080950494]
	TIME [epoch: 11.8 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8073497671350668		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.8073497671350668 | validation: 0.7955603780320104]
	TIME [epoch: 11.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8058716637230061		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.8058716637230061 | validation: 0.7801921050738576]
	TIME [epoch: 11.8 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.807596062613025		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.807596062613025 | validation: 0.7803031416660153]
	TIME [epoch: 11.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7914120296601551		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.7914120296601551 | validation: 0.8307916904775985]
	TIME [epoch: 11.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7732553006661886		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.7732553006661886 | validation: 0.7808693057853822]
	TIME [epoch: 11.8 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8049763798269175		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.8049763798269175 | validation: 0.7820120797767693]
	TIME [epoch: 11.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8058255464695394		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.8058255464695394 | validation: 0.7223146520435529]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.770709631845357		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.770709631845357 | validation: 0.8272063979321209]
	TIME [epoch: 11.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.776907068625777		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.776907068625777 | validation: 0.7993784038606138]
	TIME [epoch: 11.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7700382094844285		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.7700382094844285 | validation: 0.7821922319825879]
	TIME [epoch: 11.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7778117264411988		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.7778117264411988 | validation: 0.8088809945003334]
	TIME [epoch: 11.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.782700231643106		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.782700231643106 | validation: 0.7877229828315564]
	TIME [epoch: 11.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7772409162062712		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.7772409162062712 | validation: 0.8757177417958002]
	TIME [epoch: 11.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7938406438101576		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.7938406438101576 | validation: 0.7508870903013087]
	TIME [epoch: 11.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.774926891115439		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.774926891115439 | validation: 0.7665508571894498]
	TIME [epoch: 11.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7934746176954519		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.7934746176954519 | validation: 0.7915915300682332]
	TIME [epoch: 11.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7721051366928057		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.7721051366928057 | validation: 0.7190652756282487]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7821529117884793		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.7821529117884793 | validation: 0.8315750467186432]
	TIME [epoch: 11.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7733286949691369		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.7733286949691369 | validation: 0.7555324938340053]
	TIME [epoch: 11.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7689025187928292		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.7689025187928292 | validation: 0.7769919965810086]
	TIME [epoch: 11.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.76620618880808		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.76620618880808 | validation: 0.8050724849468434]
	TIME [epoch: 11.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8041798220519445		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.8041798220519445 | validation: 0.8160908975310162]
	TIME [epoch: 11.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7953920002035205		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.7953920002035205 | validation: 0.8286207327435628]
	TIME [epoch: 11.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7841067923159711		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.7841067923159711 | validation: 0.7562757098105879]
	TIME [epoch: 11.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7465961672249758		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.7465961672249758 | validation: 0.7966834745982403]
	TIME [epoch: 11.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7783667853012911		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.7783667853012911 | validation: 0.7865002874785763]
	TIME [epoch: 11.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7624446845454329		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.7624446845454329 | validation: 0.7267537166284386]
	TIME [epoch: 11.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.757096225548219		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.757096225548219 | validation: 0.7668828345147749]
	TIME [epoch: 11.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7562357010271408		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.7562357010271408 | validation: 0.7817904457030567]
	TIME [epoch: 11.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7907048515924733		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.7907048515924733 | validation: 0.7681222471192156]
	TIME [epoch: 11.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7695299588319922		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.7695299588319922 | validation: 0.7867030591526409]
	TIME [epoch: 11.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7679904299405803		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.7679904299405803 | validation: 0.8120325895425767]
	TIME [epoch: 11.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7749112079280889		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.7749112079280889 | validation: 0.7777539104769229]
	TIME [epoch: 11.8 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7486048562065963		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.7486048562065963 | validation: 0.7569118615557211]
	TIME [epoch: 11.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7755849481640481		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.7755849481640481 | validation: 0.754664646658149]
	TIME [epoch: 11.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.754579637974766		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.754579637974766 | validation: 0.7535662156085343]
	TIME [epoch: 11.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7807357674756246		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.7807357674756246 | validation: 0.7713993889368242]
	TIME [epoch: 11.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7588704848911354		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.7588704848911354 | validation: 0.7675922450233243]
	TIME [epoch: 11.8 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7432960031375281		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.7432960031375281 | validation: 0.7452729804109859]
	TIME [epoch: 11.8 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7575275468984531		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.7575275468984531 | validation: 0.7710660984733019]
	TIME [epoch: 11.8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8153763074251799		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.8153763074251799 | validation: 0.7620786467383459]
	TIME [epoch: 11.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7485158266645991		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.7485158266645991 | validation: 0.7417866153413515]
	TIME [epoch: 11.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7383741941531349		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.7383741941531349 | validation: 0.7581100186930773]
	TIME [epoch: 11.8 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7741352840360098		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.7741352840360098 | validation: 0.7744162670846246]
	TIME [epoch: 11.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7351160702062103		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.7351160702062103 | validation: 0.7949093659179759]
	TIME [epoch: 11.8 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7438746114268959		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.7438746114268959 | validation: 0.8417461383173153]
	TIME [epoch: 11.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7871779829740962		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.7871779829740962 | validation: 0.7246894400903463]
	TIME [epoch: 11.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7682044096223745		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.7682044096223745 | validation: 0.7347654615841362]
	TIME [epoch: 11.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7795828365253774		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.7795828365253774 | validation: 0.7444806104151007]
	TIME [epoch: 11.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7449532996238264		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.7449532996238264 | validation: 0.8047045671227655]
	TIME [epoch: 11.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7647182286303232		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.7647182286303232 | validation: 0.7996174895505336]
	TIME [epoch: 11.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7682934328940527		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.7682934328940527 | validation: 0.82087098888486]
	TIME [epoch: 11.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7488270589261519		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.7488270589261519 | validation: 0.7397973662345827]
	TIME [epoch: 11.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.748304784263033		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.748304784263033 | validation: 0.7675153482171506]
	TIME [epoch: 11.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7407966270108693		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.7407966270108693 | validation: 0.7716397671787452]
	TIME [epoch: 11.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7574508673862846		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.7574508673862846 | validation: 0.7053663814952436]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7368617550877844		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.7368617550877844 | validation: 0.9156776723066231]
	TIME [epoch: 11.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7981470258792294		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.7981470258792294 | validation: 0.8524489907015504]
	TIME [epoch: 11.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7690112898216785		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.7690112898216785 | validation: 0.8169617052023646]
	TIME [epoch: 11.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7618491219267169		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.7618491219267169 | validation: 0.8030002055654492]
	TIME [epoch: 11.8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7481163057753668		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.7481163057753668 | validation: 0.7333244102052665]
	TIME [epoch: 11.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7367316400954401		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.7367316400954401 | validation: 0.7498034392567294]
	TIME [epoch: 11.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7578796911364319		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.7578796911364319 | validation: 0.768823354710072]
	TIME [epoch: 11.8 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7380797351756001		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.7380797351756001 | validation: 0.7141461907460098]
	TIME [epoch: 11.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7337212564543691		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.7337212564543691 | validation: 0.7314781327913485]
	TIME [epoch: 11.8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7521476083434823		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.7521476083434823 | validation: 0.7214602038267921]
	TIME [epoch: 11.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7735046478157118		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.7735046478157118 | validation: 0.8015449361866769]
	TIME [epoch: 11.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7817430253317154		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.7817430253317154 | validation: 0.8157156208649029]
	TIME [epoch: 11.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7357813754103273		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.7357813754103273 | validation: 0.8385798702972405]
	TIME [epoch: 11.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7595143059619089		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.7595143059619089 | validation: 0.7558794306445119]
	TIME [epoch: 11.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7327523153098656		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.7327523153098656 | validation: 0.747159514639144]
	TIME [epoch: 11.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7521119170920941		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.7521119170920941 | validation: 0.7982275905844713]
	TIME [epoch: 11.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7385472394943042		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.7385472394943042 | validation: 0.7832548107477556]
	TIME [epoch: 11.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7669943095987526		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.7669943095987526 | validation: 0.7376327043818743]
	TIME [epoch: 11.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7332076193986126		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.7332076193986126 | validation: 0.7798470605395047]
	TIME [epoch: 11.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7297075577184071		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.7297075577184071 | validation: 0.7336330501079462]
	TIME [epoch: 11.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7453946019247711		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.7453946019247711 | validation: 0.7944229202739144]
	TIME [epoch: 11.8 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7207420579415014		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.7207420579415014 | validation: 0.7622293668016471]
	TIME [epoch: 11.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7446010754564237		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.7446010754564237 | validation: 0.796920235502059]
	TIME [epoch: 11.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7529495837672472		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.7529495837672472 | validation: 0.778274663385273]
	TIME [epoch: 11.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.740611667826859		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.740611667826859 | validation: 0.7483116561802797]
	TIME [epoch: 11.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7320955959381736		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.7320955959381736 | validation: 0.8235470696943831]
	TIME [epoch: 11.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7394944658595464		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.7394944658595464 | validation: 0.7181340320034457]
	TIME [epoch: 11.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7268737252473172		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.7268737252473172 | validation: 0.7158783072475914]
	TIME [epoch: 11.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7032025871705678		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.7032025871705678 | validation: 0.7164347056626917]
	TIME [epoch: 11.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7482343602762644		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.7482343602762644 | validation: 0.7469694684882753]
	TIME [epoch: 11.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7213960532894752		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.7213960532894752 | validation: 0.7572752378711789]
	TIME [epoch: 11.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7359419953475134		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.7359419953475134 | validation: 0.7776191226098929]
	TIME [epoch: 11.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7391892145453327		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.7391892145453327 | validation: 0.7424661675169908]
	TIME [epoch: 11.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7320142734987292		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.7320142734987292 | validation: 0.7565589681966561]
	TIME [epoch: 11.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7116687198271767		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.7116687198271767 | validation: 0.7377899948704219]
	TIME [epoch: 11.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7248140732340844		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.7248140732340844 | validation: 0.7781366672781325]
	TIME [epoch: 11.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7306238734993683		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.7306238734993683 | validation: 0.7238416632518418]
	TIME [epoch: 11.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.757753456100396		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.757753456100396 | validation: 0.7061168524820786]
	TIME [epoch: 11.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7118025134833967		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.7118025134833967 | validation: 0.726581537198504]
	TIME [epoch: 11.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.734638559077759		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.734638559077759 | validation: 0.7215472316210294]
	TIME [epoch: 11.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7225446344249715		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.7225446344249715 | validation: 0.7201171576298551]
	TIME [epoch: 11.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7099317071860154		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.7099317071860154 | validation: 0.7037083237259043]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7205215392836445		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.7205215392836445 | validation: 0.6884353123614055]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7895463986182678		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.7895463986182678 | validation: 0.7750982963118265]
	TIME [epoch: 11.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7428859268157655		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.7428859268157655 | validation: 0.7451622899341963]
	TIME [epoch: 11.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7246398587494105		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.7246398587494105 | validation: 0.7393556402907435]
	TIME [epoch: 11.8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7251476783367211		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.7251476783367211 | validation: 0.7337442309995562]
	TIME [epoch: 11.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7235188705503467		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.7235188705503467 | validation: 0.7023240923484058]
	TIME [epoch: 11.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7144921970549624		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.7144921970549624 | validation: 0.7249823913130178]
	TIME [epoch: 11.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7153582436923086		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.7153582436923086 | validation: 0.7037123308329789]
	TIME [epoch: 11.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7124616316647334		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.7124616316647334 | validation: 0.7368958106263173]
	TIME [epoch: 11.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7274777930161918		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.7274777930161918 | validation: 0.7142815181650144]
	TIME [epoch: 11.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6996927160521689		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.6996927160521689 | validation: 0.7194092732737105]
	TIME [epoch: 11.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7464455949987252		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.7464455949987252 | validation: 0.7470312543873525]
	TIME [epoch: 11.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7356773449789252		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.7356773449789252 | validation: 0.6916759190477103]
	TIME [epoch: 11.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.701479474140047		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.701479474140047 | validation: 0.6746955914958632]
	TIME [epoch: 11.8 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7141750276654238		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.7141750276654238 | validation: 0.7198007614138933]
	TIME [epoch: 11.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7077446616519062		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.7077446616519062 | validation: 0.7943775437131972]
	TIME [epoch: 11.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7304373597527907		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.7304373597527907 | validation: 0.7461056046211981]
	TIME [epoch: 11.8 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7141464390449191		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.7141464390449191 | validation: 0.7226930265130078]
	TIME [epoch: 11.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7091925800867623		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.7091925800867623 | validation: 0.72947805907563]
	TIME [epoch: 11.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.704666171506255		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.704666171506255 | validation: 0.7053440475608622]
	TIME [epoch: 11.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7163401113043747		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.7163401113043747 | validation: 0.7175421253105498]
	TIME [epoch: 11.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7082077016296737		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.7082077016296737 | validation: 0.7698285231381561]
	TIME [epoch: 11.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7121925110170532		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.7121925110170532 | validation: 0.7271009477958363]
	TIME [epoch: 11.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7115113336007229		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.7115113336007229 | validation: 0.7616277620120004]
	TIME [epoch: 11.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.699571538163787		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.699571538163787 | validation: 0.7247705785779823]
	TIME [epoch: 11.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7010318494245769		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.7010318494245769 | validation: 0.7409059718931936]
	TIME [epoch: 11.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7231974411421869		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.7231974411421869 | validation: 0.7101153695829623]
	TIME [epoch: 11.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7074292619900227		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.7074292619900227 | validation: 0.703138557399772]
	TIME [epoch: 11.9 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7033241680574047		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.7033241680574047 | validation: 0.7382484946972971]
	TIME [epoch: 11.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7175269510450698		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.7175269510450698 | validation: 0.7166780903253951]
	TIME [epoch: 11.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6974218641496615		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.6974218641496615 | validation: 0.6885796799243353]
	TIME [epoch: 440 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.703845213559789		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.703845213559789 | validation: 0.7454352645403526]
	TIME [epoch: 25.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7076960236373903		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.7076960236373903 | validation: 0.7050164965151018]
	TIME [epoch: 25.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6947060731238197		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.6947060731238197 | validation: 0.7065261258598703]
	TIME [epoch: 25.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7025093658680621		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.7025093658680621 | validation: 0.6995708130011791]
	TIME [epoch: 25.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7026623536427569		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.7026623536427569 | validation: 0.6866056608091602]
	TIME [epoch: 25.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7094897574609589		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.7094897574609589 | validation: 0.6966803527010581]
	TIME [epoch: 25.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7060401053502292		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.7060401053502292 | validation: 0.6985358908762538]
	TIME [epoch: 25.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6964600802779471		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.6964600802779471 | validation: 0.7233981540982481]
	TIME [epoch: 25.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6976012095169266		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.6976012095169266 | validation: 0.7272482353632127]
	TIME [epoch: 25.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6965264699071906		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.6965264699071906 | validation: 0.6910166518318219]
	TIME [epoch: 25.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7167452880117716		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.7167452880117716 | validation: 0.7188705945864109]
	TIME [epoch: 25.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6921674916932419		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.6921674916932419 | validation: 0.7060934149776861]
	TIME [epoch: 25.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6880791554085466		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.6880791554085466 | validation: 0.7233744225018379]
	TIME [epoch: 25.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7080672666169143		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.7080672666169143 | validation: 0.6964581680609173]
	TIME [epoch: 25.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.697389893409581		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.697389893409581 | validation: 0.6892238508266229]
	TIME [epoch: 25.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6886118744756158		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.6886118744756158 | validation: 0.7058156135884516]
	TIME [epoch: 25.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.715671626602415		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.715671626602415 | validation: 0.7437094219008296]
	TIME [epoch: 25.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.720862325250047		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.720862325250047 | validation: 0.683000036597676]
	TIME [epoch: 25.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6894210092587265		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.6894210092587265 | validation: 0.6779766257098876]
	TIME [epoch: 25.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7021936642977303		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.7021936642977303 | validation: 0.7316595935353025]
	TIME [epoch: 25.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6979319369457612		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.6979319369457612 | validation: 0.7371642641574482]
	TIME [epoch: 25.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7107002098845308		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.7107002098845308 | validation: 0.7033531680001494]
	TIME [epoch: 25.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7038590034091619		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.7038590034091619 | validation: 0.7121449807032417]
	TIME [epoch: 25.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6927091795975886		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.6927091795975886 | validation: 0.7004867286422146]
	TIME [epoch: 25.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7011743843371625		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.7011743843371625 | validation: 0.6727522015265466]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6985587655393357		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.6985587655393357 | validation: 0.6799920370380117]
	TIME [epoch: 25.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.687615351530462		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.687615351530462 | validation: 0.6703905441332073]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6905141004464896		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.6905141004464896 | validation: 0.7431148110171444]
	TIME [epoch: 25.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7030644416213921		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.7030644416213921 | validation: 0.7245671603791212]
	TIME [epoch: 25.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6923588002325212		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.6923588002325212 | validation: 0.6938341481568167]
	TIME [epoch: 25.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7037705449463271		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.7037705449463271 | validation: 0.7093040231391957]
	TIME [epoch: 25.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6923986823861652		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.6923986823861652 | validation: 0.6851009026630109]
	TIME [epoch: 25.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6853277303903974		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.6853277303903974 | validation: 0.6851435999552841]
	TIME [epoch: 25.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6785070816023167		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.6785070816023167 | validation: 0.682237684651543]
	TIME [epoch: 25.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6712545636026095		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.6712545636026095 | validation: 0.6922420492349612]
	TIME [epoch: 25.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7069019302517207		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.7069019302517207 | validation: 0.7000938740299608]
	TIME [epoch: 25.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7006291793145636		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.7006291793145636 | validation: 0.6991243502783911]
	TIME [epoch: 25.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6857097310441742		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.6857097310441742 | validation: 0.6859603623286371]
	TIME [epoch: 25.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6888000964329116		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.6888000964329116 | validation: 0.6897890470527117]
	TIME [epoch: 25.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6862643004324213		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.6862643004324213 | validation: 0.6803402268430906]
	TIME [epoch: 25.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6675022714792309		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.6675022714792309 | validation: 0.6867779165345299]
	TIME [epoch: 25.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6995240694907091		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.6995240694907091 | validation: 0.7039930138365258]
	TIME [epoch: 25.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6863080568429927		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.6863080568429927 | validation: 0.6958914175899453]
	TIME [epoch: 25.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6767540702458216		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.6767540702458216 | validation: 0.6989387230920519]
	TIME [epoch: 25.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6932090119506037		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.6932090119506037 | validation: 0.682031538326098]
	TIME [epoch: 25.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6825538099982673		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.6825538099982673 | validation: 0.7014720789906028]
	TIME [epoch: 25.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6808052092275847		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.6808052092275847 | validation: 0.7392669402195332]
	TIME [epoch: 25.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6854357298655219		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.6854357298655219 | validation: 0.6778614068909351]
	TIME [epoch: 25.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6868821780530795		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.6868821780530795 | validation: 0.6808517048951969]
	TIME [epoch: 25.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6645906590593202		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.6645906590593202 | validation: 0.7596398450437216]
	TIME [epoch: 25.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.691132536369732		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.691132536369732 | validation: 0.7151692633882918]
	TIME [epoch: 25.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6817720425350723		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.6817720425350723 | validation: 0.7085859700382162]
	TIME [epoch: 25.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6779351526648244		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.6779351526648244 | validation: 0.7260469998594845]
	TIME [epoch: 25.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6823320555332675		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.6823320555332675 | validation: 0.690258465084177]
	TIME [epoch: 25.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.669018156052697		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.669018156052697 | validation: 0.7152832526913021]
	TIME [epoch: 25.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6849227043063779		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.6849227043063779 | validation: 0.6817055277339379]
	TIME [epoch: 25.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6839305317572655		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.6839305317572655 | validation: 0.6764695478672098]
	TIME [epoch: 25.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6764994021353578		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.6764994021353578 | validation: 0.6894243587070041]
	TIME [epoch: 25.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6686576403161046		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.6686576403161046 | validation: 0.6746624283614342]
	TIME [epoch: 25.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6878355059643078		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.6878355059643078 | validation: 0.720864945534962]
	TIME [epoch: 25.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6816536651418359		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.6816536651418359 | validation: 0.7228303665469047]
	TIME [epoch: 25.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6760766077549935		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.6760766077549935 | validation: 0.6946387502814126]
	TIME [epoch: 25.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6818212917400468		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.6818212917400468 | validation: 0.7179667410378494]
	TIME [epoch: 25.8 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6735293283177861		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.6735293283177861 | validation: 0.7314427885884042]
	TIME [epoch: 25.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6751396523296824		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.6751396523296824 | validation: 0.6588511033751645]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6721886911528676		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.6721886911528676 | validation: 0.6756600371836562]
	TIME [epoch: 25.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6709191755979707		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.6709191755979707 | validation: 0.6814841955945714]
	TIME [epoch: 25.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.682703323096297		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.682703323096297 | validation: 0.6860217990028468]
	TIME [epoch: 25.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6734940955634041		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.6734940955634041 | validation: 0.6739336288351016]
	TIME [epoch: 25.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6679321910386565		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.6679321910386565 | validation: 0.7286840614191342]
	TIME [epoch: 25.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6797707971801312		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.6797707971801312 | validation: 0.7178505618777226]
	TIME [epoch: 25.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6755099329412148		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.6755099329412148 | validation: 0.6773519467588088]
	TIME [epoch: 25.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6734244910167451		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.6734244910167451 | validation: 0.6796604107681146]
	TIME [epoch: 25.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6676359343931014		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.6676359343931014 | validation: 0.6664083244662106]
	TIME [epoch: 25.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.670847273748336		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.670847273748336 | validation: 0.6834672129586526]
	TIME [epoch: 25.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6694626571976634		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.6694626571976634 | validation: 0.7252840003279144]
	TIME [epoch: 25.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6881461027364999		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.6881461027364999 | validation: 0.6750780579939983]
	TIME [epoch: 25.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6646070337798714		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.6646070337798714 | validation: 0.6963322239740127]
	TIME [epoch: 25.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6831645070096743		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.6831645070096743 | validation: 0.6971620373941739]
	TIME [epoch: 25.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6702980244694196		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.6702980244694196 | validation: 0.6803344652932715]
	TIME [epoch: 25.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6888112360632129		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.6888112360632129 | validation: 0.6695167532934905]
	TIME [epoch: 25.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6774596334428131		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.6774596334428131 | validation: 0.660494941505065]
	TIME [epoch: 25.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6568946844597406		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.6568946844597406 | validation: 0.6565261234020385]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6676191554692751		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.6676191554692751 | validation: 0.661040031560932]
	TIME [epoch: 25.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.680128514917025		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.680128514917025 | validation: 0.6860185507463783]
	TIME [epoch: 25.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.665785297629803		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.665785297629803 | validation: 0.6828415076028704]
	TIME [epoch: 25.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6729598907303032		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.6729598907303032 | validation: 0.6964362804298794]
	TIME [epoch: 25.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6684334934362105		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.6684334934362105 | validation: 0.6743819569264002]
	TIME [epoch: 25.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6547912687041422		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.6547912687041422 | validation: 0.6604081167956803]
	TIME [epoch: 25.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6774243310403192		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.6774243310403192 | validation: 0.6909573643368838]
	TIME [epoch: 25.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6765563766443534		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.6765563766443534 | validation: 0.6780806786932194]
	TIME [epoch: 25.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6605127049913857		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.6605127049913857 | validation: 0.6883395971141167]
	TIME [epoch: 25.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6563658750728141		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.6563658750728141 | validation: 0.6880723669036248]
	TIME [epoch: 25.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6690543553406396		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.6690543553406396 | validation: 0.6820181030812974]
	TIME [epoch: 25.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6663710814282211		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.6663710814282211 | validation: 0.6696924811203193]
	TIME [epoch: 25.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6549355902673692		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.6549355902673692 | validation: 0.6496390416003031]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.677503415306856		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.677503415306856 | validation: 0.6804517245124713]
	TIME [epoch: 25.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6581749850540461		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.6581749850540461 | validation: 0.6696463330062394]
	TIME [epoch: 25.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6581249468479421		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.6581249468479421 | validation: 0.7217459607335878]
	TIME [epoch: 25.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.67783182524347		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.67783182524347 | validation: 0.676547793786954]
	TIME [epoch: 25.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.659984532846472		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.659984532846472 | validation: 0.7007604670204493]
	TIME [epoch: 25.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.671222770430368		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.671222770430368 | validation: 0.7174974544662036]
	TIME [epoch: 25.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6553712040216519		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.6553712040216519 | validation: 0.6802447527703464]
	TIME [epoch: 25.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6727141014624483		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.6727141014624483 | validation: 0.6748136270378253]
	TIME [epoch: 25.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6623572163709582		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.6623572163709582 | validation: 0.6718684862844506]
	TIME [epoch: 25.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6622994287548847		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.6622994287548847 | validation: 0.677591659536714]
	TIME [epoch: 25.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6776957426055811		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.6776957426055811 | validation: 0.6867359380712541]
	TIME [epoch: 25.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6927654256559796		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.6927654256559796 | validation: 0.6920541416289594]
	TIME [epoch: 25.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6824327508468758		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.6824327508468758 | validation: 0.6641037847484125]
	TIME [epoch: 25.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6586480738656507		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.6586480738656507 | validation: 0.7177824548103331]
	TIME [epoch: 25.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6706858894615569		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.6706858894615569 | validation: 0.6793063142517386]
	TIME [epoch: 25.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6652803735066711		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.6652803735066711 | validation: 0.6856605832616584]
	TIME [epoch: 25.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.659303134860108		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.659303134860108 | validation: 0.6686983019262784]
	TIME [epoch: 25.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6728737805705522		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.6728737805705522 | validation: 0.6462080566690044]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6784217126417049		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.6784217126417049 | validation: 0.6539297905379997]
	TIME [epoch: 25.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6550438280421591		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.6550438280421591 | validation: 0.6455107864377483]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6590788896700264		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.6590788896700264 | validation: 0.6655502986997548]
	TIME [epoch: 25.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6647201002240751		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.6647201002240751 | validation: 0.6731934217010411]
	TIME [epoch: 25.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6603227517024218		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.6603227517024218 | validation: 0.6633512066995798]
	TIME [epoch: 25.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6959320805169813		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.6959320805169813 | validation: 0.7246329032074242]
	TIME [epoch: 25.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7138590662764794		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.7138590662764794 | validation: 0.6613374971607835]
	TIME [epoch: 25.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6488121650927986		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.6488121650927986 | validation: 0.6953031332881338]
	TIME [epoch: 25.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6479762166376201		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.6479762166376201 | validation: 0.6706108715000035]
	TIME [epoch: 25.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6537593822648681		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.6537593822648681 | validation: 0.6635972823298528]
	TIME [epoch: 25.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6562518195138407		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.6562518195138407 | validation: 0.6727120790440757]
	TIME [epoch: 25.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6571389166105067		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.6571389166105067 | validation: 0.6977169507792433]
	TIME [epoch: 25.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6744444645623009		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.6744444645623009 | validation: 0.6562623798130591]
	TIME [epoch: 25.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6495630006559651		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.6495630006559651 | validation: 0.6599098818037327]
	TIME [epoch: 25.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.665054325827392		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.665054325827392 | validation: 0.6824094565683229]
	TIME [epoch: 25.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.655102680017875		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.655102680017875 | validation: 0.6797824676758537]
	TIME [epoch: 25.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6481429864228063		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.6481429864228063 | validation: 0.7012422989453516]
	TIME [epoch: 25.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6601827435362834		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.6601827435362834 | validation: 0.6655266347804325]
	TIME [epoch: 25.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6475823358460157		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.6475823358460157 | validation: 0.6659975007014591]
	TIME [epoch: 25.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6620021429945695		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.6620021429945695 | validation: 0.6809126618831856]
	TIME [epoch: 25.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6644784150657823		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.6644784150657823 | validation: 0.6720534979729189]
	TIME [epoch: 25.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6584450204062096		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.6584450204062096 | validation: 0.6442107589887534]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.648511834263248		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.648511834263248 | validation: 0.6679883760853893]
	TIME [epoch: 25.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.642493757694554		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.642493757694554 | validation: 0.6611738397992968]
	TIME [epoch: 25.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.654833506411169		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.654833506411169 | validation: 0.7008786753480511]
	TIME [epoch: 25.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.661002833799581		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.661002833799581 | validation: 0.6615455042255804]
	TIME [epoch: 25.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6596509176517049		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.6596509176517049 | validation: 0.6680141627056029]
	TIME [epoch: 25.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6547847482900845		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.6547847482900845 | validation: 0.6771732655525933]
	TIME [epoch: 25.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6539197989048626		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.6539197989048626 | validation: 0.6834446267908281]
	TIME [epoch: 25.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6540239287486247		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.6540239287486247 | validation: 0.6939595695331994]
	TIME [epoch: 25.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6574937807223236		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.6574937807223236 | validation: 0.6993752822722246]
	TIME [epoch: 25.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6562394523422987		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.6562394523422987 | validation: 0.641016408073322]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6599796808348974		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.6599796808348974 | validation: 0.6504664686114743]
	TIME [epoch: 25.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6455018188911059		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.6455018188911059 | validation: 0.6475991884581899]
	TIME [epoch: 25.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6423979874699549		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.6423979874699549 | validation: 0.667540320812203]
	TIME [epoch: 25.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6612025966684113		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.6612025966684113 | validation: 0.6463851133805427]
	TIME [epoch: 25.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6362398718288697		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.6362398718288697 | validation: 0.6674200441677587]
	TIME [epoch: 25.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6475548453371845		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.6475548453371845 | validation: 0.6673789356852504]
	TIME [epoch: 25.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6465611698040425		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.6465611698040425 | validation: 0.6660599813748427]
	TIME [epoch: 25.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6447548261831215		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.6447548261831215 | validation: 0.6357025489430191]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6369097784181268		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.6369097784181268 | validation: 0.6415912320803192]
	TIME [epoch: 25.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6461503897852662		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.6461503897852662 | validation: 0.6391136888104392]
	TIME [epoch: 25.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6375099035515508		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.6375099035515508 | validation: 0.6566564483594508]
	TIME [epoch: 25.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.643853064843744		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.643853064843744 | validation: 0.6685724871441687]
	TIME [epoch: 25.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6340198493575046		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.6340198493575046 | validation: 0.6846825891160453]
	TIME [epoch: 25.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6548358900322356		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.6548358900322356 | validation: 0.6699575489094471]
	TIME [epoch: 25.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6450202685412078		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.6450202685412078 | validation: 0.6535865806432272]
	TIME [epoch: 25.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6448724581628098		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.6448724581628098 | validation: 0.6550089585197619]
	TIME [epoch: 25.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.641311857687404		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.641311857687404 | validation: 0.6446013396326966]
	TIME [epoch: 25.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6455095765461544		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.6455095765461544 | validation: 0.6449103984355324]
	TIME [epoch: 25.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6423811367326712		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.6423811367326712 | validation: 0.6436390008050585]
	TIME [epoch: 25.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6359631853244415		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.6359631853244415 | validation: 0.673658493991292]
	TIME [epoch: 25.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6456347566087466		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.6456347566087466 | validation: 0.6819159551189673]
	TIME [epoch: 25.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6464106435046537		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.6464106435046537 | validation: 0.6605935834141357]
	TIME [epoch: 25.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6379967962960142		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.6379967962960142 | validation: 0.667094779605471]
	TIME [epoch: 25.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6346785126539262		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.6346785126539262 | validation: 0.6653851633703374]
	TIME [epoch: 25.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6478697850293877		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.6478697850293877 | validation: 0.6715616839917338]
	TIME [epoch: 25.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6378666160734294		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.6378666160734294 | validation: 0.6490330339948303]
	TIME [epoch: 25.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6521074298894465		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.6521074298894465 | validation: 0.6319225090619527]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6370534222113271		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.6370534222113271 | validation: 0.6547957684811251]
	TIME [epoch: 25.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.638736644082381		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.638736644082381 | validation: 0.6623788487190558]
	TIME [epoch: 25.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6408243191462499		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.6408243191462499 | validation: 0.6562199349631413]
	TIME [epoch: 25.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6462488635808759		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.6462488635808759 | validation: 0.66911735225022]
	TIME [epoch: 25.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6451122544056156		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.6451122544056156 | validation: 0.6751622427044515]
	TIME [epoch: 25.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6387945423392808		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.6387945423392808 | validation: 0.6588904994151515]
	TIME [epoch: 25.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6335369105953295		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.6335369105953295 | validation: 0.6536719528728441]
	TIME [epoch: 25.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6443391586134188		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.6443391586134188 | validation: 0.6327437675339993]
	TIME [epoch: 25.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6381029374108618		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.6381029374108618 | validation: 0.6504710138872948]
	TIME [epoch: 25.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6365709915250498		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.6365709915250498 | validation: 0.6539922576571837]
	TIME [epoch: 25.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6343738517495143		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.6343738517495143 | validation: 0.6547795306031783]
	TIME [epoch: 25.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6358476666366748		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.6358476666366748 | validation: 0.6390162736915868]
	TIME [epoch: 25.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6360232607226927		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.6360232607226927 | validation: 0.6747505792176749]
	TIME [epoch: 25.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6316562243717789		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.6316562243717789 | validation: 0.6461280366710394]
	TIME [epoch: 25.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6268179974389083		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.6268179974389083 | validation: 0.6542559098769685]
	TIME [epoch: 25.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6383046548121465		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.6383046548121465 | validation: 0.6581789308420227]
	TIME [epoch: 25.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.654831332720758		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.654831332720758 | validation: 0.6492627995772897]
	TIME [epoch: 25.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6269894269121752		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.6269894269121752 | validation: 0.6855185345529708]
	TIME [epoch: 25.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6488566906872396		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.6488566906872396 | validation: 0.6793454767615865]
	TIME [epoch: 25.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6335199268103566		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.6335199268103566 | validation: 0.6542751326135918]
	TIME [epoch: 25.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6482342876214389		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.6482342876214389 | validation: 0.6460267414996164]
	TIME [epoch: 25.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6349027909955315		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.6349027909955315 | validation: 0.6491626979738881]
	TIME [epoch: 25.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6372171209711603		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.6372171209711603 | validation: 0.6608773721257137]
	TIME [epoch: 25.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6335892669611232		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.6335892669611232 | validation: 0.637755950278742]
	TIME [epoch: 25.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6321097862929452		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.6321097862929452 | validation: 0.6422863448296171]
	TIME [epoch: 25.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6378859364637286		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.6378859364637286 | validation: 0.6300087832264548]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6389142704814821		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.6389142704814821 | validation: 0.6259876877052525]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6530420483544911		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.6530420483544911 | validation: 0.6886635932801086]
	TIME [epoch: 25.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.64729999946399		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.64729999946399 | validation: 0.6612626808435915]
	TIME [epoch: 25.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6340475489270052		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.6340475489270052 | validation: 0.6460354863956463]
	TIME [epoch: 25.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6258608131878745		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.6258608131878745 | validation: 0.674498635602713]
	TIME [epoch: 25.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.641080191962986		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.641080191962986 | validation: 0.6417870342342593]
	TIME [epoch: 25.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6265543392641345		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.6265543392641345 | validation: 0.6326968707676757]
	TIME [epoch: 25.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6277849006237854		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.6277849006237854 | validation: 0.6344825176815881]
	TIME [epoch: 25.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6331818785569377		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.6331818785569377 | validation: 0.6363993888814892]
	TIME [epoch: 25.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6331782849758315		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.6331782849758315 | validation: 0.6637167448327788]
	TIME [epoch: 25.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6366102073028436		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.6366102073028436 | validation: 0.6426081238190042]
	TIME [epoch: 25.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6320813673367986		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.6320813673367986 | validation: 0.6729709684044407]
	TIME [epoch: 25.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6364368662077025		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.6364368662077025 | validation: 0.6715140634382086]
	TIME [epoch: 25.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.634883326250501		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.634883326250501 | validation: 0.6604899788386444]
	TIME [epoch: 25.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6350248779659795		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.6350248779659795 | validation: 0.6565025779941615]
	TIME [epoch: 25.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6304387942441543		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.6304387942441543 | validation: 0.6496528954656609]
	TIME [epoch: 25.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6305125960629356		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.6305125960629356 | validation: 0.6686160562452079]
	TIME [epoch: 25.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6344213362150721		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.6344213362150721 | validation: 0.6399241621566822]
	TIME [epoch: 25.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6236355945804797		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.6236355945804797 | validation: 0.6470583111799121]
	TIME [epoch: 25.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6353640747277766		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.6353640747277766 | validation: 0.6130483007457846]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6251002648438042		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.6251002648438042 | validation: 0.6248952343795386]
	TIME [epoch: 25.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6354493448863208		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.6354493448863208 | validation: 0.6291832444822947]
	TIME [epoch: 25.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6284836425098527		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.6284836425098527 | validation: 0.6234788103627533]
	TIME [epoch: 25.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6328362070350528		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.6328362070350528 | validation: 0.6479454561986524]
	TIME [epoch: 25.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6277909842131142		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.6277909842131142 | validation: 0.6331328072968907]
	TIME [epoch: 25.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6240803569099334		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.6240803569099334 | validation: 0.6333666236301002]
	TIME [epoch: 25.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6324257157307991		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.6324257157307991 | validation: 0.6423752965026319]
	TIME [epoch: 25.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6278666008140575		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.6278666008140575 | validation: 0.6464645730787981]
	TIME [epoch: 25.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6341491958867628		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.6341491958867628 | validation: 0.6686132135401682]
	TIME [epoch: 25.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6367196753035989		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.6367196753035989 | validation: 0.6368701013029836]
	TIME [epoch: 25.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6249542962647177		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.6249542962647177 | validation: 0.6190773712144936]
	TIME [epoch: 25.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.623046953163217		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.623046953163217 | validation: 0.6273288179657335]
	TIME [epoch: 25.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.628388280108465		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.628388280108465 | validation: 0.6329415179694855]
	TIME [epoch: 25.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6294917201147275		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.6294917201147275 | validation: 0.6289270909394342]
	TIME [epoch: 25.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6203012123303976		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.6203012123303976 | validation: 0.6558425383196169]
	TIME [epoch: 25.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6288962300684782		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.6288962300684782 | validation: 0.6467194972442445]
	TIME [epoch: 25.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6331615115722189		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.6331615115722189 | validation: 0.6407008519316694]
	TIME [epoch: 25.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231533936270672		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.6231533936270672 | validation: 0.6302345584561124]
	TIME [epoch: 25.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6314541497287425		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.6314541497287425 | validation: 0.6421467462644608]
	TIME [epoch: 25.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6244266514705891		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.6244266514705891 | validation: 0.6551468624167873]
	TIME [epoch: 25.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6317777321046908		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.6317777321046908 | validation: 0.6683206490811981]
	TIME [epoch: 25.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6281347753016415		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.6281347753016415 | validation: 0.6419324748835171]
	TIME [epoch: 25.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231792449556172		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.6231792449556172 | validation: 0.6377625296291034]
	TIME [epoch: 25.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6230987782351476		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.6230987782351476 | validation: 0.6328775707867077]
	TIME [epoch: 25.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6252570308225391		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.6252570308225391 | validation: 0.6119002737540381]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.631085702522008		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.631085702522008 | validation: 0.6131362655655364]
	TIME [epoch: 25.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6110141867979958		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.6110141867979958 | validation: 0.6224008296119966]
	TIME [epoch: 25.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6151371854777247		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.6151371854777247 | validation: 0.6265340420650389]
	TIME [epoch: 25.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6247986615546026		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.6247986615546026 | validation: 0.6281443173788178]
	TIME [epoch: 25.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6177780425777657		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.6177780425777657 | validation: 0.6132438051385551]
	TIME [epoch: 25.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6173259689316544		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.6173259689316544 | validation: 0.6356688763564733]
	TIME [epoch: 25.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6207332922254346		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.6207332922254346 | validation: 0.634340978447165]
	TIME [epoch: 25.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6210732716008878		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.6210732716008878 | validation: 0.632069037127976]
	TIME [epoch: 25.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6203182760933619		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.6203182760933619 | validation: 0.6508583677041391]
	TIME [epoch: 25.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6231461947851409		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.6231461947851409 | validation: 0.6359372941409124]
	TIME [epoch: 25.4 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6104714515227326		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.6104714515227326 | validation: 0.6272406097034919]
	TIME [epoch: 25.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6258085151357211		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.6258085151357211 | validation: 0.6213887258874381]
	TIME [epoch: 25.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6067495624642347		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.6067495624642347 | validation: 0.6311664574177505]
	TIME [epoch: 25.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.615258047067688		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.615258047067688 | validation: 0.6362515576116028]
	TIME [epoch: 25.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6188635641093547		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.6188635641093547 | validation: 0.6328327507854283]
	TIME [epoch: 25.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6150237846437394		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.6150237846437394 | validation: 0.6406090365670223]
	TIME [epoch: 25.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6222082953523353		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.6222082953523353 | validation: 0.644179772300048]
	TIME [epoch: 25.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.615557605636973		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.615557605636973 | validation: 0.632040625439517]
	TIME [epoch: 25.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6086585985957591		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.6086585985957591 | validation: 0.625525625152865]
	TIME [epoch: 25.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6241510726058183		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.6241510726058183 | validation: 0.6144693238650097]
	TIME [epoch: 25.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6167350261674849		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.6167350261674849 | validation: 0.6219444993318424]
	TIME [epoch: 25.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6179576732615851		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.6179576732615851 | validation: 0.6473040745280285]
	TIME [epoch: 25.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.623542808116704		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.623542808116704 | validation: 0.6636319754209916]
	TIME [epoch: 25.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.620177795660799		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.620177795660799 | validation: 0.647449752008105]
	TIME [epoch: 25.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.624978582564693		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.624978582564693 | validation: 0.6066102552627135]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6182444495104467		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.6182444495104467 | validation: 0.6357635760924876]
	TIME [epoch: 25.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.613636830537605		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.613636830537605 | validation: 0.6206150222782063]
	TIME [epoch: 25.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6101047153352016		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.6101047153352016 | validation: 0.6325970243881383]
	TIME [epoch: 25.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6138921426851855		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.6138921426851855 | validation: 0.6425300639327574]
	TIME [epoch: 25.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6238391611137828		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.6238391611137828 | validation: 0.6359129775980865]
	TIME [epoch: 25.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6179896199368435		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.6179896199368435 | validation: 0.6191586972123142]
	TIME [epoch: 25.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6090305376938034		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.6090305376938034 | validation: 0.642720938238153]
	TIME [epoch: 25.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6103678236186104		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.6103678236186104 | validation: 0.6198806818452143]
	TIME [epoch: 25.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6103375242436462		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.6103375242436462 | validation: 0.6317178921886193]
	TIME [epoch: 25.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6174042743247973		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.6174042743247973 | validation: 0.6366364049823543]
	TIME [epoch: 25.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6187202991172848		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.6187202991172848 | validation: 0.6371542164118196]
	TIME [epoch: 25.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6155653717091407		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.6155653717091407 | validation: 0.6336109217895668]
	TIME [epoch: 25.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6124607628940084		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.6124607628940084 | validation: 0.6348908409758021]
	TIME [epoch: 25.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.615918417807193		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.615918417807193 | validation: 0.6404453553671231]
	TIME [epoch: 25.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6222546514766261		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.6222546514766261 | validation: 0.6426824378359288]
	TIME [epoch: 25.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128706612125642		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.6128706612125642 | validation: 0.6456219999679094]
	TIME [epoch: 25.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6098918309527928		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.6098918309527928 | validation: 0.6411018885235201]
	TIME [epoch: 25.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6127069631966084		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.6127069631966084 | validation: 0.6197986013898626]
	TIME [epoch: 25.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6135377070204981		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.6135377070204981 | validation: 0.6278714867069801]
	TIME [epoch: 25.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128746985311635		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.6128746985311635 | validation: 0.6319387846949226]
	TIME [epoch: 25.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6128428646638169		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.6128428646638169 | validation: 0.6246641209137556]
	TIME [epoch: 25.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6021751399963164		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.6021751399963164 | validation: 0.6510235121581704]
	TIME [epoch: 25.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6122318153134694		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.6122318153134694 | validation: 0.6317766773556]
	TIME [epoch: 25.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6165021760649776		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.6165021760649776 | validation: 0.648545870056261]
	TIME [epoch: 25.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6080285991603032		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.6080285991603032 | validation: 0.6322472422630121]
	TIME [epoch: 25.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6093218107639893		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.6093218107639893 | validation: 0.6085075537517417]
	TIME [epoch: 25.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6185652862765334		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.6185652862765334 | validation: 0.6045201196180447]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6075376534758014		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.6075376534758014 | validation: 0.6084672848336589]
	TIME [epoch: 25.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6103830325339026		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.6103830325339026 | validation: 0.620726333386864]
	TIME [epoch: 25.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6031747520413231		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.6031747520413231 | validation: 0.6208073261039576]
	TIME [epoch: 25.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6025342693567752		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.6025342693567752 | validation: 0.6145591761842797]
	TIME [epoch: 25.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6078070284261383		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.6078070284261383 | validation: 0.612519507611265]
	TIME [epoch: 25.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6004956635135218		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.6004956635135218 | validation: 0.6332934012807532]
	TIME [epoch: 25.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040735805894848		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.6040735805894848 | validation: 0.6273731181221947]
	TIME [epoch: 25.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.610596512884479		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.610596512884479 | validation: 0.6122340939534769]
	TIME [epoch: 25.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6096024865205901		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.6096024865205901 | validation: 0.6373666341244091]
	TIME [epoch: 25.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6104078825020354		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.6104078825020354 | validation: 0.6134208757989625]
	TIME [epoch: 25.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077537751295716		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.6077537751295716 | validation: 0.61267874334857]
	TIME [epoch: 25.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5985646663342906		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.5985646663342906 | validation: 0.6096161738198131]
	TIME [epoch: 25.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6012166928852942		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.6012166928852942 | validation: 0.6040234748864235]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6043340773449963		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.6043340773449963 | validation: 0.6133286095304231]
	TIME [epoch: 25.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6032755158960514		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.6032755158960514 | validation: 0.6172081684982458]
	TIME [epoch: 25.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6077956811822358		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.6077956811822358 | validation: 0.6121468960285558]
	TIME [epoch: 25.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.599727614717866		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.599727614717866 | validation: 0.618426431973425]
	TIME [epoch: 25.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5976607555016269		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.5976607555016269 | validation: 0.6147091727301515]
	TIME [epoch: 25.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6057483929701963		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.6057483929701963 | validation: 0.6152795826506905]
	TIME [epoch: 25.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5953948439032406		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.5953948439032406 | validation: 0.6067724917738984]
	TIME [epoch: 25.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.601523669287858		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.601523669287858 | validation: 0.6169819592519661]
	TIME [epoch: 25.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5980460274733911		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.5980460274733911 | validation: 0.6114999500655801]
	TIME [epoch: 25.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6053768397324824		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.6053768397324824 | validation: 0.602611086381498]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5964767186440934		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.5964767186440934 | validation: 0.6225543250888126]
	TIME [epoch: 25.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5976561979863014		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.5976561979863014 | validation: 0.6146152753099325]
	TIME [epoch: 25.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007419133221613		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.6007419133221613 | validation: 0.6098747464075758]
	TIME [epoch: 25.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.592842248606646		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.592842248606646 | validation: 0.6179385441965877]
	TIME [epoch: 25.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6026939673491711		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.6026939673491711 | validation: 0.6228027955405753]
	TIME [epoch: 25.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6004212202691315		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.6004212202691315 | validation: 0.6412882724108113]
	TIME [epoch: 25.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5990318410668038		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.5990318410668038 | validation: 0.6207188491347859]
	TIME [epoch: 25.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6025892577479125		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.6025892577479125 | validation: 0.5977928174067701]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027620905467389		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.6027620905467389 | validation: 0.6047102119936321]
	TIME [epoch: 25.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5973353862618634		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.5973353862618634 | validation: 0.6133410566210464]
	TIME [epoch: 25.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5992862637027734		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.5992862637027734 | validation: 0.6314958280277707]
	TIME [epoch: 25.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6015163818716811		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.6015163818716811 | validation: 0.611088396248785]
	TIME [epoch: 25.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6033879473217978		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.6033879473217978 | validation: 0.6485185096080748]
	TIME [epoch: 25.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018874678788747		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.6018874678788747 | validation: 0.6260864064832227]
	TIME [epoch: 25.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5966059215794131		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.5966059215794131 | validation: 0.6280149273508089]
	TIME [epoch: 25.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5946970439201975		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.5946970439201975 | validation: 0.6051319682219838]
	TIME [epoch: 25.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6000990748263075		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.6000990748263075 | validation: 0.6027491948347592]
	TIME [epoch: 25.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5950448832266861		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.5950448832266861 | validation: 0.6181824467640606]
	TIME [epoch: 25.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5946447277987721		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.5946447277987721 | validation: 0.6089661806130486]
	TIME [epoch: 25.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.592799083042228		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.592799083042228 | validation: 0.6163923059396355]
	TIME [epoch: 25.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034482512147383		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.6034482512147383 | validation: 0.6019457153982956]
	TIME [epoch: 25.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027033048826461		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.6027033048826461 | validation: 0.6177329417560861]
	TIME [epoch: 25.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5909515882215866		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.5909515882215866 | validation: 0.6045126668322196]
	TIME [epoch: 25.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6012631750230052		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.6012631750230052 | validation: 0.60786403134079]
	TIME [epoch: 25.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.596173375322206		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.596173375322206 | validation: 0.6098588632152382]
	TIME [epoch: 25.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5864354119434277		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.5864354119434277 | validation: 0.6025251726389471]
	TIME [epoch: 25.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6034825520913338		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.6034825520913338 | validation: 0.6037840761457275]
	TIME [epoch: 25.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5955535830161574		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.5955535830161574 | validation: 0.6035614774030076]
	TIME [epoch: 25.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.593030452614328		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.593030452614328 | validation: 0.5989769321218202]
	TIME [epoch: 25.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5863273674768295		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.5863273674768295 | validation: 0.6023764468958732]
	TIME [epoch: 25.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6015503758530645		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.6015503758530645 | validation: 0.6095212923642668]
	TIME [epoch: 25.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6014116575392328		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.6014116575392328 | validation: 0.609094078288281]
	TIME [epoch: 25.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.591480842215342		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.591480842215342 | validation: 0.604691525653918]
	TIME [epoch: 25.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5975116721158447		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.5975116721158447 | validation: 0.6215112904556579]
	TIME [epoch: 25.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.590839165007115		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.590839165007115 | validation: 0.6273512144958486]
	TIME [epoch: 25.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5994110852548267		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.5994110852548267 | validation: 0.592846879037187]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5946918579483024		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.5946918579483024 | validation: 0.5956726970846714]
	TIME [epoch: 25.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5892136635534606		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.5892136635534606 | validation: 0.6005975843601079]
	TIME [epoch: 25.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5912289952376254		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.5912289952376254 | validation: 0.5880494980728841]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5887417901416685		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.5887417901416685 | validation: 0.6290914688292751]
	TIME [epoch: 25.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6045302504179818		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.6045302504179818 | validation: 0.6040304297574891]
	TIME [epoch: 25.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5901541469823979		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.5901541469823979 | validation: 0.5953696565591085]
	TIME [epoch: 25.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5910550106871499		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.5910550106871499 | validation: 0.606077548208781]
	TIME [epoch: 25.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5942237524370593		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.5942237524370593 | validation: 0.5986246176475585]
	TIME [epoch: 25.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.581701014913373		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.581701014913373 | validation: 0.6066241924753939]
	TIME [epoch: 25.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5969579833149904		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.5969579833149904 | validation: 0.596375241573804]
	TIME [epoch: 25.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6040161838036633		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.6040161838036633 | validation: 0.5952150255498956]
	TIME [epoch: 25.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.587177119695869		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.587177119695869 | validation: 0.6016531241260015]
	TIME [epoch: 25.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5870521892822127		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.5870521892822127 | validation: 0.5845079817635652]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5855827346112509		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.5855827346112509 | validation: 0.6071054058426981]
	TIME [epoch: 25.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5935894949094543		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.5935894949094543 | validation: 0.5962701868858316]
	TIME [epoch: 25.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5902221374642197		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.5902221374642197 | validation: 0.5763348632776517]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_872.pth
	Model improved!!!
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5855830462741569		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.5855830462741569 | validation: 0.5866856161433871]
	TIME [epoch: 25.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5873333308823044		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.5873333308823044 | validation: 0.5971289760092071]
	TIME [epoch: 25.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5851205157692001		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.5851205157692001 | validation: 0.5989731026566669]
	TIME [epoch: 25.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5852248032502447		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.5852248032502447 | validation: 0.5973060229345051]
	TIME [epoch: 25.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.579153998247837		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.579153998247837 | validation: 0.5907124480140888]
	TIME [epoch: 25.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5814204430134919		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.5814204430134919 | validation: 0.5996605626414544]
	TIME [epoch: 25.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5843935117248815		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.5843935117248815 | validation: 0.5935346358572624]
	TIME [epoch: 25.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5934560280071541		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.5934560280071541 | validation: 0.5873765354032033]
	TIME [epoch: 25.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.579519600223022		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.579519600223022 | validation: 0.606981400508246]
	TIME [epoch: 25.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5824097920484501		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.5824097920484501 | validation: 0.5966456249430672]
	TIME [epoch: 25.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5828894090490973		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.5828894090490973 | validation: 0.6056780870747507]
	TIME [epoch: 25.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5881865729163696		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.5881865729163696 | validation: 0.595092433152637]
	TIME [epoch: 25.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.580070258435324		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.580070258435324 | validation: 0.5970121364694507]
	TIME [epoch: 25.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5826843411592189		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.5826843411592189 | validation: 0.6121691812828629]
	TIME [epoch: 25.4 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5855940196998466		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.5855940196998466 | validation: 0.6031753161451296]
	TIME [epoch: 25.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5887424694069074		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.5887424694069074 | validation: 0.6058237333251232]
	TIME [epoch: 25.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5827627616162692		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.5827627616162692 | validation: 0.5930782605802309]
	TIME [epoch: 25.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5861737912121676		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.5861737912121676 | validation: 0.5847500041748843]
	TIME [epoch: 25.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5750862416212287		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.5750862416212287 | validation: 0.5882253141440128]
	TIME [epoch: 25.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5765137607441191		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.5765137607441191 | validation: 0.6073247499273018]
	TIME [epoch: 25.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.579331804039474		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.579331804039474 | validation: 0.5825953917734373]
	TIME [epoch: 25.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5778685731774666		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.5778685731774666 | validation: 0.5870351936874934]
	TIME [epoch: 25.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5921981063515189		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.5921981063515189 | validation: 0.5908753529733346]
	TIME [epoch: 25.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5774423783193191		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.5774423783193191 | validation: 0.6017132985602043]
	TIME [epoch: 25.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5767550627884191		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.5767550627884191 | validation: 0.5782379184664855]
	TIME [epoch: 25.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.57499161519697		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.57499161519697 | validation: 0.6001193127586282]
	TIME [epoch: 25.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5828836205476202		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.5828836205476202 | validation: 0.594828996543391]
	TIME [epoch: 25.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5784868011748718		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.5784868011748718 | validation: 0.6041521494477025]
	TIME [epoch: 25.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5835714879667117		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.5835714879667117 | validation: 0.5883110654866628]
	TIME [epoch: 25.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5765137336627063		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.5765137336627063 | validation: 0.5884479366561668]
	TIME [epoch: 25.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5723577863964074		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.5723577863964074 | validation: 0.5899465379694563]
	TIME [epoch: 25.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5731836153446817		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.5731836153446817 | validation: 0.594582173872191]
	TIME [epoch: 25.4 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5804291996666561		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.5804291996666561 | validation: 0.6062353362011014]
	TIME [epoch: 25.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5808529019027115		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.5808529019027115 | validation: 0.5810263260296737]
	TIME [epoch: 25.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5790966109758813		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.5790966109758813 | validation: 0.5844034720562021]
	TIME [epoch: 25.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5737793543340275		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.5737793543340275 | validation: 0.5887202036911212]
	TIME [epoch: 25.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5822080991965806		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.5822080991965806 | validation: 0.5966090128801391]
	TIME [epoch: 25.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.578820652005743		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.578820652005743 | validation: 0.584230838802762]
	TIME [epoch: 25.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5740210870181234		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.5740210870181234 | validation: 0.5857003789821091]
	TIME [epoch: 25.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5774118612427496		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.5774118612427496 | validation: 0.5964714277142528]
	TIME [epoch: 25.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5791280825844569		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.5791280825844569 | validation: 0.5813489196117774]
	TIME [epoch: 25.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5819667094311946		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.5819667094311946 | validation: 0.5855754896543589]
	TIME [epoch: 25.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5752624425737445		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.5752624425737445 | validation: 0.6073547874944847]
	TIME [epoch: 25.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5794125657333011		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.5794125657333011 | validation: 0.5836327418379048]
	TIME [epoch: 25.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5749890521432626		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.5749890521432626 | validation: 0.5950986202862603]
	TIME [epoch: 25.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5730822707091519		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.5730822707091519 | validation: 0.5784644850117209]
	TIME [epoch: 25.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5806834436236709		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.5806834436236709 | validation: 0.5871666533692642]
	TIME [epoch: 25.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.57645564535473		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.57645564535473 | validation: 0.5835829522008937]
	TIME [epoch: 25.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5666728600434021		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.5666728600434021 | validation: 0.5809478579607622]
	TIME [epoch: 25.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5720135287233516		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.5720135287233516 | validation: 0.6001636601813897]
	TIME [epoch: 25.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5869395562943767		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.5869395562943767 | validation: 0.584103961090199]
	TIME [epoch: 25.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.576766233395552		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.576766233395552 | validation: 0.5792124001177104]
	TIME [epoch: 25.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5695920097021856		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.5695920097021856 | validation: 0.5786149801021487]
	TIME [epoch: 25.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5760950731485605		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.5760950731485605 | validation: 0.5714283773885676]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5749549643626215		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.5749549643626215 | validation: 0.5830470360149238]
	TIME [epoch: 25.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5673422560775988		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.5673422560775988 | validation: 0.5738508771025383]
	TIME [epoch: 25.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5745464959437304		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.5745464959437304 | validation: 0.5713815115976878]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5716098744861312		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.5716098744861312 | validation: 0.572510813963592]
	TIME [epoch: 25.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5711833096232127		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.5711833096232127 | validation: 0.5945047932534255]
	TIME [epoch: 25.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5735770525930884		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.5735770525930884 | validation: 0.59037958094316]
	TIME [epoch: 25.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5750866392490599		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.5750866392490599 | validation: 0.5861434234322133]
	TIME [epoch: 25.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5747783103890397		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.5747783103890397 | validation: 0.588572040352788]
	TIME [epoch: 25.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5721902689432172		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.5721902689432172 | validation: 0.5831272383198405]
	TIME [epoch: 25.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5708558622987007		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.5708558622987007 | validation: 0.5893353489482631]
	TIME [epoch: 25.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.569740941369022		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.569740941369022 | validation: 0.5885423691274757]
	TIME [epoch: 25.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5706536429057335		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.5706536429057335 | validation: 0.5894595577376789]
	TIME [epoch: 25.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.570916350019706		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.570916350019706 | validation: 0.5710216266365841]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5743422838765959		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.5743422838765959 | validation: 0.5729710377507855]
	TIME [epoch: 25.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.568039194391794		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.568039194391794 | validation: 0.578070706482775]
	TIME [epoch: 25.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5670337850217139		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.5670337850217139 | validation: 0.5853848051446524]
	TIME [epoch: 25.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5655309407321811		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.5655309407321811 | validation: 0.5858688257560382]
	TIME [epoch: 25.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5654005624668069		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.5654005624668069 | validation: 0.6138333486396899]
	TIME [epoch: 25.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5673415709666556		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.5673415709666556 | validation: 0.5955296283104566]
	TIME [epoch: 25.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5751804263543705		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.5751804263543705 | validation: 0.5948071605099229]
	TIME [epoch: 25.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5647373058039747		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.5647373058039747 | validation: 0.5850613920812309]
	TIME [epoch: 25.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5653200563626671		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.5653200563626671 | validation: 0.585488968314789]
	TIME [epoch: 25.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5706189280230703		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.5706189280230703 | validation: 0.5859946289104075]
	TIME [epoch: 25.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5689002649572577		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.5689002649572577 | validation: 0.5888614287064353]
	TIME [epoch: 25.4 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5594630809897353		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.5594630809897353 | validation: 0.5736822550196687]
	TIME [epoch: 25.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5668636973048767		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.5668636973048767 | validation: 0.5832822228763204]
	TIME [epoch: 25.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5659762819322413		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.5659762819322413 | validation: 0.5788398997063371]
	TIME [epoch: 25.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5631689197617396		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.5631689197617396 | validation: 0.5805735655322464]
	TIME [epoch: 25.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.568123031330966		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.568123031330966 | validation: 0.5715010654172393]
	TIME [epoch: 25.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5601501133328074		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.5601501133328074 | validation: 0.5808766487373012]
	TIME [epoch: 25.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5716574508561304		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.5716574508561304 | validation: 0.565724911939629]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5689837847109297		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.5689837847109297 | validation: 0.5631423745896436]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_958.pth
	Model improved!!!
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5632957030433177		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.5632957030433177 | validation: 0.5768498666630735]
	TIME [epoch: 25.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5712003660907223		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.5712003660907223 | validation: 0.5783025425158039]
	TIME [epoch: 25.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677746528767268		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.5677746528767268 | validation: 0.5798271302935047]
	TIME [epoch: 25.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.561332297557163		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.561332297557163 | validation: 0.5963597265017012]
	TIME [epoch: 25.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5639919652943668		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.5639919652943668 | validation: 0.577536867498935]
	TIME [epoch: 25.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5659556850864828		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.5659556850864828 | validation: 0.5644781520976705]
	TIME [epoch: 25.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5651034017014338		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.5651034017014338 | validation: 0.5777630277732058]
	TIME [epoch: 25.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5642729897615735		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.5642729897615735 | validation: 0.6218159282271298]
	TIME [epoch: 25.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5687149711725767		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.5687149711725767 | validation: 0.5621760123340773]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.563080311913165		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.563080311913165 | validation: 0.5682630045496969]
	TIME [epoch: 25.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5635652591632253		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.5635652591632253 | validation: 0.5761587365502678]
	TIME [epoch: 25.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5647291419520359		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.5647291419520359 | validation: 0.5714732715113926]
	TIME [epoch: 25.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5616877977504554		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.5616877977504554 | validation: 0.5737786033502691]
	TIME [epoch: 25.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5607179334635299		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.5607179334635299 | validation: 0.5681398358067041]
	TIME [epoch: 25.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5587715846274787		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.5587715846274787 | validation: 0.5639927808154048]
	TIME [epoch: 25.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5638632128208496		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.5638632128208496 | validation: 0.5730282951800161]
	TIME [epoch: 25.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677219017930053		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.5677219017930053 | validation: 0.5784145364093277]
	TIME [epoch: 25.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5636928050294354		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.5636928050294354 | validation: 0.5715356151044734]
	TIME [epoch: 25.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5695880038230798		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.5695880038230798 | validation: 0.5688128332358687]
	TIME [epoch: 25.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5546210565701721		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.5546210565701721 | validation: 0.5754310820287096]
	TIME [epoch: 25.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5640894969733384		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.5640894969733384 | validation: 0.5840442639607929]
	TIME [epoch: 25.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5615738375338923		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.5615738375338923 | validation: 0.5693289722152365]
	TIME [epoch: 25.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5610703849545737		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.5610703849545737 | validation: 0.5708531524789061]
	TIME [epoch: 25.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5629869169784778		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.5629869169784778 | validation: 0.5771314266537734]
	TIME [epoch: 25.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5628904417827566		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.5628904417827566 | validation: 0.5696682107996096]
	TIME [epoch: 25.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.565765142501731		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.565765142501731 | validation: 0.5747002356300401]
	TIME [epoch: 25.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5645331303916021		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.5645331303916021 | validation: 0.5763467015430415]
	TIME [epoch: 25.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5597387380808767		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.5597387380808767 | validation: 0.565354248555116]
	TIME [epoch: 25.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5656220897725018		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.5656220897725018 | validation: 0.5703168647952318]
	TIME [epoch: 25.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5649282881090901		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.5649282881090901 | validation: 0.5643200071416554]
	TIME [epoch: 25.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5616386038532479		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.5616386038532479 | validation: 0.5618774568579805]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_989.pth
	Model improved!!!
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5601141689734147		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.5601141689734147 | validation: 0.5687876666382317]
	TIME [epoch: 25.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5610855119971341		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.5610855119971341 | validation: 0.565272599837162]
	TIME [epoch: 25.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5602041547152734		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.5602041547152734 | validation: 0.5699502298196102]
	TIME [epoch: 25.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5609035936772359		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.5609035936772359 | validation: 0.5669057059014416]
	TIME [epoch: 25.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5594754667626688		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.5594754667626688 | validation: 0.5618676307682635]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5559471429933486		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.5559471429933486 | validation: 0.5666095433085301]
	TIME [epoch: 25.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5539451295786932		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.5539451295786932 | validation: 0.5725718107103384]
	TIME [epoch: 25.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5588313327790875		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.5588313327790875 | validation: 0.5765302419413378]
	TIME [epoch: 25.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5607090043617613		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.5607090043617613 | validation: 0.5639230618211275]
	TIME [epoch: 25.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5569362965650054		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.5569362965650054 | validation: 0.575132328919868]
	TIME [epoch: 25.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.558473938013386		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.558473938013386 | validation: 0.5708512955641389]
	TIME [epoch: 25.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5547731654551091		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.5547731654551091 | validation: 0.5723612351271051]
	TIME [epoch: 445 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561050677595917		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.5561050677595917 | validation: 0.5670207350826177]
	TIME [epoch: 54.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5525731191937899		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.5525731191937899 | validation: 0.5764637310163305]
	TIME [epoch: 54.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5591787472071668		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.5591787472071668 | validation: 0.5700073866691786]
	TIME [epoch: 54.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5616079118069074		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.5616079118069074 | validation: 0.5620704911338023]
	TIME [epoch: 54.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561377478619054		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.5561377478619054 | validation: 0.5564874603564041]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1006.pth
	Model improved!!!
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5543304676460796		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.5543304676460796 | validation: 0.5625966158683993]
	TIME [epoch: 54.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5507206140453469		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.5507206140453469 | validation: 0.5656185857121181]
	TIME [epoch: 54.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5559338214784555		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.5559338214784555 | validation: 0.5642150127408457]
	TIME [epoch: 54.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5554625421374128		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.5554625421374128 | validation: 0.5583813334605272]
	TIME [epoch: 54.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5505741904342867		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.5505741904342867 | validation: 0.5729387197990474]
	TIME [epoch: 54.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5600327257250322		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.5600327257250322 | validation: 0.5669190270177666]
	TIME [epoch: 54.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5550668921961506		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.5550668921961506 | validation: 0.5584815479655586]
	TIME [epoch: 54.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5535924076365477		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.5535924076365477 | validation: 0.5675720324992186]
	TIME [epoch: 54.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5493152745350869		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.5493152745350869 | validation: 0.5704960337999705]
	TIME [epoch: 54.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5520850382205013		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.5520850382205013 | validation: 0.5674729595979997]
	TIME [epoch: 54.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5544726770027414		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.5544726770027414 | validation: 0.5670716655746857]
	TIME [epoch: 54.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5563152418387314		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.5563152418387314 | validation: 0.562183049340709]
	TIME [epoch: 54.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480915685553254		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.5480915685553254 | validation: 0.5649948715275452]
	TIME [epoch: 54.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561864332298687		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.5561864332298687 | validation: 0.5637923755909949]
	TIME [epoch: 54.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5542523136735494		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.5542523136735494 | validation: 0.5553924045464949]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1021.pth
	Model improved!!!
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5498244956758456		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.5498244956758456 | validation: 0.5623448675279421]
	TIME [epoch: 54.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.551026503739539		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.551026503739539 | validation: 0.5632009861167225]
	TIME [epoch: 54.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5519211473266706		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.5519211473266706 | validation: 0.5712836936808671]
	TIME [epoch: 54.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5529547979976024		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.5529547979976024 | validation: 0.5807140893995961]
	TIME [epoch: 54.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5508175481049873		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.5508175481049873 | validation: 0.5557596893185243]
	TIME [epoch: 54.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5471360452515849		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.5471360452515849 | validation: 0.5726981759048662]
	TIME [epoch: 54.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5518419551589073		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.5518419551589073 | validation: 0.5681292023987675]
	TIME [epoch: 54.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5584392928244757		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.5584392928244757 | validation: 0.5668562584004]
	TIME [epoch: 54.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5514434926659328		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.5514434926659328 | validation: 0.5653937797356796]
	TIME [epoch: 54.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5560565630821674		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.5560565630821674 | validation: 0.5620193095213243]
	TIME [epoch: 54.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5509399290019088		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.5509399290019088 | validation: 0.5614231283420017]
	TIME [epoch: 54.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5536892735670735		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.5536892735670735 | validation: 0.5582046611819851]
	TIME [epoch: 54.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5515400481708503		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.5515400481708503 | validation: 0.5671842114345436]
	TIME [epoch: 54.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5530117277242248		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.5530117277242248 | validation: 0.5690033841778417]
	TIME [epoch: 54.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.554392099218205		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.554392099218205 | validation: 0.5660864511937792]
	TIME [epoch: 54.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.55136708469814		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.55136708469814 | validation: 0.570114009889601]
	TIME [epoch: 54.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5544860768734593		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.5544860768734593 | validation: 0.5581320137122665]
	TIME [epoch: 54.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.552332966377869		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.552332966377869 | validation: 0.5618152126269637]
	TIME [epoch: 54.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5487253506416894		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.5487253506416894 | validation: 0.5722320223746066]
	TIME [epoch: 54.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5553667611970563		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.5553667611970563 | validation: 0.5642497353412801]
	TIME [epoch: 54.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5508982272939716		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.5508982272939716 | validation: 0.5673480732958489]
	TIME [epoch: 54.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5514363484498734		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.5514363484498734 | validation: 0.5616506044361764]
	TIME [epoch: 54.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5498042740732415		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.5498042740732415 | validation: 0.5541322863858381]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1044.pth
	Model improved!!!
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5524085132403663		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.5524085132403663 | validation: 0.5644224987994115]
	TIME [epoch: 54.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5564632359060255		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.5564632359060255 | validation: 0.5633387222650876]
	TIME [epoch: 54.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5489769991248221		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.5489769991248221 | validation: 0.5796810705106475]
	TIME [epoch: 54.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.55117117598907		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.55117117598907 | validation: 0.5633215307627772]
	TIME [epoch: 54.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5448836429208714		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.5448836429208714 | validation: 0.5583710998949819]
	TIME [epoch: 54.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5489854481873297		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.5489854481873297 | validation: 0.5465016094258327]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1050.pth
	Model improved!!!
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465015804693192		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.5465015804693192 | validation: 0.5605646496173791]
	TIME [epoch: 54.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.551109494981404		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.551109494981404 | validation: 0.5612912521319522]
	TIME [epoch: 54.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5515922512600976		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.5515922512600976 | validation: 0.5573766367426278]
	TIME [epoch: 54.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.549900606034812		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.549900606034812 | validation: 0.5738228252114084]
	TIME [epoch: 54.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5503489666772755		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.5503489666772755 | validation: 0.5539894204090311]
	TIME [epoch: 54.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5472938945074124		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.5472938945074124 | validation: 0.5594818122129397]
	TIME [epoch: 54.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5475895067409827		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.5475895067409827 | validation: 0.5582381524970602]
	TIME [epoch: 54.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5488837922675857		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.5488837922675857 | validation: 0.5666521064397723]
	TIME [epoch: 54.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5471836593781328		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.5471836593781328 | validation: 0.5611414480547013]
	TIME [epoch: 54.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5504505246261535		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.5504505246261535 | validation: 0.5710989111696144]
	TIME [epoch: 54.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5541825818673864		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.5541825818673864 | validation: 0.5657724065787643]
	TIME [epoch: 54.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5517735066422033		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.5517735066422033 | validation: 0.5559326820408521]
	TIME [epoch: 54.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.548525090410976		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.548525090410976 | validation: 0.5634390959255131]
	TIME [epoch: 54.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454303352629777		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.5454303352629777 | validation: 0.5651589744614582]
	TIME [epoch: 54.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.560285233710526		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.560285233710526 | validation: 0.5526302775096295]
	TIME [epoch: 54.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5493061520751898		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.5493061520751898 | validation: 0.5610457585053501]
	TIME [epoch: 54.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.545575042239712		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.545575042239712 | validation: 0.5586955455157344]
	TIME [epoch: 54.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.547683475833532		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.547683475833532 | validation: 0.5694810798664409]
	TIME [epoch: 54.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450879916491786		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.5450879916491786 | validation: 0.5607228310340864]
	TIME [epoch: 54.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.549608844940225		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.549608844940225 | validation: 0.5481155338884407]
	TIME [epoch: 54.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5452326368991485		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.5452326368991485 | validation: 0.5770614251218273]
	TIME [epoch: 54.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5533489777127018		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.5533489777127018 | validation: 0.5547436042700219]
	TIME [epoch: 54.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5441713236393345		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.5441713236393345 | validation: 0.5515644911186276]
	TIME [epoch: 54.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5483332273926713		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.5483332273926713 | validation: 0.550241047924893]
	TIME [epoch: 54.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5503489357355368		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.5503489357355368 | validation: 0.5725164171664126]
	TIME [epoch: 54.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5516686611366044		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.5516686611366044 | validation: 0.5589227689672479]
	TIME [epoch: 54.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5481893120497106		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.5481893120497106 | validation: 0.5567656206222046]
	TIME [epoch: 54.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5451388131326526		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.5451388131326526 | validation: 0.5643218067895863]
	TIME [epoch: 54.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5435772528228066		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.5435772528228066 | validation: 0.5508043222670775]
	TIME [epoch: 54.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5413528146256157		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.5413528146256157 | validation: 0.5556933955453649]
	TIME [epoch: 54.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5471019746133956		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.5471019746133956 | validation: 0.5678377012318929]
	TIME [epoch: 54.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5447518460730474		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.5447518460730474 | validation: 0.5651459759585348]
	TIME [epoch: 54.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5470784724263615		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.5470784724263615 | validation: 0.5624979280624451]
	TIME [epoch: 54.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450734368852465		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.5450734368852465 | validation: 0.5523100228152282]
	TIME [epoch: 54.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5468775164266431		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.5468775164266431 | validation: 0.5619734806085702]
	TIME [epoch: 54.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.544107056687692		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.544107056687692 | validation: 0.5545645709096814]
	TIME [epoch: 54.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5441156252431616		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.5441156252431616 | validation: 0.5606556877535778]
	TIME [epoch: 54.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480366669909162		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.5480366669909162 | validation: 0.5608607243026618]
	TIME [epoch: 54.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5405029405409095		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.5405029405409095 | validation: 0.5505567497891897]
	TIME [epoch: 54.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5461656500803818		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.5461656500803818 | validation: 0.5531070913945874]
	TIME [epoch: 54.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.544934107745684		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.544934107745684 | validation: 0.5571456372462236]
	TIME [epoch: 54.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5433340151069743		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.5433340151069743 | validation: 0.5600914192635932]
	TIME [epoch: 54.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454493717697424		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.5454493717697424 | validation: 0.5476338669276952]
	TIME [epoch: 54.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5442255940430809		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.5442255940430809 | validation: 0.5475747015651586]
	TIME [epoch: 54.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5439504458560063		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.5439504458560063 | validation: 0.5541649357016853]
	TIME [epoch: 54.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5465852707569554		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.5465852707569554 | validation: 0.548315388126345]
	TIME [epoch: 54.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5457554889129537		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.5457554889129537 | validation: 0.5471964502959187]
	TIME [epoch: 54.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.540850117770212		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.540850117770212 | validation: 0.5544616946428714]
	TIME [epoch: 54.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5446332699979236		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.5446332699979236 | validation: 0.55290021594619]
	TIME [epoch: 54.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5439324684081024		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.5439324684081024 | validation: 0.5502976846349077]
	TIME [epoch: 54.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5440908780971381		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.5440908780971381 | validation: 0.5481124322041782]
	TIME [epoch: 54.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5466817398811086		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.5466817398811086 | validation: 0.5499657661070183]
	TIME [epoch: 54.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5437584638094106		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.5437584638094106 | validation: 0.5496387635825989]
	TIME [epoch: 54.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5422694596457556		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.5422694596457556 | validation: 0.5551092738203338]
	TIME [epoch: 54.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5468383162188102		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.5468383162188102 | validation: 0.5560593106724931]
	TIME [epoch: 54.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5423959099978316		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.5423959099978316 | validation: 0.546096382754107]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1106.pth
	Model improved!!!
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5416828307818917		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.5416828307818917 | validation: 0.55061873384084]
	TIME [epoch: 54.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5422224301541436		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.5422224301541436 | validation: 0.5445269476265842]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1108.pth
	Model improved!!!
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5402464552273755		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.5402464552273755 | validation: 0.5522846009446739]
	TIME [epoch: 54.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5431358895840472		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.5431358895840472 | validation: 0.549932690411816]
	TIME [epoch: 54.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5445331165344199		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.5445331165344199 | validation: 0.5463308457748006]
	TIME [epoch: 54.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5367863397996521		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.5367863397996521 | validation: 0.5460451006814242]
	TIME [epoch: 54.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450064474599783		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.5450064474599783 | validation: 0.5589830575134715]
	TIME [epoch: 54.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.543241854984126		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.543241854984126 | validation: 0.5506829415530596]
	TIME [epoch: 54.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5438376811522995		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.5438376811522995 | validation: 0.5599361798515525]
	TIME [epoch: 54.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5401427289886123		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.5401427289886123 | validation: 0.5568128696846555]
	TIME [epoch: 54.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5431116775028373		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.5431116775028373 | validation: 0.5544063155756985]
	TIME [epoch: 54.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5419587230767742		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.5419587230767742 | validation: 0.5503996449987064]
	TIME [epoch: 54.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5389125108020483		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.5389125108020483 | validation: 0.5497957740065749]
	TIME [epoch: 54.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5413455817162135		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.5413455817162135 | validation: 0.5469585597752847]
	TIME [epoch: 54.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5390650035179546		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.5390650035179546 | validation: 0.548427294326902]
	TIME [epoch: 54.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5420170934454612		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.5420170934454612 | validation: 0.5584237071892422]
	TIME [epoch: 54.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5401499826316584		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.5401499826316584 | validation: 0.5534595835618182]
	TIME [epoch: 54.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5420426226553388		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.5420426226553388 | validation: 0.5502160523217682]
	TIME [epoch: 54.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5398118559143114		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.5398118559143114 | validation: 0.5545234417266319]
	TIME [epoch: 54.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5391710647190335		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.5391710647190335 | validation: 0.5485441361455706]
	TIME [epoch: 54.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5361913781845616		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.5361913781845616 | validation: 0.5435026596070393]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5358241586930981		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.5358241586930981 | validation: 0.5530480338499763]
	TIME [epoch: 54.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5428631001241047		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.5428631001241047 | validation: 0.553323749461922]
	TIME [epoch: 54.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5377594112754134		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.5377594112754134 | validation: 0.5413955827001884]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1130.pth
	Model improved!!!
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5411474948303879		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.5411474948303879 | validation: 0.5502991898043041]
	TIME [epoch: 54.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5433792505692485		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.5433792505692485 | validation: 0.5638990356245877]
	TIME [epoch: 54.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5427397148390702		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.5427397148390702 | validation: 0.5433685392688676]
	TIME [epoch: 54.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5379502368826841		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.5379502368826841 | validation: 0.5482754607471771]
	TIME [epoch: 54.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.537609181925303		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.537609181925303 | validation: 0.5543247958385915]
	TIME [epoch: 54.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454224101546111		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.5454224101546111 | validation: 0.5466436006092465]
	TIME [epoch: 54.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5396569332912766		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.5396569332912766 | validation: 0.5417307197125324]
	TIME [epoch: 54.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5382603688789509		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.5382603688789509 | validation: 0.5513924326168302]
	TIME [epoch: 54.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5366527132155356		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.5366527132155356 | validation: 0.5486236278130988]
	TIME [epoch: 54.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5406108265693653		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.5406108265693653 | validation: 0.5444135029745303]
	TIME [epoch: 54.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5371702041450888		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.5371702041450888 | validation: 0.5530091733186295]
	TIME [epoch: 54.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5398172003936321		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.5398172003936321 | validation: 0.5465523103822325]
	TIME [epoch: 54.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5372840503212376		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.5372840503212376 | validation: 0.5489346506098427]
	TIME [epoch: 54.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5370801616401925		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.5370801616401925 | validation: 0.5473645825279845]
	TIME [epoch: 54.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5398306675297696		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.5398306675297696 | validation: 0.5511407713701891]
	TIME [epoch: 54.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5397507139628461		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.5397507139628461 | validation: 0.5514119758125016]
	TIME [epoch: 54.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.543950026943762		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.543950026943762 | validation: 0.5494438225103566]
	TIME [epoch: 54.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5384222004032603		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.5384222004032603 | validation: 0.5436509010265427]
	TIME [epoch: 54.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5374500541553271		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.5374500541553271 | validation: 0.540212026138749]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5372872575460168		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.5372872575460168 | validation: 0.5370997208659112]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1150.pth
	Model improved!!!
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5323154423482372		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.5323154423482372 | validation: 0.5474503974156489]
	TIME [epoch: 54.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5352180161466389		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.5352180161466389 | validation: 0.5497554099110369]
	TIME [epoch: 54.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5390577048774238		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.5390577048774238 | validation: 0.5443377986967689]
	TIME [epoch: 54.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5379695379446846		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.5379695379446846 | validation: 0.5460574903127209]
	TIME [epoch: 54.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5341362028010539		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.5341362028010539 | validation: 0.5439136470630779]
	TIME [epoch: 54.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5359329578698017		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.5359329578698017 | validation: 0.5418041826018158]
	TIME [epoch: 54.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.539822090152852		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.539822090152852 | validation: 0.547387034259811]
	TIME [epoch: 54.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5365111269177112		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.5365111269177112 | validation: 0.5444348605872222]
	TIME [epoch: 54.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.536688410353061		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.536688410353061 | validation: 0.540609539777533]
	TIME [epoch: 54.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5369110666606843		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.5369110666606843 | validation: 0.5374743432625744]
	TIME [epoch: 54.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5354481477930605		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.5354481477930605 | validation: 0.5416688868109789]
	TIME [epoch: 54.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338045374600545		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.5338045374600545 | validation: 0.538806659503756]
	TIME [epoch: 54.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5352084865254826		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.5352084865254826 | validation: 0.554647298992299]
	TIME [epoch: 54.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5397432886212321		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.5397432886212321 | validation: 0.5478227108263004]
	TIME [epoch: 54.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338402302800783		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.5338402302800783 | validation: 0.5453408207552897]
	TIME [epoch: 54.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5392217822024422		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.5392217822024422 | validation: 0.5476413015060994]
	TIME [epoch: 54.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5374782872697864		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.5374782872697864 | validation: 0.5459610240207737]
	TIME [epoch: 54.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5348197182290534		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.5348197182290534 | validation: 0.5421285888441766]
	TIME [epoch: 54.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5353208057444935		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.5353208057444935 | validation: 0.542866285836358]
	TIME [epoch: 54.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5343800290279115		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.5343800290279115 | validation: 0.5427595609367225]
	TIME [epoch: 54.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5393560668944208		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.5393560668944208 | validation: 0.5454341458422253]
	TIME [epoch: 54.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5340379195274036		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.5340379195274036 | validation: 0.5411871982125646]
	TIME [epoch: 54.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338377859135337		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.5338377859135337 | validation: 0.5409481611201247]
	TIME [epoch: 54.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5309599132174541		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.5309599132174541 | validation: 0.5460254476351675]
	TIME [epoch: 54.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5328652164017174		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.5328652164017174 | validation: 0.5449128764520751]
	TIME [epoch: 54.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5354220844725345		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.5354220844725345 | validation: 0.5460524621470674]
	TIME [epoch: 54.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5332635762102953		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.5332635762102953 | validation: 0.5435780882480236]
	TIME [epoch: 54.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5322640525503733		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.5322640525503733 | validation: 0.5422988552336189]
	TIME [epoch: 54.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338918096564159		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.5338918096564159 | validation: 0.5394661675847512]
	TIME [epoch: 54.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5320121181992242		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.5320121181992242 | validation: 0.5403501804358092]
	TIME [epoch: 54.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5299181326941327		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.5299181326941327 | validation: 0.5406047189474292]
	TIME [epoch: 54.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.535000710223653		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.535000710223653 | validation: 0.543365204526992]
	TIME [epoch: 54.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5340123270201975		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.5340123270201975 | validation: 0.5428413465781433]
	TIME [epoch: 54.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5343340618499326		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.5343340618499326 | validation: 0.5408147964691723]
	TIME [epoch: 54.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5315282188907631		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.5315282188907631 | validation: 0.5412843049439617]
	TIME [epoch: 54.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5332057669971526		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.5332057669971526 | validation: 0.5432282365959586]
	TIME [epoch: 54.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.531845344072095		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.531845344072095 | validation: 0.540459158450262]
	TIME [epoch: 54.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5366695395799028		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.5366695395799028 | validation: 0.5333809116429001]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1188.pth
	Model improved!!!
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5336453988863517		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.5336453988863517 | validation: 0.5384806290243482]
	TIME [epoch: 54.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5302557440787015		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.5302557440787015 | validation: 0.5412139795758015]
	TIME [epoch: 54.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5323945846122514		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.5323945846122514 | validation: 0.5583262812237404]
	TIME [epoch: 54.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5371853935150154		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.5371853935150154 | validation: 0.5341400345956955]
	TIME [epoch: 54.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5307065684852272		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.5307065684852272 | validation: 0.5443941745346922]
	TIME [epoch: 54.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5317756066422924		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.5317756066422924 | validation: 0.5494775619945982]
	TIME [epoch: 54.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.540786406410799		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.540786406410799 | validation: 0.5368182915446031]
	TIME [epoch: 54.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5326548060908197		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.5326548060908197 | validation: 0.5315302639772832]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1196.pth
	Model improved!!!
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5305942151980355		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.5305942151980355 | validation: 0.543510782589898]
	TIME [epoch: 54.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5313988805450742		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.5313988805450742 | validation: 0.5442032084261]
	TIME [epoch: 54.2 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303491421894437		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.5303491421894437 | validation: 0.5381191373708575]
	TIME [epoch: 54.2 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303036872308441		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.5303036872308441 | validation: 0.5410361845438177]
	TIME [epoch: 54.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5299636156811195		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.5299636156811195 | validation: 0.5372707715110779]
	TIME [epoch: 54.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5318255618218435		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.5318255618218435 | validation: 0.5360080059639241]
	TIME [epoch: 54.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5293009726786827		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.5293009726786827 | validation: 0.5338248774292692]
	TIME [epoch: 54.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5307734313440227		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.5307734313440227 | validation: 0.542935216196061]
	TIME [epoch: 54.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5328136808759885		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.5328136808759885 | validation: 0.5326089686028436]
	TIME [epoch: 54.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5341941513749674		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.5341941513749674 | validation: 0.5385989718274937]
	TIME [epoch: 54.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5335399286297322		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.5335399286297322 | validation: 0.5391420266151347]
	TIME [epoch: 54.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.532531775471139		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.532531775471139 | validation: 0.5361493402750963]
	TIME [epoch: 54.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5291690543503291		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.5291690543503291 | validation: 0.5333091297029187]
	TIME [epoch: 54.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5335376375325908		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.5335376375325908 | validation: 0.5300218138288968]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1210.pth
	Model improved!!!
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5323087605878715		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.5323087605878715 | validation: 0.5380076106090856]
	TIME [epoch: 54.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5291828608586624		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.5291828608586624 | validation: 0.5382019744496449]
	TIME [epoch: 54.6 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303289808279786		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.5303289808279786 | validation: 0.5323415544407093]
	TIME [epoch: 54.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5312666019803933		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.5312666019803933 | validation: 0.5335146959230427]
	TIME [epoch: 54.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5311311271137934		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.5311311271137934 | validation: 0.535858583555763]
	TIME [epoch: 54.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5322129796834144		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.5322129796834144 | validation: 0.5315242909566771]
	TIME [epoch: 54.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5326803664482963		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.5326803664482963 | validation: 0.5380366359153388]
	TIME [epoch: 54.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5286505575761784		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.5286505575761784 | validation: 0.5370659685799286]
	TIME [epoch: 54.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5309575376068959		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.5309575376068959 | validation: 0.5353529953486429]
	TIME [epoch: 54.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.529678352097317		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.529678352097317 | validation: 0.5364206431322056]
	TIME [epoch: 54.4 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5289602097505851		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.5289602097505851 | validation: 0.5361927586841365]
	TIME [epoch: 54.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5288310518400778		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.5288310518400778 | validation: 0.5348562429625326]
	TIME [epoch: 54.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5299654762375076		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.5299654762375076 | validation: 0.5342189219948331]
	TIME [epoch: 54.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5283236408222293		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.5283236408222293 | validation: 0.5362359834764099]
	TIME [epoch: 54.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5292700371016358		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.5292700371016358 | validation: 0.5370289029615378]
	TIME [epoch: 54.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5295416465892888		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.5295416465892888 | validation: 0.5426797616526148]
	TIME [epoch: 54.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5261489745904592		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.5261489745904592 | validation: 0.5322303713816441]
	TIME [epoch: 54.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5286361237191745		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.5286361237191745 | validation: 0.5356899127046265]
	TIME [epoch: 54.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5287437471719376		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.5287437471719376 | validation: 0.5324857936987022]
	TIME [epoch: 54.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5294516727035476		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.5294516727035476 | validation: 0.5363732089832858]
	TIME [epoch: 54.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5295815448012772		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.5295815448012772 | validation: 0.5327054554438672]
	TIME [epoch: 54.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.528172902145879		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.528172902145879 | validation: 0.5337334259867716]
	TIME [epoch: 54.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5291897802913851		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.5291897802913851 | validation: 0.5350139531866824]
	TIME [epoch: 54.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5268212364544814		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.5268212364544814 | validation: 0.5382966869673969]
	TIME [epoch: 54.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5269603205461028		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.5269603205461028 | validation: 0.5355902184995079]
	TIME [epoch: 54.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5273015320696911		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.5273015320696911 | validation: 0.5344009119357013]
	TIME [epoch: 54.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5248492857583027		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.5248492857583027 | validation: 0.5352470562869961]
	TIME [epoch: 54.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.52937053072603		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.52937053072603 | validation: 0.5372558439074677]
	TIME [epoch: 54.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5302151738877964		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.5302151738877964 | validation: 0.5302005925864905]
	TIME [epoch: 54.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5292681640432648		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.5292681640432648 | validation: 0.5310231172485684]
	TIME [epoch: 54.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5274007811143645		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.5274007811143645 | validation: 0.5350816505345586]
	TIME [epoch: 54.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5242382328311308		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.5242382328311308 | validation: 0.5308159952979792]
	TIME [epoch: 54.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5273642686049266		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.5273642686049266 | validation: 0.5307016956651135]
	TIME [epoch: 54.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5292804289448901		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.5292804289448901 | validation: 0.5281943648046074]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1244.pth
	Model improved!!!
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5303632182723009		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.5303632182723009 | validation: 0.5300583096803873]
	TIME [epoch: 54.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5269945385198884		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.5269945385198884 | validation: 0.529846401064279]
	TIME [epoch: 54.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5258250482841842		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.5258250482841842 | validation: 0.5298417035576728]
	TIME [epoch: 54.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5281491755992754		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.5281491755992754 | validation: 0.5340231375713587]
	TIME [epoch: 54.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5256150697595967		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.5256150697595967 | validation: 0.5335635545190933]
	TIME [epoch: 54.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5232286013816815		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.5232286013816815 | validation: 0.5381438665096994]
	TIME [epoch: 54.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5256306954417174		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.5256306954417174 | validation: 0.5309163761173856]
	TIME [epoch: 54.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5252941986919013		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.5252941986919013 | validation: 0.5309328739765848]
	TIME [epoch: 54.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5248930003849048		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.5248930003849048 | validation: 0.5247292077048777]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1253.pth
	Model improved!!!
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5263661341806183		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.5263661341806183 | validation: 0.5299698060173545]
	TIME [epoch: 54.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5257797574832225		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.5257797574832225 | validation: 0.5321166987557607]
	TIME [epoch: 54.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5243880653307574		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.5243880653307574 | validation: 0.5324474554158157]
	TIME [epoch: 54.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5257792632029299		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.5257792632029299 | validation: 0.5296462431284934]
	TIME [epoch: 54.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5245879366589878		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.5245879366589878 | validation: 0.5299892279841741]
	TIME [epoch: 54.2 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5259601905399269		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.5259601905399269 | validation: 0.5390058375577345]
	TIME [epoch: 54.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5247340496609132		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.5247340496609132 | validation: 0.5357873389777024]
	TIME [epoch: 54.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5242826808753851		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.5242826808753851 | validation: 0.5319877703341249]
	TIME [epoch: 54.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5260224437758604		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.5260224437758604 | validation: 0.533527494376436]
	TIME [epoch: 54.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5240644789246914		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.5240644789246914 | validation: 0.5307529950950226]
	TIME [epoch: 54.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.522798816362047		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.522798816362047 | validation: 0.5301597559147939]
	TIME [epoch: 54.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5213797270136862		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.5213797270136862 | validation: 0.5272737470011684]
	TIME [epoch: 54.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5247073247876253		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.5247073247876253 | validation: 0.5297218321402111]
	TIME [epoch: 54.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5231786201171433		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.5231786201171433 | validation: 0.5349683786032]
	TIME [epoch: 54.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5268367452935392		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.5268367452935392 | validation: 0.525222954691936]
	TIME [epoch: 54.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228943168155521		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.5228943168155521 | validation: 0.5307720716302052]
	TIME [epoch: 54.2 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5242723386310416		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.5242723386310416 | validation: 0.5277969673015106]
	TIME [epoch: 54.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5246460750510626		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.5246460750510626 | validation: 0.5277618413363838]
	TIME [epoch: 54.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5221838745783332		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.5221838745783332 | validation: 0.5308266614484489]
	TIME [epoch: 54.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5246682112541561		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.5246682112541561 | validation: 0.530890123522481]
	TIME [epoch: 54.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5233481316889688		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.5233481316889688 | validation: 0.5295136240230429]
	TIME [epoch: 54.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228047191606743		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.5228047191606743 | validation: 0.530708952556278]
	TIME [epoch: 54.2 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5237427676528348		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.5237427676528348 | validation: 0.5259831786368836]
	TIME [epoch: 54.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5227716835981323		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.5227716835981323 | validation: 0.5272054676108373]
	TIME [epoch: 54.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5244341476840619		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.5244341476840619 | validation: 0.527783184339977]
	TIME [epoch: 54.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.522323832763277		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.522323832763277 | validation: 0.5230287491286953]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1279.pth
	Model improved!!!
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5234544870510831		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.5234544870510831 | validation: 0.5278510914810346]
	TIME [epoch: 54.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5202611700245372		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.5202611700245372 | validation: 0.5288030366507505]
	TIME [epoch: 54.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5216038653908405		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.5216038653908405 | validation: 0.5340221577062124]
	TIME [epoch: 54.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5240462137198848		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.5240462137198848 | validation: 0.5292256237628]
	TIME [epoch: 54.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5257635920772331		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.5257635920772331 | validation: 0.5387491892633884]
	TIME [epoch: 54.4 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5247838129951524		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.5247838129951524 | validation: 0.5261444800914985]
	TIME [epoch: 54.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5208788819177248		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.5208788819177248 | validation: 0.5241459860033452]
	TIME [epoch: 54.4 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5206653425316564		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.5206653425316564 | validation: 0.5270772283001206]
	TIME [epoch: 54.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5219413849570611		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.5219413849570611 | validation: 0.5267409047555179]
	TIME [epoch: 54.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5196972832454692		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.5196972832454692 | validation: 0.5248426781120018]
	TIME [epoch: 54.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228613399402606		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.5228613399402606 | validation: 0.5325977284093423]
	TIME [epoch: 54.2 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5225230744606576		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.5225230744606576 | validation: 0.5370672713863872]
	TIME [epoch: 54.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5241817022917017		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.5241817022917017 | validation: 0.5212914505377411]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1292.pth
	Model improved!!!
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228903392119547		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.5228903392119547 | validation: 0.5248933369796456]
	TIME [epoch: 54.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5225047762250065		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.5225047762250065 | validation: 0.5290895898600569]
	TIME [epoch: 54.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5206396170726344		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.5206396170726344 | validation: 0.5278473307932927]
	TIME [epoch: 54.3 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5225658193658146		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.5225658193658146 | validation: 0.5225384795254919]
	TIME [epoch: 54.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5209748013496375		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.5209748013496375 | validation: 0.5311211008394969]
	TIME [epoch: 54.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.519355887542815		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.519355887542815 | validation: 0.5289817248140358]
	TIME [epoch: 54.2 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.52071146257616		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.52071146257616 | validation: 0.5262562797164914]
	TIME [epoch: 54.2 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5210791222432483		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.5210791222432483 | validation: 0.5300195476406055]
	TIME [epoch: 54.2 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5217590299160362		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.5217590299160362 | validation: 0.5287589324265023]
	TIME [epoch: 54.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5214711041765753		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.5214711041765753 | validation: 0.5257771775334903]
	TIME [epoch: 54.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.521790204503608		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.521790204503608 | validation: 0.5261299001113128]
	TIME [epoch: 54.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5197170288430911		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.5197170288430911 | validation: 0.5248578719944177]
	TIME [epoch: 54.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193566065875054		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.5193566065875054 | validation: 0.5231051590068638]
	TIME [epoch: 54.2 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.518485070815988		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.518485070815988 | validation: 0.5255821437114312]
	TIME [epoch: 54.2 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5202684788592401		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.5202684788592401 | validation: 0.5274798207996498]
	TIME [epoch: 54.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5195279660066341		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.5195279660066341 | validation: 0.527127864312962]
	TIME [epoch: 54.2 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193533371724196		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.5193533371724196 | validation: 0.5287822016407836]
	TIME [epoch: 54.2 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5208948286937753		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.5208948286937753 | validation: 0.5232654444702083]
	TIME [epoch: 54.2 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.519906564299103		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.519906564299103 | validation: 0.5206621648626543]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1311.pth
	Model improved!!!
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5196112113337805		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.5196112113337805 | validation: 0.5222539570967328]
	TIME [epoch: 54.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5217594058402725		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.5217594058402725 | validation: 0.5264222111450193]
	TIME [epoch: 54.2 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5194777158803149		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.5194777158803149 | validation: 0.5260339043530469]
	TIME [epoch: 54.2 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5188306878708173		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.5188306878708173 | validation: 0.527472711720197]
	TIME [epoch: 54.2 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5215313149024726		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.5215313149024726 | validation: 0.5208412940716809]
	TIME [epoch: 54.2 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5181460112449577		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.5181460112449577 | validation: 0.5268444139191393]
	TIME [epoch: 54.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5182827394927249		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.5182827394927249 | validation: 0.5210090577567025]
	TIME [epoch: 54.3 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193287853800359		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.5193287853800359 | validation: 0.5255658343623557]
	TIME [epoch: 54.2 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5173588686083255		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.5173588686083255 | validation: 0.5306564206472932]
	TIME [epoch: 54.3 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5193694442789808		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.5193694442789808 | validation: 0.5232958346198249]
	TIME [epoch: 54.2 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.518197515870161		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.518197515870161 | validation: 0.5267089602425096]
	TIME [epoch: 54.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5191403231532605		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.5191403231532605 | validation: 0.5261537916781401]
	TIME [epoch: 54.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5186883208151467		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.5186883208151467 | validation: 0.5255001860449768]
	TIME [epoch: 54.2 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5201996765679645		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.5201996765679645 | validation: 0.5220993419705852]
	TIME [epoch: 54.2 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5157618209966971		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.5157618209966971 | validation: 0.5205395671078565]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1326.pth
	Model improved!!!
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.520398883567295		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.520398883567295 | validation: 0.5207263713870869]
	TIME [epoch: 54.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5179287326487011		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.5179287326487011 | validation: 0.5203502018205048]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1328.pth
	Model improved!!!
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5195830566184982		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 0.5195830566184982 | validation: 0.522772745835623]
	TIME [epoch: 54.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5183749536673984		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 0.5183749536673984 | validation: 0.5197100907959005]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1330.pth
	Model improved!!!
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5170205867454387		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 0.5170205867454387 | validation: 0.5262846518349507]
	TIME [epoch: 54.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5189728321773454		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 0.5189728321773454 | validation: 0.5219188273254861]
	TIME [epoch: 54.2 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5186131000019978		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 0.5186131000019978 | validation: 0.5200997089355249]
	TIME [epoch: 54.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5174593253175709		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 0.5174593253175709 | validation: 0.526074133395573]
	TIME [epoch: 54.2 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164404988692023		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 0.5164404988692023 | validation: 0.5235488125998485]
	TIME [epoch: 54.2 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5177690572332023		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 0.5177690572332023 | validation: 0.5223323897528529]
	TIME [epoch: 54.2 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5173473304550658		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 0.5173473304550658 | validation: 0.5194323262803386]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1337.pth
	Model improved!!!
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5152943879567912		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 0.5152943879567912 | validation: 0.5179108785836934]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1338.pth
	Model improved!!!
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5166218133060315		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 0.5166218133060315 | validation: 0.5198009234736121]
	TIME [epoch: 54.2 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5186101419844968		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 0.5186101419844968 | validation: 0.5205602030352359]
	TIME [epoch: 54.2 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5179021947966975		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 0.5179021947966975 | validation: 0.5137144674163401]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1341.pth
	Model improved!!!
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5149970622751593		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 0.5149970622751593 | validation: 0.5169539800009272]
	TIME [epoch: 54.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164931076077866		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 0.5164931076077866 | validation: 0.5232900081855033]
	TIME [epoch: 54.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5164494583807666		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 0.5164494583807666 | validation: 0.5216427921802859]
	TIME [epoch: 54.4 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5173926286800881		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 0.5173926286800881 | validation: 0.5197328196520179]
	TIME [epoch: 54.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5165897795607312		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 0.5165897795607312 | validation: 0.5229696498629545]
	TIME [epoch: 54.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5160314718829884		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.5160314718829884 | validation: 0.5176062150966372]
	TIME [epoch: 54.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.515329278215816		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 0.515329278215816 | validation: 0.5221666446542544]
	TIME [epoch: 54.4 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5204266202667773		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 0.5204266202667773 | validation: 0.5192372019764027]
	TIME [epoch: 54.4 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5175826352023254		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 0.5175826352023254 | validation: 0.5164612783749285]
	TIME [epoch: 54.4 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5160479735100931		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 0.5160479735100931 | validation: 0.5191261430929772]
	TIME [epoch: 54.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5146380169275119		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 0.5146380169275119 | validation: 0.5177146479391667]
	TIME [epoch: 54.4 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5162494076615812		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 0.5162494076615812 | validation: 0.5158159339467796]
	TIME [epoch: 54.4 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.517128155729616		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 0.517128155729616 | validation: 0.5161906519980763]
	TIME [epoch: 54.4 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5190407003891733		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 0.5190407003891733 | validation: 0.5146348849474385]
	TIME [epoch: 54.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5176602363450189		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 0.5176602363450189 | validation: 0.5131843801898505]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1356.pth
	Model improved!!!
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5189103582823024		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 0.5189103582823024 | validation: 0.5155899759281113]
	TIME [epoch: 54.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5168425812148963		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 0.5168425812148963 | validation: 0.5123989568726179]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1358.pth
	Model improved!!!
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.514583054239548		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 0.514583054239548 | validation: 0.5197287936138807]
	TIME [epoch: 54.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5143049051663671		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 0.5143049051663671 | validation: 0.5202367598125189]
	TIME [epoch: 54.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.516333054802834		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 0.516333054802834 | validation: 0.5219505065467894]
	TIME [epoch: 54.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.516491833013423		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 0.516491833013423 | validation: 0.5146591948162851]
	TIME [epoch: 54.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5162702617337818		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 0.5162702617337818 | validation: 0.5136980404208302]
	TIME [epoch: 54.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5171453940698547		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 0.5171453940698547 | validation: 0.5168248108320606]
	TIME [epoch: 54.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5186615819663016		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 0.5186615819663016 | validation: 0.5153747902355807]
	TIME [epoch: 54.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5137647549260826		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 0.5137647549260826 | validation: 0.5112172198076392]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1366.pth
	Model improved!!!
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.515925393320356		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 0.515925393320356 | validation: 0.5149461354924267]
	TIME [epoch: 54.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5152681266583685		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 0.5152681266583685 | validation: 0.51069206174188]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1368.pth
	Model improved!!!
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5145319962627863		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 0.5145319962627863 | validation: 0.5156395392551028]
	TIME [epoch: 54.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5148385047939829		[learning rate: 9.3243e-05]
	Learning Rate: 9.32429e-05
	LOSS [training: 0.5148385047939829 | validation: 0.5145507687529386]
	TIME [epoch: 54.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5150879090890196		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 0.5150879090890196 | validation: 0.5187719957413239]
	TIME [epoch: 54.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.513721311777283		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 0.513721311777283 | validation: 0.5128199218241133]
	TIME [epoch: 54.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5152163758588205		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 0.5152163758588205 | validation: 0.5129678272739258]
	TIME [epoch: 54.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5138592310724303		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 0.5138592310724303 | validation: 0.5160182068078325]
	TIME [epoch: 54.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5168585121261465		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 0.5168585121261465 | validation: 0.5130222170136989]
	TIME [epoch: 54.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5146093463092553		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 0.5146093463092553 | validation: 0.5132632429147508]
	TIME [epoch: 54.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5161230512503808		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 0.5161230512503808 | validation: 0.5134503999377219]
	TIME [epoch: 54.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5122881507498175		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 0.5122881507498175 | validation: 0.5122959366096648]
	TIME [epoch: 54.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5142038755860039		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 0.5142038755860039 | validation: 0.5082001127291453]
	TIME [epoch: 54.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1379.pth
	Model improved!!!
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5145349946908768		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 0.5145349946908768 | validation: 0.5163379145824079]
	TIME [epoch: 54.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5144866429680228		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 0.5144866429680228 | validation: 0.513789036625134]
	TIME [epoch: 54.3 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5150916750256394		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 0.5150916750256394 | validation: 0.5188642669545717]
	TIME [epoch: 54.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5165380863484164		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 0.5165380863484164 | validation: 0.5210982382991426]
	TIME [epoch: 54.3 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5140015868420308		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 0.5140015868420308 | validation: 0.5108248622021687]
	TIME [epoch: 54.2 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5162734499658379		[learning rate: 8.8418e-05]
	Learning Rate: 8.84176e-05
	LOSS [training: 0.5162734499658379 | validation: 0.5144208507043142]
	TIME [epoch: 54.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.513106551932424		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.513106551932424 | validation: 0.5179546785359609]
	TIME [epoch: 54.2 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5161914535110997		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 0.5161914535110997 | validation: 0.5162963955083754]
	TIME [epoch: 54.3 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.513660757146261		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 0.513660757146261 | validation: 0.509345877202285]
	TIME [epoch: 54.2 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5126526686492597		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 0.5126526686492597 | validation: 0.5135257849115049]
	TIME [epoch: 54.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5145356225319282		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 0.5145356225319282 | validation: 0.5221859696111084]
	TIME [epoch: 54.2 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.514745393130159		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 0.514745393130159 | validation: 0.5106670677419237]
	TIME [epoch: 54.2 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5145419657933846		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 0.5145419657933846 | validation: 0.5135172755062427]
	TIME [epoch: 54.3 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5137458792438029		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 0.5137458792438029 | validation: 0.5207004856085811]
	TIME [epoch: 54.2 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5126011805823824		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 0.5126011805823824 | validation: 0.5166380390708823]
	TIME [epoch: 54.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5154692490140362		[learning rate: 8.534e-05]
	Learning Rate: 8.53403e-05
	LOSS [training: 0.5154692490140362 | validation: 0.5115113030162806]
	TIME [epoch: 54.2 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5140983009816348		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 0.5140983009816348 | validation: 0.5169327266856933]
	TIME [epoch: 54.2 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5130319893502537		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 0.5130319893502537 | validation: 0.5084406971131534]
	TIME [epoch: 54.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5132054845310918		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 0.5132054845310918 | validation: 0.5082963618328646]
	TIME [epoch: 54.2 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5162202537909064		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 0.5162202537909064 | validation: 0.5125317514999199]
	TIME [epoch: 54.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5141536511509782		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 0.5141536511509782 | validation: 0.5125474927410799]
	TIME [epoch: 54.2 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.512166713505911		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 0.512166713505911 | validation: 0.5223041250960165]
	TIME [epoch: 54.2 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5129715362311041		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 0.5129715362311041 | validation: 0.5148035768652156]
	TIME [epoch: 54.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5130440221607443		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 0.5130440221607443 | validation: 0.5095881709830913]
	TIME [epoch: 54.2 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5131876788525049		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 0.5131876788525049 | validation: 0.5167467601453865]
	TIME [epoch: 54.2 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5137205559527209		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 0.5137205559527209 | validation: 0.516482058146954]
	TIME [epoch: 54.2 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5133203673374744		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 0.5133203673374744 | validation: 0.5143169633443284]
	TIME [epoch: 54.3 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5125361152100256		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 0.5125361152100256 | validation: 0.5221688911480148]
	TIME [epoch: 54.2 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5118741110246746		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 0.5118741110246746 | validation: 0.5082686105046198]
	TIME [epoch: 54.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5108547775858712		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 0.5108547775858712 | validation: 0.5149159993448813]
	TIME [epoch: 54.2 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5133446908838986		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 0.5133446908838986 | validation: 0.5106948386405809]
	TIME [epoch: 54.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5127847318986626		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 0.5127847318986626 | validation: 0.5081479872814962]
	TIME [epoch: 54.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v2r_4_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v2r_4_v_mmd1_1411.pth
	Model improved!!!
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5123169922087327		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 0.5123169922087327 | validation: 0.5129647557165739]
	TIME [epoch: 54.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5127967431736995		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 0.5127967431736995 | validation: 0.5136040440049618]
	TIME [epoch: 54.4 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5109604214448022		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 0.5109604214448022 | validation: 0.5137377628347345]
	TIME [epoch: 54.4 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5124482859239812		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 0.5124482859239812 | validation: 0.5085110660322549]
	TIME [epoch: 54.4 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5118728352596439		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 0.5118728352596439 | validation: 0.512896690443831]
	TIME [epoch: 54.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5127457996036244		[learning rate: 7.8942e-05]
	Learning Rate: 7.89419e-05
	LOSS [training: 0.5127457996036244 | validation: 0.5144544121362469]
	TIME [epoch: 54.3 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5135407478315501		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 0.5135407478315501 | validation: 0.5196816937728469]
	TIME [epoch: 54.4 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5109931020019092		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 0.5109931020019092 | validation: 0.5128593805739906]
	TIME [epoch: 54.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5138290665746204		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 0.5138290665746204 | validation: 0.5273637786267679]
	TIME [epoch: 54.4 sec]
EPOCH 1421/2000:
	Training over batches...
