Args:
Namespace(name='model_phi1_4a_distortion_v2_0_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_0/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_0/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.025831710547208786, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1205478297

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.822510007087439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.822510007087439 | validation: 6.136646755287008]
	TIME [epoch: 158 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.395311294487169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.395311294487169 | validation: 7.034530402369924]
	TIME [epoch: 0.806 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.8680301534247725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8680301534247725 | validation: 6.129749781282191]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.964139556545253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.964139556545253 | validation: 5.4575508859776765]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.932155475265687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.932155475265687 | validation: 5.249130545272834]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.546018463123893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.546018463123893 | validation: 5.3287643959975775]
	TIME [epoch: 0.694 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.240212153009778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.240212153009778 | validation: 4.41071945905634]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.693565598754438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.693565598754438 | validation: 4.5663236034416785]
	TIME [epoch: 0.693 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.493314043006782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.493314043006782 | validation: 4.7434776368050295]
	TIME [epoch: 0.693 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.567544449950729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.567544449950729 | validation: 4.246916685739267]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.879142472605282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.879142472605282 | validation: 4.631965183960139]
	TIME [epoch: 0.696 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.914226695056974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.914226695056974 | validation: 3.950645371073566]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3316175661863685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3316175661863685 | validation: 3.7852829405228885]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.402804312084687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.402804312084687 | validation: 5.01429371286467]
	TIME [epoch: 0.695 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.989811802273836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.989811802273836 | validation: 4.112036225262077]
	TIME [epoch: 0.695 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.224553436081907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.224553436081907 | validation: 3.649880683170091]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1749407640199774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1749407640199774 | validation: 4.008985469214799]
	TIME [epoch: 0.697 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.918027328458464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.918027328458464 | validation: 3.789751626672128]
	TIME [epoch: 0.696 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7999744891154217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7999744891154217 | validation: 3.2418306929642977]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8455950513263644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8455950513263644 | validation: 4.165031750780827]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.167682534734853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.167682534734853 | validation: 2.8583126991002596]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.012109077825245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.012109077825245 | validation: 3.3935411352165428]
	TIME [epoch: 0.696 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5979469913575666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5979469913575666 | validation: 3.4283866083458805]
	TIME [epoch: 0.694 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5455017224451892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5455017224451892 | validation: 2.819918108925512]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5621995452821023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5621995452821023 | validation: 3.7180910520134223]
	TIME [epoch: 0.696 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0346721958185903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0346721958185903 | validation: 2.3516695207514426]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.780811440377334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.780811440377334 | validation: 3.21105789812888]
	TIME [epoch: 0.693 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.394289829988799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.394289829988799 | validation: 2.698961733328384]
	TIME [epoch: 0.693 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.309548427006664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.309548427006664 | validation: 2.798298666802964]
	TIME [epoch: 0.694 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2840538435017828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2840538435017828 | validation: 2.1246535291334783]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9055220572356144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9055220572356144 | validation: 2.8718621377082676]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5320395722335736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5320395722335736 | validation: 2.0464754157683447]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3351000579304735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3351000579304735 | validation: 3.742340938750562]
	TIME [epoch: 0.694 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7473151965570977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7473151965570977 | validation: 2.057957341189154]
	TIME [epoch: 0.695 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.39353816085249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.39353816085249 | validation: 2.632197089166186]
	TIME [epoch: 0.694 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3767711222605117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3767711222605117 | validation: 2.369423080628874]
	TIME [epoch: 0.693 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.138880927875791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.138880927875791 | validation: 2.6251715414928927]
	TIME [epoch: 0.693 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.16328998617304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.16328998617304 | validation: 1.7889008030823441]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.613614439456682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.613614439456682 | validation: 2.634956554029522]
	TIME [epoch: 0.693 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1406169974186677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1406169974186677 | validation: 1.9278378310296298]
	TIME [epoch: 0.702 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1139062227391165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1139062227391165 | validation: 2.570655116341223]
	TIME [epoch: 0.693 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.27533787306414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.27533787306414 | validation: 1.7571474879523663]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.221559822461346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.221559822461346 | validation: 2.3824205303262205]
	TIME [epoch: 0.696 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.099839596557518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.099839596557518 | validation: 1.7162748247659285]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.120327223422503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.120327223422503 | validation: 2.5304231772861607]
	TIME [epoch: 0.699 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.14828625047351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.14828625047351 | validation: 1.6476882129844252]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1529276005254743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1529276005254743 | validation: 2.6151211482098162]
	TIME [epoch: 0.698 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.065208124377144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.065208124377144 | validation: 1.873608157955004]
	TIME [epoch: 0.695 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0767988265126163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0767988265126163 | validation: 2.532150420903047]
	TIME [epoch: 0.694 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.036559428857956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.036559428857956 | validation: 1.629332905427215]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9832503856929984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9832503856929984 | validation: 2.2456571016437836]
	TIME [epoch: 0.697 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1542936194092404		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.1542936194092404 | validation: 1.6226833089257455]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1170406359212324		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.1170406359212324 | validation: 2.0964561808218143]
	TIME [epoch: 0.695 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0008484728161857		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.0008484728161857 | validation: 1.5565150679268873]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9116858240110113		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.9116858240110113 | validation: 2.329042155420775]
	TIME [epoch: 0.694 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9929524449398401		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.9929524449398401 | validation: 1.522847480825189]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.046282602044045		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 2.046282602044045 | validation: 2.2030976081003413]
	TIME [epoch: 0.694 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8892701351161392		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.8892701351161392 | validation: 1.6120274323730066]
	TIME [epoch: 0.691 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8340095591007963		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.8340095591007963 | validation: 2.193801140342478]
	TIME [epoch: 0.69 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.899554048191497		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.899554048191497 | validation: 1.4536919850823642]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.995948083015607		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.995948083015607 | validation: 2.024751838167129]
	TIME [epoch: 0.693 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9941726769630956		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.9941726769630956 | validation: 1.5375380466293802]
	TIME [epoch: 0.695 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8383383109761873		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.8383383109761873 | validation: 1.9824858132820928]
	TIME [epoch: 0.691 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.86785074618056		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.86785074618056 | validation: 1.4044603160106162]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9195260265945835		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.9195260265945835 | validation: 2.0779395434051144]
	TIME [epoch: 0.694 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8094539233111937		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.8094539233111937 | validation: 1.5334447265865816]
	TIME [epoch: 0.695 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8131123041906272		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.8131123041906272 | validation: 2.1477005955136557]
	TIME [epoch: 0.693 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8431367978728248		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.8431367978728248 | validation: 1.4422416593571965]
	TIME [epoch: 0.694 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7868639770702834		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.7868639770702834 | validation: 1.8834089011605686]
	TIME [epoch: 0.694 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7569165226521033		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.7569165226521033 | validation: 1.4142820542061587]
	TIME [epoch: 0.696 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8746160813641919		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.8746160813641919 | validation: 1.9078643846778218]
	TIME [epoch: 0.693 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9523822847097552		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.9523822847097552 | validation: 1.4249566471567552]
	TIME [epoch: 0.695 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7663439086591068		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.7663439086591068 | validation: 1.7430735092148375]
	TIME [epoch: 0.694 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.739893276069721		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.739893276069721 | validation: 1.3782735906817916]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7950889109154715		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.7950889109154715 | validation: 1.9978499232566138]
	TIME [epoch: 0.704 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7732902719405814		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7732902719405814 | validation: 1.4510906653626183]
	TIME [epoch: 0.697 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7681231050893587		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.7681231050893587 | validation: 1.9990456332264985]
	TIME [epoch: 0.695 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7631029378400584		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.7631029378400584 | validation: 1.4355785291975147]
	TIME [epoch: 0.694 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6969687438266454		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.6969687438266454 | validation: 1.8494692655831138]
	TIME [epoch: 0.695 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7175095235795772		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.7175095235795772 | validation: 1.3718554730167283]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7777163703438819		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.7777163703438819 | validation: 1.786000068336771]
	TIME [epoch: 0.695 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8171269595656696		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.8171269595656696 | validation: 1.4462261182388858]
	TIME [epoch: 0.694 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7729896871292061		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.7729896871292061 | validation: 1.6676755827423166]
	TIME [epoch: 0.694 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7235173017924417		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.7235173017924417 | validation: 1.3472474754689825]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6645735142395313		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.6645735142395313 | validation: 1.808449799834505]
	TIME [epoch: 0.693 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.714277074195211		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.714277074195211 | validation: 1.3780919172796908]
	TIME [epoch: 0.694 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7788010044084173		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.7788010044084173 | validation: 1.819340193850772]
	TIME [epoch: 0.697 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6914837672835705		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.6914837672835705 | validation: 1.4669444761580543]
	TIME [epoch: 0.693 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6597824477676397		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.6597824477676397 | validation: 1.847141512251298]
	TIME [epoch: 0.693 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6980912101474495		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.6980912101474495 | validation: 1.383978301726647]
	TIME [epoch: 0.693 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6805272676470657		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.6805272676470657 | validation: 1.7506612088781772]
	TIME [epoch: 0.693 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.665512447087996		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.665512447087996 | validation: 1.34168841829608]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.703474261311604		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.703474261311604 | validation: 1.7417238673681383]
	TIME [epoch: 0.693 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7837703854681126		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.7837703854681126 | validation: 1.4137380405610842]
	TIME [epoch: 0.694 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7245479835183415		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.7245479835183415 | validation: 1.6251252677410015]
	TIME [epoch: 0.692 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.676322510442276		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.676322510442276 | validation: 1.3387218824979474]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6240575051382968		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.6240575051382968 | validation: 1.6707728762671625]
	TIME [epoch: 0.693 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6423535708624835		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.6423535708624835 | validation: 1.3899980451839062]
	TIME [epoch: 0.691 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.753873125216571		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.753873125216571 | validation: 1.8643473997577773]
	TIME [epoch: 0.69 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7057873871174911		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.7057873871174911 | validation: 1.5030464046810235]
	TIME [epoch: 0.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.652003087448703		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.652003087448703 | validation: 1.5847036110909247]
	TIME [epoch: 0.694 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6072813657971028		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6072813657971028 | validation: 1.4290318052467725]
	TIME [epoch: 0.691 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5752379109447583		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.5752379109447583 | validation: 1.6287202941006962]
	TIME [epoch: 0.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6075528220017925		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6075528220017925 | validation: 1.4719126995743175]
	TIME [epoch: 0.69 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.709944128184904		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.709944128184904 | validation: 2.0285678960816873]
	TIME [epoch: 0.691 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7845700260641213		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.7845700260641213 | validation: 1.3460266085112065]
	TIME [epoch: 0.689 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6722084203679684		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.6722084203679684 | validation: 1.6884862250974788]
	TIME [epoch: 0.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7171136471697577		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.7171136471697577 | validation: 1.3413985253649168]
	TIME [epoch: 0.693 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5943106363904782		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.5943106363904782 | validation: 1.680442228928353]
	TIME [epoch: 0.691 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.650849563908575		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.650849563908575 | validation: 1.2928210121372083]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6729192044064167		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.6729192044064167 | validation: 1.5886429535384292]
	TIME [epoch: 0.695 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.539509650479667		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.539509650479667 | validation: 1.3135168738355536]
	TIME [epoch: 0.695 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5464197885410835		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.5464197885410835 | validation: 1.6793875019101578]
	TIME [epoch: 0.699 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6260044638201923		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.6260044638201923 | validation: 1.2713081872599112]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6248720852628213		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.6248720852628213 | validation: 1.5324249206984455]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5727933257417184		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.5727933257417184 | validation: 1.3157889410450185]
	TIME [epoch: 0.693 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5862295722798863		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.5862295722798863 | validation: 1.601868545743394]
	TIME [epoch: 0.693 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6776378369293163		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.6776378369293163 | validation: 1.4035111135014249]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6299023339844638		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.6299023339844638 | validation: 1.528157814271933]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6094592714168807		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.6094592714168807 | validation: 1.338436289645621]
	TIME [epoch: 0.694 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5279760490938492		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.5279760490938492 | validation: 1.4013065200018826]
	TIME [epoch: 0.692 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5098004706455352		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5098004706455352 | validation: 1.236100859672666]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5356708567164965		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5356708567164965 | validation: 1.9051692727185774]
	TIME [epoch: 0.695 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.777631998610848		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.777631998610848 | validation: 1.3146327330365328]
	TIME [epoch: 0.694 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6189232155027973		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.6189232155027973 | validation: 1.6651910527303515]
	TIME [epoch: 0.694 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6012566869317675		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.6012566869317675 | validation: 1.4956501420491473]
	TIME [epoch: 0.694 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5666858186281218		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.5666858186281218 | validation: 1.3797942969361014]
	TIME [epoch: 0.694 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4865059380935548		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.4865059380935548 | validation: 1.4134179479992741]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.478942680884911		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.478942680884911 | validation: 1.2372583623107738]
	TIME [epoch: 0.698 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5568635943641909		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.5568635943641909 | validation: 1.7547605980621452]
	TIME [epoch: 0.699 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7103009252215629		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.7103009252215629 | validation: 1.2267560999248468]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5072967704872366		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.5072967704872366 | validation: 1.4859551515130822]
	TIME [epoch: 0.699 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4700278762293453		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.4700278762293453 | validation: 1.3276176355033742]
	TIME [epoch: 0.695 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5198703519157828		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.5198703519157828 | validation: 1.87887262646274]
	TIME [epoch: 0.694 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6461969843291047		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.6461969843291047 | validation: 1.3321155496020072]
	TIME [epoch: 0.695 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5193465108567352		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.5193465108567352 | validation: 1.5326111195679557]
	TIME [epoch: 0.695 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.498370416102599		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 1.498370416102599 | validation: 1.2817536584402824]
	TIME [epoch: 0.695 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.472945104819745		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.472945104819745 | validation: 1.5329888806685685]
	TIME [epoch: 0.695 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.486735273495368		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 1.486735273495368 | validation: 1.21064472920383]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4863272360967148		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.4863272360967148 | validation: 1.5200113557081532]
	TIME [epoch: 0.697 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5011880988491169		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.5011880988491169 | validation: 1.2217640344400649]
	TIME [epoch: 0.693 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5728388703546876		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5728388703546876 | validation: 1.5691218856904778]
	TIME [epoch: 0.694 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6566898261021663		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.6566898261021663 | validation: 1.4049264540524244]
	TIME [epoch: 0.692 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4817411434665246		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.4817411434665246 | validation: 1.2671379120863744]
	TIME [epoch: 0.698 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4234238493152305		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.4234238493152305 | validation: 1.4975578191521863]
	TIME [epoch: 0.692 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4521551976735418		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.4521551976735418 | validation: 1.2228117643742875]
	TIME [epoch: 0.692 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.53842791185118		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.53842791185118 | validation: 1.5000436900017926]
	TIME [epoch: 0.691 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4439345336313756		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.4439345336313756 | validation: 1.2219742808614222]
	TIME [epoch: 0.692 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3995236759640945		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.3995236759640945 | validation: 1.4318947480689925]
	TIME [epoch: 0.691 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4106050798091583		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.4106050798091583 | validation: 1.2864361862982507]
	TIME [epoch: 0.691 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4829217879024716		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.4829217879024716 | validation: 1.6119293172174978]
	TIME [epoch: 0.692 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4970151083782344		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.4970151083782344 | validation: 1.3927007895587964]
	TIME [epoch: 0.701 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5009036916615737		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.5009036916615737 | validation: 1.4326496837113716]
	TIME [epoch: 0.697 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.524293038398867		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.524293038398867 | validation: 1.5363572255901765]
	TIME [epoch: 0.697 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.56361184601807		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.56361184601807 | validation: 1.152392870360562]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4362616365381349		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.4362616365381349 | validation: 1.5144849870384123]
	TIME [epoch: 0.698 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4550513936933198		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.4550513936933198 | validation: 1.1694150655530107]
	TIME [epoch: 0.696 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.400144960542775		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.400144960542775 | validation: 1.3951043254239615]
	TIME [epoch: 0.697 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3892086958305294		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.3892086958305294 | validation: 1.1713062234059661]
	TIME [epoch: 0.695 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.393560511409539		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.393560511409539 | validation: 1.4859304456784244]
	TIME [epoch: 0.696 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.411325719558701		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.411325719558701 | validation: 1.146350549279044]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3799687959505225		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.3799687959505225 | validation: 1.424455138409754]
	TIME [epoch: 0.698 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3795707549760392		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.3795707549760392 | validation: 1.263060575560444]
	TIME [epoch: 0.699 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4026581810271728		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.4026581810271728 | validation: 1.6194636323179332]
	TIME [epoch: 0.697 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5265146933926048		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.5265146933926048 | validation: 1.5760084126365503]
	TIME [epoch: 0.696 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6107976890935651		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.6107976890935651 | validation: 1.1467429409082308]
	TIME [epoch: 0.696 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4025970322971506		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.4025970322971506 | validation: 1.4212217269378564]
	TIME [epoch: 0.697 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3935351003534016		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.3935351003534016 | validation: 1.149958540345966]
	TIME [epoch: 0.696 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4008510888584584		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.4008510888584584 | validation: 1.3706024623355024]
	TIME [epoch: 0.697 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349309326767206		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.349309326767206 | validation: 1.1515576557476557]
	TIME [epoch: 0.695 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3346400231420754		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.3346400231420754 | validation: 1.3480275198251446]
	TIME [epoch: 0.695 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3490976629113436		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3490976629113436 | validation: 1.1569748286327588]
	TIME [epoch: 0.695 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3574310910985365		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.3574310910985365 | validation: 1.3877416977508747]
	TIME [epoch: 0.695 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.367589615337572		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.367589615337572 | validation: 1.2475592862183342]
	TIME [epoch: 0.696 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3880887184997281		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.3880887184997281 | validation: 1.4903982666073607]
	TIME [epoch: 0.696 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.472707321392841		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.472707321392841 | validation: 1.4957865465605509]
	TIME [epoch: 0.694 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5372653759828756		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.5372653759828756 | validation: 1.1015888892234216]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3986202200608029		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3986202200608029 | validation: 1.3438560589301916]
	TIME [epoch: 0.696 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.359353549523353		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.359353549523353 | validation: 1.0648275001218894]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3458446379156386		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3458446379156386 | validation: 1.2774996677959398]
	TIME [epoch: 0.697 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3211762222475585		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.3211762222475585 | validation: 1.0717210438280131]
	TIME [epoch: 0.695 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3060960141251405		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3060960141251405 | validation: 1.2813967897916836]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3079437199610318		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3079437199610318 | validation: 1.0319670126062033]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.33425161283816		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.33425161283816 | validation: 1.279986945320538]
	TIME [epoch: 0.696 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.331455761474425		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.331455761474425 | validation: 1.0941403300521093]
	TIME [epoch: 0.697 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.368287664616299		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.368287664616299 | validation: 1.388916417520294]
	TIME [epoch: 0.697 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5006190491997204		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.5006190491997204 | validation: 1.3929159741927089]
	TIME [epoch: 0.695 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4252544050658467		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.4252544050658467 | validation: 1.2031221080879284]
	TIME [epoch: 0.693 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3756779104382861		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.3756779104382861 | validation: 1.3673390962327252]
	TIME [epoch: 0.694 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3231075728644683		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.3231075728644683 | validation: 1.0672316185145112]
	TIME [epoch: 0.694 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.281256799847605		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.281256799847605 | validation: 1.2452782247364418]
	TIME [epoch: 0.704 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2642157555971767		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.2642157555971767 | validation: 1.0625614029735164]
	TIME [epoch: 0.696 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2747892917428183		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.2747892917428183 | validation: 1.258700236295461]
	TIME [epoch: 0.695 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2865466640212062		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.2865466640212062 | validation: 1.0267038064897436]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.268299811832113		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.268299811832113 | validation: 1.2600273996060738]
	TIME [epoch: 0.695 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2695648042003864		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.2695648042003864 | validation: 1.0114782914786165]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2454351786603155		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.2454351786603155 | validation: 1.2111832614960378]
	TIME [epoch: 0.696 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2803073080276928		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.2803073080276928 | validation: 1.1081628117867073]
	TIME [epoch: 0.693 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.370254929744948		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.370254929744948 | validation: 1.585029942504786]
	TIME [epoch: 0.694 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6613783506549527		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.6613783506549527 | validation: 1.4402818541798406]
	TIME [epoch: 0.695 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3641954024602307		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.3641954024602307 | validation: 0.9763414400886603]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.284928027143924		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.284928027143924 | validation: 1.1932260787773494]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2512951211425332		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.2512951211425332 | validation: 1.0881940373829289]
	TIME [epoch: 1.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2537504502672012		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.2537504502672012 | validation: 1.1910334845136648]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2983715568424907		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.2983715568424907 | validation: 1.3462135987296358]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.347769438316548		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.347769438316548 | validation: 1.201554919326441]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3399347033550648		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3399347033550648 | validation: 1.300386411938748]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.273113205738565		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.273113205738565 | validation: 1.0236176217427875]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2274433681377726		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.2274433681377726 | validation: 1.1975556766786923]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.229305742220069		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.229305742220069 | validation: 0.9852021379859249]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.228335408647457		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.228335408647457 | validation: 1.1930406140883072]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235012506804357		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.2235012506804357 | validation: 0.9817573469157012]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.204834916526813		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.204834916526813 | validation: 1.1246105158184758]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.196969376209031		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.196969376209031 | validation: 0.974834999875573]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2389314615619658		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.2389314615619658 | validation: 1.2412986886855681]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3144570924421464		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.3144570924421464 | validation: 1.2403299904625391]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3940420243910938		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.3940420243910938 | validation: 1.317395266896616]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.443075655096779		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.443075655096779 | validation: 1.4469484181044514]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3252560161102207		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.3252560161102207 | validation: 0.971466467375155]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1933749579356008		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.1933749579356008 | validation: 1.0767176539406362]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1697508311964329		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.1697508311964329 | validation: 1.0291728323308813]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.174917582890943		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.174917582890943 | validation: 1.1091324950406678]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2154508522813812		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.2154508522813812 | validation: 1.158667656709629]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2714855380657482		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2714855380657482 | validation: 1.2074243459878824]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.340128920588507		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.340128920588507 | validation: 1.2946326256777791]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2627035526780046		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.2627035526780046 | validation: 1.0000932096896404]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1810794732109826		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.1810794732109826 | validation: 1.1612630648970368]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1821318451128273		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.1821318451128273 | validation: 0.9440290675333293]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1709373341813405		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.1709373341813405 | validation: 1.1653285751209042]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1790316376871746		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.1790316376871746 | validation: 0.9274365607019721]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1790899316883632		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.1790899316883632 | validation: 1.128675705133609]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165036845632938		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.165036845632938 | validation: 0.9752767597733645]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156813954721431		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.156813954721431 | validation: 1.2131843932660382]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2175950722888775		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.2175950722888775 | validation: 1.3331794311162817]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.433699045193956		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.433699045193956 | validation: 1.1843115190052467]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2829480876392978		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2829480876392978 | validation: 1.0582777107179513]
	TIME [epoch: 1.36 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1779014457025647		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.1779014457025647 | validation: 0.9687398228702637]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1310525750285825		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.1310525750285825 | validation: 1.048329858407415]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1270974804641098		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.1270974804641098 | validation: 0.9151838680577898]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1639612225597418		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.1639612225597418 | validation: 1.1504493186282418]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.204179417416702		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.204179417416702 | validation: 0.8798150911349394]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1748310814052736		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.1748310814052736 | validation: 1.0582432254941536]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253227029850874		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.1253227029850874 | validation: 0.9400541275750496]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137419362894831		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.1137419362894831 | validation: 1.0103244030140075]
	TIME [epoch: 1.37 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1551074994117758		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.1551074994117758 | validation: 1.3024901430726836]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.339238126532852		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.339238126532852 | validation: 1.2678351977522833]
	TIME [epoch: 1.35 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3939659963968944		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.3939659963968944 | validation: 1.123801473944533]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140924038616973		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.140924038616973 | validation: 0.8656429806591392]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127289408827759		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.127289408827759 | validation: 1.0820407382086548]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1458687930418925		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.1458687930418925 | validation: 0.857970828974755]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336816227852788		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.1336816227852788 | validation: 1.0348907577679964]
	TIME [epoch: 1.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1197873080278444		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.1197873080278444 | validation: 0.9500392412020647]
	TIME [epoch: 1.35 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1090843348667354		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.1090843348667354 | validation: 1.0491386551492368]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1548397761694917		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.1548397761694917 | validation: 1.2234085694011871]
	TIME [epoch: 1.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.248180157494973		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.248180157494973 | validation: 1.1303914891538642]
	TIME [epoch: 1.35 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2545701112624095		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.2545701112624095 | validation: 1.0858628492980549]
	TIME [epoch: 1.35 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1415482675971655		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.1415482675971655 | validation: 0.923460283179299]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0800717811550116		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.0800717811550116 | validation: 0.9959591059235215]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0795605103504988		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.0795605103504988 | validation: 0.895826982028936]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0821521779101155		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0821521779101155 | validation: 1.018023577635933]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0820972053867233		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.0820972053867233 | validation: 0.9080864364570673]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118917658346375		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.118917658346375 | validation: 1.222974619834529]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.197164434032996		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.197164434032996 | validation: 1.0801439543270077]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2368674825638075		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.2368674825638075 | validation: 1.0496010039926014]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.178768260629424		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.178768260629424 | validation: 1.0514868786148448]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1513784862722398		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.1513784862722398 | validation: 0.8979447250102978]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102529039872003		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.102529039872003 | validation: 0.967262924672255]
	TIME [epoch: 1.36 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0797391706989095		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.0797391706989095 | validation: 0.8914659925365411]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066842138765438		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.066842138765438 | validation: 0.9461044081852569]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0575687040499817		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.0575687040499817 | validation: 0.884157686545944]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0708208582485648		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.0708208582485648 | validation: 0.9917596778805561]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0958159763144426		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0958159763144426 | validation: 0.8937656433255783]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14395837398545		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.14395837398545 | validation: 1.0921633296383835]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1712692939603164		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.1712692939603164 | validation: 1.0015254156923354]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107832981897224		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.107832981897224 | validation: 0.9661731574550996]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1721275415600494		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.1721275415600494 | validation: 1.3002268551003213]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2057366575720172		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.2057366575720172 | validation: 0.8617282396173309]
	TIME [epoch: 1.35 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0550451016338236		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.0550451016338236 | validation: 0.865635164637256]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0304277714464778		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.0304277714464778 | validation: 0.9487274768088825]
	TIME [epoch: 1.35 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036407171476329		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.036407171476329 | validation: 0.8169893934103378]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036793126119755		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.036793126119755 | validation: 0.9952517917811736]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0568187815027628		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.0568187815027628 | validation: 0.783779288459999]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0715929813137648		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.0715929813137648 | validation: 0.9960342778969835]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0443886919683285		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.0443886919683285 | validation: 0.7897889013303484]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0322653610222665		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.0322653610222665 | validation: 0.9496759460179647]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0135839276277463		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0135839276277463 | validation: 0.8011627088911795]
	TIME [epoch: 1.37 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025794870451966		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.025794870451966 | validation: 1.0384805891222388]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0674138082743223		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.0674138082743223 | validation: 1.1524198054484154]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2921372975831344		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.2921372975831344 | validation: 1.3174339822876335]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3751402439616265		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.3751402439616265 | validation: 0.9991606888231239]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0582839368772121		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.0582839368772121 | validation: 0.8325216015387433]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0072818495029277		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.0072818495029277 | validation: 0.9320900806479924]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0416973654863997		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.0416973654863997 | validation: 0.9338177157805436]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0850260079301104		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.0850260079301104 | validation: 1.0196710066934713]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1011210623575098		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.1011210623575098 | validation: 0.9727451899549338]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0916990110302651		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0916990110302651 | validation: 0.8899871698160251]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0326194427751199		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.0326194427751199 | validation: 0.8578702552569664]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126876003323972		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0126876003323972 | validation: 0.874777061330314]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0060339597777634		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.0060339597777634 | validation: 0.845278213397231]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0092359301254863		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.0092359301254863 | validation: 1.0088507509683222]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037265028901069		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.037265028901069 | validation: 0.830552911582018]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.063405627417329		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.063405627417329 | validation: 1.0473879776923287]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0704974423621156		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.0704974423621156 | validation: 0.8747015002908018]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030825029000795		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.030825029000795 | validation: 0.8700327675341768]
	TIME [epoch: 1.35 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048844540500309		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.048844540500309 | validation: 1.00967869989753]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1022726277561636		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1022726277561636 | validation: 0.8745439702596969]
	TIME [epoch: 1.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0717236082172614		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.0717236082172614 | validation: 0.9223534573749714]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0328035197114995		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.0328035197114995 | validation: 0.9141895226361604]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0175811401297095		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.0175811401297095 | validation: 0.8270521339940662]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0135911477651656		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.0135911477651656 | validation: 0.9673254543906797]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0178671754161253		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.0178671754161253 | validation: 0.8232573496834569]
	TIME [epoch: 1.36 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010737685807398		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.010737685807398 | validation: 0.9545735347159596]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170839837673893		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.0170839837673893 | validation: 0.9045467917897014]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0365394403515975		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.0365394403515975 | validation: 0.9400865331298978]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.072786700056781		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.072786700056781 | validation: 0.9733596550880272]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0740485472477777		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.0740485472477777 | validation: 0.8438581035955264]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0081309379704342		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.0081309379704342 | validation: 0.9063712768947362]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.992127539675833		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.992127539675833 | validation: 0.7974644109486235]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9720555808093867		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.9720555808093867 | validation: 0.8359028796200145]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.975324333434194		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.975324333434194 | validation: 0.8294764545235505]
	TIME [epoch: 1.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896143762946169		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.9896143762946169 | validation: 0.9378319281735547]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0393519942272387		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.0393519942272387 | validation: 0.9784694292593911]
	TIME [epoch: 1.35 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586006186486216		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.0586006186486216 | validation: 0.8893580425266381]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0629017821570583		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.0629017821570583 | validation: 1.0048069705464324]
	TIME [epoch: 1.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0228755902686337		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.0228755902686337 | validation: 0.7287741795297195]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9893213099465026		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.9893213099465026 | validation: 0.9325917259242988]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9728046040956602		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.9728046040956602 | validation: 0.7300183782402896]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9580594536735819		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.9580594536735819 | validation: 0.8357357199531452]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9521003427934249		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.9521003427934249 | validation: 0.7573578358004078]
	TIME [epoch: 1.37 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9625914249284734		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.9625914249284734 | validation: 0.8986584338714544]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206273827537922		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.0206273827537922 | validation: 0.8882614203083925]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0687828912601032		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.0687828912601032 | validation: 0.9742179526057009]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1017595561490492		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.1017595561490492 | validation: 0.9815965894818035]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011930161600096		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.011930161600096 | validation: 0.7485307551224685]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9515662472380148		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.9515662472380148 | validation: 0.8763668380608955]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9503229505232292		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.9503229505232292 | validation: 0.7327799159836186]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9401196480643151		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.9401196480643151 | validation: 0.8932646328040129]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470294094623186		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.9470294094623186 | validation: 0.7362926379017393]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529995374954414		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.9529995374954414 | validation: 0.9093677546497192]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593633417321681		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.9593633417321681 | validation: 0.7831838910275147]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9633219588727288		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.9633219588727288 | validation: 0.9473314323289511]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0513728992353075		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0513728992353075 | validation: 1.0530311632882972]
	TIME [epoch: 1.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1676551351990583		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.1676551351990583 | validation: 0.7845314312760507]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9652885297243528		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.9652885297243528 | validation: 0.7817058973015447]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222611769780694		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.9222611769780694 | validation: 0.7934958876684702]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226580115358843		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.9226580115358843 | validation: 0.7361894221764755]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9252622669692707		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.9252622669692707 | validation: 0.8704980614449916]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312632986299118		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.9312632986299118 | validation: 0.6937505358388473]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9643310787820383		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.9643310787820383 | validation: 0.9468164292491607]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856441854842584		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.9856441854842584 | validation: 0.7664145255760746]
	TIME [epoch: 1.37 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9738489847919843		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.9738489847919843 | validation: 0.9192613183876676]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036156250537036		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.036156250537036 | validation: 1.0736032503434865]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0739359329301608		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.0739359329301608 | validation: 0.795267237393902]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9909927634444244		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.9909927634444244 | validation: 0.8560907653353879]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463303044544463		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.9463303044544463 | validation: 0.7663715055643965]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232283942980916		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.9232283942980916 | validation: 0.8189129427320732]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311517555125058		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.9311517555125058 | validation: 0.794077739911045]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388359648205754		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.9388359648205754 | validation: 0.8142719715187324]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9476899008555302		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.9476899008555302 | validation: 0.8537868236618403]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9745954748843481		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.9745954748843481 | validation: 0.8784099121706153]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0007051400674583		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.0007051400674583 | validation: 0.8625829570325287]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9769146164428729		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.9769146164428729 | validation: 0.863804995983066]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9489625881898793		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.9489625881898793 | validation: 0.7332904231401915]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343433640245289		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.9343433640245289 | validation: 0.8726832863461603]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9324579369939789		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.9324579369939789 | validation: 0.6978610198369583]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218129719177968		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.9218129719177968 | validation: 0.8362316626289019]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9083231409454158		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.9083231409454158 | validation: 0.6835933870866095]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137030528384423		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.9137030528384423 | validation: 0.8534891437778056]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305961719425164		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.9305961719425164 | validation: 0.7610727256596932]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778512433488226		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.9778512433488226 | validation: 0.9824101344696281]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0634566709790538		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.0634566709790538 | validation: 0.9378377946597581]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0139205367307147		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.0139205367307147 | validation: 0.7639386863597366]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9316962783889327		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.9316962783889327 | validation: 0.83044653532892]
	TIME [epoch: 1.35 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911427637496847		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.911427637496847 | validation: 0.6990745957971026]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899341076794315		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.899341076794315 | validation: 0.8248371080723953]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011377684740787		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.9011377684740787 | validation: 0.7513597007722299]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992807033058565		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.8992807033058565 | validation: 0.831867881394225]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436640143236837		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.9436640143236837 | validation: 0.9028439149410747]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0006061904453227		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.0006061904453227 | validation: 0.8968894917752703]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0191791627339029		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.0191791627339029 | validation: 0.8143320095814416]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9351613087795707		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.9351613087795707 | validation: 0.7856920291556296]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054191538821903		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.9054191538821903 | validation: 0.7463490134123835]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8832774129767853		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.8832774129767853 | validation: 0.8549322384218763]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957153526484768		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.8957153526484768 | validation: 0.7421994307486381]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131373951793631		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.9131373951793631 | validation: 0.8975024321127925]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199554642681605		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.9199554642681605 | validation: 0.6950783507738303]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9180016830357847		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.9180016830357847 | validation: 0.8516904836871446]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262902235868579		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9262902235868579 | validation: 0.8307213118728551]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.949287051084209		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.949287051084209 | validation: 0.8528307407900257]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9986390402714824		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.9986390402714824 | validation: 0.9210463590092858]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0001006488142887		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.0001006488142887 | validation: 0.7483731988875997]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189811136572874		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.9189811136572874 | validation: 0.7890167447559235]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839346164741526		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.8839346164741526 | validation: 0.751752102694197]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826729825876438		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.8826729825876438 | validation: 0.7582863267802312]
	TIME [epoch: 1.37 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748467986946988		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.8748467986946988 | validation: 0.7259502693114522]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793695618205558		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.8793695618205558 | validation: 0.8584758609761809]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084086330278069		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.9084086330278069 | validation: 0.7213445864847883]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9337437400545868		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.9337437400545868 | validation: 0.9172085993521469]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293731277637471		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.9293731277637471 | validation: 0.8507350228030752]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9732381824912674		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.9732381824912674 | validation: 0.8118160294740996]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.018665451191535		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.018665451191535 | validation: 0.8369731766536216]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9345128011666904		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.9345128011666904 | validation: 0.768798264527627]
	TIME [epoch: 1.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847435539840055		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.8847435539840055 | validation: 0.7309418468042227]
	TIME [epoch: 1.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708603869829705		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.8708603869829705 | validation: 0.7933605807129692]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729099510448919		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.8729099510448919 | validation: 0.7064108910605535]
	TIME [epoch: 1.35 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906362791702159		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.8906362791702159 | validation: 0.8062014731662718]
	TIME [epoch: 1.35 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8914355620231845		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.8914355620231845 | validation: 0.7324659415734165]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.914134582831516		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.914134582831516 | validation: 0.8670833108102575]
	TIME [epoch: 1.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9605315124650077		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.9605315124650077 | validation: 0.8701989547808392]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987517066555641		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.9987517066555641 | validation: 0.7615949115367326]
	TIME [epoch: 1.35 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053775092288183		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.9053775092288183 | validation: 0.7599433563475657]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8781637473077254		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.8781637473077254 | validation: 0.7503516240276593]
	TIME [epoch: 1.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8700926808089517		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.8700926808089517 | validation: 0.7479693845459997]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797449559014345		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.8797449559014345 | validation: 0.7793514433237991]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821951009068332		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.8821951009068332 | validation: 0.7134410909353652]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974740867757933		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.8974740867757933 | validation: 0.9150615503921069]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960418035230456		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.960418035230456 | validation: 0.8092664142692976]
	TIME [epoch: 1.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972424733943904		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.972424733943904 | validation: 0.8495636346369866]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127468322191922		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.9127468322191922 | validation: 0.7781735376217158]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806585948279013		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.8806585948279013 | validation: 0.6967119390855668]
	TIME [epoch: 1.35 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798420943603108		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.8798420943603108 | validation: 0.8542189061229059]
	TIME [epoch: 1.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006245135830898		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.9006245135830898 | validation: 0.6653100619738544]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9144897241038507		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.9144897241038507 | validation: 0.8105981820140825]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951479950548678		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.8951479950548678 | validation: 0.766574906071313]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821363025970255		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.8821363025970255 | validation: 0.7706871241498576]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009237913343947		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.9009237913343947 | validation: 0.8784964544164504]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9530031393005733		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.9530031393005733 | validation: 0.7835430156866083]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342223439454976		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.9342223439454976 | validation: 0.7851606910569723]
	TIME [epoch: 1.35 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8965682036710756		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.8965682036710756 | validation: 0.7354860989002274]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8718991997015484		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.8718991997015484 | validation: 0.777022697754659]
	TIME [epoch: 1.35 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650603847963949		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.8650603847963949 | validation: 0.7266380462820169]
	TIME [epoch: 1.35 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825482027929629		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.8825482027929629 | validation: 0.8170373282144587]
	TIME [epoch: 1.35 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857572787001641		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.8857572787001641 | validation: 0.7613147803339249]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997603042484008		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.8997603042484008 | validation: 0.8134143937038356]
	TIME [epoch: 1.35 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311238622570406		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.9311238622570406 | validation: 0.8574046445545189]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9240660944779597		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.9240660944779597 | validation: 0.7309017877824442]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013644215326929		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.9013644215326929 | validation: 0.7748103296856836]
	TIME [epoch: 1.37 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881141866529058		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.881141866529058 | validation: 0.7299630603042964]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701596616766714		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.8701596616766714 | validation: 0.7730277414898288]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8696496932741734		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.8696496932741734 | validation: 0.7514636558730691]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687538115966428		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.8687538115966428 | validation: 0.770438530231746]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810026467937311		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.8810026467937311 | validation: 0.7707760399583989]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901648464102769		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.8901648464102769 | validation: 0.8036690522752696]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027172631348401		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.9027172631348401 | validation: 0.8567991311553039]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245449288759342		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.9245449288759342 | validation: 0.7576226002417057]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262685625296595		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.9262685625296595 | validation: 0.8076676173473027]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936022443475315		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.8936022443475315 | validation: 0.7093967411199733]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604522241306964		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.8604522241306964 | validation: 0.7453007576280508]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540221621118333		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.8540221621118333 | validation: 0.7754209202903766]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579616454665104		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.8579616454665104 | validation: 0.6838430671749866]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797963244657702		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.8797963244657702 | validation: 0.8183854068495426]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.88939507204069		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.88939507204069 | validation: 0.7069226606761791]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171537389841422		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.9171537389841422 | validation: 0.8014335900972525]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028078087466335		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.9028078087466335 | validation: 0.8068102886210277]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.888919485368108		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.888919485368108 | validation: 0.7134338175081227]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8995568963941034		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.8995568963941034 | validation: 0.8414460741759007]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910540558060328		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.8910540558060328 | validation: 0.7315212029721868]
	TIME [epoch: 1.36 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644396237044748		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.8644396237044748 | validation: 0.7471103448716188]
	TIME [epoch: 1.36 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8665674765060177		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.8665674765060177 | validation: 0.7851236284658265]
	TIME [epoch: 1.37 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775595907276277		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.8775595907276277 | validation: 0.7413245089644723]
	TIME [epoch: 1.36 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873552049755553		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.8873552049755553 | validation: 0.7835949420552307]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919133590971623		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.8919133590971623 | validation: 0.777541402011598]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864357096596563		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.8864357096596563 | validation: 0.7388139412387138]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723854898449092		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.8723854898449092 | validation: 0.83749872322382]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885561880796198		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.885561880796198 | validation: 0.703030100382342]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574833326138309		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.8574833326138309 | validation: 0.7655390680678412]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617204952117865		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.8617204952117865 | validation: 0.7569823814349109]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855484450089316		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.855484450089316 | validation: 0.7219382645338568]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810561546957584		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.8810561546957584 | validation: 0.8614836776787951]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197078000594976		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.9197078000594976 | validation: 0.72784643810094]
	TIME [epoch: 1.35 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8970995510538594		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.8970995510538594 | validation: 0.7396083115448724]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734289733069862		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.8734289733069862 | validation: 0.7739833225303356]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632412600962842		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.8632412600962842 | validation: 0.6890847789275623]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632643954340598		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.8632643954340598 | validation: 0.7813549576014936]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8662163414764583		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.8662163414764583 | validation: 0.689405334059841]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744618353802441		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.8744618353802441 | validation: 0.8492599820490891]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886648192494215		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.8886648192494215 | validation: 0.7431504457420992]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894783546037425		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.8894783546037425 | validation: 0.7715988008524879]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876371445287304		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.876371445287304 | validation: 0.787106454418327]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821051403488989		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.8821051403488989 | validation: 0.7066247817952306]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804412928049271		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.8804412928049271 | validation: 0.7735773424042542]
	TIME [epoch: 1.36 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604503492837131		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.8604503492837131 | validation: 0.727844483120463]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854035734710159		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.854035734710159 | validation: 0.7476951008755253]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458257891729553		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.8458257891729553 | validation: 0.7335834041889118]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457502712788931		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.8457502712788931 | validation: 0.7302020730690955]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416151271358708		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.8416151271358708 | validation: 0.775159417885154]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723251113687307		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.8723251113687307 | validation: 0.7721833410107396]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123507282708349		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.9123507282708349 | validation: 0.8807534979644811]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287556936514316		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.9287556936514316 | validation: 0.738766902138555]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629923954264669		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.8629923954264669 | validation: 0.7089673006364343]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425988007454224		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.8425988007454224 | validation: 0.8002016888785749]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431396644667831		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.8431396644667831 | validation: 0.6619464337610479]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477881688250105		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.8477881688250105 | validation: 0.7822680999059961]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442874227703496		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.8442874227703496 | validation: 0.657478478565777]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460567723197542		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.8460567723197542 | validation: 0.772307997153983]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474123069269612		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.8474123069269612 | validation: 0.7390027217499715]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757616168735401		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.8757616168735401 | validation: 0.822283694232602]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923218479726535		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.923218479726535 | validation: 0.8436703737529809]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9165406632041652		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.9165406632041652 | validation: 0.7658919232754817]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859267461695671		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.859267461695671 | validation: 0.7414250833553427]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478073877424496		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.8478073877424496 | validation: 0.7056137291268183]
	TIME [epoch: 176 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394195779282009		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.8394195779282009 | validation: 0.7566390834473586]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8395231566664176		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.8395231566664176 | validation: 0.6864952952939367]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406791947122776		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.8406791947122776 | validation: 0.7681079916551974]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430423028286471		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.8430423028286471 | validation: 0.6955501977674801]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337504871259558		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.8337504871259558 | validation: 0.7348979202040979]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569396740750851		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.8569396740750851 | validation: 0.7388030771805479]
	TIME [epoch: 2.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8660368963477998		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.8660368963477998 | validation: 0.7928228996583302]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9296614268155627		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.9296614268155627 | validation: 0.8640592455341946]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923925360751007		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.923925360751007 | validation: 0.7525582314466295]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650841193835122		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.8650841193835122 | validation: 0.7674476677569754]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443375994222035		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.8443375994222035 | validation: 0.6867307013347569]
	TIME [epoch: 2.67 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321536672226583		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.8321536672226583 | validation: 0.7389947039450776]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415878624128087		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.8415878624128087 | validation: 0.6851829497581056]
	TIME [epoch: 2.69 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314438038592912		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.8314438038592912 | validation: 0.7617547804664228]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451615672004964		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.8451615672004964 | validation: 0.6820840681149392]
	TIME [epoch: 2.67 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481776617968336		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.8481776617968336 | validation: 0.7813233350914817]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740905672001281		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.8740905672001281 | validation: 0.769229134507387]
	TIME [epoch: 2.67 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090703369489026		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.9090703369489026 | validation: 0.7637397166827476]
	TIME [epoch: 2.67 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916878390741929		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.916878390741929 | validation: 0.7825142631620476]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687118607508625		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.8687118607508625 | validation: 0.6722902047583412]
	TIME [epoch: 2.67 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838070802292055		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.838070802292055 | validation: 0.7615897634190754]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352096453150964		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.8352096453150964 | validation: 0.670454859067181]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8282096405218431		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.8282096405218431 | validation: 0.7386965268437132]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391345919648712		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.8391345919648712 | validation: 0.7413130571767657]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331018684836591		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.8331018684836591 | validation: 0.7443621947440561]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305010283895857		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.8305010283895857 | validation: 0.7297101816290912]
	TIME [epoch: 2.67 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404622959390315		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.8404622959390315 | validation: 0.7955271447018775]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648708955025552		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.8648708955025552 | validation: 0.7898884521308269]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9117319549221446		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.9117319549221446 | validation: 0.7956432982937875]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9121039469959292		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.9121039469959292 | validation: 0.8318440881031709]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692780225244924		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.8692780225244924 | validation: 0.6723645671816005]
	TIME [epoch: 2.67 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375312631379308		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.8375312631379308 | validation: 0.7515425916802014]
	TIME [epoch: 2.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364298116128168		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.8364298116128168 | validation: 0.7308018011501027]
	TIME [epoch: 2.67 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323972991825478		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.8323972991825478 | validation: 0.6783417403339659]
	TIME [epoch: 2.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481973099252846		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.8481973099252846 | validation: 0.7950059158717578]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8523719446568108		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.8523719446568108 | validation: 0.7278233482495794]
	TIME [epoch: 2.68 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484810991423706		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.8484810991423706 | validation: 0.7426091285231391]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610189934793284		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.8610189934793284 | validation: 0.7776421746283206]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693897341557224		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.8693897341557224 | validation: 0.6978789516647471]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579751677925447		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.8579751677925447 | validation: 0.7469575557659702]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8386635238697028		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.8386635238697028 | validation: 0.7265150363747849]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321801510116609		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.8321801510116609 | validation: 0.7013411341036543]
	TIME [epoch: 2.67 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353217539050423		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.8353217539050423 | validation: 0.7323025646695259]
	TIME [epoch: 2.67 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8391811095101172		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.8391811095101172 | validation: 0.6937642857796503]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372343250957726		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.8372343250957726 | validation: 0.7841383449638126]
	TIME [epoch: 2.67 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588412277942546		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.8588412277942546 | validation: 0.7542443369861191]
	TIME [epoch: 2.67 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685203906140201		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.8685203906140201 | validation: 0.7226734886389095]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758384374172299		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.8758384374172299 | validation: 0.7672895008380134]
	TIME [epoch: 2.67 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588193101464526		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.8588193101464526 | validation: 0.708148027690418]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8355440490581774		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.8355440490581774 | validation: 0.6945532536158041]
	TIME [epoch: 2.67 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279289115502863		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.8279289115502863 | validation: 0.761350138032581]
	TIME [epoch: 2.67 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.836806584605977		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.836806584605977 | validation: 0.6622653713508257]
	TIME [epoch: 2.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365434855064818		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.8365434855064818 | validation: 0.7444533072000326]
	TIME [epoch: 2.67 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280208948722516		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.8280208948722516 | validation: 0.7001135463965462]
	TIME [epoch: 2.67 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438404570940742		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.8438404570940742 | validation: 0.7368034852407179]
	TIME [epoch: 2.67 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460304462713171		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.8460304462713171 | validation: 0.8053795892846545]
	TIME [epoch: 2.67 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905417129131425		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.8905417129131425 | validation: 0.8078591060639188]
	TIME [epoch: 2.67 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804247988360456		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.8804247988360456 | validation: 0.6952562346137119]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419667262687314		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.8419667262687314 | validation: 0.7751110397691017]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431023740957259		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.8431023740957259 | validation: 0.6626201829701411]
	TIME [epoch: 2.67 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261263690776721		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.8261263690776721 | validation: 0.7275232838245889]
	TIME [epoch: 2.67 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325791264802581		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.8325791264802581 | validation: 0.719651568971753]
	TIME [epoch: 2.67 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837955741841495		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.837955741841495 | validation: 0.7016998954067003]
	TIME [epoch: 2.67 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8383337535546234		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.8383337535546234 | validation: 0.738998791281047]
	TIME [epoch: 2.67 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443279773649985		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.8443279773649985 | validation: 0.7406634187171852]
	TIME [epoch: 2.67 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569098240463331		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.8569098240463331 | validation: 0.7424726129511658]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8511069635580734		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.8511069635580734 | validation: 0.7523343238189738]
	TIME [epoch: 2.67 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467293837012548		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.8467293837012548 | validation: 0.7312802981272754]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445136587661066		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8445136587661066 | validation: 0.7015860513872401]
	TIME [epoch: 2.68 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418974662935581		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.8418974662935581 | validation: 0.7158308127482769]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327805540603136		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.8327805540603136 | validation: 0.7530198498751206]
	TIME [epoch: 2.67 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392665995956938		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.8392665995956938 | validation: 0.7210216258905892]
	TIME [epoch: 2.67 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459639481550724		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.8459639481550724 | validation: 0.7800505027811]
	TIME [epoch: 2.67 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868797288951908		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.868797288951908 | validation: 0.735793853386648]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672009449392366		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.8672009449392366 | validation: 0.7170408320783285]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8335043191945193		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.8335043191945193 | validation: 0.712687746074461]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8316037658439139		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.8316037658439139 | validation: 0.7335220933034683]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819870478382729		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.819870478382729 | validation: 0.6983483107735071]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269048381238736		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.8269048381238736 | validation: 0.7570743642791021]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243430389577451		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.8243430389577451 | validation: 0.6673573531416932]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478720412935455		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.8478720412935455 | validation: 0.7964669243981559]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8523192628623781		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.8523192628623781 | validation: 0.7460945977003559]
	TIME [epoch: 2.67 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549094211680955		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.8549094211680955 | validation: 0.7229755877017939]
	TIME [epoch: 2.67 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8662795034834494		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.8662795034834494 | validation: 0.7407840986039359]
	TIME [epoch: 2.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492938383067147		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.8492938383067147 | validation: 0.7198355300195253]
	TIME [epoch: 2.67 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331791797473835		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.8331791797473835 | validation: 0.6863459104882577]
	TIME [epoch: 2.67 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220105934428596		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.8220105934428596 | validation: 0.7307663474378513]
	TIME [epoch: 2.67 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336041704744648		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.8336041704744648 | validation: 0.6582694220132617]
	TIME [epoch: 2.67 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198734310152815		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.8198734310152815 | validation: 0.7006779049468653]
	TIME [epoch: 2.67 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203640329077174		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.8203640329077174 | validation: 0.7431599819207705]
	TIME [epoch: 2.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218604757249048		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.8218604757249048 | validation: 0.6652704427333471]
	TIME [epoch: 2.67 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228245446094996		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.8228245446094996 | validation: 0.7433686273629125]
	TIME [epoch: 2.67 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262927741488324		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.8262927741488324 | validation: 0.6808947338949042]
	TIME [epoch: 2.67 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8366821882455865		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.8366821882455865 | validation: 0.7727599028690465]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_0_v_mmd4_20250519_143807/states/model_phi1_4a_distortion_v2_0_v_mmd4_595.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1346.086 seconds.
