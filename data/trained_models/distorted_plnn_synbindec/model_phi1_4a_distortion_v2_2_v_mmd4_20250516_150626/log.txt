Args:
Namespace(name='model_phi1_4a_distortion_v2_2_v_mmd4', outdir='out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v2_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.020249879, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 864150642

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.731409745299539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.731409745299539 | validation: 5.314206310853212]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.573399436468035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.573399436468035 | validation: 5.354325221955207]
	TIME [epoch: 0.759 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.365413181648679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.365413181648679 | validation: 5.072612139150159]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1434780717683415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1434780717683415 | validation: 4.6107445140589105]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.801098330526872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.801098330526872 | validation: 4.627805944384811]
	TIME [epoch: 0.701 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.740034611057743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.740034611057743 | validation: 4.330519291920727]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.623964313177975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.623964313177975 | validation: 4.126461429668082]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.400829670622435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.400829670622435 | validation: 3.8620002696832842]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.159336080069726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.159336080069726 | validation: 3.8613609893333236]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.216450498778521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.216450498778521 | validation: 3.935693095295927]
	TIME [epoch: 0.695 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.22283075867993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.22283075867993 | validation: 3.216334531167438]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.820812912991114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.820812912991114 | validation: 3.140884876342243]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.596082057319693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.596082057319693 | validation: 3.1304965016682704]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5731764814245457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5731764814245457 | validation: 2.8920530314459727]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.494731878751368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.494731878751368 | validation: 2.8657175548203746]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3512245493520885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3512245493520885 | validation: 2.58684307001335]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2718156362320565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2718156362320565 | validation: 2.7119257301364406]
	TIME [epoch: 0.697 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3130785488938943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3130785488938943 | validation: 2.8422826128710477]
	TIME [epoch: 0.696 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5387010044710783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5387010044710783 | validation: 2.501491529679389]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.176748699452036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.176748699452036 | validation: 2.305397238502691]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.007561147894734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.007561147894734 | validation: 2.210332872734795]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9282690528279605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9282690528279605 | validation: 2.1194754996516267]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8717392826170283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8717392826170283 | validation: 2.15582549052657]
	TIME [epoch: 0.695 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8529867020815693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8529867020815693 | validation: 2.0928847313324455]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.919337680959401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.919337680959401 | validation: 2.712551705453258]
	TIME [epoch: 0.696 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0033969473120505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0033969473120505 | validation: 1.9019793620273309]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.657981818291638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.657981818291638 | validation: 1.8882669854806118]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6775605404052043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6775605404052043 | validation: 2.4971541394752736]
	TIME [epoch: 0.694 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8142916271425316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8142916271425316 | validation: 2.170571028049514]
	TIME [epoch: 0.698 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8207893635776053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8207893635776053 | validation: 2.3941599911819096]
	TIME [epoch: 0.696 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0623906375233254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0623906375233254 | validation: 2.0238989853623033]
	TIME [epoch: 0.694 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6227445032422394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6227445032422394 | validation: 1.9779774070572682]
	TIME [epoch: 0.695 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5105694232566385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5105694232566385 | validation: 1.8916489465438528]
	TIME [epoch: 0.694 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.482999957760572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.482999957760572 | validation: 1.9384750381722655]
	TIME [epoch: 0.699 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4509242112833602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4509242112833602 | validation: 1.961170909735011]
	TIME [epoch: 0.697 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4475985815342765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4475985815342765 | validation: 2.1412525081781673]
	TIME [epoch: 0.695 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5024270219251052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5024270219251052 | validation: 2.033889482859395]
	TIME [epoch: 0.694 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4921608131492894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4921608131492894 | validation: 2.158486963498564]
	TIME [epoch: 0.697 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.404509866377201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.404509866377201 | validation: 1.959685302853573]
	TIME [epoch: 0.695 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2931237249056156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2931237249056156 | validation: 2.01098100048671]
	TIME [epoch: 0.695 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2379685319655747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2379685319655747 | validation: 1.9506500185999063]
	TIME [epoch: 0.694 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.17516452850167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.17516452850167 | validation: 2.106108503679427]
	TIME [epoch: 0.698 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1606293090523083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1606293090523083 | validation: 2.0467720880624145]
	TIME [epoch: 0.697 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2051632211304923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2051632211304923 | validation: 2.5384733917496156]
	TIME [epoch: 0.691 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.384480757815138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.384480757815138 | validation: 2.1657282458464713]
	TIME [epoch: 0.689 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2781838750437613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2781838750437613 | validation: 2.2149347940553596]
	TIME [epoch: 0.688 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0835456605698206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0835456605698206 | validation: 2.181967599884782]
	TIME [epoch: 0.69 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9817890933086495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9817890933086495 | validation: 2.2352714506015476]
	TIME [epoch: 0.693 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9534259583267053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9534259583267053 | validation: 2.164253187453177]
	TIME [epoch: 0.693 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.936501597337469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.936501597337469 | validation: 2.5232835970174587]
	TIME [epoch: 0.694 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9868027097121195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9868027097121195 | validation: 2.3759147829337692]
	TIME [epoch: 0.693 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.252842186833187		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.252842186833187 | validation: 2.4658301802844926]
	TIME [epoch: 0.694 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1187649914362696		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.1187649914362696 | validation: 2.3491721670974752]
	TIME [epoch: 0.695 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8616589855543557		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.8616589855543557 | validation: 2.290996930280209]
	TIME [epoch: 0.693 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8185596498265246		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.8185596498265246 | validation: 2.2921477335957223]
	TIME [epoch: 0.693 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.816499627107726		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.816499627107726 | validation: 2.279703072569587]
	TIME [epoch: 0.693 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.846302444412683		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.846302444412683 | validation: 2.5732678840620333]
	TIME [epoch: 0.696 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9926870549354128		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.9926870549354128 | validation: 2.2919313142322775]
	TIME [epoch: 0.695 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.05840946424656		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 2.05840946424656 | validation: 2.381768663526489]
	TIME [epoch: 0.698 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.806870721019333		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.806870721019333 | validation: 2.219107385161273]
	TIME [epoch: 0.692 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7467613463555478		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.7467613463555478 | validation: 2.2628204969940944]
	TIME [epoch: 0.692 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7204460403163393		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.7204460403163393 | validation: 2.1979572844242905]
	TIME [epoch: 0.692 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.755789017416944		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.755789017416944 | validation: 3.373488057438791]
	TIME [epoch: 0.692 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3400964658728403		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.3400964658728403 | validation: 2.2542048646922614]
	TIME [epoch: 0.692 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.052822694921933		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.052822694921933 | validation: 2.1562293131714165]
	TIME [epoch: 0.692 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.908237922983243		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.908237922983243 | validation: 2.210442103926327]
	TIME [epoch: 0.691 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7276610641361634		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.7276610641361634 | validation: 2.4210207173792115]
	TIME [epoch: 0.691 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7716015243856895		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.7716015243856895 | validation: 2.117287200637366]
	TIME [epoch: 0.693 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7097178119833287		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.7097178119833287 | validation: 2.1404467925407604]
	TIME [epoch: 0.691 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7213987221507094		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.7213987221507094 | validation: 2.1688316767863323]
	TIME [epoch: 0.693 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7096350601980785		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.7096350601980785 | validation: 2.3460547079048504]
	TIME [epoch: 0.692 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.733192130293451		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.733192130293451 | validation: 2.131277664187519]
	TIME [epoch: 0.696 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7935932593266657		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.7935932593266657 | validation: 2.4126734870924156]
	TIME [epoch: 0.694 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8260679210068387		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.8260679210068387 | validation: 2.1661227156358955]
	TIME [epoch: 0.693 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8016308967595656		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.8016308967595656 | validation: 2.2168551102537934]
	TIME [epoch: 0.693 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6933095308119146		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.6933095308119146 | validation: 2.139734413696472]
	TIME [epoch: 0.693 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.668249968393293		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.668249968393293 | validation: 2.209954210035569]
	TIME [epoch: 0.695 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.657309687762692		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.657309687762692 | validation: 2.115803716347806]
	TIME [epoch: 0.695 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6498593686650818		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.6498593686650818 | validation: 2.1494174132837194]
	TIME [epoch: 0.694 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6339575200149705		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.6339575200149705 | validation: 2.158582940066348]
	TIME [epoch: 0.694 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6232737357175768		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.6232737357175768 | validation: 2.037826395900578]
	TIME [epoch: 0.694 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6391807120929134		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.6391807120929134 | validation: 2.608652070491576]
	TIME [epoch: 0.694 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8719277987975926		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.8719277987975926 | validation: 2.422563401857987]
	TIME [epoch: 0.693 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4401506661649366		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.4401506661649366 | validation: 1.9929956826482276]
	TIME [epoch: 0.693 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9715952983103449		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.9715952983103449 | validation: 2.3254064990711756]
	TIME [epoch: 0.692 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.776944747298876		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.776944747298876 | validation: 2.28317508442851]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7522457849900748		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.7522457849900748 | validation: 1.9867590122557601]
	TIME [epoch: 0.69 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.710384563461218		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.710384563461218 | validation: 2.0247825840646936]
	TIME [epoch: 0.689 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6812534577170613		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.6812534577170613 | validation: 2.1665626331670853]
	TIME [epoch: 0.691 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6625644380240068		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.6625644380240068 | validation: 2.100264065192904]
	TIME [epoch: 0.689 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6251155910815218		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.6251155910815218 | validation: 2.0701407260617075]
	TIME [epoch: 0.689 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6355958986645183		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.6355958986645183 | validation: 2.145371699944056]
	TIME [epoch: 0.691 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6237164962558408		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 1.6237164962558408 | validation: 2.0623265533403377]
	TIME [epoch: 0.691 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6239309075145878		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.6239309075145878 | validation: 2.1386966994388135]
	TIME [epoch: 0.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6168979330109239		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.6168979330109239 | validation: 1.9979835382515283]
	TIME [epoch: 0.689 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6370352953253196		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.6370352953253196 | validation: 2.408379970898573]
	TIME [epoch: 0.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.739569773565351		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.739569773565351 | validation: 2.054263625385433]
	TIME [epoch: 0.696 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.010610867941382		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.010610867941382 | validation: 1.8939414755125399]
	TIME [epoch: 0.696 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7344355229760136		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 1.7344355229760136 | validation: 2.5074510533528724]
	TIME [epoch: 0.696 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8158962533105136		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.8158962533105136 | validation: 1.9061644128337563]
	TIME [epoch: 0.694 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6800426782377793		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.6800426782377793 | validation: 1.9252274137422267]
	TIME [epoch: 0.698 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6817661367341474		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.6817661367341474 | validation: 2.059974268649881]
	TIME [epoch: 0.696 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6160256302494462		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.6160256302494462 | validation: 2.0791875234666293]
	TIME [epoch: 0.698 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6101879749108405		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6101879749108405 | validation: 2.0238440365316195]
	TIME [epoch: 0.7 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6015100064279184		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.6015100064279184 | validation: 2.0521838776181265]
	TIME [epoch: 0.704 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5943676026665492		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.5943676026665492 | validation: 2.0416371869637735]
	TIME [epoch: 0.695 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6185158056992492		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.6185158056992492 | validation: 2.0761235200428954]
	TIME [epoch: 0.694 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.639018845169831		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.639018845169831 | validation: 2.0512706480602247]
	TIME [epoch: 0.694 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6590191986397558		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.6590191986397558 | validation: 2.077525444875319]
	TIME [epoch: 0.697 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.577348175148369		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.577348175148369 | validation: 1.8771339626125112]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.634726206145449		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.634726206145449 | validation: 2.4462062969103804]
	TIME [epoch: 0.693 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8218863313892413		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.8218863313892413 | validation: 2.0199616247218444]
	TIME [epoch: 0.692 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0611396836747096		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.0611396836747096 | validation: 1.8730495211987277]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8760439120421155		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.8760439120421155 | validation: 1.9370795392100497]
	TIME [epoch: 0.695 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6556791123878924		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.6556791123878924 | validation: 2.2720814057491334]
	TIME [epoch: 0.693 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7034463665019541		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.7034463665019541 | validation: 1.8932612371202162]
	TIME [epoch: 0.694 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5821080905535023		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.5821080905535023 | validation: 1.9103430092983438]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5691322207694058		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.5691322207694058 | validation: 2.026512734529434]
	TIME [epoch: 0.693 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5560474081619946		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.5560474081619946 | validation: 1.9188951317544156]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.535832563028134		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.535832563028134 | validation: 1.965001201243451]
	TIME [epoch: 0.693 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5249673593335251		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.5249673593335251 | validation: 1.9261060008015265]
	TIME [epoch: 0.701 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5197477222482787		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5197477222482787 | validation: 1.963974798797471]
	TIME [epoch: 0.695 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5242643810619885		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.5242643810619885 | validation: 1.827858523155327]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5043707197413017		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.5043707197413017 | validation: 2.013743013942148]
	TIME [epoch: 0.694 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5443827042297267		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.5443827042297267 | validation: 1.8224475312262145]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.828765151652963		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.828765151652963 | validation: 1.9764787177769954]
	TIME [epoch: 0.693 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.539091837842198		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.539091837842198 | validation: 1.7628088997069442]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6069244064628185		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.6069244064628185 | validation: 1.9755122977393769]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5294904951921826		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.5294904951921826 | validation: 1.7485678512276395]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.663912825814466		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.663912825814466 | validation: 1.8778245037210626]
	TIME [epoch: 0.698 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4772578756065269		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 1.4772578756065269 | validation: 1.792451118925289]
	TIME [epoch: 0.693 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4750227517332315		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.4750227517332315 | validation: 1.830450600633508]
	TIME [epoch: 0.693 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4623952813328362		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 1.4623952813328362 | validation: 1.7699574591687786]
	TIME [epoch: 0.693 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4555744793113246		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.4555744793113246 | validation: 1.7580644945433699]
	TIME [epoch: 0.694 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.436465393849938		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 1.436465393849938 | validation: 1.6220488428930118]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4497224523861394		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.4497224523861394 | validation: 2.9100586772026737]
	TIME [epoch: 0.697 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5644777853122287		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.5644777853122287 | validation: 1.9627639927781004]
	TIME [epoch: 0.693 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.14698583670196		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.14698583670196 | validation: 1.8156281116344493]
	TIME [epoch: 0.695 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0660367602942915		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.0660367602942915 | validation: 1.653741357449804]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.72405257590642		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.72405257590642 | validation: 1.9127422197917952]
	TIME [epoch: 0.693 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5486141163143738		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.5486141163143738 | validation: 2.0119264193386015]
	TIME [epoch: 0.693 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5900069452801742		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.5900069452801742 | validation: 1.640042729120339]
	TIME [epoch: 0.692 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5471209514568856		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 1.5471209514568856 | validation: 1.7073491851949414]
	TIME [epoch: 0.69 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4537492574662054		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.4537492574662054 | validation: 1.8790448168228544]
	TIME [epoch: 0.689 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5203825544569929		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 1.5203825544569929 | validation: 1.6350863279070702]
	TIME [epoch: 0.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5801588891032259		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.5801588891032259 | validation: 1.7053021438507046]
	TIME [epoch: 0.69 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4448536921717368		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 1.4448536921717368 | validation: 1.9139777250208707]
	TIME [epoch: 0.695 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.527068351257601		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.527068351257601 | validation: 1.6276894375414301]
	TIME [epoch: 0.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6235077696150308		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 1.6235077696150308 | validation: 1.6410216649424783]
	TIME [epoch: 0.692 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4678603873475202		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.4678603873475202 | validation: 1.964406193913754]
	TIME [epoch: 0.692 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5714701848902388		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 1.5714701848902388 | validation: 1.6216880968950507]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6176676667330139		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 1.6176676667330139 | validation: 1.640936890342468]
	TIME [epoch: 0.695 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4840864721726221		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 1.4840864721726221 | validation: 1.9398673236445327]
	TIME [epoch: 0.694 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5495445162254684		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.5495445162254684 | validation: 1.6126257229864842]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5437857949755158		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.5437857949755158 | validation: 1.6313649986822663]
	TIME [epoch: 0.696 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4301647938683582		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.4301647938683582 | validation: 1.866717125962848]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5279581773500308		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 1.5279581773500308 | validation: 1.576546984986318]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.55763842100824		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.55763842100824 | validation: 1.6064742148573534]
	TIME [epoch: 0.699 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4257285848653842		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 1.4257285848653842 | validation: 1.9134380705274752]
	TIME [epoch: 0.701 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5563210038228943		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.5563210038228943 | validation: 1.5676565984539306]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5622412269184702		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 1.5622412269184702 | validation: 1.5886913681151107]
	TIME [epoch: 0.699 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4445364814968267		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 1.4445364814968267 | validation: 1.8674013525591442]
	TIME [epoch: 0.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5085264059589114		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 1.5085264059589114 | validation: 1.5581073019654665]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.49410416409454		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.49410416409454 | validation: 1.6099930136259601]
	TIME [epoch: 0.701 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3858964964565963		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.3858964964565963 | validation: 1.798710942609452]
	TIME [epoch: 0.694 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4797293601281325		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.4797293601281325 | validation: 1.5513484342688741]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5341444132390798		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.5341444132390798 | validation: 1.543276248079307]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.388074076440716		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.388074076440716 | validation: 1.7894014616006453]
	TIME [epoch: 0.698 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5319297989728864		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 1.5319297989728864 | validation: 1.5243507485206738]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5725019100914819		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.5725019100914819 | validation: 1.5283003486453548]
	TIME [epoch: 0.698 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.412602499281876		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 1.412602499281876 | validation: 1.8312046232932375]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5364506387738575		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.5364506387738575 | validation: 1.506505624035616]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5136274931016573		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 1.5136274931016573 | validation: 1.612994621941585]
	TIME [epoch: 0.697 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.407221824297282		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.407221824297282 | validation: 1.689137646068105]
	TIME [epoch: 0.696 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.423247530583264		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 1.423247530583264 | validation: 1.4650502952312057]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4614332498230747		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 1.4614332498230747 | validation: 1.5360512751852984]
	TIME [epoch: 0.699 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3484365997901564		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 1.3484365997901564 | validation: 1.5671967391804824]
	TIME [epoch: 0.695 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3596714972887727		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.3596714972887727 | validation: 1.416120770175369]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4275642706263707		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 1.4275642706263707 | validation: 1.631869489527562]
	TIME [epoch: 0.698 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3907897991473044		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3907897991473044 | validation: 1.4095620360257737]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4367383743032036		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 1.4367383743032036 | validation: 1.551170643555922]
	TIME [epoch: 0.697 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3437993483541368		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.3437993483541368 | validation: 1.398002548172447]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3725457343689544		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 1.3725457343689544 | validation: 1.6235508118345772]
	TIME [epoch: 0.699 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3998066412079004		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.3998066412079004 | validation: 1.4247092605423204]
	TIME [epoch: 0.696 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5308389356936374		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 1.5308389356936374 | validation: 1.474480293796516]
	TIME [epoch: 0.699 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.307457982047124		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.307457982047124 | validation: 1.4613027273726082]
	TIME [epoch: 0.696 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2986561515559094		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 1.2986561515559094 | validation: 1.4145499917777475]
	TIME [epoch: 0.696 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3650973633160228		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.3650973633160228 | validation: 1.6237769037134862]
	TIME [epoch: 0.696 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4605129130506513		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 1.4605129130506513 | validation: 1.4315331406677456]
	TIME [epoch: 0.697 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6255668935544274		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.6255668935544274 | validation: 1.4201841635497237]
	TIME [epoch: 0.696 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.354614814355377		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 1.354614814355377 | validation: 1.8212359099703095]
	TIME [epoch: 0.695 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.606760057746601		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.606760057746601 | validation: 1.432819893993331]
	TIME [epoch: 0.695 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5241579586408736		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 1.5241579586408736 | validation: 1.4471382005378384]
	TIME [epoch: 0.697 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3814282860250353		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 1.3814282860250353 | validation: 1.7003149812063698]
	TIME [epoch: 0.695 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4746367979025794		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 1.4746367979025794 | validation: 1.37985386111004]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3884796223057685		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 1.3884796223057685 | validation: 1.4707426139994355]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3269382477967508		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 1.3269382477967508 | validation: 1.5503837158908063]
	TIME [epoch: 0.693 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3589524652024083		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.3589524652024083 | validation: 1.3414252555847264]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3728729345280717		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 1.3728729345280717 | validation: 1.4908775050111445]
	TIME [epoch: 0.694 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2981931316868554		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.2981931316868554 | validation: 1.3770403766696526]
	TIME [epoch: 0.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2891636474974075		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 1.2891636474974075 | validation: 1.377477014065383]
	TIME [epoch: 174 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2670243599934383		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 1.2670243599934383 | validation: 1.4156081266238414]
	TIME [epoch: 1.37 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2856858967337954		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 1.2856858967337954 | validation: 1.2947882131066977]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3206039035287132		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 1.3206039035287132 | validation: 1.6230474431777742]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4527075696854723		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 1.4527075696854723 | validation: 1.423975634572438]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6229172675956502		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 1.6229172675956502 | validation: 1.3192579627512608]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3110130734360643		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 1.3110130734360643 | validation: 1.758892860469909]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.582613659703052		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 1.582613659703052 | validation: 1.3449628112967655]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4628015893424322		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 1.4628015893424322 | validation: 1.3470049344285517]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3257290576809697		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 1.3257290576809697 | validation: 1.6167202022808387]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.432374213196501		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 1.432374213196501 | validation: 1.3218246467286139]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.353602346053786		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 1.353602346053786 | validation: 1.3410852703855867]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2690137959726424		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 1.2690137959726424 | validation: 1.4586601444287837]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3389006104598624		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.3389006104598624 | validation: 1.319356313134286]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.356880509989262		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 1.356880509989262 | validation: 1.4095873894623479]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2699498325833753		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.2699498325833753 | validation: 1.3346427425760479]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.260110250382642		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 1.260110250382642 | validation: 1.3211762646334346]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2789350237938197		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 1.2789350237938197 | validation: 1.3729593662083417]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2962232085059808		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 1.2962232085059808 | validation: 1.28961511386055]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3447182101067594		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.3447182101067594 | validation: 1.4806111890043017]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3367106019011152		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 1.3367106019011152 | validation: 1.2727135553259359]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4285257324343947		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.4285257324343947 | validation: 1.4371788346906325]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3112621493054817		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.3112621493054817 | validation: 1.2688320902819858]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2702941800400391		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 1.2702941800400391 | validation: 1.347114778108959]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2650346640905619		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.2650346640905619 | validation: 1.2648014455822514]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3054834706188925		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 1.3054834706188925 | validation: 1.4815449927662427]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3340315752048326		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 1.3340315752048326 | validation: 1.2406432442989024]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.405730525908447		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 1.405730525908447 | validation: 1.4082080159538823]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2699086982188017		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 1.2699086982188017 | validation: 1.2675289236295015]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.25786486392595		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 1.25786486392595 | validation: 1.3297603226223225]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.25147332418243		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 1.25147332418243 | validation: 1.2375136077627662]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2690184843945034		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 1.2690184843945034 | validation: 1.4090248138119152]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3122406744552582		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 1.3122406744552582 | validation: 1.315098754808312]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4926578284283774		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.4926578284283774 | validation: 1.4208932722043137]
	TIME [epoch: 1.35 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.292332348126043		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 1.292332348126043 | validation: 1.2427990832611742]
	TIME [epoch: 1.36 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2471428206582278		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 1.2471428206582278 | validation: 1.32253090813271]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244192626282588		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 1.244192626282588 | validation: 1.228673908569482]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2569209867957427		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 1.2569209867957427 | validation: 1.4117331331993554]
	TIME [epoch: 1.36 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3215499817828282		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 1.3215499817828282 | validation: 1.2486974988414086]
	TIME [epoch: 1.36 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4629315742812066		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 1.4629315742812066 | validation: 1.361046690305266]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2586671367118318		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 1.2586671367118318 | validation: 1.2636453686299247]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.239807355003468		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 1.239807355003468 | validation: 1.2977538746153081]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.241159517959151		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 1.241159517959151 | validation: 1.2810551235335959]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2476026673428156		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 1.2476026673428156 | validation: 1.2558043110491435]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3134701334410743		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 1.3134701334410743 | validation: 1.438894169328689]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3487047989828227		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 1.3487047989828227 | validation: 1.263152585611939]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4842995174442783		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 1.4842995174442783 | validation: 1.3427210109930319]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2556854891354376		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 1.2556854891354376 | validation: 1.2789623761043636]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2477848850895672		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 1.2477848850895672 | validation: 1.2548812284772213]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2570452983914877		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 1.2570452983914877 | validation: 1.3370805216686785]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.277801749182914		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 1.277801749182914 | validation: 1.2048388100446235]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3811804102166876		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 1.3811804102166876 | validation: 1.3801098849987852]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.287349762773708		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 1.287349762773708 | validation: 1.2051191216083215]
	TIME [epoch: 1.35 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2506570517188649		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 1.2506570517188649 | validation: 1.307590930293251]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2609388413460219		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 1.2609388413460219 | validation: 1.1921448232557126]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3058716483006152		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 1.3058716483006152 | validation: 1.3841400340018752]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.294110483544915		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 1.294110483544915 | validation: 1.1949592753786575]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3196241985863686		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.3196241985863686 | validation: 1.3285467423410386]
	TIME [epoch: 1.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2596831360470275		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 1.2596831360470275 | validation: 1.1836514275098438]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2609206229813603		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.2609206229813603 | validation: 1.334393616178649]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.259115690552802		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 1.259115690552802 | validation: 1.1623357745416238]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2809718123854967		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 1.2809718123854967 | validation: 1.342565309055815]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2755280498534212		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 1.2755280498534212 | validation: 1.2016414283870964]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3116622942535205		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 1.3116622942535205 | validation: 1.3204857265075198]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2741341633836853		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 1.2741341633836853 | validation: 1.1799590645027862]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2569427265041966		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 1.2569427265041966 | validation: 1.2914363487787925]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455391465885601		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 1.2455391465885601 | validation: 1.1510158752349706]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2547164507420296		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 1.2547164507420296 | validation: 1.3463088162136156]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2823326728613043		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 1.2823326728613043 | validation: 1.162461991310719]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3256700834633102		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 1.3256700834633102 | validation: 1.3250498500485177]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2646806664006216		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 1.2646806664006216 | validation: 1.169509797524105]
	TIME [epoch: 1.35 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.266650324264935		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.266650324264935 | validation: 1.2945202689260453]
	TIME [epoch: 1.35 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244489356835145		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 1.244489356835145 | validation: 1.2119698060180484]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2691483885924748		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 1.2691483885924748 | validation: 1.336981016813721]
	TIME [epoch: 1.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2712214351899906		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 1.2712214351899906 | validation: 1.1511084207081967]
	TIME [epoch: 1.36 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2690210996592768		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 1.2690210996592768 | validation: 1.3050373679282885]
	TIME [epoch: 1.35 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2622366139539911		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 1.2622366139539911 | validation: 1.1559888550403614]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2821180006744555		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 1.2821180006744555 | validation: 1.338960734353454]
	TIME [epoch: 1.35 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2773686212585778		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.2773686212585778 | validation: 1.1527417072609574]
	TIME [epoch: 1.36 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26409122262036		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.26409122262036 | validation: 1.2719904903265251]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.234853073835063		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.234853073835063 | validation: 1.1610060123435366]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2337342554567048		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.2337342554567048 | validation: 1.2639704912762746]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2408819097868091		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.2408819097868091 | validation: 1.1690746112867354]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2767626312851237		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.2767626312851237 | validation: 1.3160079042175898]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2685750846638204		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.2685750846638204 | validation: 1.1418302419173785]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2913678115857656		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.2913678115857656 | validation: 1.2733769460631426]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2478127586050205		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.2478127586050205 | validation: 1.1234070196758477]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2412586339733087		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.2412586339733087 | validation: 1.2559208501933279]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2363043929074176		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.2363043929074176 | validation: 1.1303619253614117]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2418929000607173		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.2418929000607173 | validation: 1.2897121813262098]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2505035376591067		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.2505035376591067 | validation: 1.1549084797301654]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3077205746617646		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.3077205746617646 | validation: 1.3095432791693826]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2679354441407729		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.2679354441407729 | validation: 1.129617862684556]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2424996660791363		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.2424996660791363 | validation: 1.2299959506258984]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20857665081064		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.20857665081064 | validation: 1.1373563648674032]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210894662227927		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.210894662227927 | validation: 1.2357054673534507]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215872220155492		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.215872220155492 | validation: 1.106768403871123]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2536981336214756		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.2536981336214756 | validation: 1.3435154599074275]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3054865096915493		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.3054865096915493 | validation: 1.1426426274902102]
	TIME [epoch: 1.35 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3307234367193865		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 1.3307234367193865 | validation: 1.2550817800000738]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235103163793608		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 1.2235103163793608 | validation: 1.1410490185549829]
	TIME [epoch: 1.36 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2052757216199783		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 1.2052757216199783 | validation: 1.1440047523755597]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2014746974209616		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 1.2014746974209616 | validation: 1.1646638935320892]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1967765601421854		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 1.1967765601421854 | validation: 1.1345334373339935]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1940623002942674		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 1.1940623002942674 | validation: 1.1343194140304171]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1932910789741293		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 1.1932910789741293 | validation: 1.2131694719132313]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2167828143492687		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 1.2167828143492687 | validation: 1.1280746065365541]
	TIME [epoch: 1.36 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3089621219311167		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 1.3089621219311167 | validation: 1.4348578733705526]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3769353136203029		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 1.3769353136203029 | validation: 1.1131297634001367]
	TIME [epoch: 1.35 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.316436847561472		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 1.316436847561472 | validation: 1.2032554244490186]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2096688513183498		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 1.2096688513183498 | validation: 1.119068738355813]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1953765611873408		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 1.1953765611873408 | validation: 1.1309944501023317]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1897159332979232		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 1.1897159332979232 | validation: 1.1343515835476876]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1846003966490204		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 1.1846003966490204 | validation: 1.1379266444879776]
	TIME [epoch: 1.35 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1840667911128648		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 1.1840667911128648 | validation: 1.1185541043696667]
	TIME [epoch: 1.35 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1825498267879269		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 1.1825498267879269 | validation: 1.1087995111573978]
	TIME [epoch: 1.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1896073977741		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 1.1896073977741 | validation: 1.2064075087314243]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2348057924529006		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 1.2348057924529006 | validation: 1.1416572115318449]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3815115237311835		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 1.3815115237311835 | validation: 1.3455753330506863]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2994881890772205		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.2994881890772205 | validation: 1.0890915735396982]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2579967636730418		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 1.2579967636730418 | validation: 1.2165997235977113]
	TIME [epoch: 1.37 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2315229464359814		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.2315229464359814 | validation: 1.0764756856543318]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.206892557531998		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 1.206892557531998 | validation: 1.170858969044535]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1990689386157525		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.1990689386157525 | validation: 1.0950227790127804]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2254072810321024		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 1.2254072810321024 | validation: 1.3162946613891293]
	TIME [epoch: 1.35 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2713766251657908		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 1.2713766251657908 | validation: 1.0997787314331897]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2738997844719666		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 1.2738997844719666 | validation: 1.210394493698315]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2091397638105152		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 1.2091397638105152 | validation: 1.107617530205509]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1882974893897806		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 1.1882974893897806 | validation: 1.1569048418405792]
	TIME [epoch: 1.35 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2002410749641794		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 1.2002410749641794 | validation: 1.0889452529268007]
	TIME [epoch: 1.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2100737835940139		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 1.2100737835940139 | validation: 1.1915386110721036]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235679304413285		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 1.2235679304413285 | validation: 1.0909967714663782]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2457630338427896		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 1.2457630338427896 | validation: 1.3004862620946163]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2781892144190363		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 1.2781892144190363 | validation: 1.1241931263011873]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2644569950433084		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 1.2644569950433084 | validation: 1.177936743499118]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194821714907916		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 1.194821714907916 | validation: 1.1031248555802908]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1848461356721691		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 1.1848461356721691 | validation: 1.1274988068651632]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1849953388996355		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 1.1849953388996355 | validation: 1.0985785154564223]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183533442870738		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 1.183533442870738 | validation: 1.1369506276721921]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.18814656438373		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 1.18814656438373 | validation: 1.0733422527184548]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2269046911332167		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 1.2269046911332167 | validation: 1.2755890503199026]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2844107517134313		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.2844107517134313 | validation: 1.1440379834313323]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3259275033298397		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 1.3259275033298397 | validation: 1.2305936566136664]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2210715006340378		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.2210715006340378 | validation: 1.1149289646981895]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1911902234663025		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 1.1911902234663025 | validation: 1.1303482942471337]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1844527065672816		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 1.1844527065672816 | validation: 1.0920145213719719]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1832077386362685		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 1.1832077386362685 | validation: 1.121942558528902]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.18380709520138		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 1.18380709520138 | validation: 1.0686220414389211]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1980527891376227		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 1.1980527891376227 | validation: 1.222166395506658]
	TIME [epoch: 1.35 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2480179295968352		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 1.2480179295968352 | validation: 1.0726744360441949]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.312176464999566		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 1.312176464999566 | validation: 1.274791539188103]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2346235692290897		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 1.2346235692290897 | validation: 1.0939154706303427]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1937299217184938		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 1.1937299217184938 | validation: 1.1297613592173108]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1878586897291716		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 1.1878586897291716 | validation: 1.0806810815563914]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2041145565048128		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 1.2041145565048128 | validation: 1.1844625142026985]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2201218129269156		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 1.2201218129269156 | validation: 1.0764203668787429]
	TIME [epoch: 1.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2403160609401536		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 1.2403160609401536 | validation: 1.2041300372747354]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2210019603113589		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.2210019603113589 | validation: 1.0679789265501707]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2139649758452666		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 1.2139649758452666 | validation: 1.1974792048531782]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222320466912598		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 1.222320466912598 | validation: 1.0625580335035936]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2149824841107648		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 1.2149824841107648 | validation: 1.1672611170257092]
	TIME [epoch: 1.35 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2060167777608843		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 1.2060167777608843 | validation: 1.0843297837134251]
	TIME [epoch: 1.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210593963478166		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 1.210593963478166 | validation: 1.1809944682841436]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2110229963093655		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 1.2110229963093655 | validation: 1.0804697514509536]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073243191107224		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 1.2073243191107224 | validation: 1.1863140116844746]
	TIME [epoch: 1.35 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2009620590043657		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 1.2009620590043657 | validation: 1.082348970502054]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2054319342943445		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 1.2054319342943445 | validation: 1.1490342449359379]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195631339363887		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 1.195631339363887 | validation: 1.0595200026407054]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.200976389540579		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 1.200976389540579 | validation: 1.1727495346577435]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.207528777189262		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 1.207528777189262 | validation: 1.0582161013350806]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2094088133768897		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 1.2094088133768897 | validation: 1.1652932289191704]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2042230515480443		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 1.2042230515480443 | validation: 1.0585295071634242]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2146186610751706		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 1.2146186610751706 | validation: 1.1952298646527337]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2164354740970251		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 1.2164354740970251 | validation: 1.071445318164074]
	TIME [epoch: 1.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2200648029325332		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 1.2200648029325332 | validation: 1.2123379697685264]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2105399483456		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 1.2105399483456 | validation: 1.0597884243298825]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1925428503919153		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 1.1925428503919153 | validation: 1.1545917676855908]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1957365212197797		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 1.1957365212197797 | validation: 1.0603265952565442]
	TIME [epoch: 1.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1942436703213861		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 1.1942436703213861 | validation: 1.1692828990659718]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2108440168153567		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 1.2108440168153567 | validation: 1.0554623744729923]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.217502643625835		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 1.217502643625835 | validation: 1.1846880373716917]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2075402336958194		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 1.2075402336958194 | validation: 1.0411108859530789]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2006816061344414		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 1.2006816061344414 | validation: 1.1519267687570531]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192089789634085		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 1.192089789634085 | validation: 1.0408581252954943]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.192546016484675		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 1.192546016484675 | validation: 1.152796846738857]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1928287296946791		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 1.1928287296946791 | validation: 1.050140064558297]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2022294829730842		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 1.2022294829730842 | validation: 1.1683844755036998]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2071248710348548		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 1.2071248710348548 | validation: 1.053165093167221]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2152166008520024		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 1.2152166008520024 | validation: 1.1802861175000772]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2117850242305337		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.2117850242305337 | validation: 1.0595866557870937]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1979848149491494		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 1.1979848149491494 | validation: 1.1188674723927134]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1891098368156157		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.1891098368156157 | validation: 1.0431393652818437]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1863482468242357		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 1.1863482468242357 | validation: 1.1009879718416842]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1681811331133678		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 1.1681811331133678 | validation: 1.0415375537589566]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1969202822082574		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 1.1969202822082574 | validation: 1.1988172533401213]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.22979507449334		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 1.22979507449334 | validation: 1.045679974303765]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2177639193583523		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 1.2177639193583523 | validation: 1.172608692637543]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1960432007161543		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 1.1960432007161543 | validation: 1.0624111776236973]
	TIME [epoch: 1.35 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067146394306334		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 1.2067146394306334 | validation: 1.1276495689719703]
	TIME [epoch: 1.35 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1919043078094136		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 1.1919043078094136 | validation: 1.0499638159632105]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1886054382662254		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 1.1886054382662254 | validation: 1.1451939518385899]
	TIME [epoch: 1.36 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1867088393449867		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 1.1867088393449867 | validation: 1.0243881585423602]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1968138589673456		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 1.1968138589673456 | validation: 1.1278211295129148]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1939090265310581		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 1.1939090265310581 | validation: 1.0381034105396336]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1878450252791317		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 1.1878450252791317 | validation: 1.1500184455643228]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191189703451395		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 1.191189703451395 | validation: 1.0577670454187003]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1964399955107736		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 1.1964399955107736 | validation: 1.1521238572751258]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2029268504670771		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 1.2029268504670771 | validation: 1.0346662079030449]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2017399194647498		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 1.2017399194647498 | validation: 1.1226973304014705]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.19474899149944		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 1.19474899149944 | validation: 1.0471996439532405]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1900841468951415		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 1.1900841468951415 | validation: 1.116186868212792]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1786612778836365		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 1.1786612778836365 | validation: 1.0271816634348236]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770942037864038		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 1.1770942037864038 | validation: 1.1166377472912161]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1819172702200649		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 1.1819172702200649 | validation: 1.0307938656733986]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1975671249036632		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 1.1975671249036632 | validation: 1.1501428515863688]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2079757141252048		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 1.2079757141252048 | validation: 1.041251799804856]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2113241562404957		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 1.2113241562404957 | validation: 1.158290299903053]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1990922940315434		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 1.1990922940315434 | validation: 1.0293722439958537]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1830424361942666		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 1.1830424361942666 | validation: 1.087901216123391]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1771733300179819		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 1.1771733300179819 | validation: 1.03288195326275]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1685721747028155		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 1.1685721747028155 | validation: 1.0939580057651956]
	TIME [epoch: 1.36 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1602015689470606		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 1.1602015689470606 | validation: 1.009631105174276]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1681329549095893		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 1.1681329549095893 | validation: 1.1421417668650666]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2005069573000622		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 1.2005069573000622 | validation: 1.012426653211314]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2162700752150581		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 1.2162700752150581 | validation: 1.1912265927500134]
	TIME [epoch: 1.36 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2179181774701617		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 1.2179181774701617 | validation: 1.0427342010774732]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2023692762805869		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 1.2023692762805869 | validation: 1.0892184445858744]
	TIME [epoch: 1.35 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1757518362142019		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 1.1757518362142019 | validation: 1.0505216761722629]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1698242829685577		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 1.1698242829685577 | validation: 1.086013855827323]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165260348949246		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 1.165260348949246 | validation: 1.022729144420158]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1612038492829149		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 1.1612038492829149 | validation: 1.0663585476073616]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1754055018368395		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 1.1754055018368395 | validation: 1.0184045048914907]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1832409297128115		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 1.1832409297128115 | validation: 1.1350542733702387]
	TIME [epoch: 1.35 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1880998776862104		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 1.1880998776862104 | validation: 1.0288640833922098]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2218378564293713		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 1.2218378564293713 | validation: 1.1607876507824355]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2218927257796863		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 1.2218927257796863 | validation: 1.0219394337197831]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1842871875395318		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 1.1842871875395318 | validation: 1.0912680933797876]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.181174439794272		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 1.181174439794272 | validation: 1.044896591524678]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1785689063085274		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 1.1785689063085274 | validation: 1.0755323769500265]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1728943690058253		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 1.1728943690058253 | validation: 1.0220356327165934]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1725050525751797		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 1.1725050525751797 | validation: 1.0974696051297685]
	TIME [epoch: 1.35 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1785861250838694		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 1.1785861250838694 | validation: 1.015187901167967]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1962303118907016		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 1.1962303118907016 | validation: 1.1374115931230955]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1966062246655136		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 1.1966062246655136 | validation: 1.0213015396159786]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1939229083246863		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 1.1939229083246863 | validation: 1.0968050408630556]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1708338488036194		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 1.1708338488036194 | validation: 1.0066331280803122]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679908855428431		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 1.1679908855428431 | validation: 1.0659450864837716]
	TIME [epoch: 1.36 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16332039928475		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 1.16332039928475 | validation: 1.0042428857892893]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1731903484074606		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 1.1731903484074606 | validation: 1.0933417373486936]
	TIME [epoch: 1.35 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.183717977738487		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 1.183717977738487 | validation: 1.015528453492449]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1982417180337352		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 1.1982417180337352 | validation: 1.1563280633849455]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2088760799073577		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 1.2088760799073577 | validation: 1.0225766147514068]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1874535175916339		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 1.1874535175916339 | validation: 1.0984161085773543]
	TIME [epoch: 1.35 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1711084340394222		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 1.1711084340394222 | validation: 1.017670776886455]
	TIME [epoch: 1.35 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.162836186384388		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 1.162836186384388 | validation: 1.0735585509250167]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1701081115196257		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 1.1701081115196257 | validation: 1.0124664127382705]
	TIME [epoch: 1.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1672813050557396		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 1.1672813050557396 | validation: 1.1057456592681447]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1820601581678036		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 1.1820601581678036 | validation: 1.0089718909737801]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187148249836423		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 1.187148249836423 | validation: 1.1270767351209157]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1920051082069822		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 1.1920051082069822 | validation: 1.0160254939351645]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1922020135241016		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 1.1922020135241016 | validation: 1.0866322923570637]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1690361512514031		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 1.1690361512514031 | validation: 1.0166208903945912]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1622076619054278		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 1.1622076619054278 | validation: 1.0647604743018062]
	TIME [epoch: 1.35 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1530559627212082		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 1.1530559627212082 | validation: 1.0213317474091785]
	TIME [epoch: 1.35 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1565432387492929		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 1.1565432387492929 | validation: 1.0583498800953657]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1668926519558869		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 1.1668926519558869 | validation: 1.0312477814851355]
	TIME [epoch: 1.36 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172182755290466		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 1.172182755290466 | validation: 1.0936249818657515]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.194025885416941		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 1.194025885416941 | validation: 1.0058242726195774]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.198437837358895		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 1.198437837358895 | validation: 1.1457597656666045]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1921058233687374		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 1.1921058233687374 | validation: 1.0090899321535847]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1835666070115651		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 1.1835666070115651 | validation: 1.0757768717217115]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1664914396808421		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 1.1664914396808421 | validation: 1.012547503820637]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1616306653966038		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 1.1616306653966038 | validation: 1.0455704974809226]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1535900014823965		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 1.1535900014823965 | validation: 1.0201376161370594]
	TIME [epoch: 1.35 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1516089828680367		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 1.1516089828680367 | validation: 1.0728768780234919]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1748203327543063		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 1.1748203327543063 | validation: 1.0074922782011169]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1883329449774716		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 1.1883329449774716 | validation: 1.1300002686349737]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2058932072167057		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 1.2058932072167057 | validation: 1.0155076461594394]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1939955598756735		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 1.1939955598756735 | validation: 1.0764953592048625]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1633073381681052		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 1.1633073381681052 | validation: 1.016570080351958]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1515971618086451		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 1.1515971618086451 | validation: 1.0100945914703687]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1512379786428728		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 1.1512379786428728 | validation: 1.0246627485491686]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498736635688513		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 1.1498736635688513 | validation: 1.0020838825915843]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1547734078534		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 1.1547734078534 | validation: 1.0330866080259102]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1483250280278845		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 1.1483250280278845 | validation: 0.9858522432780785]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1688233083042767		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 1.1688233083042767 | validation: 1.110733305377148]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.204935601133495		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 1.204935601133495 | validation: 1.0232326136649925]
	TIME [epoch: 1.36 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.229438882824225		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 1.229438882824225 | validation: 1.0866401066249647]
	TIME [epoch: 1.36 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.170824139394021		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 1.170824139394021 | validation: 1.010101500722439]
	TIME [epoch: 1.36 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1579522922281615		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 1.1579522922281615 | validation: 1.034160904187553]
	TIME [epoch: 1.36 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154567703250908		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 1.154567703250908 | validation: 1.005010974602051]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1407261869161398		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 1.1407261869161398 | validation: 1.0448357999992366]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1570411228049124		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 1.1570411228049124 | validation: 0.9916403742919431]
	TIME [epoch: 1.36 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1620178800627994		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 1.1620178800627994 | validation: 1.0924288875662596]
	TIME [epoch: 1.36 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1820790233319947		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 1.1820790233319947 | validation: 1.0098253315374766]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2136545922991784		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 1.2136545922991784 | validation: 1.079391379908943]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1744033449965436		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 1.1744033449965436 | validation: 0.9892317555088372]
	TIME [epoch: 1.36 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1658037536414432		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 1.1658037536414432 | validation: 1.0316625962414594]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1568662534713847		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 1.1568662534713847 | validation: 1.0233831376772617]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1500634184347467		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 1.1500634184347467 | validation: 1.012844216188866]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1515490809006597		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 1.1515490809006597 | validation: 1.027639681405379]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154268593236664		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 1.154268593236664 | validation: 0.995251885912704]
	TIME [epoch: 2.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1540907710608383		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 1.1540907710608383 | validation: 1.0874150588624294]
	TIME [epoch: 2.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179429667543099		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 1.179429667543099 | validation: 0.9998396081634159]
	TIME [epoch: 2.69 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2052215701724398		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 1.2052215701724398 | validation: 1.1251099866734335]
	TIME [epoch: 2.68 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1804572760139178		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 1.1804572760139178 | validation: 0.9956440967204628]
	TIME [epoch: 2.69 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1695008997412202		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 1.1695008997412202 | validation: 1.032026645096113]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1581291228648969		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 1.1581291228648969 | validation: 0.9909055062076441]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155898846058809		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 1.155898846058809 | validation: 1.0341502890036303]
	TIME [epoch: 2.68 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1534994415147453		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 1.1534994415147453 | validation: 0.9807824923673352]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1626043672077464		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 1.1626043672077464 | validation: 1.0330767016096691]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615445457621394		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 1.1615445457621394 | validation: 0.9713100678976926]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1558138116883976		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 1.1558138116883976 | validation: 1.0699089581737542]
	TIME [epoch: 2.68 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17722813513144		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 1.17722813513144 | validation: 0.9981556535816672]
	TIME [epoch: 2.68 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.187288295073526		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 1.187288295073526 | validation: 1.110072233442695]
	TIME [epoch: 2.68 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172188472881799		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 1.172188472881799 | validation: 0.9840561807064208]
	TIME [epoch: 2.69 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165340541187663		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 1.165340541187663 | validation: 1.058979555625558]
	TIME [epoch: 2.68 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164695988018644		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 1.164695988018644 | validation: 0.9847590847906226]
	TIME [epoch: 2.69 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554001519013617		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 1.1554001519013617 | validation: 1.0346996270030093]
	TIME [epoch: 2.68 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1565778864232454		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 1.1565778864232454 | validation: 0.9806283294869957]
	TIME [epoch: 2.68 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.165259869614281		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 1.165259869614281 | validation: 1.056980305845992]
	TIME [epoch: 2.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1601017757663368		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 1.1601017757663368 | validation: 0.9648650026466691]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1599589964797952		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 1.1599589964797952 | validation: 1.0438585898582595]
	TIME [epoch: 2.68 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1603933595129032		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 1.1603933595129032 | validation: 0.9762709363910904]
	TIME [epoch: 2.68 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1576021493551931		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 1.1576021493551931 | validation: 1.061467360650459]
	TIME [epoch: 2.68 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1610340753861084		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 1.1610340753861084 | validation: 0.9641535545902088]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1707925403242723		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 1.1707925403242723 | validation: 1.0616845708786167]
	TIME [epoch: 2.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.170742770543244		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 1.170742770543244 | validation: 0.9829768654486322]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.172318457655324		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 1.172318457655324 | validation: 1.0283646551513042]
	TIME [epoch: 2.68 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1525316185353478		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 1.1525316185353478 | validation: 0.9902471883945545]
	TIME [epoch: 2.68 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1545675402848345		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 1.1545675402848345 | validation: 1.013196614998986]
	TIME [epoch: 2.68 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1531652145644606		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 1.1531652145644606 | validation: 0.9770941476904043]
	TIME [epoch: 2.68 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453518903009463		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 1.1453518903009463 | validation: 1.0141395164074012]
	TIME [epoch: 2.68 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1450606006078212		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 1.1450606006078212 | validation: 0.981939950367766]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1511505013590466		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 1.1511505013590466 | validation: 1.0114241366742633]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1520372236198162		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 1.1520372236198162 | validation: 0.9708594768431587]
	TIME [epoch: 2.68 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1588249890520659		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 1.1588249890520659 | validation: 1.0399802568434886]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1688235277152357		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 1.1688235277152357 | validation: 0.9718627792656026]
	TIME [epoch: 2.69 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2032629001773065		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 1.2032629001773065 | validation: 1.141619586620611]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2056339146835857		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 1.2056339146835857 | validation: 0.9783915004423207]
	TIME [epoch: 2.68 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1621488808560174		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 1.1621488808560174 | validation: 0.981575792676845]
	TIME [epoch: 2.68 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474410972080438		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 1.1474410972080438 | validation: 0.9956604435637278]
	TIME [epoch: 2.68 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474009263160372		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 1.1474009263160372 | validation: 0.9872159268795733]
	TIME [epoch: 2.68 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1435208867700828		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 1.1435208867700828 | validation: 1.0081878917277998]
	TIME [epoch: 2.68 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1457059687956777		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 1.1457059687956777 | validation: 0.9586195148708723]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574792470898971		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 1.1574792470898971 | validation: 1.0329974707167868]
	TIME [epoch: 2.68 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1531242275415803		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 1.1531242275415803 | validation: 0.9506382951099376]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16037414087093		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 1.16037414087093 | validation: 1.0749582488766376]
	TIME [epoch: 2.68 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770947111787997		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 1.1770947111787997 | validation: 0.9682386833235852]
	TIME [epoch: 2.68 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1743053091331088		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 1.1743053091331088 | validation: 1.0394641707276049]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1662822876816716		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 1.1662822876816716 | validation: 0.9760776262291128]
	TIME [epoch: 2.68 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1444798935960714		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 1.1444798935960714 | validation: 0.9869812064168131]
	TIME [epoch: 2.68 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498854591988894		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 1.1498854591988894 | validation: 0.9952974430305059]
	TIME [epoch: 2.68 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.145699802435932		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 1.145699802435932 | validation: 0.9749214573128908]
	TIME [epoch: 2.69 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.153938942450013		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 1.153938942450013 | validation: 1.042328611828891]
	TIME [epoch: 2.68 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1757457000539229		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 1.1757457000539229 | validation: 0.9611195449542198]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177309860358278		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 1.177309860358278 | validation: 1.0563804776249957]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1628501560080093		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 1.1628501560080093 | validation: 0.9706449910139519]
	TIME [epoch: 2.68 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1564616501842895		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 1.1564616501842895 | validation: 1.009057792953078]
	TIME [epoch: 2.68 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417376271888187		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 1.1417376271888187 | validation: 0.9772841367157291]
	TIME [epoch: 2.69 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.153029701172777		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 1.153029701172777 | validation: 0.9953519633791503]
	TIME [epoch: 2.68 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1408583605962652		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 1.1408583605962652 | validation: 0.9691602459548503]
	TIME [epoch: 2.68 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1443470669712092		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 1.1443470669712092 | validation: 1.0142136565913444]
	TIME [epoch: 2.68 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1532427555426088		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 1.1532427555426088 | validation: 0.979782523009871]
	TIME [epoch: 2.68 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1566434188530874		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 1.1566434188530874 | validation: 1.0580818471062536]
	TIME [epoch: 2.68 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.171468272987982		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 1.171468272987982 | validation: 0.9616914826489382]
	TIME [epoch: 2.68 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1732592598245293		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 1.1732592598245293 | validation: 1.0467196551489406]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1615160845117152		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 1.1615160845117152 | validation: 0.9678121804955785]
	TIME [epoch: 2.68 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149666751438931		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 1.149666751438931 | validation: 0.98609988672149]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1487025496079832		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 1.1487025496079832 | validation: 0.9838977041968393]
	TIME [epoch: 2.69 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143336521337326		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 1.143336521337326 | validation: 0.9751610414046468]
	TIME [epoch: 2.71 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1414697402358842		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 1.1414697402358842 | validation: 0.975497431446577]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146028313128582		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 1.146028313128582 | validation: 0.9932997730044619]
	TIME [epoch: 2.68 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1431128859739412		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 1.1431128859739412 | validation: 0.9800046669241298]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474760869294671		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 1.1474760869294671 | validation: 1.0102163819573478]
	TIME [epoch: 2.68 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1538292721479537		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 1.1538292721479537 | validation: 0.9596454106554623]
	TIME [epoch: 2.68 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1879696644656395		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 1.1879696644656395 | validation: 1.0746853077582452]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1852356646403863		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 1.1852356646403863 | validation: 0.9559485605809916]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1575698553597864		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 1.1575698553597864 | validation: 0.9990574055617124]
	TIME [epoch: 2.68 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142621496318201		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 1.142621496318201 | validation: 0.9637327478730203]
	TIME [epoch: 2.68 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1360854759592989		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 1.1360854759592989 | validation: 0.9748718986413323]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1366914516561153		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 1.1366914516561153 | validation: 0.9645488098009312]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1470044026715092		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 1.1470044026715092 | validation: 0.9888452180473017]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439188151303246		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 1.1439188151303246 | validation: 0.978321292045314]
	TIME [epoch: 2.68 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149600450150136		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 1.149600450150136 | validation: 0.9859611352970742]
	TIME [epoch: 2.68 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.145591770071056		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 1.145591770071056 | validation: 0.9553715436715282]
	TIME [epoch: 2.69 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1604675569639784		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 1.1604675569639784 | validation: 1.0862214522215392]
	TIME [epoch: 2.68 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188513827574595		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 1.188513827574595 | validation: 0.9478172300521351]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169719703640725		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 1.169719703640725 | validation: 0.9826757520425812]
	TIME [epoch: 2.68 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1516532114111873		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 1.1516532114111873 | validation: 0.9917613843309564]
	TIME [epoch: 2.68 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1476115463927108		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 1.1476115463927108 | validation: 0.9611654403886558]
	TIME [epoch: 2.68 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1464015645097185		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 1.1464015645097185 | validation: 0.9926566120498649]
	TIME [epoch: 2.68 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1393314309088782		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 1.1393314309088782 | validation: 0.9679433969604754]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1405552378493764		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 1.1405552378493764 | validation: 0.9824100570011608]
	TIME [epoch: 2.68 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1470586576070951		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 1.1470586576070951 | validation: 0.9583611014153451]
	TIME [epoch: 2.68 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1386348624920328		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 1.1386348624920328 | validation: 1.0302707199745846]
	TIME [epoch: 2.68 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1623321309078396		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 1.1623321309078396 | validation: 0.949678844628053]
	TIME [epoch: 2.68 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746925140386097		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 1.1746925140386097 | validation: 1.0079416654675395]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1489003770358108		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 1.1489003770358108 | validation: 0.9673088104178483]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1385880951927672		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 1.1385880951927672 | validation: 0.9823786599609812]
	TIME [epoch: 2.68 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1401491067019047		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 1.1401491067019047 | validation: 0.965761776928753]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1472521053462514		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 1.1472521053462514 | validation: 0.9558187478906657]
	TIME [epoch: 2.68 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1404272172011258		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 1.1404272172011258 | validation: 1.0172147788328638]
	TIME [epoch: 2.68 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1485007085780796		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 1.1485007085780796 | validation: 0.9582783735097813]
	TIME [epoch: 2.69 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542954924257864		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 1.1542954924257864 | validation: 1.0088780043141274]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.156804345163546		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 1.156804345163546 | validation: 0.9494591888898937]
	TIME [epoch: 2.69 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1485483194493673		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 1.1485483194493673 | validation: 1.0020213168627592]
	TIME [epoch: 2.68 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.149210310179503		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 1.149210310179503 | validation: 0.9510212735604932]
	TIME [epoch: 2.68 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439727887177256		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 1.1439727887177256 | validation: 0.9696396429864228]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1465578958309532		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 1.1465578958309532 | validation: 0.9488118917893869]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1443935236741927		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 1.1443935236741927 | validation: 1.0124633148390634]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.148875208120371		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 1.148875208120371 | validation: 0.9448126231243492]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1534707590877165		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 1.1534707590877165 | validation: 1.0122950425515505]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1587964734587928		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 1.1587964734587928 | validation: 0.9528143011040705]
	TIME [epoch: 2.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508497514757032		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 1.1508497514757032 | validation: 1.0073995027832519]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.155229434703589		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 1.155229434703589 | validation: 0.9538424750271539]
	TIME [epoch: 2.68 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14248754370597		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 1.14248754370597 | validation: 0.9743302336859956]
	TIME [epoch: 2.68 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1415580332164665		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 1.1415580332164665 | validation: 0.9480416043558194]
	TIME [epoch: 2.68 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137569493882789		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 1.137569493882789 | validation: 0.9545879736539974]
	TIME [epoch: 2.68 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1365946132809313		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 1.1365946132809313 | validation: 0.957613510944658]
	TIME [epoch: 2.71 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1405246567072973		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 1.1405246567072973 | validation: 0.9685753830687972]
	TIME [epoch: 2.68 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1408932720716374		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 1.1408932720716374 | validation: 0.9520401169221592]
	TIME [epoch: 2.68 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1460250700471386		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 1.1460250700471386 | validation: 1.0042280009188775]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151834609196495		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 1.151834609196495 | validation: 0.9412117948991807]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1750512895251577		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 1.1750512895251577 | validation: 1.0346306115004358]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.164797853144507		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 1.164797853144507 | validation: 0.941671731886375]
	TIME [epoch: 2.68 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1475530303686536		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 1.1475530303686536 | validation: 0.955241602214628]
	TIME [epoch: 2.68 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139091027062788		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 1.139091027062788 | validation: 0.9620591576810891]
	TIME [epoch: 2.68 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14043025266624		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 1.14043025266624 | validation: 0.9550610238462123]
	TIME [epoch: 2.68 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1429793465667848		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 1.1429793465667848 | validation: 0.9545775218750934]
	TIME [epoch: 2.68 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130052366393814		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 1.130052366393814 | validation: 0.98016749028842]
	TIME [epoch: 2.68 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1357473814504595		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 1.1357473814504595 | validation: 0.9551660607526018]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.137267767287533		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 1.137267767287533 | validation: 1.0118356416614354]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1488237654263205		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 1.1488237654263205 | validation: 0.9557191979622826]
	TIME [epoch: 2.68 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.176484054328643		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 1.176484054328643 | validation: 1.0256261013983963]
	TIME [epoch: 2.68 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1571621811080595		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 1.1571621811080595 | validation: 0.9416641519140779]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321551215226027		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 1.1321551215226027 | validation: 0.9587071666165295]
	TIME [epoch: 2.68 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1401292138560515		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 1.1401292138560515 | validation: 0.9882746323871393]
	TIME [epoch: 2.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439321043184816		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 1.1439321043184816 | validation: 0.9459716700861662]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1361712317041823		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 1.1361712317041823 | validation: 0.9715741998505898]
	TIME [epoch: 2.68 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1298899474210655		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 1.1298899474210655 | validation: 0.927066567947181]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1350587371131717		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 1.1350587371131717 | validation: 0.9781427190228517]
	TIME [epoch: 2.67 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474943981590848		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 1.1474943981590848 | validation: 0.9448798004214933]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417658744578016		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 1.1417658744578016 | validation: 1.0147734091091938]
	TIME [epoch: 2.67 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1450860463804196		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 1.1450860463804196 | validation: 0.9305942196329092]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1458089180286875		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 1.1458089180286875 | validation: 1.0079012613230256]
	TIME [epoch: 2.68 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453665474172803		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 1.1453665474172803 | validation: 0.944237681907384]
	TIME [epoch: 2.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.146500256847652		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 1.146500256847652 | validation: 0.9746583587238089]
	TIME [epoch: 2.68 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1403714088079764		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 1.1403714088079764 | validation: 0.9447435244987137]
	TIME [epoch: 2.68 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1369079678474705		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 1.1369079678474705 | validation: 0.992465389546635]
	TIME [epoch: 2.68 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142425458030068		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 1.142425458030068 | validation: 0.9446444778689408]
	TIME [epoch: 2.68 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1505933811947515		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 1.1505933811947515 | validation: 1.0089899504560589]
	TIME [epoch: 2.68 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15135046974987		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 1.15135046974987 | validation: 0.9488608129213113]
	TIME [epoch: 2.68 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1317683440532518		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 1.1317683440532518 | validation: 0.9672602479192698]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370325546388103		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 1.1370325546388103 | validation: 0.9661508130962319]
	TIME [epoch: 2.68 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130832796627121		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 1.130832796627121 | validation: 0.9474026858281619]
	TIME [epoch: 2.68 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1339695747079919		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 1.1339695747079919 | validation: 0.9727189395883841]
	TIME [epoch: 2.69 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1437403718190273		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 1.1437403718190273 | validation: 0.9386418548160397]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.145513498281219		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 1.145513498281219 | validation: 1.0188153696963766]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567923945286478		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 1.1567923945286478 | validation: 0.9484964510153415]
	TIME [epoch: 2.68 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1491417280598748		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 1.1491417280598748 | validation: 0.9791250663030882]
	TIME [epoch: 2.68 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1375910204152977		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 1.1375910204152977 | validation: 0.96448951116898]
	TIME [epoch: 2.68 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1335158201538496		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 1.1335158201538496 | validation: 0.9478073679852632]
	TIME [epoch: 2.68 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1416064904550218		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 1.1416064904550218 | validation: 0.970393342069459]
	TIME [epoch: 2.68 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134387721557707		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 1.134387721557707 | validation: 0.930768800856043]
	TIME [epoch: 2.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1380643053817898		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 1.1380643053817898 | validation: 0.9929330626963296]
	TIME [epoch: 2.68 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1401343846924599		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 1.1401343846924599 | validation: 0.9332926209194975]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1525747180644736		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 1.1525747180644736 | validation: 0.9913688503894674]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1517264910345366		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 1.1517264910345366 | validation: 0.932476083115488]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140201176307363		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 1.140201176307363 | validation: 0.9558226795878172]
	TIME [epoch: 2.68 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1322017879723532		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 1.1322017879723532 | validation: 0.9484591237811213]
	TIME [epoch: 2.68 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1376006139811314		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 1.1376006139811314 | validation: 0.9616061603509394]
	TIME [epoch: 2.68 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1329629550518099		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 1.1329629550518099 | validation: 0.9343940015366979]
	TIME [epoch: 2.68 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1390985064667742		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 1.1390985064667742 | validation: 0.9765268446656062]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139521021814085		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 1.139521021814085 | validation: 0.925889689981776]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1439595497377049		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 1.1439595497377049 | validation: 1.0035435978837162]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1527663347099986		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 1.1527663347099986 | validation: 0.9411313332089135]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.154988395305147		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 1.154988395305147 | validation: 0.9909582080863623]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1357986566104177		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 1.1357986566104177 | validation: 0.9563860603455261]
	TIME [epoch: 2.67 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1351595571874342		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 1.1351595571874342 | validation: 0.9480021221804424]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1331694349345494		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 1.1331694349345494 | validation: 0.9540597967926856]
	TIME [epoch: 2.68 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130079386904819		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 1.130079386904819 | validation: 0.9421861388246292]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1355601775658415		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 1.1355601775658415 | validation: 0.9500966965529607]
	TIME [epoch: 2.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1291127380157202		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 1.1291127380157202 | validation: 0.9339064197996969]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.138601810990293		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 1.138601810990293 | validation: 0.9498118812849864]
	TIME [epoch: 2.68 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13979651952834		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 1.13979651952834 | validation: 0.9248059037580948]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1382235969352874		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 1.1382235969352874 | validation: 0.9729622958935544]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498730001248516		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 1.1498730001248516 | validation: 0.9093119534812614]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.158092015152665		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 1.158092015152665 | validation: 1.0266980942855994]
	TIME [epoch: 2.67 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.157348717138475		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 1.157348717138475 | validation: 0.9320808262102624]
	TIME [epoch: 2.67 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1435118250257952		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 1.1435118250257952 | validation: 0.9542533358223045]
	TIME [epoch: 2.68 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1358759583948648		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 1.1358759583948648 | validation: 0.9461852454749313]
	TIME [epoch: 2.67 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336990778531328		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 1.1336990778531328 | validation: 0.9306634718032278]
	TIME [epoch: 2.68 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1357337032322246		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 1.1357337032322246 | validation: 0.9310854588280194]
	TIME [epoch: 2.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1286306037298823		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 1.1286306037298823 | validation: 0.9328748326577383]
	TIME [epoch: 2.68 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1329963401143512		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 1.1329963401143512 | validation: 0.9395783645335]
	TIME [epoch: 2.68 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132390618685173		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 1.132390618685173 | validation: 0.9514375359037524]
	TIME [epoch: 2.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324856974107762		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 1.1324856974107762 | validation: 0.9210791023833995]
	TIME [epoch: 2.68 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.139031992655705		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 1.139031992655705 | validation: 0.9780295044875807]
	TIME [epoch: 2.68 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1498294033089755		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 1.1498294033089755 | validation: 0.924472574917693]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1574690372414251		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 1.1574690372414251 | validation: 1.002193350506266]
	TIME [epoch: 2.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1423883057434667		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 1.1423883057434667 | validation: 0.9206595413580573]
	TIME [epoch: 2.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1256686196228747		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 1.1256686196228747 | validation: 0.9297423447008072]
	TIME [epoch: 2.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1295092551865524		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 1.1295092551865524 | validation: 0.9591947566878769]
	TIME [epoch: 2.67 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345093885461606		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 1.1345093885461606 | validation: 0.9317617223425061]
	TIME [epoch: 2.67 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1285407737524136		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 1.1285407737524136 | validation: 0.9353830911403993]
	TIME [epoch: 2.67 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1291137238558195		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 1.1291137238558195 | validation: 0.9299952598066913]
	TIME [epoch: 2.67 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1324323356414505		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 1.1324323356414505 | validation: 0.9376102193883217]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131584442622821		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 1.131584442622821 | validation: 0.9396676273281491]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1292816810973656		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 1.1292816810973656 | validation: 0.9220446766327637]
	TIME [epoch: 2.67 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1282776004041686		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 1.1282776004041686 | validation: 0.9694400130399456]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13866507565459		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 1.13866507565459 | validation: 0.9250553524926262]
	TIME [epoch: 2.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1543589847623665		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 1.1543589847623665 | validation: 1.0015262991109608]
	TIME [epoch: 2.68 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.152691017099948		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 1.152691017099948 | validation: 0.9410509403780388]
	TIME [epoch: 2.68 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370797012850147		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 1.1370797012850147 | validation: 0.9323115021058798]
	TIME [epoch: 2.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1308835536576518		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 1.1308835536576518 | validation: 0.9544601301231226]
	TIME [epoch: 2.67 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13099536613253		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 1.13099536613253 | validation: 0.9277758472132881]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1320695484462542		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 1.1320695484462542 | validation: 0.9663165921952622]
	TIME [epoch: 2.67 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1380881500293925		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 1.1380881500293925 | validation: 0.9220316215483904]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1346313980708498		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 1.1346313980708498 | validation: 0.9547696978454248]
	TIME [epoch: 2.67 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132804869142064		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 1.132804869142064 | validation: 0.9328015368508868]
	TIME [epoch: 2.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1386724816833902		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 1.1386724816833902 | validation: 0.9647659482971408]
	TIME [epoch: 2.67 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1267663885880348		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 1.1267663885880348 | validation: 0.9264075921440876]
	TIME [epoch: 2.67 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1314579103585658		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 1.1314579103585658 | validation: 0.9351616746734716]
	TIME [epoch: 2.68 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1272214149914697		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 1.1272214149914697 | validation: 0.9454379355602044]
	TIME [epoch: 2.67 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1276960084069572		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 1.1276960084069572 | validation: 0.9461316661985273]
	TIME [epoch: 2.67 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130896063690049		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 1.130896063690049 | validation: 0.94673917177319]
	TIME [epoch: 2.67 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321809488384005		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 1.1321809488384005 | validation: 0.9367140076558318]
	TIME [epoch: 2.68 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135171490555797		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 1.135171490555797 | validation: 0.9958615074955456]
	TIME [epoch: 2.67 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453098615985264		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 1.1453098615985264 | validation: 0.921972164156718]
	TIME [epoch: 2.68 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1668836504172382		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 1.1668836504172382 | validation: 0.9482428305456904]
	TIME [epoch: 2.67 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.14250829514132		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 1.14250829514132 | validation: 0.9419699902563208]
	TIME [epoch: 2.67 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1343907124353931		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 1.1343907124353931 | validation: 0.9190193527209658]
	TIME [epoch: 2.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306153957793605		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 1.1306153957793605 | validation: 0.9565179763570275]
	TIME [epoch: 2.67 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253219197563302		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 1.1253219197563302 | validation: 0.9269379148946275]
	TIME [epoch: 2.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130626482295717		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 1.130626482295717 | validation: 0.9584673231649843]
	TIME [epoch: 2.67 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1238866813037276		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 1.1238866813037276 | validation: 0.9148059826646167]
	TIME [epoch: 2.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247746330056827		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 1.1247746330056827 | validation: 0.950616206003922]
	TIME [epoch: 2.67 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127572170808566		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 1.127572170808566 | validation: 0.9310002638224507]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306978231165195		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 1.1306978231165195 | validation: 0.9503753392182699]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1345799650790043		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 1.1345799650790043 | validation: 0.9194753029973675]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370676775664021		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 1.1370676775664021 | validation: 0.9541562233257598]
	TIME [epoch: 2.67 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1254163219673046		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 1.1254163219673046 | validation: 0.9175140622160957]
	TIME [epoch: 2.67 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.131183533731745		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 1.131183533731745 | validation: 0.939944914485867]
	TIME [epoch: 2.67 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1368039356992972		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 1.1368039356992972 | validation: 0.9173415734539226]
	TIME [epoch: 2.67 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1321980856907816		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 1.1321980856907816 | validation: 0.9861891234444446]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1365399850545028		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 1.1365399850545028 | validation: 0.9137137968454216]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417718830596755		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 1.1417718830596755 | validation: 0.9498970998993355]
	TIME [epoch: 2.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1328241338574778		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 1.1328241338574778 | validation: 0.9289681720365821]
	TIME [epoch: 2.67 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126957403742583		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 1.126957403742583 | validation: 0.9439574571148862]
	TIME [epoch: 2.68 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303328574573275		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 1.1303328574573275 | validation: 0.9244157736767296]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1327197605446484		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 1.1327197605446484 | validation: 0.9239657011715592]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1341427088105862		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 1.1341427088105862 | validation: 0.9127476876268658]
	TIME [epoch: 2.67 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127583870395269		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 1.127583870395269 | validation: 0.9385060614029254]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1288386290806904		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 1.1288386290806904 | validation: 0.9229877832193081]
	TIME [epoch: 2.67 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302730913110082		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 1.1302730913110082 | validation: 0.9384790640320789]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221207667676776		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 1.1221207667676776 | validation: 0.9086179509706401]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391929238724794		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 1.1391929238724794 | validation: 0.9749199028158065]
	TIME [epoch: 2.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1453593263309814		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 1.1453593263309814 | validation: 0.9112180338868865]
	TIME [epoch: 2.69 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1422498345754775		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 1.1422498345754775 | validation: 0.9431416801000412]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1292158997836876		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 1.1292158997836876 | validation: 0.9354164813509209]
	TIME [epoch: 2.68 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1326456805304101		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 1.1326456805304101 | validation: 0.921295505090141]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127311170335837		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 1.127311170335837 | validation: 0.9594086284617118]
	TIME [epoch: 2.67 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1307150551649505		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 1.1307150551649505 | validation: 0.9069699464517792]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1377162782053458		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 1.1377162782053458 | validation: 0.9315758551584381]
	TIME [epoch: 2.68 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1304163760482788		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 1.1304163760482788 | validation: 0.9092530721908059]
	TIME [epoch: 2.69 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1314852086228853		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 1.1314852086228853 | validation: 0.9294294866811733]
	TIME [epoch: 2.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271345670567383		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 1.1271345670567383 | validation: 0.9144623346156877]
	TIME [epoch: 2.69 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120664344378201		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 1.120664344378201 | validation: 0.927855728483999]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234860925348127		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 1.1234860925348127 | validation: 0.9438532281536297]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1270143925542153		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 1.1270143925542153 | validation: 0.909045388976155]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1313589764128187		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 1.1313589764128187 | validation: 0.9703441345691073]
	TIME [epoch: 2.68 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1301834278744296		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 1.1301834278744296 | validation: 0.9220106037292415]
	TIME [epoch: 2.68 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271812087615498		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 1.1271812087615498 | validation: 0.9475308309773816]
	TIME [epoch: 2.69 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1228114288100755		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 1.1228114288100755 | validation: 0.9164633422404137]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1283232111744013		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 1.1283232111744013 | validation: 0.9291559749655356]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1213828656396323		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 1.1213828656396323 | validation: 0.9270370063823393]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1214082619687702		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 1.1214082619687702 | validation: 0.9347084751877741]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1242656113539153		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 1.1242656113539153 | validation: 0.9042797406860261]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391255022247244		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 1.1391255022247244 | validation: 0.9609140818690011]
	TIME [epoch: 2.67 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1378978635181565		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 1.1378978635181565 | validation: 0.9028586298174445]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1394550267253722		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 1.1394550267253722 | validation: 0.9496988000710908]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1314331327139702		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 1.1314331327139702 | validation: 0.9182814446624179]
	TIME [epoch: 2.69 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1295950463442377		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 1.1295950463442377 | validation: 0.921548523406049]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1301834367348809		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 1.1301834367348809 | validation: 0.935672742765442]
	TIME [epoch: 2.69 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.122478485837006		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 1.122478485837006 | validation: 0.9210618605944815]
	TIME [epoch: 2.69 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1292426581376558		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 1.1292426581376558 | validation: 0.9314244149415227]
	TIME [epoch: 2.69 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306655240705867		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 1.1306655240705867 | validation: 0.9221795402311932]
	TIME [epoch: 2.69 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1239094734971111		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 1.1239094734971111 | validation: 0.9261036465155521]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1231314729972912		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 1.1231314729972912 | validation: 0.916796459015584]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1262366187114738		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 1.1262366187114738 | validation: 0.916535210113357]
	TIME [epoch: 2.69 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121873321527816		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 1.121873321527816 | validation: 0.9334613031081269]
	TIME [epoch: 2.69 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.128950618355929		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 1.128950618355929 | validation: 0.8992922180467421]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1341371601929293		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 1.1341371601929293 | validation: 0.9822179931921629]
	TIME [epoch: 2.69 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1451004344359599		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 1.1451004344359599 | validation: 0.8966130187414358]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_795.pth
	Model improved!!!
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1438669536352872		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 1.1438669536352872 | validation: 0.9214817724190841]
	TIME [epoch: 2.69 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.132071924853899		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 1.132071924853899 | validation: 0.9254965550232308]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1253136002542705		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 1.1253136002542705 | validation: 0.8975892923582454]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1299855948934496		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 1.1299855948934496 | validation: 0.9165681625800902]
	TIME [epoch: 2.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.119668703003783		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 1.119668703003783 | validation: 0.933217666250807]
	TIME [epoch: 2.69 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1178122972029034		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 1.1178122972029034 | validation: 0.9206204832495768]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234640956336057		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 1.1234640956336057 | validation: 0.9176064493318106]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125857311275551		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 1.125857311275551 | validation: 0.9341559163073039]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1301447152693334		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 1.1301447152693334 | validation: 0.8982493268146613]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135688267516199		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 1.135688267516199 | validation: 0.9710696584376717]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136107764823338		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 1.136107764823338 | validation: 0.9053611052498015]
	TIME [epoch: 2.68 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1255575273208211		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 1.1255575273208211 | validation: 0.9149290255711255]
	TIME [epoch: 2.69 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1210516948583047		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 1.1210516948583047 | validation: 0.9314729610796727]
	TIME [epoch: 2.68 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1273142830498974		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 1.1273142830498974 | validation: 0.9152174979463313]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124686039461411		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 1.124686039461411 | validation: 0.9457496959883127]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1258266680652207		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 1.1258266680652207 | validation: 0.9060526543832866]
	TIME [epoch: 2.68 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1326179987587788		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 1.1326179987587788 | validation: 0.9271661441329497]
	TIME [epoch: 2.68 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118439382440833		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 1.118439382440833 | validation: 0.9127784265581154]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1213521521546843		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 1.1213521521546843 | validation: 0.9238133169295755]
	TIME [epoch: 2.69 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12202691788477		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 1.12202691788477 | validation: 0.907034574168256]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1281179869709599		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 1.1281179869709599 | validation: 0.9520150356208007]
	TIME [epoch: 2.69 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1262833315227128		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 1.1262833315227128 | validation: 0.8996312900112602]
	TIME [epoch: 2.69 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1232022702826883		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 1.1232022702826883 | validation: 0.9457137456698252]
	TIME [epoch: 2.69 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1266746879214338		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 1.1266746879214338 | validation: 0.9021905032616456]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1280397606104584		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 1.1280397606104584 | validation: 0.9402778870985637]
	TIME [epoch: 2.68 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234985776933053		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 1.1234985776933053 | validation: 0.9161108448663516]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1147319093325405		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 1.1147319093325405 | validation: 0.9132764552610665]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1258601548161966		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 1.1258601548161966 | validation: 0.9047896964497213]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199429413293371		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 1.1199429413293371 | validation: 0.9212689939535896]
	TIME [epoch: 2.69 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1270949909084034		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 1.1270949909084034 | validation: 0.9074049275006163]
	TIME [epoch: 2.68 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1223793296531013		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 1.1223793296531013 | validation: 0.941945257829951]
	TIME [epoch: 2.68 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1245403167587733		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 1.1245403167587733 | validation: 0.8990307933811161]
	TIME [epoch: 2.68 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209658841709693		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 1.1209658841709693 | validation: 0.941047927635482]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1238490259041363		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 1.1238490259041363 | validation: 0.8986539557518918]
	TIME [epoch: 2.68 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1250186588120092		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 1.1250186588120092 | validation: 0.9442779653141455]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1206306746713908		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 1.1206306746713908 | validation: 0.9216493296057284]
	TIME [epoch: 2.68 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185055406524975		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 1.1185055406524975 | validation: 0.8973826089509913]
	TIME [epoch: 2.69 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209602088948214		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 1.1209602088948214 | validation: 0.9394775711783958]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200120428559204		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 1.1200120428559204 | validation: 0.9127073066265862]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1202207362548253		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 1.1202207362548253 | validation: 0.9375436814271204]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1300432981087485		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 1.1300432981087485 | validation: 0.917629673410751]
	TIME [epoch: 2.69 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1233665925633647		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 1.1233665925633647 | validation: 0.9246048287714492]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1229777674598935		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 1.1229777674598935 | validation: 0.9183629748807199]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181506034937059		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 1.1181506034937059 | validation: 0.9217010018042793]
	TIME [epoch: 2.68 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1219601760001854		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 1.1219601760001854 | validation: 0.9351194034848398]
	TIME [epoch: 2.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126632528605536		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 1.126632528605536 | validation: 0.9005278480844396]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158075164991892		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 1.1158075164991892 | validation: 0.9385071689889316]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13056516966792		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 1.13056516966792 | validation: 0.896668422454953]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1297298855738314		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 1.1297298855738314 | validation: 0.9488878133599894]
	TIME [epoch: 2.68 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221815306347538		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 1.1221815306347538 | validation: 0.9011045697252368]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1201569065122023		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 1.1201569065122023 | validation: 0.9216864745251055]
	TIME [epoch: 2.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1190038755537182		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 1.1190038755537182 | validation: 0.9084812069177168]
	TIME [epoch: 2.68 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121944614650682		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 1.121944614650682 | validation: 0.9164748413975098]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1192756563271722		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 1.1192756563271722 | validation: 0.8994683246143652]
	TIME [epoch: 2.68 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221866922961292		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 1.1221866922961292 | validation: 0.9219127031843812]
	TIME [epoch: 2.68 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123017427159282		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 1.123017427159282 | validation: 0.9352435212280391]
	TIME [epoch: 2.69 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185809733578556		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 1.1185809733578556 | validation: 0.9126756835738136]
	TIME [epoch: 2.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1168478771415582		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 1.1168478771415582 | validation: 0.9297607272235372]
	TIME [epoch: 2.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1208271764725073		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 1.1208271764725073 | validation: 0.899861069962304]
	TIME [epoch: 2.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1362603386793069		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 1.1362603386793069 | validation: 0.9481313010857841]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1267537069318647		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 1.1267537069318647 | validation: 0.9118279575369823]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181462810363325		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 1.1181462810363325 | validation: 0.9188114997072333]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123261275776035		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 1.123261275776035 | validation: 0.9069349518233729]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1190976786097593		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 1.1190976786097593 | validation: 0.8973727981863867]
	TIME [epoch: 2.69 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142772957493867		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 1.1142772957493867 | validation: 0.9114511607978454]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185303920931167		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 1.1185303920931167 | validation: 0.9041490088525287]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1234856985920503		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 1.1234856985920503 | validation: 0.9116640908840332]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1222718397250455		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 1.1222718397250455 | validation: 0.8978838748115472]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121257563424233		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 1.121257563424233 | validation: 0.9372800650813018]
	TIME [epoch: 2.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120926894006671		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 1.120926894006671 | validation: 0.893778465276156]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_865.pth
	Model improved!!!
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200841697646864		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 1.1200841697646864 | validation: 0.9200106372110753]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1222795879009184		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 1.1222795879009184 | validation: 0.9024312962168297]
	TIME [epoch: 2.69 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181438663599474		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 1.1181438663599474 | validation: 0.9299548372077141]
	TIME [epoch: 2.69 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1180949452263638		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 1.1180949452263638 | validation: 0.9029444556723366]
	TIME [epoch: 2.69 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120562486580278		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 1.120562486580278 | validation: 0.9268500531536348]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1240831250682763		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 1.1240831250682763 | validation: 0.9127222797169623]
	TIME [epoch: 2.69 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117956441068858		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 1.117956441068858 | validation: 0.9055417246388476]
	TIME [epoch: 2.69 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1138949346068345		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 1.1138949346068345 | validation: 0.89714509226001]
	TIME [epoch: 2.69 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1194930989275822		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 1.1194930989275822 | validation: 0.9066569145053684]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209111686875233		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 1.1209111686875233 | validation: 0.9372324364911212]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123276970067139		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 1.123276970067139 | validation: 0.9114410800290069]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1202730179366007		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 1.1202730179366007 | validation: 0.9457015113217772]
	TIME [epoch: 2.69 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1239781744059771		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 1.1239781744059771 | validation: 0.8974476344005113]
	TIME [epoch: 2.68 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1204411941337704		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 1.1204411941337704 | validation: 0.9101640733102869]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1192270996900502		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 1.1192270996900502 | validation: 0.9269802163741236]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150402794501917		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 1.1150402794501917 | validation: 0.9175384395520851]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182987188090652		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 1.1182987188090652 | validation: 0.910053675802025]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1128551444198098		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 1.1128551444198098 | validation: 0.8994982411718818]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161478139478267		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 1.1161478139478267 | validation: 0.9056393296878711]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161858081692486		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 1.1161858081692486 | validation: 0.9133487217963024]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1177214853560395		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 1.1177214853560395 | validation: 0.8972741454323248]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1154832510813781		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 1.1154832510813781 | validation: 0.9264529666021224]
	TIME [epoch: 2.69 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1200509642707022		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 1.1200509642707022 | validation: 0.9040766614661564]
	TIME [epoch: 2.69 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1274567331304826		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 1.1274567331304826 | validation: 0.9480384306250542]
	TIME [epoch: 2.69 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1220787733033033		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 1.1220787733033033 | validation: 0.9127361212820972]
	TIME [epoch: 2.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1205300948748436		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 1.1205300948748436 | validation: 0.9092975827702324]
	TIME [epoch: 2.71 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1174650158131971		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 1.1174650158131971 | validation: 0.9181587461641197]
	TIME [epoch: 2.69 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1125126676737411		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 1.1125126676737411 | validation: 0.8886837440778464]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1165491107847403		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 1.1165491107847403 | validation: 0.9226544075989971]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182791011123445		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 1.1182791011123445 | validation: 0.9000811912737753]
	TIME [epoch: 2.69 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181146342374257		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 1.1181146342374257 | validation: 0.8917352234947509]
	TIME [epoch: 2.68 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1188529549282749		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 1.1188529549282749 | validation: 0.9203016770587228]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1182005879993804		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 1.1182005879993804 | validation: 0.8895007596735284]
	TIME [epoch: 2.69 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160479058164527		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 1.1160479058164527 | validation: 0.9357868717989644]
	TIME [epoch: 2.68 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209533339109603		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 1.1209533339109603 | validation: 0.8905198744561255]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160464112727468		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 1.1160464112727468 | validation: 0.9198009048195916]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113378139443661		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 1.113378139443661 | validation: 0.9251425370854293]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.116729700973907		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 1.116729700973907 | validation: 0.900077637805714]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1105555728975065		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 1.1105555728975065 | validation: 0.9333700748278967]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127243745003488		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 1.1127243745003488 | validation: 0.8998924833381425]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1221193918218264		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 1.1221193918218264 | validation: 0.9231650516724463]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171000266750661		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 1.1171000266750661 | validation: 0.9101083499746023]
	TIME [epoch: 2.69 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183808808639315		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 1.1183808808639315 | validation: 0.9018972241429921]
	TIME [epoch: 2.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113444334725785		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 1.113444334725785 | validation: 0.8969798113466283]
	TIME [epoch: 2.69 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173666288350865		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 1.1173666288350865 | validation: 0.918617379910225]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1187261052622808		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 1.1187261052622808 | validation: 0.8882267819538608]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_911.pth
	Model improved!!!
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1164259770076075		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 1.1164259770076075 | validation: 0.9251947332336296]
	TIME [epoch: 2.68 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1083252979250362		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 1.1083252979250362 | validation: 0.9052981211730764]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.118638086644419		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 1.118638086644419 | validation: 0.9292885046329605]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167226714000107		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 1.1167226714000107 | validation: 0.8966646792692846]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160958346664729		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 1.1160958346664729 | validation: 0.8932104302016063]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1173508281329547		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 1.1173508281329547 | validation: 0.887875920964545]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_917.pth
	Model improved!!!
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160965309223811		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 1.1160965309223811 | validation: 0.9044961806343033]
	TIME [epoch: 2.67 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1110900365893233		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 1.1110900365893233 | validation: 0.8956488302065614]
	TIME [epoch: 2.67 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150124067320903		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 1.1150124067320903 | validation: 0.9012233918111465]
	TIME [epoch: 2.67 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156574623910818		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 1.1156574623910818 | validation: 0.9237171535932213]
	TIME [epoch: 2.67 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1144723935605496		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 1.1144723935605496 | validation: 0.8893130163547976]
	TIME [epoch: 2.67 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.123792706779055		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 1.123792706779055 | validation: 0.936828178742659]
	TIME [epoch: 2.67 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117797043793879		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 1.117797043793879 | validation: 0.9055000579897379]
	TIME [epoch: 2.67 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1123432204556032		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 1.1123432204556032 | validation: 0.8985292917270056]
	TIME [epoch: 2.67 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1132113717764391		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 1.1132113717764391 | validation: 0.9147626233771513]
	TIME [epoch: 2.67 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167911414633356		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 1.1167911414633356 | validation: 0.8998292942842897]
	TIME [epoch: 2.69 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158118229014666		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 1.1158118229014666 | validation: 0.916537699639426]
	TIME [epoch: 2.67 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1183727139232107		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 1.1183727139232107 | validation: 0.9027294179614391]
	TIME [epoch: 2.69 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115210829643247		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 1.1115210829643247 | validation: 0.8961473457466248]
	TIME [epoch: 2.67 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1199799014705687		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 1.1199799014705687 | validation: 0.8887081013294282]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1164231941559601		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 1.1164231941559601 | validation: 0.9162896190128601]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196273432468078		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 1.1196273432468078 | validation: 0.8851468909364066]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114065218427064		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 1.114065218427064 | validation: 0.9220840895154142]
	TIME [epoch: 2.67 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1156546592587793		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 1.1156546592587793 | validation: 0.8911564022496246]
	TIME [epoch: 2.67 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1094549570814194		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 1.1094549570814194 | validation: 0.8986179921773996]
	TIME [epoch: 2.66 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101381775194152		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 1.1101381775194152 | validation: 0.9189330400025626]
	TIME [epoch: 2.67 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1175378933536009		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 1.1175378933536009 | validation: 0.8889800473563707]
	TIME [epoch: 2.67 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185962040438784		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 1.1185962040438784 | validation: 0.910208396574687]
	TIME [epoch: 2.67 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1112440732414042		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 1.1112440732414042 | validation: 0.900760437526563]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101555267837566		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 1.1101555267837566 | validation: 0.9150019855374204]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1169494856287083		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 1.1169494856287083 | validation: 0.8869671316745364]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1135296442565623		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 1.1135296442565623 | validation: 0.9112326342079076]
	TIME [epoch: 2.67 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1130438621740188		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 1.1130438621740188 | validation: 0.8967712138104189]
	TIME [epoch: 2.67 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127323718452236		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 1.1127323718452236 | validation: 0.8943707131466605]
	TIME [epoch: 2.67 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171408598422088		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 1.1171408598422088 | validation: 0.8939260606827004]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108190880218094		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 1.1108190880218094 | validation: 0.9013926269952715]
	TIME [epoch: 2.66 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185125658541657		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 1.1185125658541657 | validation: 0.8953807741756692]
	TIME [epoch: 2.67 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1130791641753894		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 1.1130791641753894 | validation: 0.9100992653791904]
	TIME [epoch: 2.67 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.106766366497359		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 1.106766366497359 | validation: 0.8919199348474773]
	TIME [epoch: 2.67 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111765707661643		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 1.111765707661643 | validation: 0.9042865578708481]
	TIME [epoch: 2.67 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1114109639816643		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 1.1114109639816643 | validation: 0.8807237578811151]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137445178131844		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 1.1137445178131844 | validation: 0.8997934459395988]
	TIME [epoch: 2.68 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1104854068958248		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 1.1104854068958248 | validation: 0.8749725878967471]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1170353885117914		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 1.1170353885117914 | validation: 0.9220817667356576]
	TIME [epoch: 2.67 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167521812440626		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 1.1167521812440626 | validation: 0.8905736691516518]
	TIME [epoch: 2.67 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196203664113327		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 1.1196203664113327 | validation: 0.9074411901313693]
	TIME [epoch: 2.67 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1116356944665602		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 1.1116356944665602 | validation: 0.9059321975151567]
	TIME [epoch: 2.67 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113578468410163		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 1.113578468410163 | validation: 0.9099510396382172]
	TIME [epoch: 2.67 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107266217851319		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 1.107266217851319 | validation: 0.8998344807245833]
	TIME [epoch: 2.69 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1127118667299067		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 1.1127118667299067 | validation: 0.8896248736933429]
	TIME [epoch: 2.68 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1123309517591164		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 1.1123309517591164 | validation: 0.9022767563214847]
	TIME [epoch: 2.69 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1167629680631566		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 1.1167629680631566 | validation: 0.8856226075841467]
	TIME [epoch: 2.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1113214479496452		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 1.1113214479496452 | validation: 0.9105397252917031]
	TIME [epoch: 2.69 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108391000826225		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 1.1108391000826225 | validation: 0.8844769577634951]
	TIME [epoch: 2.68 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075460908215846		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 1.1075460908215846 | validation: 0.9083218076295264]
	TIME [epoch: 2.68 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1094354223535563		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 1.1094354223535563 | validation: 0.9042343485099779]
	TIME [epoch: 2.68 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1137496110679033		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 1.1137496110679033 | validation: 0.9001687608850967]
	TIME [epoch: 2.68 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107649890533696		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 1.107649890533696 | validation: 0.8730348874646874]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1117360672211751		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 1.1117360672211751 | validation: 0.9304343761150303]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11757027808988		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 1.11757027808988 | validation: 0.8798412379393286]
	TIME [epoch: 2.67 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1143589234885694		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 1.1143589234885694 | validation: 0.9000775718390174]
	TIME [epoch: 2.67 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1119047967673483		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 1.1119047967673483 | validation: 0.9015619568831291]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1054890915238234		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 1.1054890915238234 | validation: 0.8951691408733773]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1017078060118164		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 1.1017078060118164 | validation: 0.9066695894502722]
	TIME [epoch: 2.67 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1084215522490253		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 1.1084215522490253 | validation: 0.8949417124862903]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1067760823856128		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 1.1067760823856128 | validation: 0.876340231309262]
	TIME [epoch: 2.67 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1114860156573656		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 1.1114860156573656 | validation: 0.9121891779261223]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078034913940094		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 1.1078034913940094 | validation: 0.8944168438058974]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052504500197244		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 1.1052504500197244 | validation: 0.8809131489686511]
	TIME [epoch: 2.69 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1105025646294835		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 1.1105025646294835 | validation: 0.9030659494796297]
	TIME [epoch: 2.69 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095223486504455		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 1.1095223486504455 | validation: 0.908931599837773]
	TIME [epoch: 2.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111620196468365		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 1.111620196468365 | validation: 0.9052950857340902]
	TIME [epoch: 2.69 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1134346516479163		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 1.1134346516479163 | validation: 0.9026921500835615]
	TIME [epoch: 2.68 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1151670219675396		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 1.1151670219675396 | validation: 0.882801455372147]
	TIME [epoch: 2.69 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1049314966108372		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 1.1049314966108372 | validation: 0.9171340729664328]
	TIME [epoch: 2.68 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1092317461170984		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 1.1092317461170984 | validation: 0.8837243938607611]
	TIME [epoch: 2.68 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1121019976345283		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 1.1121019976345283 | validation: 0.9151267202104135]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1089954047795803		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 1.1089954047795803 | validation: 0.8881741462676344]
	TIME [epoch: 2.68 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102112376611561		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 1.1102112376611561 | validation: 0.9082140680062776]
	TIME [epoch: 2.68 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105889129980715		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 1.105889129980715 | validation: 0.8979521167276427]
	TIME [epoch: 2.68 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1037816665703577		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 1.1037816665703577 | validation: 0.8868811840197162]
	TIME [epoch: 2.69 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1086106341522028		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 1.1086106341522028 | validation: 0.9167544515908852]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056855310655012		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 1.1056855310655012 | validation: 0.8884533946149621]
	TIME [epoch: 2.69 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1095837843976049		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 1.1095837843976049 | validation: 0.8930198153556589]
	TIME [epoch: 2.69 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100047302457522		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 1.100047302457522 | validation: 0.9054096119446493]
	TIME [epoch: 2.69 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1050819384436656		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 1.1050819384436656 | validation: 0.8737047629465494]
	TIME [epoch: 2.68 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1087148113311462		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 1.1087148113311462 | validation: 0.9153884894254237]
	TIME [epoch: 2.69 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1081608351674086		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 1.1081608351674086 | validation: 0.889137346519603]
	TIME [epoch: 2.69 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108510629030692		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 1.1108510629030692 | validation: 0.9150106883337692]
	TIME [epoch: 2.69 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1076398403395016		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 1.1076398403395016 | validation: 0.9002210075368803]
	TIME [epoch: 184 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.11208584302454		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 1.11208584302454 | validation: 0.8875919542154599]
	TIME [epoch: 5.75 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1000150538765754		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 1.1000150538765754 | validation: 0.9022074522583492]
	TIME [epoch: 5.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1136909049448134		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 1.1136909049448134 | validation: 0.8928965227640842]
	TIME [epoch: 5.74 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1063174780858487		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 1.1063174780858487 | validation: 0.900643379100428]
	TIME [epoch: 5.74 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1159511946350453		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 1.1159511946350453 | validation: 0.8670872992548484]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_1006.pth
	Model improved!!!
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1097782206708595		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 1.1097782206708595 | validation: 0.9017241766345911]
	TIME [epoch: 5.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1098026840860713		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 1.1098026840860713 | validation: 0.8893081201851312]
	TIME [epoch: 5.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.115862873507106		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 1.115862873507106 | validation: 0.9097377143514077]
	TIME [epoch: 5.74 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1106264931399323		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 1.1106264931399323 | validation: 0.9030042226747321]
	TIME [epoch: 5.73 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102478598007457		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 1.1102478598007457 | validation: 0.8898972004891772]
	TIME [epoch: 5.74 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1101979071283534		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 1.1101979071283534 | validation: 0.9120118705818652]
	TIME [epoch: 5.74 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1113078632756317		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 1.1113078632756317 | validation: 0.8949595001935354]
	TIME [epoch: 5.74 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108805885834075		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 1.108805885834075 | validation: 0.8910133193063349]
	TIME [epoch: 5.74 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1106641567032887		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 1.1106641567032887 | validation: 0.9178376081252957]
	TIME [epoch: 5.73 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1060552049241639		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 1.1060552049241639 | validation: 0.8899574722088097]
	TIME [epoch: 5.73 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1106326718249684		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 1.1106326718249684 | validation: 0.9021523973985451]
	TIME [epoch: 5.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052129830367592		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 1.1052129830367592 | validation: 0.889586786541047]
	TIME [epoch: 5.73 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0987542450443077		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 1.0987542450443077 | validation: 0.8893616630145331]
	TIME [epoch: 5.74 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.104397274575955		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 1.104397274575955 | validation: 0.9034259974072132]
	TIME [epoch: 5.73 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1034676837538873		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 1.1034676837538873 | validation: 0.8972532453311918]
	TIME [epoch: 5.74 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1073365004192965		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 1.1073365004192965 | validation: 0.8906291349804163]
	TIME [epoch: 5.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1010082860193513		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 1.1010082860193513 | validation: 0.9003224255241765]
	TIME [epoch: 5.73 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075052551950113		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 1.1075052551950113 | validation: 0.8859434573658316]
	TIME [epoch: 5.73 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1065590758359225		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 1.1065590758359225 | validation: 0.8854363900178381]
	TIME [epoch: 5.82 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1015547193331066		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 1.1015547193331066 | validation: 0.8932477406250992]
	TIME [epoch: 5.77 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103600944443136		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 1.103600944443136 | validation: 0.8888711327047908]
	TIME [epoch: 5.73 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1043901656016362		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 1.1043901656016362 | validation: 0.9205256209641682]
	TIME [epoch: 5.74 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1117882962063828		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 1.1117882962063828 | validation: 0.8783758486964747]
	TIME [epoch: 5.74 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1125045282476516		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 1.1125045282476516 | validation: 0.9119371542109396]
	TIME [epoch: 5.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108648709701024		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 1.108648709701024 | validation: 0.9092809241266814]
	TIME [epoch: 5.73 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103128980934867		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 1.103128980934867 | validation: 0.8839017139850779]
	TIME [epoch: 5.74 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1021828783264531		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 1.1021828783264531 | validation: 0.8951895670685887]
	TIME [epoch: 5.73 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1018026701867258		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 1.1018026701867258 | validation: 0.8767522520532446]
	TIME [epoch: 5.73 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1078711477799794		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 1.1078711477799794 | validation: 0.8936938814473486]
	TIME [epoch: 5.74 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.101227267141524		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 1.101227267141524 | validation: 0.8755496348927472]
	TIME [epoch: 5.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1071447783465787		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 1.1071447783465787 | validation: 0.9007148982365321]
	TIME [epoch: 5.73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108798682717768		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 1.108798682717768 | validation: 0.8893504504003581]
	TIME [epoch: 5.73 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064212805529403		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 1.1064212805529403 | validation: 0.8929548438719999]
	TIME [epoch: 5.73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1021367470684524		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 1.1021367470684524 | validation: 0.9041740179623702]
	TIME [epoch: 5.74 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1048829121242578		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 1.1048829121242578 | validation: 0.8770894845940526]
	TIME [epoch: 5.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052332406948304		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 1.1052332406948304 | validation: 0.9054861314447454]
	TIME [epoch: 5.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107596132978249		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 1.107596132978249 | validation: 0.8935645271901921]
	TIME [epoch: 5.74 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1051349084212132		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 1.1051349084212132 | validation: 0.8958144278131513]
	TIME [epoch: 5.73 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1050762931810085		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 1.1050762931810085 | validation: 0.9035562669489283]
	TIME [epoch: 5.74 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1077170449995888		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 1.1077170449995888 | validation: 0.8945259709356107]
	TIME [epoch: 5.74 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1097035896211747		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 1.1097035896211747 | validation: 0.8794575699740297]
	TIME [epoch: 5.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0999672974969419		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 1.0999672974969419 | validation: 0.8922147616127307]
	TIME [epoch: 5.73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0977681355946545		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 1.0977681355946545 | validation: 0.8977873664618223]
	TIME [epoch: 5.73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107281093412255		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 1.107281093412255 | validation: 0.8914052099700103]
	TIME [epoch: 5.73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102663764286968		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 1.102663764286968 | validation: 0.9183318750370486]
	TIME [epoch: 5.73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.105475593458829		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 1.105475593458829 | validation: 0.8972939051173658]
	TIME [epoch: 5.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031521953928383		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 1.1031521953928383 | validation: 0.8811463518318146]
	TIME [epoch: 5.73 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1040684413989281		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 1.1040684413989281 | validation: 0.8977753051945196]
	TIME [epoch: 5.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1056726289930356		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 1.1056726289930356 | validation: 0.8877653043640312]
	TIME [epoch: 5.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0958689282362135		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 1.0958689282362135 | validation: 0.896765908199654]
	TIME [epoch: 5.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.099202636315157		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 1.099202636315157 | validation: 0.9056631793411913]
	TIME [epoch: 5.75 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102611759032		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 1.102611759032 | validation: 0.8969108847288155]
	TIME [epoch: 5.75 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052054297990297		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 1.1052054297990297 | validation: 0.9138134483085193]
	TIME [epoch: 5.75 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.106357762934729		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 1.106357762934729 | validation: 0.8754666066697188]
	TIME [epoch: 5.74 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1044832890719676		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 1.1044832890719676 | validation: 0.8959950307947449]
	TIME [epoch: 5.74 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0975070181345468		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 1.0975070181345468 | validation: 0.9104241792459477]
	TIME [epoch: 5.74 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1037302339001023		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 1.1037302339001023 | validation: 0.8864636765775619]
	TIME [epoch: 5.73 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1043080219382215		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 1.1043080219382215 | validation: 0.8835607007536894]
	TIME [epoch: 5.73 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1053522761171368		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 1.1053522761171368 | validation: 0.9111286996002502]
	TIME [epoch: 5.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1012850311299112		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 1.1012850311299112 | validation: 0.8827330626921777]
	TIME [epoch: 5.74 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102521883391062		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 1.102521883391062 | validation: 0.8848866414063956]
	TIME [epoch: 5.73 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1064346483676446		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 1.1064346483676446 | validation: 0.9060922226563882]
	TIME [epoch: 5.73 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1014395804804427		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 1.1014395804804427 | validation: 0.8778731087451974]
	TIME [epoch: 5.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1015108140870298		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 1.1015108140870298 | validation: 0.8968425257699733]
	TIME [epoch: 5.74 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102434340713682		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 1.102434340713682 | validation: 0.9007388413544917]
	TIME [epoch: 5.74 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1047002598726012		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 1.1047002598726012 | validation: 0.8858773973053742]
	TIME [epoch: 5.74 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031167382083846		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 1.1031167382083846 | validation: 0.8966192140412961]
	TIME [epoch: 5.73 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1074467395453047		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 1.1074467395453047 | validation: 0.8944561774426725]
	TIME [epoch: 5.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0983047246917794		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 1.0983047246917794 | validation: 0.891522408789249]
	TIME [epoch: 5.73 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107157476730875		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 1.107157476730875 | validation: 0.8856609519352606]
	TIME [epoch: 5.73 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0985465279308384		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 1.0985465279308384 | validation: 0.9167548926802636]
	TIME [epoch: 5.74 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994612611093626		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 1.0994612611093626 | validation: 0.8945416383906745]
	TIME [epoch: 5.75 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024706585863984		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 1.1024706585863984 | validation: 0.8978935977898375]
	TIME [epoch: 5.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1007799033754837		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 1.1007799033754837 | validation: 0.9062830462742941]
	TIME [epoch: 5.74 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1047963695080965		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 1.1047963695080965 | validation: 0.883365832853929]
	TIME [epoch: 5.74 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1012246128721872		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 1.1012246128721872 | validation: 0.8825729279717383]
	TIME [epoch: 5.75 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1028100072239755		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 1.1028100072239755 | validation: 0.8846059052119177]
	TIME [epoch: 5.74 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1037739422050346		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 1.1037739422050346 | validation: 0.8859609103507365]
	TIME [epoch: 5.74 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994489728526424		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 1.0994489728526424 | validation: 0.8858073314671793]
	TIME [epoch: 5.73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0999226785274971		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 1.0999226785274971 | validation: 0.8919274949210995]
	TIME [epoch: 5.75 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1041358480358228		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 1.1041358480358228 | validation: 0.8931194258871504]
	TIME [epoch: 5.75 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0983118267898937		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 1.0983118267898937 | validation: 0.888444019243535]
	TIME [epoch: 5.74 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.101780781596075		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 1.101780781596075 | validation: 0.8816409491259928]
	TIME [epoch: 5.74 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075220373947632		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 1.1075220373947632 | validation: 0.8771179124787356]
	TIME [epoch: 5.74 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1012615747418726		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 1.1012615747418726 | validation: 0.8878066344374225]
	TIME [epoch: 5.74 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.100846696192221		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 1.100846696192221 | validation: 0.8995909121213252]
	TIME [epoch: 5.74 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.101532173901951		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 1.101532173901951 | validation: 0.8901563061384619]
	TIME [epoch: 5.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098427806763488		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 1.098427806763488 | validation: 0.876560389944147]
	TIME [epoch: 5.73 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1000418814766972		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 1.1000418814766972 | validation: 0.8881270474960736]
	TIME [epoch: 5.73 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1031100256697561		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 1.1031100256697561 | validation: 0.8790250645752036]
	TIME [epoch: 5.74 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024092172274926		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 1.1024092172274926 | validation: 0.899100197063463]
	TIME [epoch: 5.74 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0990660946797035		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 1.0990660946797035 | validation: 0.8781226863464733]
	TIME [epoch: 5.74 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1061366929352876		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 1.1061366929352876 | validation: 0.8974570182933435]
	TIME [epoch: 5.73 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0983734117680672		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 1.0983734117680672 | validation: 0.8885977406959832]
	TIME [epoch: 5.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10265777376869		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 1.10265777376869 | validation: 0.8788755597301212]
	TIME [epoch: 5.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1014031334298922		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 1.1014031334298922 | validation: 0.8916823436421819]
	TIME [epoch: 5.74 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1001111837012623		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 1.1001111837012623 | validation: 0.8923356620244993]
	TIME [epoch: 5.73 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024805026615827		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 1.1024805026615827 | validation: 0.8874961838339971]
	TIME [epoch: 5.73 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0991253207652354		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 1.0991253207652354 | validation: 0.8823021099714599]
	TIME [epoch: 5.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0973163425610968		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 1.0973163425610968 | validation: 0.9097381818647685]
	TIME [epoch: 5.74 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0997197448296194		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 1.0997197448296194 | validation: 0.8817691247940346]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v2_2_v_mmd4_20250516_150626/states/model_phi1_4a_distortion_v2_2_v_mmd4_1107.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3284.975 seconds.
