Args:
Namespace(name='model_phi1_1a_distortion_v1r_1_v_mmd1', outdir='out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v1r_1/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_1a_distortion_v1r_1/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.07407755, 0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1650530491

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.526825919574096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.526825919574096 | validation: 6.222114946012336]
	TIME [epoch: 377 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.461581999292986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.461581999292986 | validation: 6.907268909329884]
	TIME [epoch: 6.18 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.252440394190108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.252440394190108 | validation: 5.237179376269226]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.2639233780265995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2639233780265995 | validation: 5.188220061834609]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.917982363735762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.917982363735762 | validation: 4.600281925498469]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.906318729251216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.906318729251216 | validation: 4.507763278811334]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.562963617411721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.562963617411721 | validation: 4.206035861266239]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.32371078095781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.32371078095781 | validation: 4.762878390290274]
	TIME [epoch: 6.14 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4035381359270875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4035381359270875 | validation: 4.184776500624045]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.098608367872959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.098608367872959 | validation: 3.880465654416405]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9928047855078113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9928047855078113 | validation: 3.8076638242397154]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8647967012422146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8647967012422146 | validation: 3.7409459356243873]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6986059208725806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6986059208725806 | validation: 3.9546707233109624]
	TIME [epoch: 6.14 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8556140821406575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8556140821406575 | validation: 3.826058971274918]
	TIME [epoch: 6.14 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7230539041794826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7230539041794826 | validation: 3.764392849187118]
	TIME [epoch: 6.13 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7033622418324206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7033622418324206 | validation: 3.7761652916926596]
	TIME [epoch: 6.12 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6732421125980315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6732421125980315 | validation: 3.6037582714571608]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5919146565448106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5919146565448106 | validation: 3.699703618101119]
	TIME [epoch: 6.19 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6134159008067455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6134159008067455 | validation: 3.556977645398589]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5120891713917373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5120891713917373 | validation: 3.5250981219101023]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4952952155044636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4952952155044636 | validation: 3.5194529907693584]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5698378516144658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5698378516144658 | validation: 3.579367124593057]
	TIME [epoch: 6.15 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4739303002263275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4739303002263275 | validation: 3.448254263035066]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.419812549106323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.419812549106323 | validation: 3.447617669237056]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.464298338950771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.464298338950771 | validation: 3.4724859127499927]
	TIME [epoch: 6.15 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4218546197760853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4218546197760853 | validation: 3.4095170541919106]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3794594191887883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3794594191887883 | validation: 3.4015272731818547]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.419787550829286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.419787550829286 | validation: 3.3881048661731104]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3551856961684936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3551856961684936 | validation: 3.3625203426313486]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.374131722842106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374131722842106 | validation: 3.356838112679325]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.332309267815287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.332309267815287 | validation: 3.333536126427477]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3123830809383126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3123830809383126 | validation: 3.3096420095580106]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.314838844711193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.314838844711193 | validation: 3.372524247601566]
	TIME [epoch: 6.27 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3202983142037352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3202983142037352 | validation: 3.31638042687357]
	TIME [epoch: 6.16 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280454408648861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.280454408648861 | validation: 3.2786720021490243]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2670299877953193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2670299877953193 | validation: 3.3084663811119013]
	TIME [epoch: 6.24 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2780643868193384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2780643868193384 | validation: 3.2348333957444177]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2163225396184414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2163225396184414 | validation: 3.2477732866356837]
	TIME [epoch: 6.15 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2564289061193286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2564289061193286 | validation: 3.293014640504186]
	TIME [epoch: 6.15 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1926652324081117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1926652324081117 | validation: 3.146324975998777]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.131980906970539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.131980906970539 | validation: 3.262199143199143]
	TIME [epoch: 6.25 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.241271133246415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.241271133246415 | validation: 3.2152480502413114]
	TIME [epoch: 6.16 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5516393530200037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5516393530200037 | validation: 4.161314627979332]
	TIME [epoch: 6.16 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.991398542092008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.991398542092008 | validation: 3.485196007719866]
	TIME [epoch: 6.18 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3312074951754633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3312074951754633 | validation: 3.238537558998371]
	TIME [epoch: 6.17 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.194327956556456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.194327956556456 | validation: 3.1562296730816515]
	TIME [epoch: 6.18 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.146379358991179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.146379358991179 | validation: 3.1457776068667096]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1334084900881862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1334084900881862 | validation: 3.1321017112005283]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1115178468408944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1115178468408944 | validation: 3.1016981018903733]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0840601873886873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0840601873886873 | validation: 3.063987404329229]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0530160580711505		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 3.0530160580711505 | validation: 3.022928869732377]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.42200365654667		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 3.42200365654667 | validation: 3.6000243524422393]
	TIME [epoch: 6.26 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3324121030947578		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 3.3324121030947578 | validation: 3.0848266041445935]
	TIME [epoch: 6.18 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.174345280070646		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 4.174345280070646 | validation: 5.79970583431945]
	TIME [epoch: 6.17 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.822240265338106		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 5.822240265338106 | validation: 5.175510270028724]
	TIME [epoch: 6.18 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.324708059501901		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 4.324708059501901 | validation: 3.180268246523334]
	TIME [epoch: 6.19 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.147769858990789		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.147769858990789 | validation: 3.1075789330308257]
	TIME [epoch: 6.36 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0920455829822457		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.0920455829822457 | validation: 3.079709948291304]
	TIME [epoch: 6.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0668558523681515		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.0668558523681515 | validation: 3.065380184001791]
	TIME [epoch: 6.19 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0489264151436504		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.0489264151436504 | validation: 3.050861702085651]
	TIME [epoch: 6.2 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.038207463801165		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.038207463801165 | validation: 3.0330529269435496]
	TIME [epoch: 6.19 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0196527630152485		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.0196527630152485 | validation: 3.015369406804375]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.001300488545759		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.001300488545759 | validation: 2.992119834493061]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.974704188044182		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 2.974704188044182 | validation: 2.959722424823987]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9337981730481704		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 2.9337981730481704 | validation: 2.916690399232798]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.965593507545447		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 2.965593507545447 | validation: 2.885187584887274]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.886477031864983		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 2.886477031864983 | validation: 2.872572563221889]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3244028823296747		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.3244028823296747 | validation: 3.420013948912262]
	TIME [epoch: 6.29 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635947104252704		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.635947104252704 | validation: 3.387490752717785]
	TIME [epoch: 6.28 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.555410106876707		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.555410106876707 | validation: 3.4122713766199544]
	TIME [epoch: 6.21 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2678770139843136		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 3.2678770139843136 | validation: 3.3493511120683035]
	TIME [epoch: 6.21 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29730932002676		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.29730932002676 | validation: 3.3523377732107758]
	TIME [epoch: 6.22 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366288676223663		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.366288676223663 | validation: 3.309741759365696]
	TIME [epoch: 6.2 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3098469092185683		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.3098469092185683 | validation: 3.639970260024927]
	TIME [epoch: 6.22 sec]
EPOCH 75/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 3.6598695022999412		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.6598695022999412 | validation: 4.04300885323453]
	TIME [epoch: 396 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.8813025567928126		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 4.8813025567928126 | validation: 5.239780891859368]
	TIME [epoch: 6.17 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.980907178495157		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 4.980907178495157 | validation: 4.792758945085788]
	TIME [epoch: 6.15 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1281303309808015		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 4.1281303309808015 | validation: 3.933471474603559]
	TIME [epoch: 6.15 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261823324117586		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.261823324117586 | validation: 3.000263779522995]
	TIME [epoch: 6.15 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.487863987949925		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 2.487863987949925 | validation: 2.188750223293341]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1670496684501868		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 2.1670496684501868 | validation: 2.8774709966030616]
	TIME [epoch: 6.15 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22990105019341		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.22990105019341 | validation: 2.963707436731255]
	TIME [epoch: 6.14 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.170891332229013		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 3.170891332229013 | validation: 2.6903134619271594]
	TIME [epoch: 6.14 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.328281230448824		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.328281230448824 | validation: 5.422979105532318]
	TIME [epoch: 6.14 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.894694213877937		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 5.894694213877937 | validation: 5.417122218673794]
	TIME [epoch: 6.44 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.928807161004068		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.928807161004068 | validation: 2.631723470813816]
	TIME [epoch: 6.14 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.983921057374311		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 2.983921057374311 | validation: 2.863157555489229]
	TIME [epoch: 6.14 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8596423012364296		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 2.8596423012364296 | validation: 2.596067508013105]
	TIME [epoch: 6.14 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.618793352948685		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 2.618793352948685 | validation: 2.436861436122289]
	TIME [epoch: 6.15 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4888409841328594		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 2.4888409841328594 | validation: 2.327230359404102]
	TIME [epoch: 6.15 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3848772231394393		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 2.3848772231394393 | validation: 2.202507814906208]
	TIME [epoch: 6.14 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.307163137667326		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 2.307163137667326 | validation: 2.086989548112257]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2217464588073783		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 2.2217464588073783 | validation: 1.9436245132779186]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0325377533604074		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 2.0325377533604074 | validation: 1.8131157421526614]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6870292636182533		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 1.6870292636182533 | validation: 1.5886577055472566]
	TIME [epoch: 6.15 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4598068858463802		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 1.4598068858463802 | validation: 1.620677574171295]
	TIME [epoch: 6.16 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3566305619385997		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 1.3566305619385997 | validation: 1.3882373277246858]
	TIME [epoch: 6.17 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3267895180613678		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 1.3267895180613678 | validation: 1.3069278674683034]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2088917694275196		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.2088917694275196 | validation: 1.2282906944836995]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2630080870181728		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 1.2630080870181728 | validation: 1.175746109438291]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.156571487565985		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 1.156571487565985 | validation: 1.6724526596030014]
	TIME [epoch: 6.19 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6161330319662026		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 1.6161330319662026 | validation: 1.1959019877794388]
	TIME [epoch: 6.19 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0958902377387247		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 1.0958902377387247 | validation: 1.0608590550840167]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0218255694967846		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 1.0218255694967846 | validation: 1.030320970669546]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9829926514812275		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.9829926514812275 | validation: 0.9229789929928796]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.112054998740976		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 1.112054998740976 | validation: 1.0317397749657327]
	TIME [epoch: 6.2 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9749393296523615		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.9749393296523615 | validation: 0.8671694115461692]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8897231276988404		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.8897231276988404 | validation: 0.9575749863283676]
	TIME [epoch: 6.2 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9326125268733301		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.9326125268733301 | validation: 0.9295410299261313]
	TIME [epoch: 6.2 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9002380909135334		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.9002380909135334 | validation: 0.9788039751881792]
	TIME [epoch: 6.21 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7862605993390415		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.7862605993390415 | validation: 0.752594387907974]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7019569303991402		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.7019569303991402 | validation: 0.8669945700574391]
	TIME [epoch: 6.31 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8647213963643942		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.8647213963643942 | validation: 0.7725977120647087]
	TIME [epoch: 6.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7187116740199192		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.7187116740199192 | validation: 0.8253518877125248]
	TIME [epoch: 6.18 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6326748219322824		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.6326748219322824 | validation: 0.9665830727283262]
	TIME [epoch: 6.19 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8477348121289636		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.8477348121289636 | validation: 0.6230458568966769]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.719552804444463		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.719552804444463 | validation: 0.6377141603267631]
	TIME [epoch: 6.27 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.575339281087393		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.575339281087393 | validation: 0.7229195515574653]
	TIME [epoch: 6.17 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6442559062036518		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.6442559062036518 | validation: 0.5822559624664589]
	TIME [epoch: 6.16 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5968811442403534		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.5968811442403534 | validation: 0.6641979753582867]
	TIME [epoch: 6.25 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6084323091573721		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.6084323091573721 | validation: 0.5487506746559226]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5176465681829061		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.5176465681829061 | validation: 0.631527192928782]
	TIME [epoch: 6.26 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6401510362159036		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.6401510362159036 | validation: 0.5540525613751903]
	TIME [epoch: 6.2 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49807137363836584		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.49807137363836584 | validation: 0.6649087858982055]
	TIME [epoch: 6.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5975012289517312		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.5975012289517312 | validation: 0.5580741262758071]
	TIME [epoch: 6.21 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5338834087459424		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.5338834087459424 | validation: 0.42742781877939673]
	TIME [epoch: 6.18 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5630404839212114		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.5630404839212114 | validation: 0.4665377925956765]
	TIME [epoch: 6.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.443870863921308		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.443870863921308 | validation: 0.4142534157190976]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5099035683784718		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.5099035683784718 | validation: 0.7195582521637194]
	TIME [epoch: 6.21 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5045408838169536		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.5045408838169536 | validation: 0.5479957788038081]
	TIME [epoch: 6.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4819395652065258		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.4819395652065258 | validation: 0.4829041148390629]
	TIME [epoch: 6.18 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43473818381069396		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.43473818381069396 | validation: 0.5219838315179405]
	TIME [epoch: 6.18 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45087493465676315		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.45087493465676315 | validation: 0.49167938298548697]
	TIME [epoch: 6.17 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4215371979217244		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.4215371979217244 | validation: 0.6757722845700089]
	TIME [epoch: 6.17 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5475521196779053		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.5475521196779053 | validation: 0.43308175469432575]
	TIME [epoch: 6.17 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3896320575359093		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.3896320575359093 | validation: 0.3367953759201576]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.471325339639302		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.471325339639302 | validation: 0.4380673305552987]
	TIME [epoch: 6.27 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36550581656236963		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.36550581656236963 | validation: 0.42914457094120784]
	TIME [epoch: 6.2 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39215399841712556		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.39215399841712556 | validation: 0.5995864203088067]
	TIME [epoch: 6.2 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46794142907502306		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.46794142907502306 | validation: 0.413926411704181]
	TIME [epoch: 6.2 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34304747284717635		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.34304747284717635 | validation: 0.536151421359115]
	TIME [epoch: 6.29 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48570046863021143		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.48570046863021143 | validation: 0.25599942115439034]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31654422018954376		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.31654422018954376 | validation: 0.37491456532542916]
	TIME [epoch: 6.29 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40418848992749584		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.40418848992749584 | validation: 0.458271042870687]
	TIME [epoch: 6.21 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37506535427854126		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.37506535427854126 | validation: 0.42675934557424317]
	TIME [epoch: 6.19 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3013405555978921		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.3013405555978921 | validation: 0.5335569700839171]
	TIME [epoch: 6.17 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4820027643752916		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.4820027643752916 | validation: 0.33071340089113593]
	TIME [epoch: 6.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34892296465732586		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.34892296465732586 | validation: 0.6602044839965313]
	TIME [epoch: 6.16 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37761545931586354		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.37761545931586354 | validation: 0.3640095872470865]
	TIME [epoch: 6.19 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35376520221173696		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.35376520221173696 | validation: 0.3882616144020816]
	TIME [epoch: 6.18 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3700727568224127		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.3700727568224127 | validation: 0.2731041623265764]
	TIME [epoch: 6.2 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32921215909160045		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.32921215909160045 | validation: 0.3057717940637869]
	TIME [epoch: 6.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36179683069815527		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.36179683069815527 | validation: 0.4518181819060242]
	TIME [epoch: 6.19 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37285506125109885		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.37285506125109885 | validation: 0.3307715438558001]
	TIME [epoch: 6.21 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2242046354543924		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.2242046354543924 | validation: 0.3526471442856596]
	TIME [epoch: 6.21 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4113572672347174		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.4113572672347174 | validation: 0.35394912154191954]
	TIME [epoch: 6.21 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32191511519911836		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.32191511519911836 | validation: 0.24342390094064512]
	TIME [epoch: 6.21 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3341205596620084		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.3341205596620084 | validation: 0.4063582757048453]
	TIME [epoch: 6.28 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786084534397172		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2786084534397172 | validation: 0.43049307811386683]
	TIME [epoch: 6.18 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32749357777759763		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.32749357777759763 | validation: 0.4327610463692803]
	TIME [epoch: 6.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3844529473784285		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.3844529473784285 | validation: 0.2550952547384333]
	TIME [epoch: 6.21 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34402939814771283		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.34402939814771283 | validation: 0.2589701657557917]
	TIME [epoch: 6.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23816645919647317		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.23816645919647317 | validation: 0.24847459609124942]
	TIME [epoch: 6.22 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3772234779426723		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.3772234779426723 | validation: 0.38104311261576845]
	TIME [epoch: 6.23 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28867836335238806		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.28867836335238806 | validation: 0.3004447378108769]
	TIME [epoch: 6.21 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3459192685451095		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.3459192685451095 | validation: 0.25620489320261475]
	TIME [epoch: 6.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29427699141168345		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.29427699141168345 | validation: 0.25499959374122755]
	TIME [epoch: 6.19 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28366298162344344		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.28366298162344344 | validation: 0.2923352695178697]
	TIME [epoch: 6.18 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.293648024533782		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.293648024533782 | validation: 0.26631785221898774]
	TIME [epoch: 6.18 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31116255703095524		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.31116255703095524 | validation: 0.27847363103262]
	TIME [epoch: 6.19 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28744776169384667		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.28744776169384667 | validation: 0.2206031767710671]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2602453835614587		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.2602453835614587 | validation: 0.3195058346567852]
	TIME [epoch: 6.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24216265635759293		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.24216265635759293 | validation: 0.3716315409914372]
	TIME [epoch: 6.25 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3026169216651321		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.3026169216651321 | validation: 0.37289434776369895]
	TIME [epoch: 6.2 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2507074937650248		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.2507074937650248 | validation: 0.18970176023090135]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24880527239937733		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.24880527239937733 | validation: 0.48207716180145244]
	TIME [epoch: 6.24 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3074704652329252		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.3074704652329252 | validation: 0.20963815620633902]
	TIME [epoch: 6.17 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2458812800272608		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.2458812800272608 | validation: 0.20843866248107168]
	TIME [epoch: 6.17 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2560745339683395		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.2560745339683395 | validation: 0.26201825307614846]
	TIME [epoch: 6.18 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30856607484416054		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.30856607484416054 | validation: 0.31665868460514457]
	TIME [epoch: 6.17 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23265142821050178		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.23265142821050178 | validation: 0.24839930761426918]
	TIME [epoch: 6.17 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656015426172679		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.2656015426172679 | validation: 0.24707181708713777]
	TIME [epoch: 6.18 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22992606507134578		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.22992606507134578 | validation: 0.19370287667245645]
	TIME [epoch: 6.18 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20869747438928016		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.20869747438928016 | validation: 0.517865229530572]
	TIME [epoch: 6.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32621603217056033		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.32621603217056033 | validation: 0.2151734855019737]
	TIME [epoch: 6.19 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22904295949793518		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.22904295949793518 | validation: 0.2910623024449986]
	TIME [epoch: 6.21 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20592199687922408		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.20592199687922408 | validation: 0.18302103804727776]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30787152361416503		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.30787152361416503 | validation: 0.2908835520281249]
	TIME [epoch: 6.28 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2006734965827486		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.2006734965827486 | validation: 0.24213547327095014]
	TIME [epoch: 6.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.231684785214435		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.231684785214435 | validation: 0.1697695013826684]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26009836991139657		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.26009836991139657 | validation: 0.17983498415937837]
	TIME [epoch: 6.19 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2500874979699589		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.2500874979699589 | validation: 0.21265427549812177]
	TIME [epoch: 6.19 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2325594910780436		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.2325594910780436 | validation: 0.16875539141986562]
	TIME [epoch: 6.19 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20340847287872438		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.20340847287872438 | validation: 0.2767058466306942]
	TIME [epoch: 6.19 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24452736794329274		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.24452736794329274 | validation: 0.2702261541208289]
	TIME [epoch: 6.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22817264568198584		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.22817264568198584 | validation: 0.1609086606552656]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1749186720687257		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.1749186720687257 | validation: 0.2986913495907004]
	TIME [epoch: 6.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2696945766728013		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.2696945766728013 | validation: 0.3980927309911406]
	TIME [epoch: 6.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24761450372261565		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.24761450372261565 | validation: 0.18801468610555594]
	TIME [epoch: 6.21 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20549957603827887		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.20549957603827887 | validation: 0.21692324901931953]
	TIME [epoch: 6.19 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24882959606983399		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.24882959606983399 | validation: 0.12510986830626686]
	TIME [epoch: 407 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17045503399758044		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.17045503399758044 | validation: 0.2507962317121373]
	TIME [epoch: 12.1 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20539706163360966		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.20539706163360966 | validation: 0.2275688966414786]
	TIME [epoch: 12.1 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2669404510708338		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.2669404510708338 | validation: 0.1328276447279704]
	TIME [epoch: 12.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14324506230163617		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.14324506230163617 | validation: 0.13355532658294553]
	TIME [epoch: 12.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1805951095104378		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.1805951095104378 | validation: 0.4124378272566096]
	TIME [epoch: 12.1 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29483828173712745		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.29483828173712745 | validation: 0.20195941161327374]
	TIME [epoch: 12.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17929806767963768		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.17929806767963768 | validation: 0.1577557733743617]
	TIME [epoch: 12.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2419853388353324		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.2419853388353324 | validation: 0.13516673448038685]
	TIME [epoch: 12.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17186954076444544		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.17186954076444544 | validation: 0.18793355680912338]
	TIME [epoch: 12.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1596319329663865		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.1596319329663865 | validation: 0.16920073401022007]
	TIME [epoch: 12.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19823628643632177		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.19823628643632177 | validation: 0.1762757304490497]
	TIME [epoch: 12.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.275301182595424		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.275301182595424 | validation: 0.12030821472979011]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13833403400789662		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.13833403400789662 | validation: 0.23627569296338963]
	TIME [epoch: 12.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24767807688068863		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.24767807688068863 | validation: 0.12666251044297006]
	TIME [epoch: 12.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13102972943052904		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.13102972943052904 | validation: 0.1723737996408995]
	TIME [epoch: 12.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2094784663914513		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.2094784663914513 | validation: 0.1927489454832323]
	TIME [epoch: 12.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2334590558820124		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.2334590558820124 | validation: 0.15096863541897015]
	TIME [epoch: 12.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13125360728544772		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.13125360728544772 | validation: 0.15374946790016172]
	TIME [epoch: 12.1 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22489046361422932		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.22489046361422932 | validation: 0.19116239331311383]
	TIME [epoch: 12.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12877849129856833		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.12877849129856833 | validation: 0.19302305751988996]
	TIME [epoch: 12.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20919421338512886		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.20919421338512886 | validation: 0.1749139726466577]
	TIME [epoch: 12.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19552754045226778		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.19552754045226778 | validation: 0.11430280015565727]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17394575350504018		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.17394575350504018 | validation: 0.13238474642429848]
	TIME [epoch: 12.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18546823432348344		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.18546823432348344 | validation: 0.13431061325286592]
	TIME [epoch: 12.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15474682630579892		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.15474682630579892 | validation: 0.21785794350490043]
	TIME [epoch: 12.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15552207413914926		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.15552207413914926 | validation: 0.2904993269794303]
	TIME [epoch: 12.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23461410529643018		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.23461410529643018 | validation: 0.14468261814338457]
	TIME [epoch: 12.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10357604320408305		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.10357604320408305 | validation: 0.09569509042847052]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18357419546819492		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.18357419546819492 | validation: 0.18629280700491652]
	TIME [epoch: 12.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20428693754596877		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.20428693754596877 | validation: 0.12843567713685133]
	TIME [epoch: 12.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15107338254319724		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.15107338254319724 | validation: 0.16371357430122963]
	TIME [epoch: 12.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14341717025245831		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.14341717025245831 | validation: 0.10742424478456442]
	TIME [epoch: 12.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18408854844717665		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.18408854844717665 | validation: 0.10184962825452812]
	TIME [epoch: 12.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16253013218052892		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.16253013218052892 | validation: 0.2646694129964419]
	TIME [epoch: 12.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17820574679113826		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.17820574679113826 | validation: 0.1547798463244578]
	TIME [epoch: 12.1 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12490291028413289		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.12490291028413289 | validation: 0.22137789534300148]
	TIME [epoch: 12.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21685363416508185		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.21685363416508185 | validation: 0.11623194505557496]
	TIME [epoch: 12.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09679397237035847		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.09679397237035847 | validation: 0.09426675363035175]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16393177962121838		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.16393177962121838 | validation: 0.16329973446332624]
	TIME [epoch: 12.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19227962331255724		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.19227962331255724 | validation: 0.15579031544346672]
	TIME [epoch: 12.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14255077387198306		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.14255077387198306 | validation: 0.11964988784315325]
	TIME [epoch: 12.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14863891967602796		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.14863891967602796 | validation: 0.17685971978180512]
	TIME [epoch: 12.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19391396089957752		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.19391396089957752 | validation: 0.23029680608461087]
	TIME [epoch: 12.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13610728562607016		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.13610728562607016 | validation: 0.16672859607245177]
	TIME [epoch: 12.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14931298405615906		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.14931298405615906 | validation: 0.1025423040527095]
	TIME [epoch: 12.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14064374111737782		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.14064374111737782 | validation: 0.24268872883395062]
	TIME [epoch: 12.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15540069402701762		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.15540069402701762 | validation: 0.09032132207916105]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17688925787277246		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.17688925787277246 | validation: 0.15860810997635727]
	TIME [epoch: 12.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15131769294113823		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.15131769294113823 | validation: 0.25950275835027686]
	TIME [epoch: 12.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14080378575447944		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.14080378575447944 | validation: 0.13018983680069302]
	TIME [epoch: 12.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1433173903532945		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.1433173903532945 | validation: 0.14369362407975333]
	TIME [epoch: 12.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12151362576731467		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.12151362576731467 | validation: 0.32821662314982414]
	TIME [epoch: 12.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15563899948500604		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.15563899948500604 | validation: 0.07477484195919107]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12627788567052356		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.12627788567052356 | validation: 0.2338629179125991]
	TIME [epoch: 12.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20437490033736966		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.20437490033736966 | validation: 0.13601189183074158]
	TIME [epoch: 12.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10804951673225413		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.10804951673225413 | validation: 0.11880534069311435]
	TIME [epoch: 12.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14871760368572537		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.14871760368572537 | validation: 0.1220810781681928]
	TIME [epoch: 12.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11746792139764599		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.11746792139764599 | validation: 0.11556325773314605]
	TIME [epoch: 12.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.143459548013552		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.143459548013552 | validation: 0.28459321222896006]
	TIME [epoch: 12.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1668590585384327		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.1668590585384327 | validation: 0.0987910379551048]
	TIME [epoch: 12.2 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11432394670910445		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.11432394670910445 | validation: 0.17883127664583526]
	TIME [epoch: 12.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17034029119808047		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.17034029119808047 | validation: 0.12680529278776473]
	TIME [epoch: 12.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09714175586740756		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09714175586740756 | validation: 0.06682561302427284]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07058482999523276		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.07058482999523276 | validation: 0.2409717009243199]
	TIME [epoch: 12.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2378429338513614		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.2378429338513614 | validation: 0.12051459307010555]
	TIME [epoch: 12.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08325466335331876		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.08325466335331876 | validation: 0.15380896697011254]
	TIME [epoch: 12.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1570395843481654		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.1570395843481654 | validation: 0.11024565599764263]
	TIME [epoch: 12.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0930188506052699		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.0930188506052699 | validation: 0.15921162475104433]
	TIME [epoch: 12.2 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1488290379005695		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.1488290379005695 | validation: 0.11004317344030137]
	TIME [epoch: 12.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1210751518152757		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.1210751518152757 | validation: 0.19674096792287715]
	TIME [epoch: 12.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1531557932160576		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.1531557932160576 | validation: 0.08635744547787794]
	TIME [epoch: 12.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1058160383680883		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.1058160383680883 | validation: 0.09026582119961082]
	TIME [epoch: 12.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.115202698676549		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.115202698676549 | validation: 0.1745171433678684]
	TIME [epoch: 12.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13285013071330506		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.13285013071330506 | validation: 0.29592939982488337]
	TIME [epoch: 12.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14995731444588095		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.14995731444588095 | validation: 0.0696590929439031]
	TIME [epoch: 12.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07159628843165694		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.07159628843165694 | validation: 0.17251349901071933]
	TIME [epoch: 12.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16374757123337547		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.16374757123337547 | validation: 0.08395528648969025]
	TIME [epoch: 12.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11223440495347256		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11223440495347256 | validation: 0.10865435050517616]
	TIME [epoch: 12.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1200521113095497		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.1200521113095497 | validation: 0.23483655946635346]
	TIME [epoch: 12.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15087132607418016		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.15087132607418016 | validation: 0.11035017882497322]
	TIME [epoch: 12.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10472973316806054		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.10472973316806054 | validation: 0.06741609921210151]
	TIME [epoch: 12.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13760892926967958		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.13760892926967958 | validation: 0.13872037425216915]
	TIME [epoch: 12.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1463009400574344		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1463009400574344 | validation: 0.10262035918457292]
	TIME [epoch: 12.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06631839424990424		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.06631839424990424 | validation: 0.06789688090895857]
	TIME [epoch: 12.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10853095294881956		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.10853095294881956 | validation: 0.05985979779365559]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16710352496197795		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.16710352496197795 | validation: 0.0846595631648431]
	TIME [epoch: 12.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1184501408427378		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.1184501408427378 | validation: 0.10465855422138022]
	TIME [epoch: 12.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10606297888261855		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.10606297888261855 | validation: 0.10754082401847445]
	TIME [epoch: 12.2 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1069374736133776		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.1069374736133776 | validation: 0.06858328224089894]
	TIME [epoch: 12.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10924769791874098		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.10924769791874098 | validation: 0.11173232759454098]
	TIME [epoch: 12.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10686323024458552		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.10686323024458552 | validation: 0.08688849830741696]
	TIME [epoch: 12.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12239452303028403		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.12239452303028403 | validation: 0.11010359950806868]
	TIME [epoch: 12.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08093420883291147		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.08093420883291147 | validation: 0.11204382200166513]
	TIME [epoch: 12.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16685303227207132		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.16685303227207132 | validation: 0.11722290150302975]
	TIME [epoch: 12.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09343032905836066		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.09343032905836066 | validation: 0.08196076862955282]
	TIME [epoch: 12.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12611656258530463		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.12611656258530463 | validation: 0.11559384650154195]
	TIME [epoch: 12.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08787721025879199		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.08787721025879199 | validation: 0.07458493580539402]
	TIME [epoch: 12.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09297213973743192		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.09297213973743192 | validation: 0.16172474858179464]
	TIME [epoch: 12.2 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14767491351018244		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.14767491351018244 | validation: 0.10186125509216651]
	TIME [epoch: 12.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0719022967089682		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.0719022967089682 | validation: 0.14296421439270474]
	TIME [epoch: 12.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0951540363448076		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.0951540363448076 | validation: 0.07450616683363917]
	TIME [epoch: 12.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1383798116373974		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.1383798116373974 | validation: 0.15127022910556637]
	TIME [epoch: 12.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10196608891699012		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.10196608891699012 | validation: 0.12442992058776436]
	TIME [epoch: 12.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06917419276416957		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.06917419276416957 | validation: 0.07653905601779573]
	TIME [epoch: 12.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08638482441688143		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.08638482441688143 | validation: 0.12691726156153127]
	TIME [epoch: 12.1 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1570473431667567		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.1570473431667567 | validation: 0.10845892291147394]
	TIME [epoch: 12.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06998591995722936		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.06998591995722936 | validation: 0.08171766733976443]
	TIME [epoch: 12.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14780815536164676		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.14780815536164676 | validation: 0.11348706509574713]
	TIME [epoch: 12.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08935550059311258		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.08935550059311258 | validation: 0.06232720389044329]
	TIME [epoch: 12.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11090787373024771		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.11090787373024771 | validation: 0.07027841207361603]
	TIME [epoch: 12.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059758530773517005		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.059758530773517005 | validation: 0.052878925226383655]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08591684836340732		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.08591684836340732 | validation: 0.1881600872839002]
	TIME [epoch: 12.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16886738981576038		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.16886738981576038 | validation: 0.08896672080493821]
	TIME [epoch: 12.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06641746144196901		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.06641746144196901 | validation: 0.06981097040715528]
	TIME [epoch: 12.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12575009136306164		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.12575009136306164 | validation: 0.15801506673292354]
	TIME [epoch: 12.2 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09565071713623602		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.09565071713623602 | validation: 0.10643992696756027]
	TIME [epoch: 12.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12031208795798601		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.12031208795798601 | validation: 0.07108273676389766]
	TIME [epoch: 12.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061598565208356104		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.061598565208356104 | validation: 0.07137650236716066]
	TIME [epoch: 12.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1179613453270712		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.1179613453270712 | validation: 0.06426210684681909]
	TIME [epoch: 12.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08966569876508049		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.08966569876508049 | validation: 0.0613091736867611]
	TIME [epoch: 12.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0984285520987177		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.0984285520987177 | validation: 0.10152588831917937]
	TIME [epoch: 12.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08398392050010624		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08398392050010624 | validation: 0.08830013477853887]
	TIME [epoch: 12.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05202888244431058		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.05202888244431058 | validation: 0.048426216342429024]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17641286313092375		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.17641286313092375 | validation: 0.0689181829172731]
	TIME [epoch: 12.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08378425629034789		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.08378425629034789 | validation: 0.08232940414892916]
	TIME [epoch: 12.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05746917208849029		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.05746917208849029 | validation: 0.08628274978718593]
	TIME [epoch: 12.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11030190569974008		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.11030190569974008 | validation: 0.11285804490240137]
	TIME [epoch: 12.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08371419546504474		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.08371419546504474 | validation: 0.0487101108964068]
	TIME [epoch: 12.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10219981681031876		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.10219981681031876 | validation: 0.12794082931258133]
	TIME [epoch: 12.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10217007125333746		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.10217007125333746 | validation: 0.10348896082761871]
	TIME [epoch: 12.2 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05301213791610426		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.05301213791610426 | validation: 0.046208384475030236]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1259072689285372		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.1259072689285372 | validation: 0.10690109624539999]
	TIME [epoch: 12.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09144246232930694		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.09144246232930694 | validation: 0.04203878333798403]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053909454610085304		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.053909454610085304 | validation: 0.13340860350545097]
	TIME [epoch: 12.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11913473290029496		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.11913473290029496 | validation: 0.05695064042064156]
	TIME [epoch: 12.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08094491535126908		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.08094491535126908 | validation: 0.07196506429979767]
	TIME [epoch: 12.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07424345948802666		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.07424345948802666 | validation: 0.10803474720718283]
	TIME [epoch: 12.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1234648491623293		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.1234648491623293 | validation: 0.1304364722989635]
	TIME [epoch: 12.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06303081389083207		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.06303081389083207 | validation: 0.04071766073905631]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08502775470807791		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.08502775470807791 | validation: 0.15358212117981437]
	TIME [epoch: 12.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10745999612848835		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.10745999612848835 | validation: 0.07664164746335435]
	TIME [epoch: 12.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08764759816084208		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.08764759816084208 | validation: 0.054833032274669395]
	TIME [epoch: 12.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05149884373282979		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.05149884373282979 | validation: 0.04564514436356151]
	TIME [epoch: 12.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12548781641257256		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.12548781641257256 | validation: 0.11742238512117253]
	TIME [epoch: 12.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08047293409919888		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.08047293409919888 | validation: 0.07039883608440038]
	TIME [epoch: 12.2 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06133080884151512		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.06133080884151512 | validation: 0.04204462398656026]
	TIME [epoch: 12.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08294628715447835		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.08294628715447835 | validation: 0.07999094520284336]
	TIME [epoch: 12.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11086526369957714		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.11086526369957714 | validation: 0.043226581768570464]
	TIME [epoch: 12.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060629721516664424		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.060629721516664424 | validation: 0.07018494685774022]
	TIME [epoch: 12.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1014374856148795		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.1014374856148795 | validation: 0.07789547242120204]
	TIME [epoch: 12.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07184696121564613		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.07184696121564613 | validation: 0.06001063359769417]
	TIME [epoch: 12.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0805162228021253		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.0805162228021253 | validation: 0.07795605440973358]
	TIME [epoch: 12.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.082313710842547		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.082313710842547 | validation: 0.0902015039663672]
	TIME [epoch: 12.2 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07467324723662673		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.07467324723662673 | validation: 0.054247301721665496]
	TIME [epoch: 12.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06374662579342011		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.06374662579342011 | validation: 0.15308103106127469]
	TIME [epoch: 12.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10752525052385792		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.10752525052385792 | validation: 0.09216379777233963]
	TIME [epoch: 12.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06391194732090526		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.06391194732090526 | validation: 0.04464453985304116]
	TIME [epoch: 12.2 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07523992041956253		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.07523992041956253 | validation: 0.07336846724702392]
	TIME [epoch: 12.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09545240419751942		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.09545240419751942 | validation: 0.06174128716565451]
	TIME [epoch: 12.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04940799752755252		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.04940799752755252 | validation: 0.05391429022361373]
	TIME [epoch: 12.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07146994125764436		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.07146994125764436 | validation: 0.0860469256099897]
	TIME [epoch: 12.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08904060420309666		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.08904060420309666 | validation: 0.06793983478553772]
	TIME [epoch: 12.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09554979788219672		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.09554979788219672 | validation: 0.04356202192503385]
	TIME [epoch: 12.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04719789313652985		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.04719789313652985 | validation: 0.055142685764593025]
	TIME [epoch: 12.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11995114410871754		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.11995114410871754 | validation: 0.04629660415962303]
	TIME [epoch: 12.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05130223361906541		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.05130223361906541 | validation: 0.049269013808730194]
	TIME [epoch: 12.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054443106599010124		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.054443106599010124 | validation: 0.11459820731580125]
	TIME [epoch: 12.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10532696795171749		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.10532696795171749 | validation: 0.06061083063989814]
	TIME [epoch: 12.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06052726824114894		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.06052726824114894 | validation: 0.08853864795651456]
	TIME [epoch: 12.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07006436409904229		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.07006436409904229 | validation: 0.06159853183153127]
	TIME [epoch: 12.2 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0929399353762235		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.0929399353762235 | validation: 0.09106313398484257]
	TIME [epoch: 12.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05831831327046676		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.05831831327046676 | validation: 0.03802891191377602]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051946911603245424		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.051946911603245424 | validation: 0.14346427579185234]
	TIME [epoch: 12.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08010028407993951		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.08010028407993951 | validation: 0.03490251209180266]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09040699935591262		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.09040699935591262 | validation: 0.07992448363598417]
	TIME [epoch: 12.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06627703973347172		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.06627703973347172 | validation: 0.056011483346770904]
	TIME [epoch: 12.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06872999800968727		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.06872999800968727 | validation: 0.06900359752400213]
	TIME [epoch: 12.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06484482687235582		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.06484482687235582 | validation: 0.03580733653212881]
	TIME [epoch: 12.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039122331552797256		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.039122331552797256 | validation: 0.05225055552679154]
	TIME [epoch: 12.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07218403976698516		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.07218403976698516 | validation: 0.1444229234853671]
	TIME [epoch: 12.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11956205277642253		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.11956205277642253 | validation: 0.04228593188964936]
	TIME [epoch: 12.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06575453520771109		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.06575453520771109 | validation: 0.06222893491010001]
	TIME [epoch: 12.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06556690489578777		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.06556690489578777 | validation: 0.04008306243870843]
	TIME [epoch: 12.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05297691034870741		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.05297691034870741 | validation: 0.08743744103809084]
	TIME [epoch: 12.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07382519992429445		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.07382519992429445 | validation: 0.046490692111119]
	TIME [epoch: 12.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07936790117542432		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.07936790117542432 | validation: 0.08553073976569445]
	TIME [epoch: 12.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07580312649706708		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.07580312649706708 | validation: 0.03647265642240224]
	TIME [epoch: 12.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04926922999195596		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.04926922999195596 | validation: 0.047613740110707715]
	TIME [epoch: 12.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09020550674123666		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.09020550674123666 | validation: 0.06992139635948968]
	TIME [epoch: 12.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06753457834913543		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.06753457834913543 | validation: 0.07309057556154434]
	TIME [epoch: 12.2 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04399002931721421		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.04399002931721421 | validation: 0.04800684484585355]
	TIME [epoch: 12.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0790107599509101		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.0790107599509101 | validation: 0.07993220774182305]
	TIME [epoch: 12.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04733605905387809		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.04733605905387809 | validation: 0.036929605923481726]
	TIME [epoch: 12.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08372444538470286		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.08372444538470286 | validation: 0.07109179407613546]
	TIME [epoch: 12.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07294361019314548		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.07294361019314548 | validation: 0.04257133175332177]
	TIME [epoch: 12.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04200448966325611		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.04200448966325611 | validation: 0.06755564708247588]
	TIME [epoch: 12.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07431670904820903		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.07431670904820903 | validation: 0.0824937866946733]
	TIME [epoch: 12.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08740959490625472		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.08740959490625472 | validation: 0.0398043023785878]
	TIME [epoch: 12.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037531753365923126		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.037531753365923126 | validation: 0.04868402844538529]
	TIME [epoch: 12.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059350314809011734		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.059350314809011734 | validation: 0.20184414814784382]
	TIME [epoch: 12.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10921737695303627		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.10921737695303627 | validation: 0.07618543350833511]
	TIME [epoch: 12.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0465674529966795		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.0465674529966795 | validation: 0.0411785397998393]
	TIME [epoch: 12.2 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055536909101960484		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.055536909101960484 | validation: 0.08942913479012518]
	TIME [epoch: 12.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06333898913465247		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.06333898913465247 | validation: 0.07681510779454957]
	TIME [epoch: 12.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06273232901876855		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.06273232901876855 | validation: 0.06487521246586647]
	TIME [epoch: 12.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08638188083313915		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.08638188083313915 | validation: 0.07067739043917529]
	TIME [epoch: 12.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04671226755814797		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.04671226755814797 | validation: 0.04596156894390971]
	TIME [epoch: 12.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05014942549091451		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.05014942549091451 | validation: 0.059520055128585256]
	TIME [epoch: 12.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07042916693210781		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.07042916693210781 | validation: 0.038932138742273165]
	TIME [epoch: 12.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03263530151527199		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.03263530151527199 | validation: 0.030866570921845936]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05622433916932322		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.05622433916932322 | validation: 0.20668045077030783]
	TIME [epoch: 12.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11204083492639216		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.11204083492639216 | validation: 0.07370769385519901]
	TIME [epoch: 12.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05541086115664728		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.05541086115664728 | validation: 0.06405576877103725]
	TIME [epoch: 12.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055512465096175044		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.055512465096175044 | validation: 0.04198620929524016]
	TIME [epoch: 12.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06463301542186436		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.06463301542186436 | validation: 0.12935894462266725]
	TIME [epoch: 12.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05961296351662117		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.05961296351662117 | validation: 0.05029142363126346]
	TIME [epoch: 12.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05851672905882277		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.05851672905882277 | validation: 0.07093045353180355]
	TIME [epoch: 12.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05525925673023149		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.05525925673023149 | validation: 0.1039339560365434]
	TIME [epoch: 12.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07458736926725103		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.07458736926725103 | validation: 0.0458203314667357]
	TIME [epoch: 12.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05000499142169382		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.05000499142169382 | validation: 0.12764637657968678]
	TIME [epoch: 12.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06383446944175697		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.06383446944175697 | validation: 0.03399562496101387]
	TIME [epoch: 12.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04211572203031111		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.04211572203031111 | validation: 0.044332200258677776]
	TIME [epoch: 12.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05747035924529095		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.05747035924529095 | validation: 0.12649850395860524]
	TIME [epoch: 12.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0962471495959645		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.0962471495959645 | validation: 0.07552033084049806]
	TIME [epoch: 12.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043153337475488565		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.043153337475488565 | validation: 0.030986868346924597]
	TIME [epoch: 12.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041310089592518		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.041310089592518 | validation: 0.07490954629884064]
	TIME [epoch: 12.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07198908213822303		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.07198908213822303 | validation: 0.04981492943771728]
	TIME [epoch: 12.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051669120387112175		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.051669120387112175 | validation: 0.04438898015698496]
	TIME [epoch: 12.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03836382847188068		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.03836382847188068 | validation: 0.06671837319943108]
	TIME [epoch: 12.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0807575068613304		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.0807575068613304 | validation: 0.07406383311753642]
	TIME [epoch: 12.2 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06256793707509004		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.06256793707509004 | validation: 0.08237751397968915]
	TIME [epoch: 12.1 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06049841857446277		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.06049841857446277 | validation: 0.050583172091974965]
	TIME [epoch: 12.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03833282217003875		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.03833282217003875 | validation: 0.031228202239962463]
	TIME [epoch: 12.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04213868046662772		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.04213868046662772 | validation: 0.11423904138891339]
	TIME [epoch: 12.2 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09378166471362363		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.09378166471362363 | validation: 0.046172585095238895]
	TIME [epoch: 12.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060280645734478694		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.060280645734478694 | validation: 0.05635876978977537]
	TIME [epoch: 12.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0627888766985733		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.0627888766985733 | validation: 0.09512214236082348]
	TIME [epoch: 12.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047808944685627304		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.047808944685627304 | validation: 0.04209366579501293]
	TIME [epoch: 12.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05226943653624367		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.05226943653624367 | validation: 0.06959852550777489]
	TIME [epoch: 12.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07585082235993487		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.07585082235993487 | validation: 0.060775695020907086]
	TIME [epoch: 12.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042047353767763335		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.042047353767763335 | validation: 0.03931881250728397]
	TIME [epoch: 12.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039592988498707474		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.039592988498707474 | validation: 0.07280171236392383]
	TIME [epoch: 12.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06959664090654813		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.06959664090654813 | validation: 0.04262678513927457]
	TIME [epoch: 12.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055338035318199176		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.055338035318199176 | validation: 0.03629013580871489]
	TIME [epoch: 12.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04343960147436568		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.04343960147436568 | validation: 0.036750422835942854]
	TIME [epoch: 12.2 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05748804312401183		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.05748804312401183 | validation: 0.048264175335853254]
	TIME [epoch: 12.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04540294150235657		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.04540294150235657 | validation: 0.09319830453286823]
	TIME [epoch: 12.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05553536147011412		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.05553536147011412 | validation: 0.06007617628948485]
	TIME [epoch: 12.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05833552188475685		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.05833552188475685 | validation: 0.05356883447049564]
	TIME [epoch: 12.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03735035022585936		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.03735035022585936 | validation: 0.05490525288307704]
	TIME [epoch: 12.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0527894971792321		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.0527894971792321 | validation: 0.06961738770049303]
	TIME [epoch: 12.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07357692490151778		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.07357692490151778 | validation: 0.04557605236803211]
	TIME [epoch: 12.2 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04133611444611942		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.04133611444611942 | validation: 0.04800691119910769]
	TIME [epoch: 12.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04434065203027004		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.04434065203027004 | validation: 0.04475278593078466]
	TIME [epoch: 12.2 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050820604170086804		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.050820604170086804 | validation: 0.1562127336751961]
	TIME [epoch: 12.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07152796153749665		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.07152796153749665 | validation: 0.03636561733698007]
	TIME [epoch: 12.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04278719992721721		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.04278719992721721 | validation: 0.03782169887061825]
	TIME [epoch: 12.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040985324909467845		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.040985324909467845 | validation: 0.055877630293459685]
	TIME [epoch: 12.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05503955246735616		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.05503955246735616 | validation: 0.04176381502923367]
	TIME [epoch: 12.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04362706749267735		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.04362706749267735 | validation: 0.06047547936882467]
	TIME [epoch: 12.2 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04745393496752548		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.04745393496752548 | validation: 0.06427213216242181]
	TIME [epoch: 12.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0688337852091112		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.0688337852091112 | validation: 0.03676402777528721]
	TIME [epoch: 12.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036333496462289366		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.036333496462289366 | validation: 0.036745762714505324]
	TIME [epoch: 12.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056038217850101005		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.056038217850101005 | validation: 0.05512894701354776]
	TIME [epoch: 12.2 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059438976049505085		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.059438976049505085 | validation: 0.03182104466657021]
	TIME [epoch: 12.2 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04151197608018926		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.04151197608018926 | validation: 0.06885035034654796]
	TIME [epoch: 12.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05086182287395482		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.05086182287395482 | validation: 0.07642012486403116]
	TIME [epoch: 12.2 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04719771077517919		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.04719771077517919 | validation: 0.05910475381802567]
	TIME [epoch: 12.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0570702031504038		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.0570702031504038 | validation: 0.0879599894915716]
	TIME [epoch: 12.2 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04869559132222903		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.04869559132222903 | validation: 0.03508411385687135]
	TIME [epoch: 12.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05347072409842453		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.05347072409842453 | validation: 0.051094294265241924]
	TIME [epoch: 12.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036418799877546454		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.036418799877546454 | validation: 0.039083907496022294]
	TIME [epoch: 12.2 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04911192474845905		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.04911192474845905 | validation: 0.08214736385597682]
	TIME [epoch: 12.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049682247838384694		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.049682247838384694 | validation: 0.032776796799752914]
	TIME [epoch: 12.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0365956145934495		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.0365956145934495 | validation: 0.04572566326880306]
	TIME [epoch: 12.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06079324628110761		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.06079324628110761 | validation: 0.032536986131879525]
	TIME [epoch: 12.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03367614784117308		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.03367614784117308 | validation: 0.04679917027806615]
	TIME [epoch: 12.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0571019407209611		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.0571019407209611 | validation: 0.06575067964807202]
	TIME [epoch: 12.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05217824751776479		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.05217824751776479 | validation: 0.03220348918175743]
	TIME [epoch: 12.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03640610841613512		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.03640610841613512 | validation: 0.07751893814256905]
	TIME [epoch: 12.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04986060468115184		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.04986060468115184 | validation: 0.03182338310631637]
	TIME [epoch: 12.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05898113471746394		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.05898113471746394 | validation: 0.03962633413120874]
	TIME [epoch: 12.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03898307959744334		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.03898307959744334 | validation: 0.048239870917861194]
	TIME [epoch: 12.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048508904186074744		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.048508904186074744 | validation: 0.06142110936492874]
	TIME [epoch: 12.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04694752176720838		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.04694752176720838 | validation: 0.03912588539909829]
	TIME [epoch: 12.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03667461919753385		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.03667461919753385 | validation: 0.03557308942059534]
	TIME [epoch: 12.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050484978943673514		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.050484978943673514 | validation: 0.08750921566585484]
	TIME [epoch: 12.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060877970663113415		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.060877970663113415 | validation: 0.030443711912328898]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03259452272036969		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.03259452272036969 | validation: 0.026846511788837894]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04116864823703684		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.04116864823703684 | validation: 0.05880829461486098]
	TIME [epoch: 12.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0447170803746868		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.0447170803746868 | validation: 0.02992013086788461]
	TIME [epoch: 12.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04128304963090376		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.04128304963090376 | validation: 0.06901402054955338]
	TIME [epoch: 12.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05232451763074462		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.05232451763074462 | validation: 0.034322680232751555]
	TIME [epoch: 12.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03406937110854192		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.03406937110854192 | validation: 0.03171517379937898]
	TIME [epoch: 12.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044908006136515806		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.044908006136515806 | validation: 0.146414143054559]
	TIME [epoch: 12.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06467480200520878		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.06467480200520878 | validation: 0.035235726605693324]
	TIME [epoch: 12.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03612938952217787		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.03612938952217787 | validation: 0.03957923351848343]
	TIME [epoch: 12.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03789236620815139		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.03789236620815139 | validation: 0.0376507101988847]
	TIME [epoch: 12.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051397057302093464		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.051397057302093464 | validation: 0.031313614776955635]
	TIME [epoch: 12.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029089864808269916		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.029089864808269916 | validation: 0.05183340966320628]
	TIME [epoch: 425 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06277644141465623		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.06277644141465623 | validation: 0.030761242584286034]
	TIME [epoch: 26 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03114347768376551		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.03114347768376551 | validation: 0.058955065925565334]
	TIME [epoch: 25.9 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044527847930039596		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.044527847930039596 | validation: 0.038254101644828606]
	TIME [epoch: 25.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028693839598289465		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.028693839598289465 | validation: 0.039558357733008614]
	TIME [epoch: 26 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053119682496814535		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.053119682496814535 | validation: 0.028167330992257336]
	TIME [epoch: 26.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048654291918663176		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.048654291918663176 | validation: 0.09806911734488713]
	TIME [epoch: 26.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04892372881973406		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.04892372881973406 | validation: 0.0350845386389206]
	TIME [epoch: 26.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030072455669547853		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.030072455669547853 | validation: 0.03131131099055775]
	TIME [epoch: 26.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032055403869647406		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.032055403869647406 | validation: 0.04499233610835983]
	TIME [epoch: 26 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06110568229756372		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.06110568229756372 | validation: 0.059126357408998506]
	TIME [epoch: 26.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03948268260028298		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.03948268260028298 | validation: 0.0277318057875934]
	TIME [epoch: 26.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03258838538649186		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.03258838538649186 | validation: 0.04278985482738616]
	TIME [epoch: 26.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03704669146906452		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.03704669146906452 | validation: 0.06236292543257504]
	TIME [epoch: 26.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044610985171027864		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.044610985171027864 | validation: 0.03137921547704302]
	TIME [epoch: 26.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03356510817224371		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.03356510817224371 | validation: 0.03350278891379919]
	TIME [epoch: 26.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04973527465248531		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.04973527465248531 | validation: 0.02638726024906677]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02778224056520588		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.02778224056520588 | validation: 0.03763638539084289]
	TIME [epoch: 26.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04314810448466952		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.04314810448466952 | validation: 0.025658914918149908]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036814328588032755		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.036814328588032755 | validation: 0.04695213069882227]
	TIME [epoch: 26.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05399397811236097		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.05399397811236097 | validation: 0.025198527587928257]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029218236861557646		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.029218236861557646 | validation: 0.033028616627707064]
	TIME [epoch: 26.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04160493347035658		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.04160493347035658 | validation: 0.028708216124170745]
	TIME [epoch: 26.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03074878486189242		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.03074878486189242 | validation: 0.044929200460813404]
	TIME [epoch: 26.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0410830248800912		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.0410830248800912 | validation: 0.028312719957163036]
	TIME [epoch: 26.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04287761445598657		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.04287761445598657 | validation: 0.10533649184149663]
	TIME [epoch: 26.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04503201683183049		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.04503201683183049 | validation: 0.02321676752551357]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02952674399503151		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.02952674399503151 | validation: 0.024144526633637274]
	TIME [epoch: 26 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03502643681259737		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.03502643681259737 | validation: 0.051237166554220115]
	TIME [epoch: 26.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044520423395468446		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.044520423395468446 | validation: 0.027871693850561867]
	TIME [epoch: 26.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03948449173526569		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.03948449173526569 | validation: 0.04230299530639453]
	TIME [epoch: 26.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03204374196828346		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.03204374196828346 | validation: 0.03204912617162947]
	TIME [epoch: 26.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04010313868521175		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.04010313868521175 | validation: 0.027188446646615433]
	TIME [epoch: 26.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03683436195170836		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.03683436195170836 | validation: 0.07419032362785366]
	TIME [epoch: 26.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20624152501565607		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.20624152501565607 | validation: 0.28138853095375516]
	TIME [epoch: 26.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1252202695974747		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.1252202695974747 | validation: 0.046269079912687694]
	TIME [epoch: 26.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03526444522551614		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.03526444522551614 | validation: 0.029414814344670588]
	TIME [epoch: 26.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028704348278476736		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.028704348278476736 | validation: 0.02886493933779241]
	TIME [epoch: 26 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0267265441492569		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.0267265441492569 | validation: 0.02523295370663446]
	TIME [epoch: 25.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02588998293244064		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.02588998293244064 | validation: 0.026117295088491775]
	TIME [epoch: 26 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028742591710226785		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.028742591710226785 | validation: 0.030760983154875007]
	TIME [epoch: 26.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0336592253602469		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.0336592253602469 | validation: 0.037352572495537535]
	TIME [epoch: 26.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03127852714652266		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.03127852714652266 | validation: 0.027706763395176172]
	TIME [epoch: 25.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031218343982128464		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.031218343982128464 | validation: 0.035383024116453575]
	TIME [epoch: 26 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041654956813332776		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.041654956813332776 | validation: 0.03937507390866346]
	TIME [epoch: 26 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03385021220719868		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.03385021220719868 | validation: 0.03418401424570602]
	TIME [epoch: 26.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030283694253089247		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.030283694253089247 | validation: 0.03468088300888246]
	TIME [epoch: 26.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034785944964566766		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.034785944964566766 | validation: 0.052766959117914375]
	TIME [epoch: 26.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04087334935200372		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.04087334935200372 | validation: 0.02971085587714163]
	TIME [epoch: 26 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03498896801273238		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.03498896801273238 | validation: 0.03351997389139483]
	TIME [epoch: 26 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03364873186584308		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.03364873186584308 | validation: 0.027886424253852394]
	TIME [epoch: 26.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038193328102337894		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.038193328102337894 | validation: 0.06321409794777247]
	TIME [epoch: 26.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03228265962615769		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.03228265962615769 | validation: 0.024965996535839266]
	TIME [epoch: 26.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032862552798893906		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.032862552798893906 | validation: 0.04705449973548844]
	TIME [epoch: 26.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034308064562175764		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.034308064562175764 | validation: 0.022639498375067804]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026886236689823872		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.026886236689823872 | validation: 0.03531757868010151]
	TIME [epoch: 26 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04012540419326805		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.04012540419326805 | validation: 0.026592399367485357]
	TIME [epoch: 26.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025408151598582337		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.025408151598582337 | validation: 0.02714250556026192]
	TIME [epoch: 26.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06428421878986366		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.06428421878986366 | validation: 0.08649385057784595]
	TIME [epoch: 26.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0498928303532447		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.0498928303532447 | validation: 0.025748142012111015]
	TIME [epoch: 26 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02573672805148279		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.02573672805148279 | validation: 0.024498474215878547]
	TIME [epoch: 26.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025677679681050662		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.025677679681050662 | validation: 0.023466550551369567]
	TIME [epoch: 26.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03634034328285577		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.03634034328285577 | validation: 0.057287995444902026]
	TIME [epoch: 26.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03965956530125133		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.03965956530125133 | validation: 0.02612211237183757]
	TIME [epoch: 26.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028031650117766566		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.028031650117766566 | validation: 0.03436888794263028]
	TIME [epoch: 26.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031072058214723353		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.031072058214723353 | validation: 0.023571746433035903]
	TIME [epoch: 26.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05170731829750011		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.05170731829750011 | validation: 0.03396105014393272]
	TIME [epoch: 26.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027960778220023307		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.027960778220023307 | validation: 0.024327137790806624]
	TIME [epoch: 26.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02784578468087774		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.02784578468087774 | validation: 0.02808467890726588]
	TIME [epoch: 26.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03480215386510505		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.03480215386510505 | validation: 0.04095211746587016]
	TIME [epoch: 26 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039006643149999264		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.039006643149999264 | validation: 0.028899675068882295]
	TIME [epoch: 26 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030165738885774684		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.030165738885774684 | validation: 0.041957885420557936]
	TIME [epoch: 26.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038215716727296914		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.038215716727296914 | validation: 0.05474075046641351]
	TIME [epoch: 26.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03489519562764915		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.03489519562764915 | validation: 0.027046174481932306]
	TIME [epoch: 26.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028996961741246594		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.028996961741246594 | validation: 0.027594720360995702]
	TIME [epoch: 26.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034626949529451126		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.034626949529451126 | validation: 0.04879991974712689]
	TIME [epoch: 26 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251347229938278		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.03251347229938278 | validation: 0.02415770838724697]
	TIME [epoch: 26 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026563404077483373		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.026563404077483373 | validation: 0.034375824978447114]
	TIME [epoch: 26 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04975547790244747		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.04975547790244747 | validation: 0.040649198883877635]
	TIME [epoch: 26.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032603813766742415		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.032603813766742415 | validation: 0.03151926063000208]
	TIME [epoch: 26.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02643569101053622		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.02643569101053622 | validation: 0.0239069328010484]
	TIME [epoch: 26 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029069711296386018		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.029069711296386018 | validation: 0.03329431219850338]
	TIME [epoch: 26.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035787427521162485		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.035787427521162485 | validation: 0.029141766359772364]
	TIME [epoch: 25.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041368890845967295		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.041368890845967295 | validation: 0.02846560318796091]
	TIME [epoch: 26 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026071589333255894		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.026071589333255894 | validation: 0.03232880515451254]
	TIME [epoch: 26.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024797208158122905		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.024797208158122905 | validation: 0.028728301686576516]
	TIME [epoch: 26.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04083063172439237		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.04083063172439237 | validation: 0.032901343745695574]
	TIME [epoch: 26.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02783823772120935		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.02783823772120935 | validation: 0.057755625370514846]
	TIME [epoch: 26.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03886099971282586		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.03886099971282586 | validation: 0.02420286377798567]
	TIME [epoch: 26 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023591806858569403		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.023591806858569403 | validation: 0.023569830617574353]
	TIME [epoch: 26.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03180511598619977		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.03180511598619977 | validation: 0.03113355668401096]
	TIME [epoch: 26.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042257145964165216		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.042257145964165216 | validation: 0.026976188695369926]
	TIME [epoch: 26.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029536726200562538		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.029536726200562538 | validation: 0.025208753644586086]
	TIME [epoch: 26.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02943879016057726		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.02943879016057726 | validation: 0.0369998108658414]
	TIME [epoch: 26 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03380570692006642		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.03380570692006642 | validation: 0.027745770999142005]
	TIME [epoch: 26.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028570935174958702		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.028570935174958702 | validation: 0.02904771176799043]
	TIME [epoch: 26.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026544579303154184		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.026544579303154184 | validation: 0.037888216011172815]
	TIME [epoch: 26.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04249663993546258		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.04249663993546258 | validation: 0.028034788772145332]
	TIME [epoch: 26.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02799236414089474		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.02799236414089474 | validation: 0.032827307784585535]
	TIME [epoch: 26.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025376287213496267		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.025376287213496267 | validation: 0.026785972229185624]
	TIME [epoch: 26 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0281383165094061		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0281383165094061 | validation: 0.02822151188723013]
	TIME [epoch: 26 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030207379670887095		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.030207379670887095 | validation: 0.046291358714135186]
	TIME [epoch: 26.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036956630110316614		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.036956630110316614 | validation: 0.028503871156804528]
	TIME [epoch: 26 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02685084208529591		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.02685084208529591 | validation: 0.027009848724795622]
	TIME [epoch: 26.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027651128339692563		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.027651128339692563 | validation: 0.029175898698555348]
	TIME [epoch: 26 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03695614790352348		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.03695614790352348 | validation: 0.040933553624582064]
	TIME [epoch: 26 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03309719510558296		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.03309719510558296 | validation: 0.04471022091933942]
	TIME [epoch: 26.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026521528388869103		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.026521528388869103 | validation: 0.036700705327799885]
	TIME [epoch: 26.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031038556167582276		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.031038556167582276 | validation: 0.023368408830721887]
	TIME [epoch: 26.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03228027956899049		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.03228027956899049 | validation: 0.024222744569035705]
	TIME [epoch: 26.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025201061877548902		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.025201061877548902 | validation: 0.03335340332548841]
	TIME [epoch: 26.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028497034918088915		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.028497034918088915 | validation: 0.03941966191498985]
	TIME [epoch: 26.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027812830903302284		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.027812830903302284 | validation: 0.0308399006007029]
	TIME [epoch: 26.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04251933672868631		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.04251933672868631 | validation: 0.02722146095959454]
	TIME [epoch: 26.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026218213872626114		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.026218213872626114 | validation: 0.02224053712295167]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02432641984510217		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.02432641984510217 | validation: 0.026556067054767367]
	TIME [epoch: 25.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03080661192620482		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.03080661192620482 | validation: 0.04061293971123424]
	TIME [epoch: 26 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03539367349184115		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.03539367349184115 | validation: 0.026142132173231386]
	TIME [epoch: 26 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02342506016446356		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.02342506016446356 | validation: 0.024142342399140607]
	TIME [epoch: 26.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034981775993052966		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.034981775993052966 | validation: 0.03396849780518195]
	TIME [epoch: 26.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025700947993047868		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.025700947993047868 | validation: 0.021971442621821498]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021851687218182515		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.021851687218182515 | validation: 0.025538989480576584]
	TIME [epoch: 26 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03498409462852306		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.03498409462852306 | validation: 0.046465903041334806]
	TIME [epoch: 26.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03446105292358072		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.03446105292358072 | validation: 0.03253865074980473]
	TIME [epoch: 26 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025497483280749886		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.025497483280749886 | validation: 0.026901894157210553]
	TIME [epoch: 26.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026445045174826064		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.026445045174826064 | validation: 0.040090112569792544]
	TIME [epoch: 26.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03385574296310069		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.03385574296310069 | validation: 0.026061561253845456]
	TIME [epoch: 26.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027840135454108163		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.027840135454108163 | validation: 0.026530440137409093]
	TIME [epoch: 26.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024317845001443253		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.024317845001443253 | validation: 0.026662779747553676]
	TIME [epoch: 26.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029750090993524395		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.029750090993524395 | validation: 0.047662837386456616]
	TIME [epoch: 26 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025518446064764652		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.025518446064764652 | validation: 0.021354560898387165]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02640895650707803		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.02640895650707803 | validation: 0.03915167852299368]
	TIME [epoch: 25.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0385521511807811		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0385521511807811 | validation: 0.024244349337557265]
	TIME [epoch: 26.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024241894025441274		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.024241894025441274 | validation: 0.025857823288670628]
	TIME [epoch: 26.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026495061658614257		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.026495061658614257 | validation: 0.02708158226729454]
	TIME [epoch: 26.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030907432521642027		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.030907432521642027 | validation: 0.02162596942106107]
	TIME [epoch: 26.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02279648003109473		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.02279648003109473 | validation: 0.02558647169433543]
	TIME [epoch: 26.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02761647004570798		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.02761647004570798 | validation: 0.02235795722517568]
	TIME [epoch: 26.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027004558944589283		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.027004558944589283 | validation: 0.030351196077725212]
	TIME [epoch: 26.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030682137345031564		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.030682137345031564 | validation: 0.022596233115631137]
	TIME [epoch: 26.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02447088344143689		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.02447088344143689 | validation: 0.03094190464868858]
	TIME [epoch: 26.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03245137345454996		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.03245137345454996 | validation: 0.042287325578617724]
	TIME [epoch: 26.1 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251944169191939		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.03251944169191939 | validation: 0.02447629882715483]
	TIME [epoch: 26.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022998709042914352		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.022998709042914352 | validation: 0.040061006637410274]
	TIME [epoch: 26.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03911382895276911		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.03911382895276911 | validation: 0.02543505993734346]
	TIME [epoch: 25.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025044085833322533		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.025044085833322533 | validation: 0.023238957269982114]
	TIME [epoch: 25.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02569883643728659		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.02569883643728659 | validation: 0.06011531703790447]
	TIME [epoch: 25.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03459837757938813		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.03459837757938813 | validation: 0.022005716869083404]
	TIME [epoch: 26.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02150590114559303		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.02150590114559303 | validation: 0.0229775578357996]
	TIME [epoch: 26.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02440482370470235		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.02440482370470235 | validation: 0.020836676114286534]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038433823120566965		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.038433823120566965 | validation: 0.022012423378763087]
	TIME [epoch: 26 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02401384828643841		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.02401384828643841 | validation: 0.01997814084212504]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02170372818642203		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.02170372818642203 | validation: 0.02203916308816803]
	TIME [epoch: 26 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026534836861176633		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.026534836861176633 | validation: 0.02651203829681558]
	TIME [epoch: 26 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023059139573525032		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.023059139573525032 | validation: 0.03458541598253764]
	TIME [epoch: 26.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03087870830305465		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.03087870830305465 | validation: 0.05222249275805022]
	TIME [epoch: 26.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02935363983330993		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.02935363983330993 | validation: 0.022405222530109535]
	TIME [epoch: 26 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03266300014748715		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.03266300014748715 | validation: 0.024901242209495328]
	TIME [epoch: 26 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021409229111829776		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.021409229111829776 | validation: 0.02104005416478944]
	TIME [epoch: 26 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025029010954153157		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.025029010954153157 | validation: 0.024424654708258407]
	TIME [epoch: 26 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021696388740700734		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.021696388740700734 | validation: 0.02151399827986171]
	TIME [epoch: 26 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037215184825615893		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.037215184825615893 | validation: 0.025587803351390895]
	TIME [epoch: 26 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02318227974723024		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.02318227974723024 | validation: 0.020355174826449475]
	TIME [epoch: 26.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019838558375187398		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.019838558375187398 | validation: 0.0200366676885979]
	TIME [epoch: 25.9 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03161194499403292		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.03161194499403292 | validation: 0.019763623480589514]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022855019295810467		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.022855019295810467 | validation: 0.020102358179736376]
	TIME [epoch: 26.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02454771911910422		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.02454771911910422 | validation: 0.026864840624874008]
	TIME [epoch: 26.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026006089669201864		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.026006089669201864 | validation: 0.02403480959807036]
	TIME [epoch: 26.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02189515599648101		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.02189515599648101 | validation: 0.025561716560755315]
	TIME [epoch: 26.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03214300948604678		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.03214300948604678 | validation: 0.022681408217797856]
	TIME [epoch: 26 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023546177784201656		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.023546177784201656 | validation: 0.02335311289966618]
	TIME [epoch: 26.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021280680323897152		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.021280680323897152 | validation: 0.02048950859462647]
	TIME [epoch: 26 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030395226675228232		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.030395226675228232 | validation: 0.023114545353453468]
	TIME [epoch: 26 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026668697149728925		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.026668697149728925 | validation: 0.024341565552399868]
	TIME [epoch: 26 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023969956366068162		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.023969956366068162 | validation: 0.038128590389098965]
	TIME [epoch: 26 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02626705599259492		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.02626705599259492 | validation: 0.019175810265699585]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029128185022395905		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.029128185022395905 | validation: 0.023214757305965256]
	TIME [epoch: 25.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024478892803708102		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.024478892803708102 | validation: 0.030324967520213367]
	TIME [epoch: 26 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022842383340380308		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.022842383340380308 | validation: 0.02583122866816515]
	TIME [epoch: 26.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028399175340019067		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.028399175340019067 | validation: 0.019681497759433036]
	TIME [epoch: 26.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02122590993528265		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.02122590993528265 | validation: 0.02050002196434511]
	TIME [epoch: 26.1 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022790199795267193		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.022790199795267193 | validation: 0.0238811657465354]
	TIME [epoch: 26.1 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027075146737735208		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.027075146737735208 | validation: 0.0206295994073253]
	TIME [epoch: 25.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02129818567026993		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.02129818567026993 | validation: 0.019867011304779355]
	TIME [epoch: 26 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02939503621240614		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.02939503621240614 | validation: 0.02635239257393758]
	TIME [epoch: 26 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023244371622544682		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.023244371622544682 | validation: 0.026607410905124352]
	TIME [epoch: 26.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028629328664740963		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.028629328664740963 | validation: 0.023515288586847025]
	TIME [epoch: 26 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020658064606249263		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.020658064606249263 | validation: 0.02397142674169694]
	TIME [epoch: 26 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020600636244234753		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.020600636244234753 | validation: 0.04193247113861924]
	TIME [epoch: 26 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03440967892664504		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.03440967892664504 | validation: 0.021519642035555283]
	TIME [epoch: 26.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021298617962046098		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.021298617962046098 | validation: 0.02076209011297278]
	TIME [epoch: 26 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02121416718574163		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.02121416718574163 | validation: 0.026336229045965852]
	TIME [epoch: 26 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026946074284808892		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.026946074284808892 | validation: 0.02336815792056067]
	TIME [epoch: 26.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023261041482388212		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.023261041482388212 | validation: 0.02541447977154257]
	TIME [epoch: 26.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02288788252029294		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.02288788252029294 | validation: 0.026041460807881254]
	TIME [epoch: 26.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02177598613992874		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.02177598613992874 | validation: 0.0214947950191062]
	TIME [epoch: 26.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023159634887779015		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.023159634887779015 | validation: 0.016138726070364262]
	TIME [epoch: 26.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03326265055769405		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.03326265055769405 | validation: 0.03765169969644304]
	TIME [epoch: 25.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030747989904778505		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.030747989904778505 | validation: 0.023637032148373562]
	TIME [epoch: 26 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02113809254248696		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.02113809254248696 | validation: 0.01831673824223752]
	TIME [epoch: 26 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026652471864977288		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.026652471864977288 | validation: 0.018484965254084582]
	TIME [epoch: 26.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021949999464156593		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.021949999464156593 | validation: 0.02433642463432103]
	TIME [epoch: 26.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027273150853270707		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.027273150853270707 | validation: 0.02337616652589166]
	TIME [epoch: 26.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024960729852209526		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.024960729852209526 | validation: 0.028973347208446163]
	TIME [epoch: 26.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020739795734209625		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.020739795734209625 | validation: 0.01689699094455085]
	TIME [epoch: 26.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01986356761301934		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.01986356761301934 | validation: 0.02326821188128682]
	TIME [epoch: 26.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030091211565129405		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.030091211565129405 | validation: 0.036498774657590424]
	TIME [epoch: 26.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026041895940252052		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.026041895940252052 | validation: 0.02079944139859135]
	TIME [epoch: 26 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01950872920679108		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.01950872920679108 | validation: 0.01872246325707029]
	TIME [epoch: 26.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026378423512656925		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.026378423512656925 | validation: 0.0187797620990578]
	TIME [epoch: 26.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021158673625983858		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.021158673625983858 | validation: 0.021175190029238497]
	TIME [epoch: 26 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01893221420027846		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.01893221420027846 | validation: 0.021009339112252612]
	TIME [epoch: 25.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022198215627625636		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.022198215627625636 | validation: 0.027284214964290544]
	TIME [epoch: 26 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02221990687672467		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.02221990687672467 | validation: 0.025430143288795348]
	TIME [epoch: 26 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030465347541415		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.030465347541415 | validation: 0.02687455495032994]
	TIME [epoch: 25.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024369300869435225		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.024369300869435225 | validation: 0.01784914051806224]
	TIME [epoch: 26 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018830117633887762		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.018830117633887762 | validation: 0.020983667239293582]
	TIME [epoch: 26.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02014091400617494		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.02014091400617494 | validation: 0.022644809216468373]
	TIME [epoch: 26.1 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030252975381797227		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.030252975381797227 | validation: 0.02196410001846914]
	TIME [epoch: 26 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022297763777120016		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.022297763777120016 | validation: 0.02126176091737814]
	TIME [epoch: 26 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0204092428294427		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0204092428294427 | validation: 0.018253946823181596]
	TIME [epoch: 26.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028813011254925414		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.028813011254925414 | validation: 0.025096435204038765]
	TIME [epoch: 26 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021771367679437475		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.021771367679437475 | validation: 0.017667555994700507]
	TIME [epoch: 26 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020344797986596524		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.020344797986596524 | validation: 0.021811699999437415]
	TIME [epoch: 26.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021005542081795343		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.021005542081795343 | validation: 0.025590951886748498]
	TIME [epoch: 26.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022421317642975672		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.022421317642975672 | validation: 0.02170853152529649]
	TIME [epoch: 26 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0208167186260452		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0208167186260452 | validation: 0.024107842110306515]
	TIME [epoch: 26.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031737982042518154		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.031737982042518154 | validation: 0.01989808963170378]
	TIME [epoch: 26.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019161530329459796		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.019161530329459796 | validation: 0.01771510628754197]
	TIME [epoch: 26.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02053719592088503		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.02053719592088503 | validation: 0.022634336870355216]
	TIME [epoch: 26.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02049322126302044		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.02049322126302044 | validation: 0.026526430851209017]
	TIME [epoch: 26.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026085063437364883		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.026085063437364883 | validation: 0.03649486186843051]
	TIME [epoch: 26.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022608187508167413		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.022608187508167413 | validation: 0.019143516486069746]
	TIME [epoch: 26.1 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021935528462362807		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.021935528462362807 | validation: 0.019092364086650333]
	TIME [epoch: 26 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021462907883371126		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.021462907883371126 | validation: 0.024789031965702472]
	TIME [epoch: 26 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022077725576123863		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.022077725576123863 | validation: 0.029140682418464594]
	TIME [epoch: 26 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021101508020632695		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.021101508020632695 | validation: 0.020350934938981102]
	TIME [epoch: 26 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020186609074750474		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.020186609074750474 | validation: 0.032416546781867596]
	TIME [epoch: 26.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027028646601019935		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.027028646601019935 | validation: 0.01866900354175791]
	TIME [epoch: 26.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019939367477751394		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.019939367477751394 | validation: 0.025292588764583967]
	TIME [epoch: 26 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022369005060356568		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.022369005060356568 | validation: 0.030662542081687214]
	TIME [epoch: 26.1 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02661990401092487		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.02661990401092487 | validation: 0.02637316747205719]
	TIME [epoch: 26 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021031272306623175		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.021031272306623175 | validation: 0.025331946646716393]
	TIME [epoch: 26 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02592248348824599		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.02592248348824599 | validation: 0.03242509587807822]
	TIME [epoch: 26 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019486742151160384		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.019486742151160384 | validation: 0.02871480533052212]
	TIME [epoch: 26.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019025534966093115		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.019025534966093115 | validation: 0.019434779549265]
	TIME [epoch: 26.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021253204902843506		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.021253204902843506 | validation: 0.026790509443527174]
	TIME [epoch: 26 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02322780401553307		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.02322780401553307 | validation: 0.04952265952368203]
	TIME [epoch: 25.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02781146488724156		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.02781146488724156 | validation: 0.022645300704612653]
	TIME [epoch: 26 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01917565041429558		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.01917565041429558 | validation: 0.02049949025931698]
	TIME [epoch: 26 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0202672674262278		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.0202672674262278 | validation: 0.019489271990245027]
	TIME [epoch: 26 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019474757033728272		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.019474757033728272 | validation: 0.02599186822699488]
	TIME [epoch: 26.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019372917796919027		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.019372917796919027 | validation: 0.0218169296892856]
	TIME [epoch: 25.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022777236309772627		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.022777236309772627 | validation: 0.030998504956738317]
	TIME [epoch: 26 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024656779955288487		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.024656779955288487 | validation: 0.030480005565561588]
	TIME [epoch: 26.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022177137012889578		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.022177137012889578 | validation: 0.019413313420217057]
	TIME [epoch: 26.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01985328789283379		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.01985328789283379 | validation: 0.01847374683741477]
	TIME [epoch: 26 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01877269448353363		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.01877269448353363 | validation: 0.017536228091052757]
	TIME [epoch: 26 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024503547814417447		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.024503547814417447 | validation: 0.019023609180479427]
	TIME [epoch: 26.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01937169181805886		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.01937169181805886 | validation: 0.024541795023727747]
	TIME [epoch: 26.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021928915027134812		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.021928915027134812 | validation: 0.018556488755067893]
	TIME [epoch: 26 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01773244393059984		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.01773244393059984 | validation: 0.017698530416678918]
	TIME [epoch: 26.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020919423868077158		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.020919423868077158 | validation: 0.03228360486419311]
	TIME [epoch: 26 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0237438562244154		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.0237438562244154 | validation: 0.016752523673375062]
	TIME [epoch: 25.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019985666365776977		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.019985666365776977 | validation: 0.022663832921692342]
	TIME [epoch: 25.9 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020603606611517514		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.020603606611517514 | validation: 0.027568033319215042]
	TIME [epoch: 26 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023376450337415932		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.023376450337415932 | validation: 0.017184245485451263]
	TIME [epoch: 26 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019757615847681383		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.019757615847681383 | validation: 0.024270380471397318]
	TIME [epoch: 26 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019950568866977846		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.019950568866977846 | validation: 0.015726869546419282]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_769.pth
	Model improved!!!
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018199318767799785		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.018199318767799785 | validation: 0.023013690370206372]
	TIME [epoch: 26.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019696153251366772		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.019696153251366772 | validation: 0.020306509862358064]
	TIME [epoch: 26 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024013809008892065		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.024013809008892065 | validation: 0.021311911514274563]
	TIME [epoch: 26 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020279552239638704		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.020279552239638704 | validation: 0.02068098811019725]
	TIME [epoch: 26 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021372474954475286		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.021372474954475286 | validation: 0.025068831223684654]
	TIME [epoch: 26.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02254785120273375		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.02254785120273375 | validation: 0.01977750435511661]
	TIME [epoch: 26 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01898643940404362		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.01898643940404362 | validation: 0.023112453649393575]
	TIME [epoch: 26 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02202856877607734		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.02202856877607734 | validation: 0.021687703089768154]
	TIME [epoch: 26 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019062203376474764		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.019062203376474764 | validation: 0.026663222607387647]
	TIME [epoch: 26.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02187490668840289		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.02187490668840289 | validation: 0.023512053258059253]
	TIME [epoch: 25.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01939037660539815		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.01939037660539815 | validation: 0.02129361071231812]
	TIME [epoch: 26 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0188576126910446		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.0188576126910446 | validation: 0.017773217682508387]
	TIME [epoch: 26 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02173142673292215		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.02173142673292215 | validation: 0.030898447572572943]
	TIME [epoch: 26 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02312820181454827		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.02312820181454827 | validation: 0.01826749367361001]
	TIME [epoch: 25.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018122453482595172		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.018122453482595172 | validation: 0.019268416340193707]
	TIME [epoch: 26 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019194668393949135		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.019194668393949135 | validation: 0.018314927431654214]
	TIME [epoch: 26.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018115843055225202		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.018115843055225202 | validation: 0.023522863349956]
	TIME [epoch: 26 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022534728792347258		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.022534728792347258 | validation: 0.019512053320960787]
	TIME [epoch: 26.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020635007490780188		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.020635007490780188 | validation: 0.021851995482899858]
	TIME [epoch: 26 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019684132222509275		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.019684132222509275 | validation: 0.018657023441564967]
	TIME [epoch: 26.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02384919515536106		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.02384919515536106 | validation: 0.029770035384680052]
	TIME [epoch: 26.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019457230094097028		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.019457230094097028 | validation: 0.019055820943607343]
	TIME [epoch: 26.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01945173668531284		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.01945173668531284 | validation: 0.02399179656385681]
	TIME [epoch: 26 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018976392716111647		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.018976392716111647 | validation: 0.020067722118811242]
	TIME [epoch: 26.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018431785276179072		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.018431785276179072 | validation: 0.019844267780216123]
	TIME [epoch: 26.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021217771842322316		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.021217771842322316 | validation: 0.018778071196950952]
	TIME [epoch: 26.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018215675969607676		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.018215675969607676 | validation: 0.02001479088390786]
	TIME [epoch: 26.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022627966551087293		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.022627966551087293 | validation: 0.016946517729323173]
	TIME [epoch: 26 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017784559771828013		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.017784559771828013 | validation: 0.023678953271247943]
	TIME [epoch: 25.9 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019218213922608973		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.019218213922608973 | validation: 0.02685023317302964]
	TIME [epoch: 26.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02349135148387639		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.02349135148387639 | validation: 0.019472161270073857]
	TIME [epoch: 26.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017663856182115426		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.017663856182115426 | validation: 0.016828755741799425]
	TIME [epoch: 26.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0192727297470878		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.0192727297470878 | validation: 0.019480729342996363]
	TIME [epoch: 26 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02130792152432934		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.02130792152432934 | validation: 0.02137119115785207]
	TIME [epoch: 26.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01848670413074603		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.01848670413074603 | validation: 0.018911304004545094]
	TIME [epoch: 26 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02084943395964695		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.02084943395964695 | validation: 0.021199409090702295]
	TIME [epoch: 26 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018840011616956854		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.018840011616956854 | validation: 0.016259369863852575]
	TIME [epoch: 25.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020266961414300524		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.020266961414300524 | validation: 0.018871911152093423]
	TIME [epoch: 25.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019561720779696437		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.019561720779696437 | validation: 0.018647825749974437]
	TIME [epoch: 26 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02005025675400607		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.02005025675400607 | validation: 0.017256078705194813]
	TIME [epoch: 26.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019288856534202713		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.019288856534202713 | validation: 0.025914433820771007]
	TIME [epoch: 25.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01943208968795823		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.01943208968795823 | validation: 0.01880243865929651]
	TIME [epoch: 26 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0212334604673121		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.0212334604673121 | validation: 0.018000899885713583]
	TIME [epoch: 26 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017800706941054		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.017800706941054 | validation: 0.01715521199235725]
	TIME [epoch: 26 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019772392753982435		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.019772392753982435 | validation: 0.019842947534162998]
	TIME [epoch: 26.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01855198186999254		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.01855198186999254 | validation: 0.0403456908534639]
	TIME [epoch: 26 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03420276697334226		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.03420276697334226 | validation: 0.018784380942484524]
	TIME [epoch: 25.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018130226437013376		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.018130226437013376 | validation: 0.021064900783125097]
	TIME [epoch: 25.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017793327697765697		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.017793327697765697 | validation: 0.015081743872289742]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016947953738508277		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.016947953738508277 | validation: 0.017935760510746288]
	TIME [epoch: 26.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017817496302779044		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.017817496302779044 | validation: 0.027725555270446522]
	TIME [epoch: 26.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02009448991340005		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.02009448991340005 | validation: 0.01899154461002424]
	TIME [epoch: 26 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019623517136578013		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.019623517136578013 | validation: 0.020448655864808568]
	TIME [epoch: 26 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018690316542629475		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.018690316542629475 | validation: 0.021296614120260446]
	TIME [epoch: 25.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020084099823934567		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.020084099823934567 | validation: 0.02916860116836957]
	TIME [epoch: 26 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017890713698215883		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.017890713698215883 | validation: 0.01832819578089733]
	TIME [epoch: 26.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017716384958968812		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.017716384958968812 | validation: 0.029634185049893416]
	TIME [epoch: 26.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019004488809235373		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.019004488809235373 | validation: 0.021083449295730243]
	TIME [epoch: 26 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01879605053991528		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.01879605053991528 | validation: 0.019852834064653616]
	TIME [epoch: 26 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021033301659562777		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.021033301659562777 | validation: 0.01750827592573723]
	TIME [epoch: 26 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018484211611448		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.018484211611448 | validation: 0.021651185814336928]
	TIME [epoch: 25.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019185106724704783		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.019185106724704783 | validation: 0.02326645754409945]
	TIME [epoch: 25.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017139659073846424		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.017139659073846424 | validation: 0.01743855628173103]
	TIME [epoch: 26 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017455371762167597		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.017455371762167597 | validation: 0.01806777192079216]
	TIME [epoch: 26.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01719827722123434		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.01719827722123434 | validation: 0.018021312290374444]
	TIME [epoch: 26 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023458961835854192		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.023458961835854192 | validation: 0.019522968399478445]
	TIME [epoch: 26 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018773946316218093		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.018773946316218093 | validation: 0.0169860269496197]
	TIME [epoch: 26.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018013882457885273		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.018013882457885273 | validation: 0.02241241711428281]
	TIME [epoch: 26 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016732812058224578		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.016732812058224578 | validation: 0.01985595913471567]
	TIME [epoch: 26 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020736295972373454		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.020736295972373454 | validation: 0.016798491569755512]
	TIME [epoch: 26.1 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0182518028082524		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.0182518028082524 | validation: 0.016879886623444855]
	TIME [epoch: 25.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01677553320670145		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.01677553320670145 | validation: 0.01944059521611918]
	TIME [epoch: 26 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02005071699718185		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.02005071699718185 | validation: 0.022292547412162474]
	TIME [epoch: 26.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018415183571213616		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.018415183571213616 | validation: 0.016629388196538593]
	TIME [epoch: 26.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017431751921472265		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.017431751921472265 | validation: 0.016953939554995953]
	TIME [epoch: 26.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01719144124676065		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.01719144124676065 | validation: 0.017522798114308756]
	TIME [epoch: 25.8 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02042131467851575		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.02042131467851575 | validation: 0.016842715324536205]
	TIME [epoch: 25.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017856484336242723		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.017856484336242723 | validation: 0.022224008417212834]
	TIME [epoch: 26 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017601476415089027		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.017601476415089027 | validation: 0.01873509223411482]
	TIME [epoch: 26.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018906056087360887		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.018906056087360887 | validation: 0.016200685870443453]
	TIME [epoch: 26.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018075537827896906		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.018075537827896906 | validation: 0.017722381660969823]
	TIME [epoch: 26.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016746176244914694		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.016746176244914694 | validation: 0.017807478180619]
	TIME [epoch: 26.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018717434359547258		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.018717434359547258 | validation: 0.01720822236963194]
	TIME [epoch: 26 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02146663266312123		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.02146663266312123 | validation: 0.018939036888020566]
	TIME [epoch: 26 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01714050851062569		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.01714050851062569 | validation: 0.014397873606755384]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01794531199127355		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.01794531199127355 | validation: 0.02515950929784179]
	TIME [epoch: 26.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01721818009563046		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.01721818009563046 | validation: 0.015211909271251087]
	TIME [epoch: 26 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019881861661532687		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.019881861661532687 | validation: 0.020523464131312348]
	TIME [epoch: 26.1 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018177451029141572		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.018177451029141572 | validation: 0.014455163085732546]
	TIME [epoch: 26.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018838885135915598		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.018838885135915598 | validation: 0.014678150142832632]
	TIME [epoch: 26.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018293422267848535		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.018293422267848535 | validation: 0.015451379048055694]
	TIME [epoch: 26.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016804319283753453		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.016804319283753453 | validation: 0.017353424054117467]
	TIME [epoch: 26 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018586222695572148		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.018586222695572148 | validation: 0.02479552856828597]
	TIME [epoch: 26 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02559733345002372		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.02559733345002372 | validation: 0.0265854563959776]
	TIME [epoch: 26 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02003942484461029		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.02003942484461029 | validation: 0.016058489501194356]
	TIME [epoch: 25.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01663023714749209		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.01663023714749209 | validation: 0.01605047108229023]
	TIME [epoch: 26 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016532280186880747		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.016532280186880747 | validation: 0.017401958992522972]
	TIME [epoch: 26 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01837097025064106		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.01837097025064106 | validation: 0.018329525137409597]
	TIME [epoch: 26 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01883842482020967		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.01883842482020967 | validation: 0.01741836058616882]
	TIME [epoch: 25.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016637172945471915		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.016637172945471915 | validation: 0.01994820354523647]
	TIME [epoch: 25.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0155303109944838		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.0155303109944838 | validation: 0.015072257132414515]
	TIME [epoch: 26 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017529418163086653		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.017529418163086653 | validation: 0.015515455606191383]
	TIME [epoch: 26.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020958254340962632		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.020958254340962632 | validation: 0.019879284542507404]
	TIME [epoch: 26 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01696168796053502		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.01696168796053502 | validation: 0.017952491813080448]
	TIME [epoch: 26 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015964117244445138		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.015964117244445138 | validation: 0.016819454189694733]
	TIME [epoch: 26 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017300986256647995		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.017300986256647995 | validation: 0.02600012519035324]
	TIME [epoch: 25.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01692812194023551		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.01692812194023551 | validation: 0.016106013003185655]
	TIME [epoch: 25.9 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017831040780474053		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.017831040780474053 | validation: 0.02315999234595442]
	TIME [epoch: 25.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017820289527225905		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.017820289527225905 | validation: 0.019669669152237196]
	TIME [epoch: 26 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02193174362891552		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.02193174362891552 | validation: 0.017599645805531583]
	TIME [epoch: 25.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016729687612026797		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.016729687612026797 | validation: 0.01669348762966027]
	TIME [epoch: 25.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016934506288928723		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.016934506288928723 | validation: 0.017371038134125896]
	TIME [epoch: 26 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017175983359834543		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.017175983359834543 | validation: 0.015901444735712682]
	TIME [epoch: 26.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01912042591697594		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.01912042591697594 | validation: 0.01939375286616015]
	TIME [epoch: 25.9 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018317590926776123		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.018317590926776123 | validation: 0.016988281158017662]
	TIME [epoch: 26 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021325231354687522		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.021325231354687522 | validation: 0.0225919163033954]
	TIME [epoch: 26.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018673066426108623		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.018673066426108623 | validation: 0.017838535291051366]
	TIME [epoch: 26.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015825392663447766		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.015825392663447766 | validation: 0.01686566566107251]
	TIME [epoch: 26.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016334334996652683		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.016334334996652683 | validation: 0.01945595819830634]
	TIME [epoch: 26.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019563810004341035		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.019563810004341035 | validation: 0.01607369553258029]
	TIME [epoch: 26 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016116069010990684		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.016116069010990684 | validation: 0.015351811522593682]
	TIME [epoch: 26.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016654608406288265		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.016654608406288265 | validation: 0.015515072693203336]
	TIME [epoch: 26.1 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0181216201787379		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0181216201787379 | validation: 0.016322151393941973]
	TIME [epoch: 26.1 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017935026857611162		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.017935026857611162 | validation: 0.017593477615797173]
	TIME [epoch: 26 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01695811421855817		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.01695811421855817 | validation: 0.015883591760796664]
	TIME [epoch: 26 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01608483374540956		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.01608483374540956 | validation: 0.015946343401351426]
	TIME [epoch: 26.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01909019639060719		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.01909019639060719 | validation: 0.01805265058097151]
	TIME [epoch: 26 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015911038231259927		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.015911038231259927 | validation: 0.05515763732575709]
	TIME [epoch: 26 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03116494301658769		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.03116494301658769 | validation: 0.020387925858048543]
	TIME [epoch: 26 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016761873190188105		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.016761873190188105 | validation: 0.0188929964597055]
	TIME [epoch: 26.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01568501905882066		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.01568501905882066 | validation: 0.01667552526682109]
	TIME [epoch: 26.1 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016117906834161956		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.016117906834161956 | validation: 0.01567181657780238]
	TIME [epoch: 26.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016728913187986076		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.016728913187986076 | validation: 0.0159147363844799]
	TIME [epoch: 26.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019984984313739105		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.019984984313739105 | validation: 0.022946163229045705]
	TIME [epoch: 26 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018235020680998724		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.018235020680998724 | validation: 0.021180246584392607]
	TIME [epoch: 26 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018162472747817647		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.018162472747817647 | validation: 0.018088528745559187]
	TIME [epoch: 26.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017970547666622382		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.017970547666622382 | validation: 0.015470136980558867]
	TIME [epoch: 26.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01627165644468726		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.01627165644468726 | validation: 0.019218348842841806]
	TIME [epoch: 26 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016163560577748034		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.016163560577748034 | validation: 0.013687836654229726]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016894523515711093		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.016894523515711093 | validation: 0.01694906405648106]
	TIME [epoch: 25.9 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01644541214796988		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.01644541214796988 | validation: 0.016760057195769333]
	TIME [epoch: 25.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015566474543683378		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.015566474543683378 | validation: 0.01997357088549951]
	TIME [epoch: 25.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01915795364154027		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.01915795364154027 | validation: 0.016017905736572066]
	TIME [epoch: 26.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018090402449360967		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.018090402449360967 | validation: 0.020184533966049092]
	TIME [epoch: 26.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017304773577784382		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.017304773577784382 | validation: 0.026564892693897987]
	TIME [epoch: 26.1 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01718505503358286		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.01718505503358286 | validation: 0.015438744258020986]
	TIME [epoch: 26 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015621751273622746		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.015621751273622746 | validation: 0.018331294865422364]
	TIME [epoch: 26 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01565206428718478		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.01565206428718478 | validation: 0.015731320867858497]
	TIME [epoch: 26 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017207504816109262		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.017207504816109262 | validation: 0.018679741812231483]
	TIME [epoch: 26.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017086618791235998		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.017086618791235998 | validation: 0.016526557668376764]
	TIME [epoch: 26 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01675018995408583		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.01675018995408583 | validation: 0.016914295304273012]
	TIME [epoch: 25.9 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015983845289353222		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.015983845289353222 | validation: 0.02083486528575277]
	TIME [epoch: 26 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01802636447922056		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.01802636447922056 | validation: 0.014409236548359714]
	TIME [epoch: 26 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01587475959615893		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.01587475959615893 | validation: 0.016816681953033187]
	TIME [epoch: 26.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01686889617061753		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.01686889617061753 | validation: 0.015896794664063078]
	TIME [epoch: 26.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01592184389970501		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.01592184389970501 | validation: 0.02265851069774552]
	TIME [epoch: 26 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016815612932010147		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.016815612932010147 | validation: 0.015369759941163701]
	TIME [epoch: 26 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039067394125385		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.039067394125385 | validation: 0.057635070365537916]
	TIME [epoch: 26 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030679434925027012		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.030679434925027012 | validation: 0.019125531578119276]
	TIME [epoch: 26.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016830645456177303		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.016830645456177303 | validation: 0.016272294397743026]
	TIME [epoch: 25.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015639788558476416		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.015639788558476416 | validation: 0.01757489879935912]
	TIME [epoch: 25.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015336633002378558		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.015336633002378558 | validation: 0.016006207419442364]
	TIME [epoch: 26 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015828084719477095		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.015828084719477095 | validation: 0.015535983511561064]
	TIME [epoch: 26 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015895798207099604		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.015895798207099604 | validation: 0.015275961994605389]
	TIME [epoch: 26 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016074661035913543		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.016074661035913543 | validation: 0.019849868830110966]
	TIME [epoch: 26 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017003651880204425		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.017003651880204425 | validation: 0.0163565366287739]
	TIME [epoch: 26.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015431821833728829		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.015431821833728829 | validation: 0.014870842729423631]
	TIME [epoch: 26 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01521471915334226		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.01521471915334226 | validation: 0.01824538687697459]
	TIME [epoch: 26 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01569517789283276		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.01569517789283276 | validation: 0.015939144041153444]
	TIME [epoch: 26.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015859992658781498		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.015859992658781498 | validation: 0.017359813678688274]
	TIME [epoch: 26.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018466611109362215		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.018466611109362215 | validation: 0.015294937872264047]
	TIME [epoch: 26.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01531134004772637		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.01531134004772637 | validation: 0.016748215676711]
	TIME [epoch: 26 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01585911658133156		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.01585911658133156 | validation: 0.016156654277574505]
	TIME [epoch: 26 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015370643718634756		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.015370643718634756 | validation: 0.019383174228684266]
	TIME [epoch: 26 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016378528182431025		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.016378528182431025 | validation: 0.019108848617042852]
	TIME [epoch: 26 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01549956380799529		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.01549956380799529 | validation: 0.01585657251490202]
	TIME [epoch: 26 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016228645733349513		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.016228645733349513 | validation: 0.026752342841501395]
	TIME [epoch: 26 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01752730157499482		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.01752730157499482 | validation: 0.01620427525838441]
	TIME [epoch: 26 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025008866660915292		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.025008866660915292 | validation: 0.021599924586193517]
	TIME [epoch: 26.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018060350664445213		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.018060350664445213 | validation: 0.014657866748472368]
	TIME [epoch: 26.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01596621887282673		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.01596621887282673 | validation: 0.015425284697236452]
	TIME [epoch: 26 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016581543432229433		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.016581543432229433 | validation: 0.015045606487199]
	TIME [epoch: 26 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015004127978642052		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.015004127978642052 | validation: 0.01828036223447211]
	TIME [epoch: 25.9 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016448523771592692		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.016448523771592692 | validation: 0.01689166867328677]
	TIME [epoch: 25.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016132853701135506		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.016132853701135506 | validation: 0.021568512180876256]
	TIME [epoch: 25.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01701943614869178		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.01701943614869178 | validation: 0.015515328762598672]
	TIME [epoch: 26 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016836678643345246		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.016836678643345246 | validation: 0.015461589095046179]
	TIME [epoch: 26.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015396129092370174		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.015396129092370174 | validation: 0.021310920516663515]
	TIME [epoch: 26.1 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017651037875459918		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.017651037875459918 | validation: 0.015869811561462683]
	TIME [epoch: 26 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016206318779110475		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.016206318779110475 | validation: 0.01791615995921266]
	TIME [epoch: 26.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015255471880191376		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.015255471880191376 | validation: 0.017914597152515067]
	TIME [epoch: 26 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604497583725464		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.01604497583725464 | validation: 0.017614375389348493]
	TIME [epoch: 26 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018343886578221454		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.018343886578221454 | validation: 0.015911570545478964]
	TIME [epoch: 26 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014588322857889947		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.014588322857889947 | validation: 0.020670109428072528]
	TIME [epoch: 26 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016940064644655667		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.016940064644655667 | validation: 0.017914420613726764]
	TIME [epoch: 26 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01600477456928494		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.01600477456928494 | validation: 0.01794729044338081]
	TIME [epoch: 26 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01595078089608144		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.01595078089608144 | validation: 0.015380539033984536]
	TIME [epoch: 26.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015873807488863387		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.015873807488863387 | validation: 0.01709480242107205]
	TIME [epoch: 26.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016247957579053923		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.016247957579053923 | validation: 0.015204353069897024]
	TIME [epoch: 26 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015276092854092226		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.015276092854092226 | validation: 0.014944189488556022]
	TIME [epoch: 26.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015848548419482637		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.015848548419482637 | validation: 0.015464249006416356]
	TIME [epoch: 26 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174656106354796		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0174656106354796 | validation: 0.01361956158002419]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_971.pth
	Model improved!!!
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015198872015204312		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.015198872015204312 | validation: 0.021126991042170987]
	TIME [epoch: 26 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01607700834248183		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.01607700834248183 | validation: 0.01517219924273498]
	TIME [epoch: 26.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019928420139449975		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.019928420139449975 | validation: 0.035281939555419986]
	TIME [epoch: 26.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02661118517586476		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.02661118517586476 | validation: 0.018125487073047796]
	TIME [epoch: 26 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017036361203153515		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.017036361203153515 | validation: 0.016412716627370733]
	TIME [epoch: 26.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017023886181262184		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.017023886181262184 | validation: 0.015581390520232406]
	TIME [epoch: 26.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015329186196314407		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.015329186196314407 | validation: 0.01912891755364063]
	TIME [epoch: 26.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014882536313371943		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.014882536313371943 | validation: 0.015921712897047226]
	TIME [epoch: 26.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015148289142468598		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.015148289142468598 | validation: 0.01790032096026862]
	TIME [epoch: 26 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01532405934078663		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.01532405934078663 | validation: 0.01893308803864642]
	TIME [epoch: 26 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015488454581398126		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.015488454581398126 | validation: 0.021218334337453473]
	TIME [epoch: 26 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016122814917291124		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.016122814917291124 | validation: 0.014544870868045387]
	TIME [epoch: 26 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01590216231634297		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.01590216231634297 | validation: 0.017712781556562675]
	TIME [epoch: 25.9 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016961206783600764		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.016961206783600764 | validation: 0.014289923947253418]
	TIME [epoch: 26 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014992616835065274		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.014992616835065274 | validation: 0.016248275897799376]
	TIME [epoch: 26 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01532358218811352		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.01532358218811352 | validation: 0.015960858197863782]
	TIME [epoch: 26.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01638335115031065		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.01638335115031065 | validation: 0.015697485995585587]
	TIME [epoch: 25.9 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015434625314202118		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.015434625314202118 | validation: 0.021894434146939947]
	TIME [epoch: 26 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01532172194074128		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.01532172194074128 | validation: 0.017755796509142184]
	TIME [epoch: 26 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017050738604758336		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.017050738604758336 | validation: 0.01551922170622351]
	TIME [epoch: 26 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016140066723900993		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.016140066723900993 | validation: 0.014704358869059637]
	TIME [epoch: 25.9 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015683171430280144		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.015683171430280144 | validation: 0.017276429330370317]
	TIME [epoch: 25.9 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016660382198525055		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.016660382198525055 | validation: 0.014608955421142463]
	TIME [epoch: 25.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015305842475750174		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.015305842475750174 | validation: 0.013471245301707773]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015690233055567528		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.015690233055567528 | validation: 0.014491481739519974]
	TIME [epoch: 26 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01536951430978172		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.01536951430978172 | validation: 0.017538785627236972]
	TIME [epoch: 26 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016706280655670445		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.016706280655670445 | validation: 0.015958880551983634]
	TIME [epoch: 26.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015876717836914828		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.015876717836914828 | validation: 0.01798712867246332]
	TIME [epoch: 26.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014974038251829093		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.014974038251829093 | validation: 0.017964145309186115]
	TIME [epoch: 26 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014526060045206343		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.014526060045206343 | validation: 0.015416739103277123]
	TIME [epoch: 406 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016697481066113514		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.016697481066113514 | validation: 0.018113735606593545]
	TIME [epoch: 54.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01510323973538619		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.01510323973538619 | validation: 0.01695167864676457]
	TIME [epoch: 54.9 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015158428434724978		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.015158428434724978 | validation: 0.014618605905957302]
	TIME [epoch: 55.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01487798465983409		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.01487798465983409 | validation: 0.014435537318992726]
	TIME [epoch: 55.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015446878808372637		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.015446878808372637 | validation: 0.01814077179483936]
	TIME [epoch: 55.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01449804048684121		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.01449804048684121 | validation: 0.015472340112835012]
	TIME [epoch: 55.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01422774215738932		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.01422774215738932 | validation: 0.015448710062690357]
	TIME [epoch: 55 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01808358941858149		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.01808358941858149 | validation: 0.014494496085906826]
	TIME [epoch: 55 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014602662828938098		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.014602662828938098 | validation: 0.014662244769653339]
	TIME [epoch: 55.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014538581032472095		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.014538581032472095 | validation: 0.01425175277986054]
	TIME [epoch: 55 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014981087051842722		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.014981087051842722 | validation: 0.019344210090717975]
	TIME [epoch: 55.1 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014967072516366364		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.014967072516366364 | validation: 0.016179818350668645]
	TIME [epoch: 55.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015285090934951869		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.015285090934951869 | validation: 0.014126993668528412]
	TIME [epoch: 55.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015049340518344388		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.015049340518344388 | validation: 0.016964996171799918]
	TIME [epoch: 55.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014770194738048551		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.014770194738048551 | validation: 0.015294417339235204]
	TIME [epoch: 55 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015536428968581461		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.015536428968581461 | validation: 0.014130920707908626]
	TIME [epoch: 55.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014901202359111847		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.014901202359111847 | validation: 0.01547662465787722]
	TIME [epoch: 54.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015140983034281352		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.015140983034281352 | validation: 0.015596243950678434]
	TIME [epoch: 55.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015834299225165438		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.015834299225165438 | validation: 0.0173625532251711]
	TIME [epoch: 55 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015166935036136517		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.015166935036136517 | validation: 0.015196804117859493]
	TIME [epoch: 55 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014672401292730095		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.014672401292730095 | validation: 0.01563922978913189]
	TIME [epoch: 54.9 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015011860403886504		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.015011860403886504 | validation: 0.016797382165951488]
	TIME [epoch: 54.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015132008224367818		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.015132008224367818 | validation: 0.013962747731991585]
	TIME [epoch: 55 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014351382292598993		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.014351382292598993 | validation: 0.015123144600039745]
	TIME [epoch: 55 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014384362886780643		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.014384362886780643 | validation: 0.01972693864173515]
	TIME [epoch: 55.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015179554651726134		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.015179554651726134 | validation: 0.013695515094707837]
	TIME [epoch: 55 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014789830689909907		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.014789830689909907 | validation: 0.015995086525895444]
	TIME [epoch: 55.1 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015385642905951506		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.015385642905951506 | validation: 0.01537412320921758]
	TIME [epoch: 55 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015348718187255628		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.015348718187255628 | validation: 0.017619083740645382]
	TIME [epoch: 55.1 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015046080349299863		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.015046080349299863 | validation: 0.013394718218704301]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0140009039123072		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.0140009039123072 | validation: 0.016005745663620993]
	TIME [epoch: 55.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014537518946042181		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.014537518946042181 | validation: 0.013202993782946693]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014366805892498872		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.014366805892498872 | validation: 0.01920450685927369]
	TIME [epoch: 55.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01481888381110896		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.01481888381110896 | validation: 0.01472242387960439]
	TIME [epoch: 55.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015279272680771432		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.015279272680771432 | validation: 0.014123901416426138]
	TIME [epoch: 55.1 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01583529375554321		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.01583529375554321 | validation: 0.013609094672864502]
	TIME [epoch: 55.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015163182799891671		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.015163182799891671 | validation: 0.013249222579500522]
	TIME [epoch: 55.1 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01486090042887326		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.01486090042887326 | validation: 0.014038827905462733]
	TIME [epoch: 55.1 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014386405816509072		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.014386405816509072 | validation: 0.014728039425792164]
	TIME [epoch: 55.1 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014695505536398123		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.014695505536398123 | validation: 0.013432688130400317]
	TIME [epoch: 55 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014866477776790404		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.014866477776790404 | validation: 0.01800193871105017]
	TIME [epoch: 55.1 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014845324319322568		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.014845324319322568 | validation: 0.014633402083282163]
	TIME [epoch: 55.1 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014388237703789912		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.014388237703789912 | validation: 0.01480559701530334]
	TIME [epoch: 55.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015530341387421095		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.015530341387421095 | validation: 0.016171657812664893]
	TIME [epoch: 55.1 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014934939553583305		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.014934939553583305 | validation: 0.013993685345027202]
	TIME [epoch: 55.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01445963308471038		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.01445963308471038 | validation: 0.017708593582743324]
	TIME [epoch: 55.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014508445298134566		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.014508445298134566 | validation: 0.022088380912452438]
	TIME [epoch: 55.1 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017494407358980366		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.017494407358980366 | validation: 0.015584859319545851]
	TIME [epoch: 55.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014721740390028718		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.014721740390028718 | validation: 0.014870890301339878]
	TIME [epoch: 55.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01444953634410118		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.01444953634410118 | validation: 0.013726297460471907]
	TIME [epoch: 55.1 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014980388605102236		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.014980388605102236 | validation: 0.016048821032229984]
	TIME [epoch: 55.1 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01495610084540502		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.01495610084540502 | validation: 0.016025760010416532]
	TIME [epoch: 55 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013945282747105		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.013945282747105 | validation: 0.014674274085662412]
	TIME [epoch: 55.1 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014101371938479693		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.014101371938479693 | validation: 0.015468338702849003]
	TIME [epoch: 55.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014846728559282572		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.014846728559282572 | validation: 0.013586537171193437]
	TIME [epoch: 55.1 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014275814848463575		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.014275814848463575 | validation: 0.01591400416207846]
	TIME [epoch: 55 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014744076642223006		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.014744076642223006 | validation: 0.013871060459501787]
	TIME [epoch: 55.1 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01569968504534633		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.01569968504534633 | validation: 0.013735984318600408]
	TIME [epoch: 55.1 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013530900989283227		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.013530900989283227 | validation: 0.015435364635094755]
	TIME [epoch: 55.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014410106084586104		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.014410106084586104 | validation: 0.013761151122677642]
	TIME [epoch: 55.1 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013688427888686008		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.013688427888686008 | validation: 0.018502142889290585]
	TIME [epoch: 55 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01439719365814491		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.01439719365814491 | validation: 0.0157059026809555]
	TIME [epoch: 55.1 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014775863342642158		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.014775863342642158 | validation: 0.015697458560894505]
	TIME [epoch: 55 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015008880713216252		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.015008880713216252 | validation: 0.017075268133070254]
	TIME [epoch: 55.1 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015105896194005992		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.015105896194005992 | validation: 0.01568077243128008]
	TIME [epoch: 55 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014241310345981386		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.014241310345981386 | validation: 0.014980111699424349]
	TIME [epoch: 55.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014582811827686906		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.014582811827686906 | validation: 0.015705801727456555]
	TIME [epoch: 55.1 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014752749167636846		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.014752749167636846 | validation: 0.012929417020453386]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1069.pth
	Model improved!!!
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014727262004684147		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.014727262004684147 | validation: 0.016387617164064606]
	TIME [epoch: 55.1 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015289333199654757		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.015289333199654757 | validation: 0.0212766363299385]
	TIME [epoch: 55.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015167465866494832		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.015167465866494832 | validation: 0.017068470663317542]
	TIME [epoch: 55.2 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014641565399473172		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.014641565399473172 | validation: 0.013185446742533165]
	TIME [epoch: 55.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014358380158216314		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.014358380158216314 | validation: 0.0143002050309567]
	TIME [epoch: 55.2 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013957035653624699		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.013957035653624699 | validation: 0.014437670319491034]
	TIME [epoch: 55.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01383293113259591		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.01383293113259591 | validation: 0.015089207136171321]
	TIME [epoch: 55.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015268696575031736		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.015268696575031736 | validation: 0.01587771374048533]
	TIME [epoch: 55 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014583220787946853		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.014583220787946853 | validation: 0.014569117878767706]
	TIME [epoch: 55.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013937173357806603		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.013937173357806603 | validation: 0.013195564183814342]
	TIME [epoch: 55.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013946490298947234		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.013946490298947234 | validation: 0.013333064948025889]
	TIME [epoch: 55 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015121690640868816		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.015121690640868816 | validation: 0.015909484668874832]
	TIME [epoch: 55 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013803511513720987		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.013803511513720987 | validation: 0.015207446725682791]
	TIME [epoch: 55.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014264798895810395		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.014264798895810395 | validation: 0.01528115484890745]
	TIME [epoch: 55.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014556788406988617		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.014556788406988617 | validation: 0.013347641842457108]
	TIME [epoch: 55 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014845778126677816		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.014845778126677816 | validation: 0.015718357564188323]
	TIME [epoch: 55.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015403125963937073		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.015403125963937073 | validation: 0.015327467346650811]
	TIME [epoch: 55.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014907137192026763		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.014907137192026763 | validation: 0.014051009466570577]
	TIME [epoch: 55.2 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013674128251800229		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.013674128251800229 | validation: 0.014027153232451763]
	TIME [epoch: 55.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014210529174819523		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.014210529174819523 | validation: 0.015796224141869184]
	TIME [epoch: 55.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014783119234237046		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.014783119234237046 | validation: 0.012786913340883442]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1090.pth
	Model improved!!!
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014670036052089233		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.014670036052089233 | validation: 0.016537208894622422]
	TIME [epoch: 55.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01351852933355015		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.01351852933355015 | validation: 0.014229936870002107]
	TIME [epoch: 55 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013866365641716		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.013866365641716 | validation: 0.013295925987445816]
	TIME [epoch: 55.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014572718679057153		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.014572718679057153 | validation: 0.013777918350345648]
	TIME [epoch: 55.1 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013575192501501486		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.013575192501501486 | validation: 0.015292966877715808]
	TIME [epoch: 55.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013750001529155592		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.013750001529155592 | validation: 0.014491437437380135]
	TIME [epoch: 55.2 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01476402870528684		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.01476402870528684 | validation: 0.021960442524954785]
	TIME [epoch: 55.1 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01530857037731571		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.01530857037731571 | validation: 0.0135957263959295]
	TIME [epoch: 55.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013156396360538003		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.013156396360538003 | validation: 0.01506458788233109]
	TIME [epoch: 55.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01359624536807925		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.01359624536807925 | validation: 0.013914145470326652]
	TIME [epoch: 55.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013796122842236755		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.013796122842236755 | validation: 0.013857267551532574]
	TIME [epoch: 55 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013819643265899528		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.013819643265899528 | validation: 0.01462573409532493]
	TIME [epoch: 55 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014277114868993364		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.014277114868993364 | validation: 0.016529524048274426]
	TIME [epoch: 55 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014644388081204822		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.014644388081204822 | validation: 0.015825482296004934]
	TIME [epoch: 55.1 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014081222361260188		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.014081222361260188 | validation: 0.015750499452127215]
	TIME [epoch: 55.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01445267641148467		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.01445267641148467 | validation: 0.015309836333878397]
	TIME [epoch: 55.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014032262307775106		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.014032262307775106 | validation: 0.014026520384716253]
	TIME [epoch: 55.1 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013581015006255753		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.013581015006255753 | validation: 0.013034147047082372]
	TIME [epoch: 55.2 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014519364324632345		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.014519364324632345 | validation: 0.015464232812782218]
	TIME [epoch: 55.1 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012988064443991199		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.012988064443991199 | validation: 0.012739779292220168]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1110.pth
	Model improved!!!
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013442086213236039		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.013442086213236039 | validation: 0.014230349898527044]
	TIME [epoch: 55.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01421575003875506		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.01421575003875506 | validation: 0.014238960429152563]
	TIME [epoch: 55.1 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013248407694117616		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.013248407694117616 | validation: 0.01588783286170303]
	TIME [epoch: 55.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014648623126181055		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.014648623126181055 | validation: 0.01350171388383528]
	TIME [epoch: 55 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013564089293570574		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.013564089293570574 | validation: 0.014835238362041012]
	TIME [epoch: 55.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013998185237529909		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.013998185237529909 | validation: 0.01504873224465056]
	TIME [epoch: 55.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01465640301731258		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.01465640301731258 | validation: 0.012922953448385678]
	TIME [epoch: 55 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01331795490324467		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.01331795490324467 | validation: 0.015976142087989843]
	TIME [epoch: 55.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013359328208238116		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.013359328208238116 | validation: 0.013949046165890173]
	TIME [epoch: 55.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014577378954285302		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.014577378954285302 | validation: 0.013828879859176671]
	TIME [epoch: 55.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014359692809794012		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.014359692809794012 | validation: 0.015107753885207692]
	TIME [epoch: 55 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01377421619013082		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.01377421619013082 | validation: 0.014175945231667066]
	TIME [epoch: 55.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014457495494980216		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.014457495494980216 | validation: 0.012538393944258989]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1123.pth
	Model improved!!!
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014283650084578669		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.014283650084578669 | validation: 0.013641761922888952]
	TIME [epoch: 55 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014622600467174347		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.014622600467174347 | validation: 0.014842166832970643]
	TIME [epoch: 55 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013358896530188989		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.013358896530188989 | validation: 0.01368182103245974]
	TIME [epoch: 55.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014523583405805292		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.014523583405805292 | validation: 0.014632269885859021]
	TIME [epoch: 55.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013304855083003084		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.013304855083003084 | validation: 0.013868392971807362]
	TIME [epoch: 55.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013559563800491559		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.013559563800491559 | validation: 0.015145441476371492]
	TIME [epoch: 55.1 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014357565557779495		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.014357565557779495 | validation: 0.013224887435025048]
	TIME [epoch: 55.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013444582189376029		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.013444582189376029 | validation: 0.014183047419670507]
	TIME [epoch: 55.2 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013226731609521604		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.013226731609521604 | validation: 0.012537689651911528]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1132.pth
	Model improved!!!
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01433796212779579		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.01433796212779579 | validation: 0.01474446851686231]
	TIME [epoch: 55.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014996844912573783		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.014996844912573783 | validation: 0.016032481300360127]
	TIME [epoch: 55 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013197797128460979		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.013197797128460979 | validation: 0.014410312707058245]
	TIME [epoch: 54.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013589987693272947		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.013589987693272947 | validation: 0.014747188661249206]
	TIME [epoch: 55.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014863121619992183		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.014863121619992183 | validation: 0.012614065494919682]
	TIME [epoch: 55.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014275549570045412		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.014275549570045412 | validation: 0.01299781345579237]
	TIME [epoch: 55.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013703322358211872		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.013703322358211872 | validation: 0.013273190086255635]
	TIME [epoch: 55.1 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01343386560911218		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.01343386560911218 | validation: 0.014383683991210438]
	TIME [epoch: 55 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013760626992946598		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.013760626992946598 | validation: 0.014034639761418156]
	TIME [epoch: 55 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013442207207879354		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.013442207207879354 | validation: 0.01603971397698206]
	TIME [epoch: 55 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014505419489768277		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.014505419489768277 | validation: 0.014597604524671863]
	TIME [epoch: 55 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014110847486834046		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.014110847486834046 | validation: 0.013572567275917094]
	TIME [epoch: 55.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014016682122407077		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.014016682122407077 | validation: 0.012753395729812076]
	TIME [epoch: 55.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01361259662189427		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.01361259662189427 | validation: 0.015892000685021456]
	TIME [epoch: 55 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014058675454595467		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.014058675454595467 | validation: 0.015496548883829026]
	TIME [epoch: 54.9 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013860247151702247		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.013860247151702247 | validation: 0.014463300108956521]
	TIME [epoch: 55.2 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014053833132660357		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.014053833132660357 | validation: 0.013520766790088852]
	TIME [epoch: 55.2 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013581824853601137		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.013581824853601137 | validation: 0.01298260176336305]
	TIME [epoch: 55.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013431524711015611		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.013431524711015611 | validation: 0.011758249475064923]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1151.pth
	Model improved!!!
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013451279866532169		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.013451279866532169 | validation: 0.014179875765424208]
	TIME [epoch: 55.2 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013557657468530233		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.013557657468530233 | validation: 0.014397065666784305]
	TIME [epoch: 55.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013724879503018407		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.013724879503018407 | validation: 0.014372032527108511]
	TIME [epoch: 55.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013777044725980559		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.013777044725980559 | validation: 0.014372095172747647]
	TIME [epoch: 55.1 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013216268958511648		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.013216268958511648 | validation: 0.014024060872458898]
	TIME [epoch: 55.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014507007651498368		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.014507007651498368 | validation: 0.015730432614991026]
	TIME [epoch: 55.1 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01371878196497743		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.01371878196497743 | validation: 0.014091611756591241]
	TIME [epoch: 55.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014147987196143243		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.014147987196143243 | validation: 0.012810813225754756]
	TIME [epoch: 55 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014839813174799236		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.014839813174799236 | validation: 0.013882336382430496]
	TIME [epoch: 55.1 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013546854492639121		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.013546854492639121 | validation: 0.014584366038213071]
	TIME [epoch: 55.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013327782282367822		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.013327782282367822 | validation: 0.014536473602343579]
	TIME [epoch: 55 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013392650934012443		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.013392650934012443 | validation: 0.014171571840844777]
	TIME [epoch: 55.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01333047275362009		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.01333047275362009 | validation: 0.014057852286461352]
	TIME [epoch: 55.2 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01357883085702619		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.01357883085702619 | validation: 0.013049764522758172]
	TIME [epoch: 55.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013735875103038144		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.013735875103038144 | validation: 0.013056213740112555]
	TIME [epoch: 55 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014063244751121393		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.014063244751121393 | validation: 0.014734975746387581]
	TIME [epoch: 55 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013888596068666966		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.013888596068666966 | validation: 0.01473991506139627]
	TIME [epoch: 55.2 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013524142698094998		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.013524142698094998 | validation: 0.01477824204366995]
	TIME [epoch: 55.1 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01330094606655928		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.01330094606655928 | validation: 0.013816750880421056]
	TIME [epoch: 55.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012905964018177424		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.012905964018177424 | validation: 0.013339012467099802]
	TIME [epoch: 55.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014197508076051388		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.014197508076051388 | validation: 0.01348707843205113]
	TIME [epoch: 55.1 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013509040825451573		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.013509040825451573 | validation: 0.014300683797933787]
	TIME [epoch: 55.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013452056984838952		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.013452056984838952 | validation: 0.014464642118822408]
	TIME [epoch: 55.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013084294963182152		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.013084294963182152 | validation: 0.015702946636351]
	TIME [epoch: 55 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01349683338408313		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.01349683338408313 | validation: 0.012563751040373154]
	TIME [epoch: 55.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013260427409472607		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.013260427409472607 | validation: 0.01697856260180763]
	TIME [epoch: 55.1 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01369273879227439		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.01369273879227439 | validation: 0.014493206965600258]
	TIME [epoch: 55.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013846593924905639		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.013846593924905639 | validation: 0.014363194580102562]
	TIME [epoch: 55.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013386723463982234		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.013386723463982234 | validation: 0.013392675356604003]
	TIME [epoch: 55.1 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013514720598185726		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.013514720598185726 | validation: 0.014564803743019532]
	TIME [epoch: 55.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013012011864562205		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.013012011864562205 | validation: 0.013284973552351646]
	TIME [epoch: 55 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013158948613292459		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.013158948613292459 | validation: 0.013931965953637614]
	TIME [epoch: 55.2 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012865300336473713		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.012865300336473713 | validation: 0.011818705514883246]
	TIME [epoch: 55.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014967855476891005		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.014967855476891005 | validation: 0.014215083027568725]
	TIME [epoch: 55.1 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013991701304942268		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.013991701304942268 | validation: 0.013090177506927973]
	TIME [epoch: 55 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013899622833144526		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.013899622833144526 | validation: 0.014021988657494221]
	TIME [epoch: 54.9 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01346907810921957		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.01346907810921957 | validation: 0.01194369499184222]
	TIME [epoch: 55.1 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013210240592239842		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.013210240592239842 | validation: 0.012081109659452846]
	TIME [epoch: 55 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012553257429146652		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.012553257429146652 | validation: 0.013482071029964548]
	TIME [epoch: 55.1 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01346192654907083		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.01346192654907083 | validation: 0.013100676492163048]
	TIME [epoch: 55 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014177765413435225		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.014177765413435225 | validation: 0.01595646864590501]
	TIME [epoch: 55 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014161306306070529		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.014161306306070529 | validation: 0.017673492639465466]
	TIME [epoch: 55 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013270205736334851		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.013270205736334851 | validation: 0.012679508705474941]
	TIME [epoch: 55.1 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013231882912143292		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.013231882912143292 | validation: 0.013083189840723004]
	TIME [epoch: 55.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01312454162007649		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.01312454162007649 | validation: 0.013056371837883943]
	TIME [epoch: 55.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013122997565697257		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.013122997565697257 | validation: 0.011589517348914329]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1197.pth
	Model improved!!!
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013272735620194535		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.013272735620194535 | validation: 0.013231707629791456]
	TIME [epoch: 55.1 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01349775907471568		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.01349775907471568 | validation: 0.014811234912869332]
	TIME [epoch: 55.1 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013550425282061244		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.013550425282061244 | validation: 0.012707941154166708]
	TIME [epoch: 55.1 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015202623085141625		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.015202623085141625 | validation: 0.01427397418441564]
	TIME [epoch: 55.2 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012935938062761693		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.012935938062761693 | validation: 0.012809662171225112]
	TIME [epoch: 55.2 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01322904185059049		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.01322904185059049 | validation: 0.01373670755986596]
	TIME [epoch: 55.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013864901040031245		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.013864901040031245 | validation: 0.014460484130549539]
	TIME [epoch: 55.1 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013870040303540977		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.013870040303540977 | validation: 0.012503583368719933]
	TIME [epoch: 55.1 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013615586078488581		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.013615586078488581 | validation: 0.015472196833508665]
	TIME [epoch: 55.1 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01359806210272101		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.01359806210272101 | validation: 0.013329446163481883]
	TIME [epoch: 55.1 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013850905240426611		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.013850905240426611 | validation: 0.012580793921787271]
	TIME [epoch: 55.1 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013208438562369036		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.013208438562369036 | validation: 0.01350950498261284]
	TIME [epoch: 55.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012907598301283903		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.012907598301283903 | validation: 0.013812164458268137]
	TIME [epoch: 55.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013092463293798817		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.013092463293798817 | validation: 0.013669742682200628]
	TIME [epoch: 55.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013586086939092377		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.013586086939092377 | validation: 0.012750746150003128]
	TIME [epoch: 55.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01263990474879299		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.01263990474879299 | validation: 0.014446790415691848]
	TIME [epoch: 55.1 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013294206096684476		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.013294206096684476 | validation: 0.014546968658777432]
	TIME [epoch: 55.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013695204276567001		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.013695204276567001 | validation: 0.014847168812619545]
	TIME [epoch: 55.1 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014072799993297188		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.014072799993297188 | validation: 0.014347448762679217]
	TIME [epoch: 55.1 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012969826448758721		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.012969826448758721 | validation: 0.013044082673790062]
	TIME [epoch: 55.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012761272431187987		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.012761272431187987 | validation: 0.012656571426884599]
	TIME [epoch: 55.1 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013016502031107146		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.013016502031107146 | validation: 0.013279300142314227]
	TIME [epoch: 55.2 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014294874554390775		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.014294874554390775 | validation: 0.013643181138133322]
	TIME [epoch: 55.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013286909488722942		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.013286909488722942 | validation: 0.013957221401577298]
	TIME [epoch: 55.1 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013491382049106007		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.013491382049106007 | validation: 0.012581783201158755]
	TIME [epoch: 55.2 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01323540763937504		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.01323540763937504 | validation: 0.013839845143916674]
	TIME [epoch: 55 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013162962771764587		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.013162962771764587 | validation: 0.015524496535636804]
	TIME [epoch: 55.2 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012898833678847886		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.012898833678847886 | validation: 0.01407808568233446]
	TIME [epoch: 55 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013820804578652014		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.013820804578652014 | validation: 0.01669843723514082]
	TIME [epoch: 55 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013434888383223155		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.013434888383223155 | validation: 0.011394077418436142]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1227.pth
	Model improved!!!
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013559941708393343		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.013559941708393343 | validation: 0.012996079852475941]
	TIME [epoch: 55 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013293986864638705		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.013293986864638705 | validation: 0.014001692585751853]
	TIME [epoch: 55.1 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013079391242223962		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.013079391242223962 | validation: 0.013747476634578076]
	TIME [epoch: 55.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013433453957525843		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.013433453957525843 | validation: 0.01221309771040607]
	TIME [epoch: 55.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013029304140562772		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.013029304140562772 | validation: 0.01442039154723726]
	TIME [epoch: 55.2 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013250157158443946		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.013250157158443946 | validation: 0.013393281568844083]
	TIME [epoch: 55.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013021999594958493		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.013021999594958493 | validation: 0.01266593472721541]
	TIME [epoch: 55.1 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013511979529787176		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.013511979529787176 | validation: 0.013454658858315868]
	TIME [epoch: 55.2 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013582815095110485		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.013582815095110485 | validation: 0.013825868928225304]
	TIME [epoch: 55.1 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013535571795524627		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.013535571795524627 | validation: 0.013644178688746952]
	TIME [epoch: 55.1 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013625476100226568		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.013625476100226568 | validation: 0.013302859475910502]
	TIME [epoch: 55.2 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017379448938916416		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.017379448938916416 | validation: 0.024325473901679655]
	TIME [epoch: 55.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01476284293018395		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.01476284293018395 | validation: 0.014675317052766885]
	TIME [epoch: 55 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012517675809146001		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.012517675809146001 | validation: 0.012284304913142363]
	TIME [epoch: 55.1 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013040877082072796		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.013040877082072796 | validation: 0.014967469289970553]
	TIME [epoch: 55.1 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013215202430748522		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.013215202430748522 | validation: 0.015617966808113316]
	TIME [epoch: 55.2 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013406291176377177		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.013406291176377177 | validation: 0.01338174133427214]
	TIME [epoch: 54.9 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013067212260611784		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.013067212260611784 | validation: 0.013125778875093854]
	TIME [epoch: 55.1 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013124266440168753		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.013124266440168753 | validation: 0.01302895726887019]
	TIME [epoch: 55.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013160620462054823		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.013160620462054823 | validation: 0.013303231958001089]
	TIME [epoch: 55.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013287576183953334		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.013287576183953334 | validation: 0.013809817202712921]
	TIME [epoch: 55.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013065387698573275		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.013065387698573275 | validation: 0.014756590276722072]
	TIME [epoch: 55.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013487063208035813		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.013487063208035813 | validation: 0.012392796674788521]
	TIME [epoch: 55.1 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012975811121644663		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.012975811121644663 | validation: 0.013165846009210114]
	TIME [epoch: 55.2 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012851440664424148		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.012851440664424148 | validation: 0.012029420743180266]
	TIME [epoch: 55.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012913900679568237		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.012913900679568237 | validation: 0.013022351733509746]
	TIME [epoch: 55 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012647226670914863		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.012647226670914863 | validation: 0.011866935901780161]
	TIME [epoch: 55.1 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013538327069283741		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.013538327069283741 | validation: 0.013865681169967288]
	TIME [epoch: 55 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012802639851468386		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.012802639851468386 | validation: 0.013581355155981443]
	TIME [epoch: 55.1 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013411275039079573		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.013411275039079573 | validation: 0.01268422894672028]
	TIME [epoch: 55.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012822679766022875		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.012822679766022875 | validation: 0.014103013344699863]
	TIME [epoch: 55 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013064730607832973		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.013064730607832973 | validation: 0.0156004215706412]
	TIME [epoch: 55.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013110028327820385		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.013110028327820385 | validation: 0.017519297159495163]
	TIME [epoch: 55.1 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013666278761114148		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.013666278761114148 | validation: 0.013099924619975147]
	TIME [epoch: 55 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01294242579948605		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.01294242579948605 | validation: 0.013995045012172514]
	TIME [epoch: 55.1 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013338328398445136		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.013338328398445136 | validation: 0.01206591548358197]
	TIME [epoch: 55.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01375183137294122		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.01375183137294122 | validation: 0.013273980610634575]
	TIME [epoch: 55.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012852289659497414		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.012852289659497414 | validation: 0.015436720127930726]
	TIME [epoch: 55.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013499938822236783		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.013499938822236783 | validation: 0.012638739777743204]
	TIME [epoch: 55.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01288487310375289		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.01288487310375289 | validation: 0.014709970405049]
	TIME [epoch: 55.1 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012826087201583118		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.012826087201583118 | validation: 0.013204685469481208]
	TIME [epoch: 55.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012563382689618173		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.012563382689618173 | validation: 0.012167643320508542]
	TIME [epoch: 55.1 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01309825197982922		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.01309825197982922 | validation: 0.011509997703088398]
	TIME [epoch: 55.1 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013640600447431196		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.013640600447431196 | validation: 0.015594737083680837]
	TIME [epoch: 55.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013099115852756395		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.013099115852756395 | validation: 0.014000929699721137]
	TIME [epoch: 55.1 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013228028423446177		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.013228028423446177 | validation: 0.01213851941448647]
	TIME [epoch: 55.1 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013379322604179107		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.013379322604179107 | validation: 0.013918410605227097]
	TIME [epoch: 55.1 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013137776638025324		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.013137776638025324 | validation: 0.012622960074877685]
	TIME [epoch: 55.1 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013536990970969916		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.013536990970969916 | validation: 0.013496823641243091]
	TIME [epoch: 55.1 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01383979141614473		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.01383979141614473 | validation: 0.01354315575671587]
	TIME [epoch: 55.1 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013317836510241371		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.013317836510241371 | validation: 0.017730258814086543]
	TIME [epoch: 55.1 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014407721251674787		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.014407721251674787 | validation: 0.01347584275374884]
	TIME [epoch: 55.1 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012922640961332804		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.012922640961332804 | validation: 0.013615389881933304]
	TIME [epoch: 55.1 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013199216724534533		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.013199216724534533 | validation: 0.013452482312298526]
	TIME [epoch: 55 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012366422240119666		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.012366422240119666 | validation: 0.013460464396257926]
	TIME [epoch: 55.1 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012899259705004423		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.012899259705004423 | validation: 0.012718641858655129]
	TIME [epoch: 55 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012640925686149456		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.012640925686149456 | validation: 0.013895729628046347]
	TIME [epoch: 55 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01326561787798577		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.01326561787798577 | validation: 0.01441279511027378]
	TIME [epoch: 55.2 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012478809494173425		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.012478809494173425 | validation: 0.013428900290609521]
	TIME [epoch: 55.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012566318486348168		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.012566318486348168 | validation: 0.013706214444313008]
	TIME [epoch: 55.2 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01313250967117721		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.01313250967117721 | validation: 0.013441523228789509]
	TIME [epoch: 55.2 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01277269568785654		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.01277269568785654 | validation: 0.012624313475138076]
	TIME [epoch: 55.1 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013045840660413206		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.013045840660413206 | validation: 0.01357335155131832]
	TIME [epoch: 55.1 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013275066856209664		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.013275066856209664 | validation: 0.012875765911555401]
	TIME [epoch: 55.1 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013063163002446502		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.013063163002446502 | validation: 0.012340090149488957]
	TIME [epoch: 55.1 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013052111060640786		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.013052111060640786 | validation: 0.012085093644662153]
	TIME [epoch: 55 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012731081269079712		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.012731081269079712 | validation: 0.013211536709188646]
	TIME [epoch: 55.1 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012994370126555372		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.012994370126555372 | validation: 0.014114209974164153]
	TIME [epoch: 55 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013040675765558698		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.013040675765558698 | validation: 0.012799785205415574]
	TIME [epoch: 55.1 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012950983266329492		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.012950983266329492 | validation: 0.012030154884411918]
	TIME [epoch: 55.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01346377296966283		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.01346377296966283 | validation: 0.012481326023821182]
	TIME [epoch: 54.9 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013413487636605742		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.013413487636605742 | validation: 0.014313243464733076]
	TIME [epoch: 55.1 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013211020007267508		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.013211020007267508 | validation: 0.013691446650109736]
	TIME [epoch: 55.1 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012830621186806668		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.012830621186806668 | validation: 0.013370611779876588]
	TIME [epoch: 55.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013005818410830615		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.013005818410830615 | validation: 0.012883956736400737]
	TIME [epoch: 55 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013180739640844406		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 0.013180739640844406 | validation: 0.014323406631161814]
	TIME [epoch: 55.1 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014099938958319515		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 0.014099938958319515 | validation: 0.012346294308320762]
	TIME [epoch: 55.1 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01253474287997629		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 0.01253474287997629 | validation: 0.012870779715349513]
	TIME [epoch: 55.2 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01218670145880504		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 0.01218670145880504 | validation: 0.013636829081965193]
	TIME [epoch: 54.9 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012725188648825235		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 0.012725188648825235 | validation: 0.01301041829002254]
	TIME [epoch: 55.1 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01286603333915438		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.01286603333915438 | validation: 0.013538622064836438]
	TIME [epoch: 55.1 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013155867124705418		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 0.013155867124705418 | validation: 0.013197571848266074]
	TIME [epoch: 55 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01304071888266336		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 0.01304071888266336 | validation: 0.01562449639593428]
	TIME [epoch: 54.9 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01304717611730392		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 0.01304717611730392 | validation: 0.013224919980387669]
	TIME [epoch: 55 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012908761234899193		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 0.012908761234899193 | validation: 0.012627472580577656]
	TIME [epoch: 54.9 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012518277350877316		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 0.012518277350877316 | validation: 0.01300472366573105]
	TIME [epoch: 55 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013776587236711203		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 0.013776587236711203 | validation: 0.011771041311517749]
	TIME [epoch: 55 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012834352728691326		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 0.012834352728691326 | validation: 0.013019453481226378]
	TIME [epoch: 55.1 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012734320174712852		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 0.012734320174712852 | validation: 0.013092337824193976]
	TIME [epoch: 55.1 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012646514619374877		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 0.012646514619374877 | validation: 0.013140942635132342]
	TIME [epoch: 55.1 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012849959757780114		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 0.012849959757780114 | validation: 0.014812514205282122]
	TIME [epoch: 54.9 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01352212445794404		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 0.01352212445794404 | validation: 0.015637869158262215]
	TIME [epoch: 55.1 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012591095185127472		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 0.012591095185127472 | validation: 0.013293829321833659]
	TIME [epoch: 55.1 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012611342250871243		[learning rate: 0.00011092]
	Learning Rate: 0.000110917
	LOSS [training: 0.012611342250871243 | validation: 0.013499865914758928]
	TIME [epoch: 55 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012331277744436773		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 0.012331277744436773 | validation: 0.014063309792738078]
	TIME [epoch: 55 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01327257508498444		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 0.01327257508498444 | validation: 0.0123954376470378]
	TIME [epoch: 55 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012722084943956338		[learning rate: 0.00010974]
	Learning Rate: 0.000109745
	LOSS [training: 0.012722084943956338 | validation: 0.01387497006011704]
	TIME [epoch: 55.1 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013321373310417248		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 0.013321373310417248 | validation: 0.013989243748456351]
	TIME [epoch: 55.2 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012831738151642033		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 0.012831738151642033 | validation: 0.013214257052469676]
	TIME [epoch: 55.1 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012294306343947474		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 0.012294306343947474 | validation: 0.012847161923453532]
	TIME [epoch: 55.1 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01259944272364635		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 0.01259944272364635 | validation: 0.012873131439203498]
	TIME [epoch: 55.1 sec]
	Saving model to: out/model_training/model_phi1_1a_distortion_v1r_1_v_mmd1_20250602_120011/states/model_phi1_1a_distortion_v1r_1_v_mmd1_1328.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 39056.725 seconds.
