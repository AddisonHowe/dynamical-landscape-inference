Args:
Namespace(name='model_phi1_4a_distortion_v1_2_v_mmd1', outdir='out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1', training_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/training', validation_data='data/training_data/distortions/paraboloids/data_phi1_4a_distortion_v1_2/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2250704520

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.437783458607986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.437783458607986 | validation: 6.737816864396257]
	TIME [epoch: 129 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.26405088678838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.26405088678838 | validation: 6.297772867830014]
	TIME [epoch: 0.483 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.913971773655184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.913971773655184 | validation: 6.233476382811457]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.829522758695321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.829522758695321 | validation: 6.186325270552436]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.378586045682738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.378586045682738 | validation: 7.315602670151631]
	TIME [epoch: 0.476 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.05345514998721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.05345514998721 | validation: 7.304486484410998]
	TIME [epoch: 0.474 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 7.039907684971031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.039907684971031 | validation: 7.247941249519127]
	TIME [epoch: 0.477 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.966246300921678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.966246300921678 | validation: 6.978256798933088]
	TIME [epoch: 0.476 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.754051711779089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.754051711779089 | validation: 6.559937692092899]
	TIME [epoch: 0.473 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.087690795211363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.087690795211363 | validation: 6.386731071163834]
	TIME [epoch: 0.475 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.962361953327407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.962361953327407 | validation: 6.236550649226985]
	TIME [epoch: 0.474 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.7237626379783775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7237626379783775 | validation: 6.075804048928009]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.227407581784871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.227407581784871 | validation: 6.168954235048005]
	TIME [epoch: 0.475 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.97901319999817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.97901319999817 | validation: 6.23782598142072]
	TIME [epoch: 0.475 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.892811429016399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.892811429016399 | validation: 5.8769893539701785]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4808743785870435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4808743785870435 | validation: 6.2218863275164455]
	TIME [epoch: 0.477 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.131806419428645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.131806419428645 | validation: 6.130652382797315]
	TIME [epoch: 0.479 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.891493854734104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.891493854734104 | validation: 5.934950741699311]
	TIME [epoch: 0.474 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3991596305105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3991596305105 | validation: 5.735009343256607]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.28107513626285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.28107513626285 | validation: 6.110896355464341]
	TIME [epoch: 0.475 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.595384792084596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.595384792084596 | validation: 5.7632957508533185]
	TIME [epoch: 0.473 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9821388014377206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9821388014377206 | validation: 5.846489156783538]
	TIME [epoch: 0.473 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.018178609250827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.018178609250827 | validation: 5.7537224743324344]
	TIME [epoch: 0.473 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.938229582033713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.938229582033713 | validation: 5.888688029254151]
	TIME [epoch: 0.474 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006874416492203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.006874416492203 | validation: 5.666053700734387]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.248392624758269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.248392624758269 | validation: 5.870336304629925]
	TIME [epoch: 0.474 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001350149571261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.001350149571261 | validation: 5.67841592468571]
	TIME [epoch: 0.473 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8864911644343865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8864911644343865 | validation: 5.807599953509925]
	TIME [epoch: 0.475 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8921620710067635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8921620710067635 | validation: 5.622138818391718]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9900722461691998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9900722461691998 | validation: 5.834288872290138]
	TIME [epoch: 0.474 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.976213649987617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.976213649987617 | validation: 5.628085480829387]
	TIME [epoch: 0.473 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.916764593531825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.916764593531825 | validation: 5.762529140912333]
	TIME [epoch: 0.475 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8419253625390533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8419253625390533 | validation: 5.58831172356953]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.871566121818704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.871566121818704 | validation: 5.762337238028432]
	TIME [epoch: 0.476 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.860628918646245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.860628918646245 | validation: 5.557883459329197]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8410388901771793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8410388901771793 | validation: 5.711727651535466]
	TIME [epoch: 0.475 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7940469440999176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7940469440999176 | validation: 5.537564116312511]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7804366476241613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7804366476241613 | validation: 5.693153702588912]
	TIME [epoch: 0.475 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.766096101478955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.766096101478955 | validation: 5.512548796023752]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7817037852946607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7817037852946607 | validation: 5.667687223094552]
	TIME [epoch: 0.475 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7556690451246744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7556690451246744 | validation: 5.512022237623035]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7677826954319618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7677826954319618 | validation: 5.618029477655685]
	TIME [epoch: 0.476 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7001872467874506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7001872467874506 | validation: 5.486283932118372]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.716914099964061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.716914099964061 | validation: 5.590194094929604]
	TIME [epoch: 0.474 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6695659058352663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6695659058352663 | validation: 5.4284205851691425]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.690905060276133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.690905060276133 | validation: 5.591633237768597]
	TIME [epoch: 0.475 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6938121754725692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6938121754725692 | validation: 5.395928612310029]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6154036398761993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6154036398761993 | validation: 5.547082987808567]
	TIME [epoch: 0.474 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.650882414427215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.650882414427215 | validation: 5.38716895417755]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.569581109452299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.569581109452299 | validation: 5.480825032106397]
	TIME [epoch: 0.476 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5754444080216183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5754444080216183 | validation: 5.353862953285939]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5399377780399206		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.5399377780399206 | validation: 5.468644526213805]
	TIME [epoch: 0.474 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.55278072020788		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.55278072020788 | validation: 5.320542664240479]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.507988810461545		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.507988810461545 | validation: 5.4350664484673095]
	TIME [epoch: 0.474 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.528828058942563		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.528828058942563 | validation: 5.313948382232127]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.609490802891015		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.609490802891015 | validation: 5.4435866801615775]
	TIME [epoch: 0.476 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6036526859313756		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.6036526859313756 | validation: 5.310982355207556]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5981929418925853		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.5981929418925853 | validation: 5.344698660042394]
	TIME [epoch: 0.474 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4366320578966314		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.4366320578966314 | validation: 5.239136753162292]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4430398869024943		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.4430398869024943 | validation: 5.352173195007133]
	TIME [epoch: 0.475 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4672715634194735		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.4672715634194735 | validation: 5.213819282289638]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4008673480159644		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.4008673480159644 | validation: 5.304327632581558]
	TIME [epoch: 0.476 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.399775569859278		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.399775569859278 | validation: 5.198484201213507]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.46629177704672		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.46629177704672 | validation: 5.307896781830084]
	TIME [epoch: 0.48 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4517532945142206		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.4517532945142206 | validation: 5.177075384008318]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.429647523233317		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.429647523233317 | validation: 5.225290335205812]
	TIME [epoch: 0.475 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.357924255389164		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.357924255389164 | validation: 5.134003420119218]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.348504408671495		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.348504408671495 | validation: 5.2086490781653305]
	TIME [epoch: 0.475 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3589503366411084		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.3589503366411084 | validation: 5.104299411732926]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3480919670680773		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.3480919670680773 | validation: 5.17050016244003]
	TIME [epoch: 0.476 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.339354017676017		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.339354017676017 | validation: 5.078652841178347]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3348835828775565		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.3348835828775565 | validation: 5.13121343104494]
	TIME [epoch: 0.476 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3109000888579034		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.3109000888579034 | validation: 5.044745740982794]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.318959608575923		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.318959608575923 | validation: 5.1002529012298305]
	TIME [epoch: 0.475 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2904086874041103		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.2904086874041103 | validation: 4.994613793700765]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2696660162272138		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.2696660162272138 | validation: 5.0604753266037195]
	TIME [epoch: 0.476 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.259991715519925		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.259991715519925 | validation: 4.969557364930642]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.24264722249574		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.24264722249574 | validation: 5.02799947950182]
	TIME [epoch: 0.474 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.236198201812747		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.236198201812747 | validation: 4.926939746800015]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.236469846699896		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.236469846699896 | validation: 4.986003453560091]
	TIME [epoch: 0.475 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2259175256476453		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.2259175256476453 | validation: 4.886893984384733]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2187754034686247		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.2187754034686247 | validation: 4.936271661013446]
	TIME [epoch: 0.476 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1956098036434355		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.1956098036434355 | validation: 4.854899426289845]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1867766009636034		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.1867766009636034 | validation: 4.908894761052359]
	TIME [epoch: 0.474 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.174070548838583		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.174070548838583 | validation: 4.807462992792421]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1647385107716226		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.1647385107716226 | validation: 4.87816069672256]
	TIME [epoch: 0.474 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1678258054310393		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.1678258054310393 | validation: 4.77895393539579]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1361741838544437		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 3.1361741838544437 | validation: 4.8361705968469915]
	TIME [epoch: 0.476 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.115469723750973		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.115469723750973 | validation: 4.74278658849122]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1132958016683507		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.1132958016683507 | validation: 4.786048693614453]
	TIME [epoch: 0.473 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.114178718272793		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 3.114178718272793 | validation: 4.718899934113]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1397731898081362		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 3.1397731898081362 | validation: 4.767710924896387]
	TIME [epoch: 0.476 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1106733407359957		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 3.1106733407359957 | validation: 4.683992837774726]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1003330867899015		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.1003330867899015 | validation: 4.715522956355978]
	TIME [epoch: 0.474 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0695327846444433		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 3.0695327846444433 | validation: 4.642135051117665]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0624223485548487		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 3.0624223485548487 | validation: 4.684222916929343]
	TIME [epoch: 0.476 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.067415332646994		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 3.067415332646994 | validation: 4.616934948395463]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.078459435570345		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 3.078459435570345 | validation: 4.665886170026879]
	TIME [epoch: 0.475 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0605666376285274		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 3.0605666376285274 | validation: 4.5822145938317265]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.035931527586024		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 3.035931527586024 | validation: 4.616753578578861]
	TIME [epoch: 0.476 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0248717113160413		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 3.0248717113160413 | validation: 4.561665502701535]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.024838185884662		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 3.024838185884662 | validation: 4.590985153399788]
	TIME [epoch: 0.474 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0225111295091955		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 3.0225111295091955 | validation: 4.529734072774228]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.040400390863579		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 3.040400390863579 | validation: 4.586333888035896]
	TIME [epoch: 0.475 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0264119100832536		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 3.0264119100832536 | validation: 4.484577489304991]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.017321675689458		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 3.017321675689458 | validation: 4.51945376974647]
	TIME [epoch: 0.474 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.987090069149975		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.987090069149975 | validation: 4.433883183893938]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.976146826694259		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.976146826694259 | validation: 4.459685696797808]
	TIME [epoch: 0.475 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9426421113389596		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.9426421113389596 | validation: 4.422193138183391]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8138856969570196		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.8138856969570196 | validation: 4.309761819708807]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7703483050578646		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.7703483050578646 | validation: 4.318649136779736]
	TIME [epoch: 0.475 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7528861873439836		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.7528861873439836 | validation: 4.37704550965193]
	TIME [epoch: 0.474 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.782938402042855		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.782938402042855 | validation: 4.257556516301053]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.713101885557151		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.713101885557151 | validation: 4.210494714857135]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.636409408870138		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.636409408870138 | validation: 4.245339796012709]
	TIME [epoch: 0.474 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6750922963730206		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.6750922963730206 | validation: 4.244172320339345]
	TIME [epoch: 0.473 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6172913316132935		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.6172913316132935 | validation: 4.158568023507438]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6057312283760297		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.6057312283760297 | validation: 4.1903083929373155]
	TIME [epoch: 0.475 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5855820742189417		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 2.5855820742189417 | validation: 4.164887160356824]
	TIME [epoch: 0.474 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.567226616460538		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 2.567226616460538 | validation: 4.12786044923833]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.55276483180286		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 2.55276483180286 | validation: 4.108147129826799]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.547977109589404		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 2.547977109589404 | validation: 4.056393126457276]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5375710757908885		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 2.5375710757908885 | validation: 4.0048157017099575]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4994416059861058		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 2.4994416059861058 | validation: 4.184454700581568]
	TIME [epoch: 0.478 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7973470461053354		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 2.7973470461053354 | validation: 3.8905126395107845]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4587261088106867		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 2.4587261088106867 | validation: 3.888756747996016]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.405807153023574		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 2.405807153023574 | validation: 4.02125647773412]
	TIME [epoch: 0.476 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.523922150567762		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 2.523922150567762 | validation: 3.8409678811047416]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4467734283469156		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 2.4467734283469156 | validation: 3.9107835350482385]
	TIME [epoch: 0.474 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4148665019398736		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 2.4148665019398736 | validation: 3.8745211777197106]
	TIME [epoch: 0.474 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3645050719564775		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 2.3645050719564775 | validation: 3.8498917215430604]
	TIME [epoch: 0.474 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3641909618730965		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 2.3641909618730965 | validation: 3.822125261565002]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3499933725386333		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 2.3499933725386333 | validation: 3.7467094600264503]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3469127501489355		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 2.3469127501489355 | validation: 3.920867841521525]
	TIME [epoch: 0.474 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4631934885372866		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 2.4631934885372866 | validation: 3.7800584517732734]
	TIME [epoch: 0.473 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3594195578235766		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 2.3594195578235766 | validation: 3.7935237688024763]
	TIME [epoch: 0.473 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3445403838470633		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 2.3445403838470633 | validation: 3.8794511049582017]
	TIME [epoch: 0.474 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3848480366431426		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 2.3848480366431426 | validation: 3.7509429694526535]
	TIME [epoch: 0.476 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3667180864521344		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 2.3667180864521344 | validation: 3.707273997402983]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.371720480081407		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 2.371720480081407 | validation: 3.8817622161846153]
	TIME [epoch: 0.476 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4332677655655206		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 2.4332677655655206 | validation: 3.9117415260387802]
	TIME [epoch: 0.476 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4600761289318895		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 2.4600761289318895 | validation: 3.7474725896539436]
	TIME [epoch: 0.475 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4138041719622447		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 2.4138041719622447 | validation: 3.827485163861701]
	TIME [epoch: 0.476 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.36194174079396		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 2.36194174079396 | validation: 3.839902490095726]
	TIME [epoch: 0.474 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6462192240221705		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 2.6462192240221705 | validation: 3.695081746249716]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.414879433355057		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 2.414879433355057 | validation: 3.8316657521957027]
	TIME [epoch: 0.477 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4157333028214363		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 2.4157333028214363 | validation: 3.6725725956551396]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3290245450938523		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 2.3290245450938523 | validation: 3.771128935326958]
	TIME [epoch: 0.475 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.34280577517753		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 2.34280577517753 | validation: 3.7086546144303014]
	TIME [epoch: 0.475 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3258522416242537		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 2.3258522416242537 | validation: 3.653569625324588]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.323436281961564		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 2.323436281961564 | validation: 3.6548067264864947]
	TIME [epoch: 0.475 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.306548080414888		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 2.306548080414888 | validation: 3.67882232776957]
	TIME [epoch: 0.474 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.306183715602853		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 2.306183715602853 | validation: 3.7948950126545435]
	TIME [epoch: 0.475 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3697376713567353		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 2.3697376713567353 | validation: 3.6678973029681674]
	TIME [epoch: 0.48 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3163451243353137		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 2.3163451243353137 | validation: 3.635043493033116]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3469472968296325		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 2.3469472968296325 | validation: 3.692987812204598]
	TIME [epoch: 0.475 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3078387393811854		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 2.3078387393811854 | validation: 3.6943439777308145]
	TIME [epoch: 0.475 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.314123887097357		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 2.314123887097357 | validation: 3.7373729235324875]
	TIME [epoch: 0.477 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.332030017747347		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 2.332030017747347 | validation: 3.7113517356889973]
	TIME [epoch: 0.474 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.309299220103361		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 2.309299220103361 | validation: 3.7320335326061866]
	TIME [epoch: 0.474 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.316436602614189		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 2.316436602614189 | validation: 3.771685577145502]
	TIME [epoch: 0.475 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.324000124190063		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 2.324000124190063 | validation: 3.6882438791967207]
	TIME [epoch: 0.475 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2977442702072564		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 2.2977442702072564 | validation: 3.7127140918429533]
	TIME [epoch: 0.474 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.313135041973789		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 2.313135041973789 | validation: 3.59821328675455]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2984570208556403		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 2.2984570208556403 | validation: 3.5733957460595236]
	TIME [epoch: 0.476 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2857811838227566		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 2.2857811838227566 | validation: 3.6136859322480257]
	TIME [epoch: 0.475 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.276576540380627		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 2.276576540380627 | validation: 3.5736087538860772]
	TIME [epoch: 0.474 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.271300433526317		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 2.271300433526317 | validation: 3.621466049050042]
	TIME [epoch: 0.475 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.280501344593215		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 2.280501344593215 | validation: 3.549768725680167]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2814606037276457		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 2.2814606037276457 | validation: 3.7712677369883347]
	TIME [epoch: 0.475 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.726986075711336		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 2.726986075711336 | validation: 3.6824283309147776]
	TIME [epoch: 0.475 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.414905659015495		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 2.414905659015495 | validation: 3.8043064053015527]
	TIME [epoch: 0.476 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4571762328721385		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 2.4571762328721385 | validation: 3.612652919938063]
	TIME [epoch: 0.475 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.299874111938745		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 2.299874111938745 | validation: 3.5917486536956944]
	TIME [epoch: 0.475 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.28619601043397		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 2.28619601043397 | validation: 3.739512882281052]
	TIME [epoch: 0.474 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.317813822136605		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 2.317813822136605 | validation: 3.6619731574011753]
	TIME [epoch: 0.474 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.279942732336327		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 2.279942732336327 | validation: 3.635676360342408]
	TIME [epoch: 0.475 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.262598451456936		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 2.262598451456936 | validation: 3.6074851063646864]
	TIME [epoch: 0.475 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2534145181877174		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 2.2534145181877174 | validation: 3.63010165808285]
	TIME [epoch: 0.475 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2592513511571077		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 2.2592513511571077 | validation: 4.018786396694567]
	TIME [epoch: 0.474 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.242771531843668		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 3.242771531843668 | validation: 4.0617048171596455]
	TIME [epoch: 0.474 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.234638812408448		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 3.234638812408448 | validation: 3.76579816278677]
	TIME [epoch: 0.474 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5390987632546222		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 2.5390987632546222 | validation: 3.6496233315762527]
	TIME [epoch: 0.475 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.43329588076763		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 2.43329588076763 | validation: 3.570133814185189]
	TIME [epoch: 0.474 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.340387640162761		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 2.340387640162761 | validation: 3.5403023410282035]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.303096702078765		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 2.303096702078765 | validation: 3.5407805618407373]
	TIME [epoch: 0.475 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.252230929403996		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 2.252230929403996 | validation: 3.513682079563491]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.24864409426772		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 2.24864409426772 | validation: 3.5173796912195656]
	TIME [epoch: 0.474 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.229867097887707		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 2.229867097887707 | validation: 3.5158154786917715]
	TIME [epoch: 0.474 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.234436823827057		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 2.234436823827057 | validation: 3.4907109652392805]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.244235216843291		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 2.244235216843291 | validation: 3.489735396707967]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.248396396035832		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 2.248396396035832 | validation: 3.494132328494565]
	TIME [epoch: 0.474 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.238734042312409		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 2.238734042312409 | validation: 3.4976525992674503]
	TIME [epoch: 0.474 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2419675052635633		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 2.2419675052635633 | validation: 3.5017430891786483]
	TIME [epoch: 0.475 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.231949001205636		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 2.231949001205636 | validation: 3.486783852349616]
	TIME [epoch: 0.473 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2268213471911023		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 2.2268213471911023 | validation: 3.490902258279448]
	TIME [epoch: 0.476 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2270279530102806		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 2.2270279530102806 | validation: 3.4731372123446436]
	TIME [epoch: 0.475 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2137742364113877		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 2.2137742364113877 | validation: 3.4461640339380084]
	TIME [epoch: 0.474 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2184625208535222		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 2.2184625208535222 | validation: 3.474706600209972]
	TIME [epoch: 0.474 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.214562613056112		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 2.214562613056112 | validation: 3.4812881232359545]
	TIME [epoch: 0.476 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.22610499334203		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 2.22610499334203 | validation: 3.6001554606001522]
	TIME [epoch: 136 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2706178143297215		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 2.2706178143297215 | validation: 3.558747530320929]
	TIME [epoch: 0.936 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2341706428332135		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 2.2341706428332135 | validation: 3.530312236200476]
	TIME [epoch: 0.926 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.222008101763616		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 2.222008101763616 | validation: 3.480212007076942]
	TIME [epoch: 0.927 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2157891042321602		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 2.2157891042321602 | validation: 3.436073863910721]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2133880801222015		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 2.2133880801222015 | validation: 3.4924379550132714]
	TIME [epoch: 0.941 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2009071618463536		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 2.2009071618463536 | validation: 3.4186818858698818]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.189287365065042		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 2.189287365065042 | validation: 3.3804679292178124]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.18233104408704		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 2.18233104408704 | validation: 3.4085943659622533]
	TIME [epoch: 0.93 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.178109288300806		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 2.178109288300806 | validation: 3.545043393386135]
	TIME [epoch: 0.928 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2243175475005192		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 2.2243175475005192 | validation: 3.3296360524279707]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.188859485030536		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 2.188859485030536 | validation: 3.333644954795711]
	TIME [epoch: 0.926 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1392260819480065		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 2.1392260819480065 | validation: 3.440878621053942]
	TIME [epoch: 0.925 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1611530913043904		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 2.1611530913043904 | validation: 3.3541968126558817]
	TIME [epoch: 0.924 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5535270796477274		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 2.5535270796477274 | validation: 3.3055888042880768]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.292693101838677		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 2.292693101838677 | validation: 3.2938389661704477]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.189762397019343		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 2.189762397019343 | validation: 3.3034959553685996]
	TIME [epoch: 0.93 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1408535889840397		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 2.1408535889840397 | validation: 3.2422328884004075]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0852589300181474		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 2.0852589300181474 | validation: 2.9401009197101144]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0328407680331573		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 2.0328407680331573 | validation: 2.827038776496219]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0039464787322787		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 2.0039464787322787 | validation: 2.6223014935361957]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.917595762966104		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.917595762966104 | validation: 2.3757402001329644]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7142915436436805		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 1.7142915436436805 | validation: 4.375449390932857]
	TIME [epoch: 0.929 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.917667255007327		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 2.917667255007327 | validation: 4.404458747112506]
	TIME [epoch: 0.927 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.96389179689374		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 2.96389179689374 | validation: 4.338240453961593]
	TIME [epoch: 0.927 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.831865556201174		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 2.831865556201174 | validation: 4.2251164650316895]
	TIME [epoch: 0.927 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6727153886867003		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 2.6727153886867003 | validation: 4.1217206382155664]
	TIME [epoch: 0.929 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.577902419865662		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 2.577902419865662 | validation: 4.052145092901995]
	TIME [epoch: 0.927 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5358085864384985		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 2.5358085864384985 | validation: 4.020565505990109]
	TIME [epoch: 0.93 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.540707538930277		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 2.540707538930277 | validation: 4.015572217745065]
	TIME [epoch: 0.929 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5421108423566445		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 2.5421108423566445 | validation: 3.987919663870807]
	TIME [epoch: 0.928 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5283850984346317		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 2.5283850984346317 | validation: 4.007069628692853]
	TIME [epoch: 0.927 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.513741289239032		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 2.513741289239032 | validation: 3.9859544238996225]
	TIME [epoch: 0.927 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.507014916079132		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 2.507014916079132 | validation: 3.9861502984321904]
	TIME [epoch: 0.928 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4908696680723916		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 2.4908696680723916 | validation: 3.927413405980716]
	TIME [epoch: 0.929 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4621243010551064		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 2.4621243010551064 | validation: 3.7866647389776675]
	TIME [epoch: 0.928 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3722924708497146		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 2.3722924708497146 | validation: 3.7919418135824774]
	TIME [epoch: 0.931 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3761094473000477		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 2.3761094473000477 | validation: 3.691717065449632]
	TIME [epoch: 0.927 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3134509725218404		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 2.3134509725218404 | validation: 3.627727380480535]
	TIME [epoch: 0.928 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2808347153536945		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 2.2808347153536945 | validation: 3.5645994255312865]
	TIME [epoch: 0.927 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2663028981119884		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 2.2663028981119884 | validation: 3.4706204682120187]
	TIME [epoch: 0.928 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2677187667735983		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 2.2677187667735983 | validation: 3.467562445138512]
	TIME [epoch: 0.929 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2409465484444815		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 2.2409465484444815 | validation: 3.471682429548154]
	TIME [epoch: 0.927 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2320008204686603		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 2.2320008204686603 | validation: 3.463982955112681]
	TIME [epoch: 0.928 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.235672881352306		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 2.235672881352306 | validation: 3.4237941860765204]
	TIME [epoch: 0.927 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2281667471520303		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 2.2281667471520303 | validation: 3.338026833806298]
	TIME [epoch: 0.929 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.227287322931538		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 2.227287322931538 | validation: 3.36937783961732]
	TIME [epoch: 0.933 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.209506378905249		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 2.209506378905249 | validation: 3.3462482454919824]
	TIME [epoch: 0.928 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.213998629182263		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 2.213998629182263 | validation: 3.3653813297216235]
	TIME [epoch: 0.927 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2061640648635477		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 2.2061640648635477 | validation: 3.417800754178586]
	TIME [epoch: 0.928 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2220331978639978		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 2.2220331978639978 | validation: 3.3027749764729735]
	TIME [epoch: 0.928 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2472426807637915		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 2.2472426807637915 | validation: 3.368688249674965]
	TIME [epoch: 0.927 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1984041544841335		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 2.1984041544841335 | validation: 3.387067804698952]
	TIME [epoch: 0.926 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1978269481717727		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 2.1978269481717727 | validation: 3.3749769140980153]
	TIME [epoch: 0.929 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1907039390907532		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 2.1907039390907532 | validation: 3.383026181345702]
	TIME [epoch: 0.927 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1935235935226447		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 2.1935235935226447 | validation: 3.3320167517033648]
	TIME [epoch: 0.926 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1919668175928164		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 2.1919668175928164 | validation: 3.3310111946266674]
	TIME [epoch: 0.929 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.169732831317315		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 2.169732831317315 | validation: 3.277945764807038]
	TIME [epoch: 0.926 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1689499589727634		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 2.1689499589727634 | validation: 3.3586781035288023]
	TIME [epoch: 0.926 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1831691304557133		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 2.1831691304557133 | validation: 3.222358117080684]
	TIME [epoch: 0.925 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1759095065631935		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 2.1759095065631935 | validation: 3.280458000029945]
	TIME [epoch: 0.926 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.162307941855626		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 2.162307941855626 | validation: 3.1863175356599993]
	TIME [epoch: 0.925 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.314289488960855		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 2.314289488960855 | validation: 3.467828240101212]
	TIME [epoch: 0.926 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2839368729168323		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 2.2839368729168323 | validation: 3.248530846188265]
	TIME [epoch: 0.925 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1955970946314656		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 2.1955970946314656 | validation: 3.2766756748150017]
	TIME [epoch: 0.924 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2608324587015436		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 2.2608324587015436 | validation: 3.2224061596721927]
	TIME [epoch: 0.925 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.197153663529878		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 2.197153663529878 | validation: 3.263711975388155]
	TIME [epoch: 0.926 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.163118759490476		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 2.163118759490476 | validation: 3.2816644453402546]
	TIME [epoch: 0.923 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1852639635056876		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 2.1852639635056876 | validation: 3.2156933028725705]
	TIME [epoch: 0.927 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.15669254062673		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 2.15669254062673 | validation: 3.1642624825308268]
	TIME [epoch: 0.924 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.150583563939479		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 2.150583563939479 | validation: 3.140068847197831]
	TIME [epoch: 0.926 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1471209933827895		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 2.1471209933827895 | validation: 3.1463224114046278]
	TIME [epoch: 0.923 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1254124871080924		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 2.1254124871080924 | validation: 3.125088867289763]
	TIME [epoch: 0.926 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1124076698827627		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 2.1124076698827627 | validation: 3.0838376158952325]
	TIME [epoch: 0.923 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1026516010223975		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 2.1026516010223975 | validation: 3.0609868422717152]
	TIME [epoch: 0.926 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.08265749859986		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 2.08265749859986 | validation: 3.092998624677276]
	TIME [epoch: 0.925 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0879577003415726		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 2.0879577003415726 | validation: 2.9280691612778784]
	TIME [epoch: 0.926 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0256782859875084		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 2.0256782859875084 | validation: 2.643140024274902]
	TIME [epoch: 0.926 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.939766257074582		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 1.939766257074582 | validation: 2.048120027989682]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.709902857715182		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 1.709902857715182 | validation: 1.310370213784961]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3603810570482582		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 1.3603810570482582 | validation: 2.1015666185056587]
	TIME [epoch: 0.926 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9080981958941294		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 1.9080981958941294 | validation: 1.0765223318700994]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3370943238232955		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 1.3370943238232955 | validation: 1.0599508444964176]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2515896201381518		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 1.2515896201381518 | validation: 0.9553824674545583]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1326506520477932		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 1.1326506520477932 | validation: 0.7558551840533596]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0738509537857568		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 1.0738509537857568 | validation: 0.7613563998263642]
	TIME [epoch: 0.926 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0733754137895608		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 1.0733754137895608 | validation: 0.7067952557453646]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4270005577607474		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 1.4270005577607474 | validation: 0.7948877450368999]
	TIME [epoch: 0.927 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0607477118116688		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 1.0607477118116688 | validation: 0.8837833847611268]
	TIME [epoch: 0.925 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0859656298102018		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 1.0859656298102018 | validation: 0.71297312845595]
	TIME [epoch: 0.925 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4905724671721754		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 1.4905724671721754 | validation: 0.7314272011653316]
	TIME [epoch: 0.925 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4363725497894593		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 1.4363725497894593 | validation: 0.669223979801328]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1679750018847173		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 1.1679750018847173 | validation: 0.6497547231049885]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1014565048161737		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 1.1014565048161737 | validation: 0.6449885140196687]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086248406270481		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 1.086248406270481 | validation: 0.6408716896234324]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0787479769258128		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 1.0787479769258128 | validation: 0.6070754126661847]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044190194278993		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 1.044190194278993 | validation: 0.6008023687401969]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0469057684603313		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 1.0469057684603313 | validation: 0.5796422824046202]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0329590784849152		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 1.0329590784849152 | validation: 0.5736915553009567]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.985683688108211		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.985683688108211 | validation: 0.529558519088263]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.910518220763597		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.910518220763597 | validation: 0.6432332489241654]
	TIME [epoch: 0.928 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9206368707266941		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.9206368707266941 | validation: 0.6853597909879228]
	TIME [epoch: 0.927 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906880001274307		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.906880001274307 | validation: 0.6599117371044549]
	TIME [epoch: 0.927 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714016719088153		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.8714016719088153 | validation: 0.6296792050329778]
	TIME [epoch: 0.933 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853788337607709		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.853788337607709 | validation: 0.5759025694498509]
	TIME [epoch: 0.927 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8254354079373286		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.8254354079373286 | validation: 0.5394536162656473]
	TIME [epoch: 0.927 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105955879877599		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.8105955879877599 | validation: 0.5413807605275862]
	TIME [epoch: 0.927 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034096697946295		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.8034096697946295 | validation: 0.5183492223181649]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812729945118531		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.7812729945118531 | validation: 0.5307211108876314]
	TIME [epoch: 0.925 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723874450381513		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.7723874450381513 | validation: 0.5415932758920855]
	TIME [epoch: 0.925 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467404541229203		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.7467404541229203 | validation: 0.5443531891713932]
	TIME [epoch: 0.925 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7360038414468375		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.7360038414468375 | validation: 0.5279531625900473]
	TIME [epoch: 0.926 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202257266130424		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.7202257266130424 | validation: 0.5144996525738472]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193423287042345		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.7193423287042345 | validation: 0.48495268442795486]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7054653684221662		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.7054653684221662 | validation: 0.5000274678949873]
	TIME [epoch: 0.926 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693968050436655		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.693968050436655 | validation: 0.5200137353906655]
	TIME [epoch: 0.925 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007155907484435		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7007155907484435 | validation: 0.5029256035795399]
	TIME [epoch: 0.925 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988544080865944		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6988544080865944 | validation: 0.5042673970047987]
	TIME [epoch: 0.926 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906570913227376		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6906570913227376 | validation: 0.5005601241426378]
	TIME [epoch: 0.925 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833015953542741		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6833015953542741 | validation: 0.48073358878444]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674264832845808		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.674264832845808 | validation: 0.45020894675017087]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746114278606774		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6746114278606774 | validation: 0.48639899179902746]
	TIME [epoch: 0.927 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637918968609734		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6637918968609734 | validation: 0.44909251644267434]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655692861311664		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.655692861311664 | validation: 0.4407337775467166]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600501344200612		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6600501344200612 | validation: 0.4349683355983587]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629964584322334		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6629964584322334 | validation: 0.4450959973857367]
	TIME [epoch: 0.928 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449596442398766		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6449596442398766 | validation: 0.4993130724651474]
	TIME [epoch: 0.93 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854395995415967		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6854395995415967 | validation: 0.4764772803220025]
	TIME [epoch: 0.927 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703978926469119		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.6703978926469119 | validation: 0.811546707166519]
	TIME [epoch: 0.928 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998724378707845		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.7998724378707845 | validation: 0.6522563380448493]
	TIME [epoch: 0.927 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094187633219117		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.7094187633219117 | validation: 0.5849149648527331]
	TIME [epoch: 0.928 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6735938614149629		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.6735938614149629 | validation: 0.523976131707491]
	TIME [epoch: 0.933 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553249843927419		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6553249843927419 | validation: 0.482742577343059]
	TIME [epoch: 0.928 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237762875990872		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6237762875990872 | validation: 0.469899241782781]
	TIME [epoch: 0.928 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177278386369593		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.6177278386369593 | validation: 0.4241482727814145]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056684431944591		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6056684431944591 | validation: 0.40357373818164616]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981081437367715		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.5981081437367715 | validation: 0.39585219213512457]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899739396111271		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.5899739396111271 | validation: 0.39968139762305266]
	TIME [epoch: 0.928 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6020691198820773		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6020691198820773 | validation: 0.4970257833731612]
	TIME [epoch: 0.928 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6210916614281791		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6210916614281791 | validation: 0.43996945001875964]
	TIME [epoch: 0.928 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994789359363811		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.5994789359363811 | validation: 0.3777148304601617]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845050967327892		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.5845050967327892 | validation: 0.3570907402356025]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5766874276125917		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5766874276125917 | validation: 0.3841283331477205]
	TIME [epoch: 0.927 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5734471703361281		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5734471703361281 | validation: 0.36210193211541736]
	TIME [epoch: 0.927 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5584467421242445		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5584467421242445 | validation: 0.3754198299687337]
	TIME [epoch: 0.927 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609646828081093		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5609646828081093 | validation: 0.3754180200321215]
	TIME [epoch: 0.926 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454052680928954		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5454052680928954 | validation: 0.3522101688266222]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5547557067069818		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5547557067069818 | validation: 0.3563689615380883]
	TIME [epoch: 0.927 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496994117679826		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5496994117679826 | validation: 0.36833156924046445]
	TIME [epoch: 0.927 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5386418825129371		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5386418825129371 | validation: 0.4011027939357499]
	TIME [epoch: 0.927 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536253740026523		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.536253740026523 | validation: 0.3845481719947563]
	TIME [epoch: 0.927 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.532448096286803		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.532448096286803 | validation: 0.3800923883843877]
	TIME [epoch: 0.927 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5475701042912126		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5475701042912126 | validation: 0.43911770932679045]
	TIME [epoch: 0.927 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5319281547314187		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.5319281547314187 | validation: 0.364965570490861]
	TIME [epoch: 0.927 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203113448298415		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.5203113448298415 | validation: 0.3286708162402521]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5156165469342865		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5156165469342865 | validation: 0.3343910113750453]
	TIME [epoch: 0.927 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5042541246031728		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5042541246031728 | validation: 0.36517388327065964]
	TIME [epoch: 0.929 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010143385364685		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.5010143385364685 | validation: 0.3392688567945236]
	TIME [epoch: 0.928 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877191954482305		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.4877191954482305 | validation: 0.32048933925005463]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48974354178716784		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.48974354178716784 | validation: 0.3843625673120752]
	TIME [epoch: 0.927 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4821210227273343		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.4821210227273343 | validation: 0.30780945121826697]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4704235410381843		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.4704235410381843 | validation: 0.3251976994929813]
	TIME [epoch: 0.928 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45665302310570355		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.45665302310570355 | validation: 0.3492971847484674]
	TIME [epoch: 0.926 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4569546785061642		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.4569546785061642 | validation: 0.311369393917887]
	TIME [epoch: 0.926 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4610281431719973		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.4610281431719973 | validation: 0.3408428403658349]
	TIME [epoch: 0.927 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45503638179420874		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.45503638179420874 | validation: 0.32715419914410654]
	TIME [epoch: 0.926 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45189381657186645		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.45189381657186645 | validation: 0.36294724014813584]
	TIME [epoch: 0.928 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44875637769815646		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.44875637769815646 | validation: 0.3931005361458227]
	TIME [epoch: 0.926 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.437694790190467		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.437694790190467 | validation: 0.29896125490130326]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4275595048774002		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.4275595048774002 | validation: 0.31309513957984125]
	TIME [epoch: 0.927 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41727577859516735		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.41727577859516735 | validation: 0.2717238739844522]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4232381227198054		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.4232381227198054 | validation: 0.3895959302842471]
	TIME [epoch: 0.925 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4352411761980557		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.4352411761980557 | validation: 0.295980272366054]
	TIME [epoch: 0.925 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4107105762324031		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4107105762324031 | validation: 0.29344624185334356]
	TIME [epoch: 0.925 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3986669822690647		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.3986669822690647 | validation: 0.3628675381051746]
	TIME [epoch: 0.925 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3924706855651286		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3924706855651286 | validation: 0.27815059828683175]
	TIME [epoch: 0.924 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38795866191534345		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.38795866191534345 | validation: 0.31016191761528383]
	TIME [epoch: 0.925 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3776827378202345		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.3776827378202345 | validation: 0.29452485898129577]
	TIME [epoch: 0.924 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36229014400571175		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.36229014400571175 | validation: 0.29796557648997796]
	TIME [epoch: 0.925 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36517852179609006		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.36517852179609006 | validation: 0.29109662925957364]
	TIME [epoch: 0.924 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594168384416541		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.3594168384416541 | validation: 0.34208114887365704]
	TIME [epoch: 0.925 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43401708208878637		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.43401708208878637 | validation: 0.3248456855213249]
	TIME [epoch: 0.924 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3705078458486669		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.3705078458486669 | validation: 0.31000750687519946]
	TIME [epoch: 0.925 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3547033696241656		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3547033696241656 | validation: 0.21722966553355622]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812134037546217		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.3812134037546217 | validation: 0.31991184749493323]
	TIME [epoch: 0.925 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35973352105454837		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.35973352105454837 | validation: 0.23985965411897137]
	TIME [epoch: 0.926 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36098816555686186		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.36098816555686186 | validation: 0.3073279446136811]
	TIME [epoch: 0.925 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31984791639025684		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.31984791639025684 | validation: 0.301566836907766]
	TIME [epoch: 0.925 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3210052828256689		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.3210052828256689 | validation: 0.23165747054287814]
	TIME [epoch: 0.925 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.352478008645408		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.352478008645408 | validation: 0.3354835618518016]
	TIME [epoch: 0.925 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35338607628031554		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.35338607628031554 | validation: 0.2534755243708455]
	TIME [epoch: 0.93 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30959595414016866		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.30959595414016866 | validation: 0.2144103638064225]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3083432112867771		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3083432112867771 | validation: 0.29637902108856534]
	TIME [epoch: 0.925 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30254783688519693		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.30254783688519693 | validation: 0.20758885901382218]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3212953831493364		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.3212953831493364 | validation: 0.29566700379630756]
	TIME [epoch: 0.927 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4021786915864823		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.4021786915864823 | validation: 0.28743946538289866]
	TIME [epoch: 0.926 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30343658478887864		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.30343658478887864 | validation: 0.2519663031063187]
	TIME [epoch: 0.925 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28820316161462683		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.28820316161462683 | validation: 0.19030220179595403]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2970827600036268		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.2970827600036268 | validation: 0.30948215223039544]
	TIME [epoch: 0.925 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051381120225605		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3051381120225605 | validation: 0.2018259642671886]
	TIME [epoch: 0.925 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873346323043127		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.2873346323043127 | validation: 0.2228270675794337]
	TIME [epoch: 0.926 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747007209966032		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.2747007209966032 | validation: 0.27560298391763943]
	TIME [epoch: 0.927 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2801929032307653		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.2801929032307653 | validation: 0.21318538819002653]
	TIME [epoch: 0.924 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713832587085147		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2713832587085147 | validation: 0.22353107999518654]
	TIME [epoch: 0.925 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2609551170663223		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.2609551170663223 | validation: 0.249927024233876]
	TIME [epoch: 0.924 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25126669587201184		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.25126669587201184 | validation: 0.1801716453224255]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25296035050317406		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.25296035050317406 | validation: 0.25549158432771607]
	TIME [epoch: 0.925 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27591109805273906		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.27591109805273906 | validation: 0.18642277159090898]
	TIME [epoch: 0.925 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052792023169733		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.3052792023169733 | validation: 0.23750943171364514]
	TIME [epoch: 0.927 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533999837437929		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.2533999837437929 | validation: 0.22750420044858122]
	TIME [epoch: 0.924 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24486629697527143		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.24486629697527143 | validation: 0.19584635584093252]
	TIME [epoch: 0.927 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26654628210698356		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.26654628210698356 | validation: 0.21742329533543972]
	TIME [epoch: 0.925 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23365132080470652		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.23365132080470652 | validation: 0.1969713435867593]
	TIME [epoch: 0.925 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22319454454675075		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.22319454454675075 | validation: 0.17979041955641478]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23199228502235528		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.23199228502235528 | validation: 0.234488987130972]
	TIME [epoch: 0.926 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2323958455592701		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2323958455592701 | validation: 0.1585617111855634]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25163520044671756		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.25163520044671756 | validation: 0.2310496405471279]
	TIME [epoch: 0.926 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22954504145170737		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.22954504145170737 | validation: 0.18389312652883152]
	TIME [epoch: 0.925 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22042320309884555		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.22042320309884555 | validation: 0.19069571477255404]
	TIME [epoch: 0.926 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23690356714172198		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.23690356714172198 | validation: 0.1962841424694001]
	TIME [epoch: 0.93 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24352707020009262		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.24352707020009262 | validation: 0.17289360446512028]
	TIME [epoch: 0.926 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21866732701109817		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.21866732701109817 | validation: 0.20034864992080742]
	TIME [epoch: 0.924 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20826404858087444		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.20826404858087444 | validation: 0.1524175702124767]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181367936786084		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.2181367936786084 | validation: 0.2505813542211411]
	TIME [epoch: 0.926 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2331106832231039		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.2331106832231039 | validation: 0.13002673119880198]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23930880935236412		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.23930880935236412 | validation: 0.2026146614034262]
	TIME [epoch: 0.925 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20953971535072824		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.20953971535072824 | validation: 0.1699560912571308]
	TIME [epoch: 0.925 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20189476772094445		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.20189476772094445 | validation: 0.17087754277492737]
	TIME [epoch: 0.925 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20379192640527413		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.20379192640527413 | validation: 0.1626681781959096]
	TIME [epoch: 0.925 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1919746662866452		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.1919746662866452 | validation: 0.1725650550173268]
	TIME [epoch: 0.927 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18846505600479638		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.18846505600479638 | validation: 0.1984666429608207]
	TIME [epoch: 0.925 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1986634932359719		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.1986634932359719 | validation: 0.1480427503471239]
	TIME [epoch: 0.924 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23143488860546776		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.23143488860546776 | validation: 0.2077706082101506]
	TIME [epoch: 0.926 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2118492524844251		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.2118492524844251 | validation: 0.1385689001283449]
	TIME [epoch: 0.925 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19753439385312188		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.19753439385312188 | validation: 0.17008106402008277]
	TIME [epoch: 0.925 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19190189107396127		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.19190189107396127 | validation: 0.2045818008935398]
	TIME [epoch: 0.925 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20825063156837226		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.20825063156837226 | validation: 0.16693022122823026]
	TIME [epoch: 0.925 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2108731081241426		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.2108731081241426 | validation: 0.17926255411063824]
	TIME [epoch: 0.925 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19738039228518453		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.19738039228518453 | validation: 0.14676103709190752]
	TIME [epoch: 0.925 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18548582931585705		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.18548582931585705 | validation: 0.18046419280530648]
	TIME [epoch: 0.924 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20028366619670585		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.20028366619670585 | validation: 0.1774650976755311]
	TIME [epoch: 0.925 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1793507501862338		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.1793507501862338 | validation: 0.15956624022437588]
	TIME [epoch: 0.925 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178989532643588		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.178989532643588 | validation: 0.1355066555234893]
	TIME [epoch: 0.925 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18593223931328626		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.18593223931328626 | validation: 0.2348293626049796]
	TIME [epoch: 0.925 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20436477321525648		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.20436477321525648 | validation: 0.13487373786510642]
	TIME [epoch: 0.925 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19450813292836522		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.19450813292836522 | validation: 0.15802898754698838]
	TIME [epoch: 0.925 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18572193927624134		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.18572193927624134 | validation: 0.14087964908613565]
	TIME [epoch: 0.925 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17495825708322263		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.17495825708322263 | validation: 0.15165433821702515]
	TIME [epoch: 0.925 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17527163446586969		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.17527163446586969 | validation: 0.1263467567763947]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1889351750551583		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.1889351750551583 | validation: 0.20959652264911754]
	TIME [epoch: 0.929 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1964627432351675		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1964627432351675 | validation: 0.14317093207824946]
	TIME [epoch: 0.933 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15859029914207148		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.15859029914207148 | validation: 0.1330142893093823]
	TIME [epoch: 0.927 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15985596189740772		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.15985596189740772 | validation: 0.14985494736776356]
	TIME [epoch: 0.927 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672645961091732		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.1672645961091732 | validation: 0.13637764253029716]
	TIME [epoch: 0.927 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16971277553198622		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.16971277553198622 | validation: 0.156600680721655]
	TIME [epoch: 0.928 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16122702410906983		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.16122702410906983 | validation: 0.11493405446430455]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17059646955168994		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.17059646955168994 | validation: 0.18528485212036366]
	TIME [epoch: 0.926 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1891278834466273		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.1891278834466273 | validation: 0.12848733055644113]
	TIME [epoch: 0.925 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19714845099266956		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.19714845099266956 | validation: 0.19191800810767107]
	TIME [epoch: 0.925 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17722395279191142		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.17722395279191142 | validation: 0.13596146140499374]
	TIME [epoch: 0.924 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15927402480841343		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.15927402480841343 | validation: 0.14555093506257016]
	TIME [epoch: 0.925 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23689009835561173		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.23689009835561173 | validation: 0.13873096438285712]
	TIME [epoch: 0.925 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15084982939670918		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.15084982939670918 | validation: 0.1433362449590884]
	TIME [epoch: 0.924 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16365919928645664		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.16365919928645664 | validation: 0.13102800502598186]
	TIME [epoch: 0.926 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14507445105833902		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.14507445105833902 | validation: 0.12718256137595507]
	TIME [epoch: 0.926 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14891415428572574		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.14891415428572574 | validation: 0.10804837678788788]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1550386152532662		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.1550386152532662 | validation: 0.14214487875664822]
	TIME [epoch: 0.924 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445984761992736		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.1445984761992736 | validation: 0.10824355349255028]
	TIME [epoch: 0.923 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1472456470927493		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.1472456470927493 | validation: 0.13438251353787198]
	TIME [epoch: 0.925 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1611747291218016		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1611747291218016 | validation: 0.11248054302775819]
	TIME [epoch: 0.925 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14472148093639528		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.14472148093639528 | validation: 0.15019308938157672]
	TIME [epoch: 0.925 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15907359197104373		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.15907359197104373 | validation: 0.10585075889812]
	TIME [epoch: 0.925 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15811000659139338		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.15811000659139338 | validation: 0.1774387222437643]
	TIME [epoch: 0.927 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17128091218590974		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.17128091218590974 | validation: 0.1408255408114661]
	TIME [epoch: 0.926 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1827040695742815		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.1827040695742815 | validation: 0.17449741323738457]
	TIME [epoch: 0.926 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841302227955403		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.15841302227955403 | validation: 0.11422760079507849]
	TIME [epoch: 0.926 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13990814975350213		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.13990814975350213 | validation: 0.10760291155580443]
	TIME [epoch: 0.926 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13459349473025514		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.13459349473025514 | validation: 0.1303652437809478]
	TIME [epoch: 0.926 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13516310488594196		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.13516310488594196 | validation: 0.1125661486883637]
	TIME [epoch: 0.927 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13302067254607697		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.13302067254607697 | validation: 0.11191150936244171]
	TIME [epoch: 0.927 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14287189927323737		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.14287189927323737 | validation: 0.1338031434387453]
	TIME [epoch: 0.932 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14201490863302904		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.14201490863302904 | validation: 0.12337902120199092]
	TIME [epoch: 0.927 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16098065999436018		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.16098065999436018 | validation: 0.15115005231923861]
	TIME [epoch: 0.926 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1851500104236564		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.1851500104236564 | validation: 0.12276095397739306]
	TIME [epoch: 0.926 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535682422261076		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.1535682422261076 | validation: 0.11772403521696201]
	TIME [epoch: 0.927 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13212685392625317		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.13212685392625317 | validation: 0.11707374420099761]
	TIME [epoch: 0.926 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450993819866309		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.1450993819866309 | validation: 0.11634687995262345]
	TIME [epoch: 0.926 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1352866240963956		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.1352866240963956 | validation: 0.1306452832508602]
	TIME [epoch: 0.927 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14083915549699644		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.14083915549699644 | validation: 0.12710995810836065]
	TIME [epoch: 0.927 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14066154663944092		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.14066154663944092 | validation: 0.1086121924055557]
	TIME [epoch: 0.926 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556977543499918		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.13556977543499918 | validation: 0.12827514191525663]
	TIME [epoch: 0.926 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284130062500968		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1284130062500968 | validation: 0.1029663932991602]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396684091864616		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.1396684091864616 | validation: 0.18879768114571976]
	TIME [epoch: 0.927 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16833226379395327		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.16833226379395327 | validation: 0.09865954110998204]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14145023337633275		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.14145023337633275 | validation: 0.11720609324609464]
	TIME [epoch: 0.927 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13757568379074567		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.13757568379074567 | validation: 0.10331267425956861]
	TIME [epoch: 0.926 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408422255698653		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.12408422255698653 | validation: 0.10961951687402374]
	TIME [epoch: 0.927 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12275116544348656		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.12275116544348656 | validation: 0.09763340136801553]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097990065053728		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.12097990065053728 | validation: 0.11305210167507114]
	TIME [epoch: 0.926 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12494029139294621		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.12494029139294621 | validation: 0.13788619575197486]
	TIME [epoch: 0.927 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13728229090122995		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.13728229090122995 | validation: 0.12119901266164518]
	TIME [epoch: 139 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16630057970732992		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.16630057970732992 | validation: 0.14487174054997484]
	TIME [epoch: 1.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1621205257875384		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1621205257875384 | validation: 0.10342868072313145]
	TIME [epoch: 1.83 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12650897090498087		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.12650897090498087 | validation: 0.12302613428478143]
	TIME [epoch: 1.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13262394075584966		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.13262394075584966 | validation: 0.11737020054434698]
	TIME [epoch: 1.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13077705551279872		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.13077705551279872 | validation: 0.12046017030391594]
	TIME [epoch: 1.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12830505010250268		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.12830505010250268 | validation: 0.10646129038566327]
	TIME [epoch: 1.83 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1246862389844938		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.1246862389844938 | validation: 0.11592585720052587]
	TIME [epoch: 1.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11833967198821979		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.11833967198821979 | validation: 0.10471282418267941]
	TIME [epoch: 1.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1214867707670252		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.1214867707670252 | validation: 0.1271572886812944]
	TIME [epoch: 1.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12552964336418415		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.12552964336418415 | validation: 0.11659360322065977]
	TIME [epoch: 1.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18149036639241348		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.18149036639241348 | validation: 0.14281868305273646]
	TIME [epoch: 1.83 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13074741150406885		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.13074741150406885 | validation: 0.12419163478862588]
	TIME [epoch: 1.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15001931488702286		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.15001931488702286 | validation: 0.11739079193364535]
	TIME [epoch: 1.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12465671633657553		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.12465671633657553 | validation: 0.09290035408233532]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12198805336026723		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.12198805336026723 | validation: 0.10654831606896008]
	TIME [epoch: 1.83 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11848260535291527		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.11848260535291527 | validation: 0.1024975626555176]
	TIME [epoch: 1.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853918688068554		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.11853918688068554 | validation: 0.11014368684115956]
	TIME [epoch: 1.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12487023497144602		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.12487023497144602 | validation: 0.10441735928441256]
	TIME [epoch: 1.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12518589873600663		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.12518589873600663 | validation: 0.1301264213919908]
	TIME [epoch: 1.83 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14761039990293515		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.14761039990293515 | validation: 0.10566683468328203]
	TIME [epoch: 1.83 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305485236231772		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1305485236231772 | validation: 0.10938262167748768]
	TIME [epoch: 1.83 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12010778336540677		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12010778336540677 | validation: 0.10411640976080534]
	TIME [epoch: 1.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11868437494027961		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.11868437494027961 | validation: 0.1032075051616464]
	TIME [epoch: 1.83 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853927887173474		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.11853927887173474 | validation: 0.1105529572641792]
	TIME [epoch: 1.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11960910374714195		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.11960910374714195 | validation: 0.10362206021267173]
	TIME [epoch: 1.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11651882885756727		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.11651882885756727 | validation: 0.10501088209187388]
	TIME [epoch: 1.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11444884919740245		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.11444884919740245 | validation: 0.10020293625192259]
	TIME [epoch: 1.82 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12243129714154892		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.12243129714154892 | validation: 0.10768763576478646]
	TIME [epoch: 1.83 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12951093085768725		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.12951093085768725 | validation: 0.10720816889569101]
	TIME [epoch: 1.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11974811555701342		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.11974811555701342 | validation: 0.09693122132470344]
	TIME [epoch: 1.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222923643268328		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.1222923643268328 | validation: 0.10175787840053703]
	TIME [epoch: 1.83 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11898179703973137		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.11898179703973137 | validation: 0.11838930590089847]
	TIME [epoch: 1.83 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12102220891812188		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.12102220891812188 | validation: 0.09300358086161478]
	TIME [epoch: 1.83 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11985092390374927		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.11985092390374927 | validation: 0.12053818902467497]
	TIME [epoch: 1.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12192504079740933		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.12192504079740933 | validation: 0.09445660850329986]
	TIME [epoch: 1.83 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12731693436189262		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.12731693436189262 | validation: 0.11516966710222053]
	TIME [epoch: 1.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12363640517734899		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.12363640517734899 | validation: 0.09471335043365971]
	TIME [epoch: 1.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1178268210606371		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.1178268210606371 | validation: 0.10497404325172802]
	TIME [epoch: 1.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11296688402033628		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.11296688402033628 | validation: 0.09922428255450404]
	TIME [epoch: 1.83 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10485869682134016		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.10485869682134016 | validation: 0.08741510194944481]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629889927094702		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.10629889927094702 | validation: 0.09208106990599947]
	TIME [epoch: 1.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10625145297894459		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.10625145297894459 | validation: 0.09600955864219167]
	TIME [epoch: 1.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1049686120887458		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.1049686120887458 | validation: 0.09660479160499169]
	TIME [epoch: 1.82 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.114872773837222		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.114872773837222 | validation: 0.10356786892140356]
	TIME [epoch: 1.83 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12703360464438476		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.12703360464438476 | validation: 0.1517697476433141]
	TIME [epoch: 1.82 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1465796544827901		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1465796544827901 | validation: 0.09569249186898263]
	TIME [epoch: 1.83 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850185034359795		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.11850185034359795 | validation: 0.10622975849996111]
	TIME [epoch: 1.83 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11984514463249253		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.11984514463249253 | validation: 0.09734981710374231]
	TIME [epoch: 1.83 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1088006378727526		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.1088006378727526 | validation: 0.08858718232241188]
	TIME [epoch: 1.83 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10323756421721987		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.10323756421721987 | validation: 0.0992799610983694]
	TIME [epoch: 1.83 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10648735193813635		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.10648735193813635 | validation: 0.08885202069410329]
	TIME [epoch: 1.82 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10790912740656147		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.10790912740656147 | validation: 0.11247702467482351]
	TIME [epoch: 1.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11414983945808675		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.11414983945808675 | validation: 0.10544582152011726]
	TIME [epoch: 1.83 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12029379656816559		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.12029379656816559 | validation: 0.09881323155245966]
	TIME [epoch: 1.82 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11727479860889199		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11727479860889199 | validation: 0.10246832574484582]
	TIME [epoch: 1.83 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10550964804634617		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.10550964804634617 | validation: 0.09873665073740179]
	TIME [epoch: 1.83 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984084607120322		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.10984084607120322 | validation: 0.11184311775874721]
	TIME [epoch: 1.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12025330991840172		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.12025330991840172 | validation: 0.09467080123734226]
	TIME [epoch: 1.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12076552966017622		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.12076552966017622 | validation: 0.11508870074155575]
	TIME [epoch: 1.82 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522675244628695		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.11522675244628695 | validation: 0.08786115210207558]
	TIME [epoch: 1.83 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755958704562645		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.10755958704562645 | validation: 0.08895719202710078]
	TIME [epoch: 1.83 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10250977413740289		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.10250977413740289 | validation: 0.09308613379862124]
	TIME [epoch: 1.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984103666204157		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.09984103666204157 | validation: 0.09275399492513893]
	TIME [epoch: 1.82 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972197666589873		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.09972197666589873 | validation: 0.09693597934493264]
	TIME [epoch: 1.82 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10159106078223769		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.10159106078223769 | validation: 0.09516753864222512]
	TIME [epoch: 1.82 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11313881531485269		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.11313881531485269 | validation: 0.09893363702899327]
	TIME [epoch: 1.83 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080890654283504		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.1080890654283504 | validation: 0.1038030742623044]
	TIME [epoch: 1.83 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1098987181900797		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1098987181900797 | validation: 0.09285693520177135]
	TIME [epoch: 1.82 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000577880163846		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.11000577880163846 | validation: 0.09772462167010193]
	TIME [epoch: 1.82 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10453169162008497		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.10453169162008497 | validation: 0.08852510454265622]
	TIME [epoch: 1.82 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1036917290998527		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.1036917290998527 | validation: 0.09546216283027063]
	TIME [epoch: 1.82 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182942365355188		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.10182942365355188 | validation: 0.09337680536037521]
	TIME [epoch: 1.83 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10870299529365877		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.10870299529365877 | validation: 0.1053034315055803]
	TIME [epoch: 1.82 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11814144934214213		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.11814144934214213 | validation: 0.09806199817417179]
	TIME [epoch: 1.83 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096660908173649		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.11096660908173649 | validation: 0.09349976091644847]
	TIME [epoch: 1.82 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10430385117413404		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.10430385117413404 | validation: 0.09180014907238672]
	TIME [epoch: 1.83 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09903399261718782		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.09903399261718782 | validation: 0.10310780818454726]
	TIME [epoch: 1.83 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1162423857607396		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.1162423857607396 | validation: 0.10643061641433829]
	TIME [epoch: 1.83 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10854269399339232		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.10854269399339232 | validation: 0.10086553346327905]
	TIME [epoch: 1.82 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10273682968144225		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.10273682968144225 | validation: 0.09799902472198083]
	TIME [epoch: 1.83 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10623474961750445		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.10623474961750445 | validation: 0.10948303501363249]
	TIME [epoch: 1.83 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10331762472893907		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.10331762472893907 | validation: 0.08757646845507577]
	TIME [epoch: 1.84 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09677282800773768		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.09677282800773768 | validation: 0.09066639326881754]
	TIME [epoch: 1.83 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09648260024915327		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.09648260024915327 | validation: 0.1149180086032319]
	TIME [epoch: 1.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11484520173408762		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.11484520173408762 | validation: 0.09414264288972675]
	TIME [epoch: 1.82 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11568206638955303		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.11568206638955303 | validation: 0.10119124410375771]
	TIME [epoch: 1.83 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12232964005777781		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.12232964005777781 | validation: 0.08997886311961527]
	TIME [epoch: 1.82 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298484558496686		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.10298484558496686 | validation: 0.0863946037226687]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09606038338234499		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.09606038338234499 | validation: 0.09281333793925899]
	TIME [epoch: 1.83 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09933298244872903		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.09933298244872903 | validation: 0.08734120172427279]
	TIME [epoch: 1.83 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09712491051924876		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.09712491051924876 | validation: 0.08596369941258988]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841396893413823		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.09841396893413823 | validation: 0.08966236444727552]
	TIME [epoch: 1.83 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972375327264514		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.09972375327264514 | validation: 0.09108094577995617]
	TIME [epoch: 1.83 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09647999511709668		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.09647999511709668 | validation: 0.08825189592699793]
	TIME [epoch: 1.83 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081437660100422		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.10081437660100422 | validation: 0.09285964765180715]
	TIME [epoch: 1.83 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10394507205972225		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.10394507205972225 | validation: 0.08612401916011692]
	TIME [epoch: 1.83 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09880896848352748		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.09880896848352748 | validation: 0.08657643866546186]
	TIME [epoch: 1.83 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968393774373423		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.0968393774373423 | validation: 0.09131079561705431]
	TIME [epoch: 1.83 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09834809876726162		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.09834809876726162 | validation: 0.11431762722461825]
	TIME [epoch: 1.83 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10983204360929101		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.10983204360929101 | validation: 0.1119535354554305]
	TIME [epoch: 1.83 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955215372085974		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.11955215372085974 | validation: 0.09104969057213981]
	TIME [epoch: 1.83 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070208808940381		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.10070208808940381 | validation: 0.08873175107324337]
	TIME [epoch: 1.83 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1052116757823319		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1052116757823319 | validation: 0.08716515797313512]
	TIME [epoch: 1.83 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244625169170523		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.09244625169170523 | validation: 0.08618531587065456]
	TIME [epoch: 1.83 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09872936939609733		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.09872936939609733 | validation: 0.0909354786103313]
	TIME [epoch: 1.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09610954918088788		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.09610954918088788 | validation: 0.08106258590618726]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758110355893759		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.09758110355893759 | validation: 0.09765837028985823]
	TIME [epoch: 1.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10140225170353599		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.10140225170353599 | validation: 0.09023668178672993]
	TIME [epoch: 1.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10240569352479838		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.10240569352479838 | validation: 0.10386785593505957]
	TIME [epoch: 1.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466185340832831		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.10466185340832831 | validation: 0.08737637359051305]
	TIME [epoch: 1.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09347379244039922		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.09347379244039922 | validation: 0.08942454750736255]
	TIME [epoch: 1.82 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022780994164582		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.09022780994164582 | validation: 0.08542867684421579]
	TIME [epoch: 1.82 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09395920937834149		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.09395920937834149 | validation: 0.0837916162608906]
	TIME [epoch: 1.82 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220006325832848		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.09220006325832848 | validation: 0.08777090192153592]
	TIME [epoch: 1.82 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717111599415706		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.09717111599415706 | validation: 0.08761841938589543]
	TIME [epoch: 1.83 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0911259131348348		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.0911259131348348 | validation: 0.0888834739799027]
	TIME [epoch: 1.82 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10054956501245954		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.10054956501245954 | validation: 0.09082062884692002]
	TIME [epoch: 1.82 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123323881768224		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.11123323881768224 | validation: 0.09742672586632423]
	TIME [epoch: 1.82 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11020129193984088		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.11020129193984088 | validation: 0.08873142278401852]
	TIME [epoch: 1.82 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09568141728525259		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.09568141728525259 | validation: 0.08888403294851083]
	TIME [epoch: 1.83 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09239665581381075		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.09239665581381075 | validation: 0.08569714805331669]
	TIME [epoch: 1.82 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09933158940651517		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.09933158940651517 | validation: 0.09560672641201434]
	TIME [epoch: 1.83 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09715931697105527		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.09715931697105527 | validation: 0.08663931632165559]
	TIME [epoch: 1.83 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09024326303369976		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.09024326303369976 | validation: 0.08487643126540684]
	TIME [epoch: 1.83 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09356478667986849		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.09356478667986849 | validation: 0.07899404769510049]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09468906303733243		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.09468906303733243 | validation: 0.09407539997557933]
	TIME [epoch: 1.82 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943893602344917		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.0943893602344917 | validation: 0.0834411063291306]
	TIME [epoch: 1.82 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925216667525336		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.0925216667525336 | validation: 0.09457711784099428]
	TIME [epoch: 1.83 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0992862925194655		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.0992862925194655 | validation: 0.08265799478613015]
	TIME [epoch: 1.82 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0963134744985518		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.0963134744985518 | validation: 0.09076847822101074]
	TIME [epoch: 1.82 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09465471341409243		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.09465471341409243 | validation: 0.08687966767666147]
	TIME [epoch: 1.83 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976413924254104		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.08976413924254104 | validation: 0.08352381078264928]
	TIME [epoch: 1.84 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900427803638031		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.0900427803638031 | validation: 0.09511812690635346]
	TIME [epoch: 1.82 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954058928425599		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.0954058928425599 | validation: 0.0924924903813319]
	TIME [epoch: 1.82 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09784867539702308		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.09784867539702308 | validation: 0.0857222531888913]
	TIME [epoch: 1.82 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788372459497358		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.08788372459497358 | validation: 0.08946326071490658]
	TIME [epoch: 1.82 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09449946084623044		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.09449946084623044 | validation: 0.09286076351592872]
	TIME [epoch: 1.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191002205498376		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.10191002205498376 | validation: 0.1012358504474902]
	TIME [epoch: 1.82 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10856079757649681		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.10856079757649681 | validation: 0.08539903920391177]
	TIME [epoch: 1.83 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09318869098239396		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.09318869098239396 | validation: 0.08075586397760864]
	TIME [epoch: 1.82 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08523460872622181		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08523460872622181 | validation: 0.08493514953106274]
	TIME [epoch: 1.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893418132309347		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.0893418132309347 | validation: 0.08133845508329673]
	TIME [epoch: 1.82 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559191184063705		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.08559191184063705 | validation: 0.08229842575211367]
	TIME [epoch: 1.83 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812038681554887		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.08812038681554887 | validation: 0.08011585386801624]
	TIME [epoch: 1.83 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714232606880014		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.08714232606880014 | validation: 0.08324940429064023]
	TIME [epoch: 1.82 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09323384613957977		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.09323384613957977 | validation: 0.08498595663875397]
	TIME [epoch: 1.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09614594924795396		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.09614594924795396 | validation: 0.08505322965446346]
	TIME [epoch: 1.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08958438574764062		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.08958438574764062 | validation: 0.09286905465144021]
	TIME [epoch: 1.83 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381254873995787		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09381254873995787 | validation: 0.08800955917422737]
	TIME [epoch: 1.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862623070059728		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.0862623070059728 | validation: 0.0868182650438881]
	TIME [epoch: 1.83 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08652969095928896		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.08652969095928896 | validation: 0.09589837621667303]
	TIME [epoch: 1.83 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09846437451806399		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.09846437451806399 | validation: 0.08560317764867928]
	TIME [epoch: 1.82 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110226050645285		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.10110226050645285 | validation: 0.08461337219016034]
	TIME [epoch: 1.83 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996377513669284		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09996377513669284 | validation: 0.0879498470991174]
	TIME [epoch: 1.82 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08486259695233199		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.08486259695233199 | validation: 0.08171465986093795]
	TIME [epoch: 1.83 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695924221958483		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.08695924221958483 | validation: 0.07938033677894869]
	TIME [epoch: 1.83 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471002245251928		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.08471002245251928 | validation: 0.0889593611382167]
	TIME [epoch: 1.83 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876227711318893		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.0876227711318893 | validation: 0.08804135120785106]
	TIME [epoch: 1.83 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244934770022287		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.09244934770022287 | validation: 0.09182435767645789]
	TIME [epoch: 1.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09746124773601723		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.09746124773601723 | validation: 0.07989214034959502]
	TIME [epoch: 1.83 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927262981441886		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.0927262981441886 | validation: 0.08658000236880624]
	TIME [epoch: 1.83 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08575817768972804		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.08575817768972804 | validation: 0.0844789342618258]
	TIME [epoch: 1.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587914569109333		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.08587914569109333 | validation: 0.09413654768043847]
	TIME [epoch: 1.83 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09318483714551822		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.09318483714551822 | validation: 0.08593169404241117]
	TIME [epoch: 1.82 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317520182926398		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.09317520182926398 | validation: 0.08114146611619895]
	TIME [epoch: 1.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09044740647205238		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.09044740647205238 | validation: 0.08568192815913202]
	TIME [epoch: 1.84 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08375428930718055		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.08375428930718055 | validation: 0.08097130151010917]
	TIME [epoch: 1.84 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639985461435772		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08639985461435772 | validation: 0.0885527904485183]
	TIME [epoch: 1.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08693086996730896		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.08693086996730896 | validation: 0.08651246059669092]
	TIME [epoch: 1.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0999659058303695		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.0999659058303695 | validation: 0.08289354098501894]
	TIME [epoch: 1.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08761134867590897		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.08761134867590897 | validation: 0.09137817540378586]
	TIME [epoch: 1.83 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714105651634309		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.08714105651634309 | validation: 0.07865816608360109]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840943506966494		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.0840943506966494 | validation: 0.07661114787267956]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972753566583073		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.07972753566583073 | validation: 0.08135317656209518]
	TIME [epoch: 1.83 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08395389199678889		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.08395389199678889 | validation: 0.0911621418554417]
	TIME [epoch: 1.83 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09092258378662922		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.09092258378662922 | validation: 0.08482003520606464]
	TIME [epoch: 1.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254671579042061		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.10254671579042061 | validation: 0.08054647600780108]
	TIME [epoch: 1.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653333324125795		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.08653333324125795 | validation: 0.08037251179244118]
	TIME [epoch: 1.83 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08489882056104929		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.08489882056104929 | validation: 0.07922054483223823]
	TIME [epoch: 1.83 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08225503374722994		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.08225503374722994 | validation: 0.08264866030422689]
	TIME [epoch: 1.83 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08624937997020028		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08624937997020028 | validation: 0.08218613007021247]
	TIME [epoch: 1.83 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817885711428752		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.08817885711428752 | validation: 0.08545490337141397]
	TIME [epoch: 1.83 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802784369859168		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.08802784369859168 | validation: 0.08302117160747247]
	TIME [epoch: 1.83 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08520004021336053		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.08520004021336053 | validation: 0.07678683314377345]
	TIME [epoch: 1.83 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07916949928265728		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07916949928265728 | validation: 0.07775124356283665]
	TIME [epoch: 1.83 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010122689879723		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.08010122689879723 | validation: 0.07994169824849487]
	TIME [epoch: 1.83 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526767657731146		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.08526767657731146 | validation: 0.08630732025865738]
	TIME [epoch: 1.83 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896072116849483		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.0896072116849483 | validation: 0.08528447056479653]
	TIME [epoch: 1.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038610955081251		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.10038610955081251 | validation: 0.08440839262332708]
	TIME [epoch: 1.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620981869100383		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.08620981869100383 | validation: 0.0828975817694081]
	TIME [epoch: 1.84 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08379944639873309		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.08379944639873309 | validation: 0.07875856627463987]
	TIME [epoch: 1.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08619531570286078		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.08619531570286078 | validation: 0.08782437649640515]
	TIME [epoch: 1.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08573866835726625		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.08573866835726625 | validation: 0.0799988119856393]
	TIME [epoch: 1.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08284107985476688		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.08284107985476688 | validation: 0.08261668092550528]
	TIME [epoch: 1.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08641113298351744		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08641113298351744 | validation: 0.08194884383859924]
	TIME [epoch: 1.83 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559336140479867		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.08559336140479867 | validation: 0.08150358043978824]
	TIME [epoch: 1.83 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08998042391025692		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.08998042391025692 | validation: 0.08195320075788895]
	TIME [epoch: 1.83 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686498708185672		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.08686498708185672 | validation: 0.08566216845721027]
	TIME [epoch: 1.83 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414661075672494		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.08414661075672494 | validation: 0.08213809532293946]
	TIME [epoch: 1.83 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07795293389703739		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.07795293389703739 | validation: 0.08027334296000688]
	TIME [epoch: 1.83 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08281502714183937		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.08281502714183937 | validation: 0.08407383941539405]
	TIME [epoch: 1.83 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08041656874795076		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.08041656874795076 | validation: 0.08347838220660228]
	TIME [epoch: 1.83 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798939114338884		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.08798939114338884 | validation: 0.09367776209548379]
	TIME [epoch: 1.83 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888458716521906		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09888458716521906 | validation: 0.07543205811905584]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08405763191423166		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.08405763191423166 | validation: 0.07738082992794626]
	TIME [epoch: 1.83 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08110007946087638		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.08110007946087638 | validation: 0.07925257820647513]
	TIME [epoch: 1.83 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0812244781933779		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0812244781933779 | validation: 0.07825423916841556]
	TIME [epoch: 1.83 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943054544328504		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.07943054544328504 | validation: 0.08441655507503343]
	TIME [epoch: 1.83 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08358735031735204		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.08358735031735204 | validation: 0.08952847451271775]
	TIME [epoch: 1.83 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0853376194541773		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.0853376194541773 | validation: 0.0765792566406215]
	TIME [epoch: 1.83 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08163750938898531		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.08163750938898531 | validation: 0.08282329265021807]
	TIME [epoch: 1.83 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271816869619446		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.08271816869619446 | validation: 0.08064766522740528]
	TIME [epoch: 1.83 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08007181528125423		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.08007181528125423 | validation: 0.07175000231243053]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08100687697470672		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.08100687697470672 | validation: 0.07074655630077974]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682631579126771		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.07682631579126771 | validation: 0.0808250247403547]
	TIME [epoch: 1.83 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07963444819084385		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.07963444819084385 | validation: 0.08126483924719316]
	TIME [epoch: 1.83 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886202521268632		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0886202521268632 | validation: 0.08438729095215303]
	TIME [epoch: 1.83 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08136587653623602		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.08136587653623602 | validation: 0.07923697851865419]
	TIME [epoch: 1.83 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08018151225956413		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.08018151225956413 | validation: 0.07337083766762979]
	TIME [epoch: 1.83 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106167563453945		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.08106167563453945 | validation: 0.07457688290574757]
	TIME [epoch: 1.83 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812825473440292		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.07812825473440292 | validation: 0.07910189706642433]
	TIME [epoch: 1.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08029775084530304		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.08029775084530304 | validation: 0.07613343007962181]
	TIME [epoch: 1.83 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09023526809202301		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.09023526809202301 | validation: 0.08509123532217185]
	TIME [epoch: 1.83 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09277458376789084		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.09277458376789084 | validation: 0.07794502048076127]
	TIME [epoch: 1.83 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09111927675986109		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.09111927675986109 | validation: 0.07545019764894496]
	TIME [epoch: 1.83 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129095918216597		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.08129095918216597 | validation: 0.07611719346607825]
	TIME [epoch: 1.83 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07966385470449323		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07966385470449323 | validation: 0.07633490036657668]
	TIME [epoch: 1.83 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07757329884703064		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.07757329884703064 | validation: 0.07559847703224622]
	TIME [epoch: 1.83 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08044226304459037		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08044226304459037 | validation: 0.0788921179485537]
	TIME [epoch: 1.83 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07654871836130696		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.07654871836130696 | validation: 0.07483901987295404]
	TIME [epoch: 1.83 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07628143539855299		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.07628143539855299 | validation: 0.0685916730737702]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07820942729322812		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.07820942729322812 | validation: 0.07590421213832223]
	TIME [epoch: 1.83 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599962500777457		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.07599962500777457 | validation: 0.07741186865451066]
	TIME [epoch: 1.83 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07820720808646339		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.07820720808646339 | validation: 0.07545473049110912]
	TIME [epoch: 1.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940182504622857		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.07940182504622857 | validation: 0.07524287440194577]
	TIME [epoch: 1.83 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257124424631798		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.08257124424631798 | validation: 0.08466374489785562]
	TIME [epoch: 1.83 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414888992097957		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.08414888992097957 | validation: 0.06941286274047802]
	TIME [epoch: 1.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08393461473876629		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.08393461473876629 | validation: 0.07777373249430486]
	TIME [epoch: 1.83 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08327330116480101		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.08327330116480101 | validation: 0.07340417505697037]
	TIME [epoch: 1.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570699961991316		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.07570699961991316 | validation: 0.07796910134480323]
	TIME [epoch: 1.83 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07516298006973418		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.07516298006973418 | validation: 0.07352566345253396]
	TIME [epoch: 1.83 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671543504099611		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07671543504099611 | validation: 0.07916107078066564]
	TIME [epoch: 1.83 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07959195901600816		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.07959195901600816 | validation: 0.07433221524584291]
	TIME [epoch: 1.83 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653093949014026		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.08653093949014026 | validation: 0.07688005899213873]
	TIME [epoch: 1.83 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07831963462555186		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.07831963462555186 | validation: 0.0706384170645509]
	TIME [epoch: 1.83 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768747765419202		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.07768747765419202 | validation: 0.07680706820833483]
	TIME [epoch: 1.83 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0800591388240467		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.0800591388240467 | validation: 0.07763875419390547]
	TIME [epoch: 1.83 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770661513003643		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.07770661513003643 | validation: 0.07739369369919769]
	TIME [epoch: 1.83 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209002043896551		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07209002043896551 | validation: 0.0729915345657416]
	TIME [epoch: 1.83 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760768066659504		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.07760768066659504 | validation: 0.07741835509032025]
	TIME [epoch: 1.83 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07494315806115348		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07494315806115348 | validation: 0.07431146527907927]
	TIME [epoch: 1.83 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412829663254783		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.07412829663254783 | validation: 0.07246786278332117]
	TIME [epoch: 1.83 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328286087190417		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.08328286087190417 | validation: 0.08211171801796187]
	TIME [epoch: 1.83 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09371388246545269		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.09371388246545269 | validation: 0.07487305854442165]
	TIME [epoch: 1.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954366732469213		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.07954366732469213 | validation: 0.07923958041733209]
	TIME [epoch: 1.83 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609309634353255		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.07609309634353255 | validation: 0.07325509352544655]
	TIME [epoch: 1.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823886167741179		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.07823886167741179 | validation: 0.07720020280071534]
	TIME [epoch: 1.83 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724137826883268		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07724137826883268 | validation: 0.07944985608194939]
	TIME [epoch: 1.83 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07573864735097503		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.07573864735097503 | validation: 0.07098271455392735]
	TIME [epoch: 1.83 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07415217036467726		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07415217036467726 | validation: 0.06894055410548015]
	TIME [epoch: 1.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615082197895873		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.07615082197895873 | validation: 0.0800805795529149]
	TIME [epoch: 1.83 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07536070599087141		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.07536070599087141 | validation: 0.07463030728995182]
	TIME [epoch: 1.83 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08021590999857237		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.08021590999857237 | validation: 0.08167211604772125]
	TIME [epoch: 1.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345066925536038		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.08345066925536038 | validation: 0.06937827197652056]
	TIME [epoch: 1.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314340964257146		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.08314340964257146 | validation: 0.07558485227099494]
	TIME [epoch: 1.83 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419029645000695		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.07419029645000695 | validation: 0.077384750755142]
	TIME [epoch: 1.83 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934624608213216		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.07934624608213216 | validation: 0.07647262880123099]
	TIME [epoch: 1.83 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07826133945681897		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.07826133945681897 | validation: 0.08379118002086083]
	TIME [epoch: 1.83 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07993930661857443		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.07993930661857443 | validation: 0.07019189038877666]
	TIME [epoch: 1.83 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497293791183936		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.07497293791183936 | validation: 0.07838904683456638]
	TIME [epoch: 1.83 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638931990099712		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.07638931990099712 | validation: 0.07415161652699752]
	TIME [epoch: 1.83 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805196633480682		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.07805196633480682 | validation: 0.08183676665603817]
	TIME [epoch: 1.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07861074306439936		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.07861074306439936 | validation: 0.07347429899255546]
	TIME [epoch: 1.83 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07660984108942355		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.07660984108942355 | validation: 0.08323064829842973]
	TIME [epoch: 1.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726434911475962		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.0726434911475962 | validation: 0.07331732619620107]
	TIME [epoch: 1.83 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565074182157587		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.07565074182157587 | validation: 0.07916641246093918]
	TIME [epoch: 1.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07496272166701115		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.07496272166701115 | validation: 0.06570390835746881]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769913509150575		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.0769913509150575 | validation: 0.0744678444474575]
	TIME [epoch: 1.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07243656368507186		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.07243656368507186 | validation: 0.06692508153653286]
	TIME [epoch: 1.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07355498545532024		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.07355498545532024 | validation: 0.078120989337594]
	TIME [epoch: 1.83 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320877856126824		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.07320877856126824 | validation: 0.07891463882886197]
	TIME [epoch: 1.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835676196280211		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.07835676196280211 | validation: 0.07984221009819704]
	TIME [epoch: 1.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07745466371797051		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.07745466371797051 | validation: 0.07140876364868169]
	TIME [epoch: 1.83 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07643383154368748		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.07643383154368748 | validation: 0.06923688621967272]
	TIME [epoch: 1.83 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267681398401171		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.07267681398401171 | validation: 0.07147106856866046]
	TIME [epoch: 1.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07336611952326953		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.07336611952326953 | validation: 0.07060910152679607]
	TIME [epoch: 1.83 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155719753139528		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.07155719753139528 | validation: 0.07744760539392687]
	TIME [epoch: 1.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024045048055019		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.07024045048055019 | validation: 0.06942184649206545]
	TIME [epoch: 1.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07229252030160364		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.07229252030160364 | validation: 0.07047783784886409]
	TIME [epoch: 1.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731604346293029		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.07731604346293029 | validation: 0.07802822031737325]
	TIME [epoch: 1.83 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622160334025345		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.07622160334025345 | validation: 0.0704541754785705]
	TIME [epoch: 1.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0730840951635877		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.0730840951635877 | validation: 0.06674948476973623]
	TIME [epoch: 1.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365153396004048		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.07365153396004048 | validation: 0.07848980207470481]
	TIME [epoch: 1.83 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07544089356014681		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.07544089356014681 | validation: 0.07303221071720117]
	TIME [epoch: 1.83 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07402691436062536		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.07402691436062536 | validation: 0.07274288549164444]
	TIME [epoch: 1.83 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07100980466346943		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.07100980466346943 | validation: 0.07587814000883862]
	TIME [epoch: 1.83 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631351460064834		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.07631351460064834 | validation: 0.07071042315106289]
	TIME [epoch: 1.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07664733108037412		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.07664733108037412 | validation: 0.06824697706099682]
	TIME [epoch: 1.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490948486983669		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.07490948486983669 | validation: 0.07136572332928186]
	TIME [epoch: 1.83 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07517713446204351		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.07517713446204351 | validation: 0.070269185828171]
	TIME [epoch: 1.83 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181352064790829		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.07181352064790829 | validation: 0.07025006240423184]
	TIME [epoch: 1.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07202355345847833		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.07202355345847833 | validation: 0.06883187018386294]
	TIME [epoch: 1.83 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337041310253843		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.07337041310253843 | validation: 0.07185780593447422]
	TIME [epoch: 1.83 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07315625827723937		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.07315625827723937 | validation: 0.07131386045970553]
	TIME [epoch: 1.83 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07556699155105935		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.07556699155105935 | validation: 0.07880094390274693]
	TIME [epoch: 1.83 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07602382459963729		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.07602382459963729 | validation: 0.07191280528379135]
	TIME [epoch: 1.83 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07990730799663939		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.07990730799663939 | validation: 0.07392912434722756]
	TIME [epoch: 1.83 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998439787083738		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.06998439787083738 | validation: 0.07553222595959308]
	TIME [epoch: 1.83 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742331257542339		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.0742331257542339 | validation: 0.0643398040591075]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728350257102756		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.0728350257102756 | validation: 0.07543327459594015]
	TIME [epoch: 1.83 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07911054802029367		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.07911054802029367 | validation: 0.07083065252405402]
	TIME [epoch: 1.83 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07342391446498253		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.07342391446498253 | validation: 0.0727761333666212]
	TIME [epoch: 1.83 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352484755279934		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.07352484755279934 | validation: 0.0726461233659237]
	TIME [epoch: 1.83 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282792306810691		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.07282792306810691 | validation: 0.06672081501414726]
	TIME [epoch: 1.83 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07327271357769101		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.07327271357769101 | validation: 0.0703171989754983]
	TIME [epoch: 1.83 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968570179232263		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.06968570179232263 | validation: 0.06885217416166461]
	TIME [epoch: 1.83 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063348526849626		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.07063348526849626 | validation: 0.0677951061004438]
	TIME [epoch: 1.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07026343338619966		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.07026343338619966 | validation: 0.06859517834463445]
	TIME [epoch: 1.83 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298342046711527		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.07298342046711527 | validation: 0.07359623944949116]
	TIME [epoch: 1.83 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06869961260593653		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.06869961260593653 | validation: 0.06477558172932038]
	TIME [epoch: 1.83 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06792829408453878		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.06792829408453878 | validation: 0.07138479593450311]
	TIME [epoch: 1.83 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186777307751133		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.07186777307751133 | validation: 0.0738034253605094]
	TIME [epoch: 1.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06915535730096213		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.06915535730096213 | validation: 0.06487909291763347]
	TIME [epoch: 1.83 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07166703057447245		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.07166703057447245 | validation: 0.0692175946018883]
	TIME [epoch: 1.83 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113787345789574		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.07113787345789574 | validation: 0.07353733632960283]
	TIME [epoch: 1.83 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714887108374498		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0714887108374498 | validation: 0.06722563705774565]
	TIME [epoch: 1.83 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874946636280423		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.06874946636280423 | validation: 0.06422201042874104]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149612384695435		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.07149612384695435 | validation: 0.06491847093228396]
	TIME [epoch: 1.83 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07077519025966289		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.07077519025966289 | validation: 0.06900625001983124]
	TIME [epoch: 1.83 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07150971843085899		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.07150971843085899 | validation: 0.06996344901499085]
	TIME [epoch: 1.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058730450884941		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.07058730450884941 | validation: 0.07124740240796289]
	TIME [epoch: 1.83 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800208098373631		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.06800208098373631 | validation: 0.0776944719546853]
	TIME [epoch: 1.83 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264140984241049		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.07264140984241049 | validation: 0.07320517662511715]
	TIME [epoch: 1.83 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07749208008027389		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.07749208008027389 | validation: 0.06776899076731929]
	TIME [epoch: 1.83 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117281363153667		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.07117281363153667 | validation: 0.06340750150993044]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258338186123524		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.07258338186123524 | validation: 0.07326231429699297]
	TIME [epoch: 1.83 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072892087376499		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.07072892087376499 | validation: 0.06699788328857881]
	TIME [epoch: 1.83 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06869243905031035		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.06869243905031035 | validation: 0.06994798164459799]
	TIME [epoch: 1.83 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07191173837879049		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.07191173837879049 | validation: 0.06744342852773184]
	TIME [epoch: 1.83 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020574567917796		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07020574567917796 | validation: 0.06530890237265614]
	TIME [epoch: 1.83 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093093660514534		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07093093660514534 | validation: 0.06993598028927607]
	TIME [epoch: 1.83 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186681402631445		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.07186681402631445 | validation: 0.06642964254763664]
	TIME [epoch: 1.83 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992175676011772		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.06992175676011772 | validation: 0.07316838693137104]
	TIME [epoch: 1.83 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883778476234335		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.06883778476234335 | validation: 0.07080431712475692]
	TIME [epoch: 1.83 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06723361263222953		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.06723361263222953 | validation: 0.06485664917883546]
	TIME [epoch: 1.83 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708880539917201		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.0708880539917201 | validation: 0.06858603122762266]
	TIME [epoch: 1.83 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168670391939509		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.07168670391939509 | validation: 0.07179315990626305]
	TIME [epoch: 1.83 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0730341063114315		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.0730341063114315 | validation: 0.07308884397943727]
	TIME [epoch: 1.83 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07862861389291557		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07862861389291557 | validation: 0.06441535866943526]
	TIME [epoch: 1.84 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911124235173029		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.06911124235173029 | validation: 0.06853008785908832]
	TIME [epoch: 1.83 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874785634392902		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.06874785634392902 | validation: 0.07649538974589724]
	TIME [epoch: 1.83 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07021052047704743		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.07021052047704743 | validation: 0.06737919309770578]
	TIME [epoch: 1.83 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06985596385432642		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.06985596385432642 | validation: 0.06989552460279602]
	TIME [epoch: 1.83 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707093552638805		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.0707093552638805 | validation: 0.06228991133331348]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07029279439447737		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.07029279439447737 | validation: 0.07146734956911291]
	TIME [epoch: 1.82 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06568712554820716		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.06568712554820716 | validation: 0.06354186502088009]
	TIME [epoch: 1.82 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06756716771753798		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06756716771753798 | validation: 0.06363320287155101]
	TIME [epoch: 1.82 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07194874052233063		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.07194874052233063 | validation: 0.06851714728277201]
	TIME [epoch: 1.82 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695774647426025		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.0695774647426025 | validation: 0.06540797925643405]
	TIME [epoch: 1.82 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802021341442555		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.06802021341442555 | validation: 0.06851790729977437]
	TIME [epoch: 1.82 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047595296078778		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.07047595296078778 | validation: 0.06028434728648681]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06749987816796243		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.06749987816796243 | validation: 0.06644669677529233]
	TIME [epoch: 1.83 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033826852644844		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.07033826852644844 | validation: 0.06915302321484967]
	TIME [epoch: 1.83 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161656936225468		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.07161656936225468 | validation: 0.07503602137244816]
	TIME [epoch: 1.83 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225974737395478		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07225974737395478 | validation: 0.06538048104920262]
	TIME [epoch: 1.83 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06587422224033405		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.06587422224033405 | validation: 0.06576273559397984]
	TIME [epoch: 1.83 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897544706169965		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.06897544706169965 | validation: 0.06624436690619895]
	TIME [epoch: 1.83 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06842908394123884		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.06842908394123884 | validation: 0.06597697091685696]
	TIME [epoch: 1.83 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667660689240465		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.06667660689240465 | validation: 0.0636612305902922]
	TIME [epoch: 1.82 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06977112552646908		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.06977112552646908 | validation: 0.06546703485125885]
	TIME [epoch: 1.83 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714617358950551		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0714617358950551 | validation: 0.06412044100385443]
	TIME [epoch: 1.83 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0734642294073321		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.0734642294073321 | validation: 0.0644413054834354]
	TIME [epoch: 1.83 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064986544041206		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.07064986544041206 | validation: 0.06472188840845645]
	TIME [epoch: 1.83 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06798726566940796		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.06798726566940796 | validation: 0.06690138494429487]
	TIME [epoch: 1.82 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06696032636934242		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.06696032636934242 | validation: 0.06789197773545856]
	TIME [epoch: 1.82 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693080286963926		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.0693080286963926 | validation: 0.06273023971747727]
	TIME [epoch: 1.82 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584432638097805		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.06584432638097805 | validation: 0.06773664983209857]
	TIME [epoch: 1.82 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07051053168078887		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.07051053168078887 | validation: 0.06085824507890595]
	TIME [epoch: 1.82 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06676709306000986		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.06676709306000986 | validation: 0.06423009616140347]
	TIME [epoch: 1.82 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06436259418283882		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.06436259418283882 | validation: 0.06009581863676854]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06517314801826771		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.06517314801826771 | validation: 0.06026625256509314]
	TIME [epoch: 1.83 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680882063928764		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.06680882063928764 | validation: 0.07316718898420535]
	TIME [epoch: 1.82 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347321038502036		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.07347321038502036 | validation: 0.057841355024191014]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06900158435759678		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.06900158435759678 | validation: 0.0689262145649581]
	TIME [epoch: 1.82 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06658916920745579		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.06658916920745579 | validation: 0.06066001426150028]
	TIME [epoch: 1.83 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576697538163088		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.06576697538163088 | validation: 0.059625158278748036]
	TIME [epoch: 1.83 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06523878089024547		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.06523878089024547 | validation: 0.061783914661363265]
	TIME [epoch: 1.83 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06786238906154862		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.06786238906154862 | validation: 0.06180409203288317]
	TIME [epoch: 1.82 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861320408732381		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.06861320408732381 | validation: 0.06093299641018568]
	TIME [epoch: 1.83 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06751577070744023		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.06751577070744023 | validation: 0.060456872440408475]
	TIME [epoch: 1.82 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820749602989158		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06820749602989158 | validation: 0.06804922067095824]
	TIME [epoch: 1.82 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666977939231627		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.0666977939231627 | validation: 0.06472155167146719]
	TIME [epoch: 1.83 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06635757550923904		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.06635757550923904 | validation: 0.059574872070365284]
	TIME [epoch: 1.82 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866952010684242		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.06866952010684242 | validation: 0.06770131530792106]
	TIME [epoch: 1.82 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06427215467828028		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.06427215467828028 | validation: 0.0646922434503751]
	TIME [epoch: 1.82 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664409256001097		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.06664409256001097 | validation: 0.06770122724185354]
	TIME [epoch: 1.82 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06569508078497942		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.06569508078497942 | validation: 0.06331381514007559]
	TIME [epoch: 1.83 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06677945992209884		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.06677945992209884 | validation: 0.06643790659161584]
	TIME [epoch: 1.83 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06501223058528394		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.06501223058528394 | validation: 0.062317415143502725]
	TIME [epoch: 1.82 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06707600789711937		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.06707600789711937 | validation: 0.06628185736174966]
	TIME [epoch: 1.83 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06930804547719781		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.06930804547719781 | validation: 0.0627706702060193]
	TIME [epoch: 1.83 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06691224156474128		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.06691224156474128 | validation: 0.07119082038260482]
	TIME [epoch: 1.83 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862084372901993		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.06862084372901993 | validation: 0.06484462557278041]
	TIME [epoch: 1.83 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06561816333903667		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.06561816333903667 | validation: 0.0613188709296209]
	TIME [epoch: 1.83 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06367022869340463		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06367022869340463 | validation: 0.06721578719446057]
	TIME [epoch: 1.83 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06809586506011128		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.06809586506011128 | validation: 0.05919448051101542]
	TIME [epoch: 1.83 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683181893713638		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.0683181893713638 | validation: 0.06610865588392446]
	TIME [epoch: 1.83 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06550495762202636		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.06550495762202636 | validation: 0.06579901836554385]
	TIME [epoch: 1.83 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06506194748324454		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.06506194748324454 | validation: 0.057709252956827474]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683911289773633		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0683911289773633 | validation: 0.062360564641473076]
	TIME [epoch: 1.83 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644086492168816		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0644086492168816 | validation: 0.07006985447209291]
	TIME [epoch: 1.83 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639650942438954		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.0639650942438954 | validation: 0.059288885497472556]
	TIME [epoch: 1.83 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06486572618722976		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06486572618722976 | validation: 0.06650816354420754]
	TIME [epoch: 1.83 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564435583904192		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.06564435583904192 | validation: 0.05656396803021041]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_915.pth
	Model improved!!!
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06637933566655091		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.06637933566655091 | validation: 0.0668003599792122]
	TIME [epoch: 1.83 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06762360429244783		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.06762360429244783 | validation: 0.06422206227279528]
	TIME [epoch: 1.83 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405505766890297		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.06405505766890297 | validation: 0.06684233661537299]
	TIME [epoch: 1.83 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358436342065396		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.06358436342065396 | validation: 0.06241443437842691]
	TIME [epoch: 1.83 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583423726109426		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06583423726109426 | validation: 0.06635645647022931]
	TIME [epoch: 1.83 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06626426124167087		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.06626426124167087 | validation: 0.058609073189606134]
	TIME [epoch: 1.83 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646214955028668		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.0646214955028668 | validation: 0.060675152588288574]
	TIME [epoch: 1.83 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496065712239656		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06496065712239656 | validation: 0.0648636328835719]
	TIME [epoch: 1.83 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06648998941187624		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.06648998941187624 | validation: 0.06385922696536546]
	TIME [epoch: 1.83 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373505352995043		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.06373505352995043 | validation: 0.06674130938579857]
	TIME [epoch: 1.83 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540358701552193		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.06540358701552193 | validation: 0.062239506318531784]
	TIME [epoch: 1.83 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06447240995399821		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.06447240995399821 | validation: 0.06218558606120598]
	TIME [epoch: 1.83 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647230572922777		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0647230572922777 | validation: 0.061739657039586684]
	TIME [epoch: 1.83 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06568903664214051		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.06568903664214051 | validation: 0.06653755657288463]
	TIME [epoch: 1.83 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06643303917587531		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.06643303917587531 | validation: 0.05689788197525138]
	TIME [epoch: 1.83 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06572465706210243		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.06572465706210243 | validation: 0.06635274671730776]
	TIME [epoch: 1.83 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425437393118735		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.06425437393118735 | validation: 0.06490273065594004]
	TIME [epoch: 1.84 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638218694489127		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.06638218694489127 | validation: 0.05578067469712894]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653304352101121		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.0653304352101121 | validation: 0.061556032587234515]
	TIME [epoch: 1.83 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06585277808502073		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.06585277808502073 | validation: 0.057382386822449394]
	TIME [epoch: 1.83 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525225982122018		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.06525225982122018 | validation: 0.06198963923447396]
	TIME [epoch: 1.83 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06566846231849295		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.06566846231849295 | validation: 0.06013418545160531]
	TIME [epoch: 1.83 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06421675402636036		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.06421675402636036 | validation: 0.06867571275268823]
	TIME [epoch: 1.83 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06615885290434056		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06615885290434056 | validation: 0.057183960696066266]
	TIME [epoch: 1.83 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825465811271415		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.06825465811271415 | validation: 0.060444403759466206]
	TIME [epoch: 1.83 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06338656429288225		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.06338656429288225 | validation: 0.057605479119152585]
	TIME [epoch: 1.83 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405121710553117		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.06405121710553117 | validation: 0.05912511520165919]
	TIME [epoch: 1.83 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435652842873994		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.06435652842873994 | validation: 0.06197785558817942]
	TIME [epoch: 1.83 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373878293215941		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.06373878293215941 | validation: 0.05760954057239951]
	TIME [epoch: 1.83 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420154739706245		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.06420154739706245 | validation: 0.06433348488858617]
	TIME [epoch: 1.83 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0656351812485321		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.0656351812485321 | validation: 0.06310567407816314]
	TIME [epoch: 1.83 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06339108918682494		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.06339108918682494 | validation: 0.06078012235207353]
	TIME [epoch: 1.83 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498124037444561		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.06498124037444561 | validation: 0.06348595585419574]
	TIME [epoch: 1.82 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497133630262672		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.06497133630262672 | validation: 0.06095468712365233]
	TIME [epoch: 1.83 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06333077910582045		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.06333077910582045 | validation: 0.05744217738394261]
	TIME [epoch: 1.83 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403081808797852		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.06403081808797852 | validation: 0.05881220721646294]
	TIME [epoch: 1.83 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06197437940495453		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.06197437940495453 | validation: 0.06084797027239433]
	TIME [epoch: 1.83 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06275111698257832		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.06275111698257832 | validation: 0.05598910882503633]
	TIME [epoch: 1.82 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310026267141625		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.06310026267141625 | validation: 0.06062003487733303]
	TIME [epoch: 1.83 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06536520397516224		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.06536520397516224 | validation: 0.06161198150152478]
	TIME [epoch: 1.83 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539323241852399		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.06539323241852399 | validation: 0.058842424407493155]
	TIME [epoch: 1.83 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06352054077074162		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.06352054077074162 | validation: 0.057128350241803655]
	TIME [epoch: 1.83 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362763467253546		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.06362763467253546 | validation: 0.05938042869227055]
	TIME [epoch: 1.83 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0650717012445401		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.0650717012445401 | validation: 0.05975749826055039]
	TIME [epoch: 1.83 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06776296656241929		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.06776296656241929 | validation: 0.05364891845512763]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06611072290906182		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.06611072290906182 | validation: 0.060226370876556336]
	TIME [epoch: 1.83 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06460196712621294		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.06460196712621294 | validation: 0.06027919955442207]
	TIME [epoch: 1.83 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06415934327219047		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.06415934327219047 | validation: 0.05974672526749908]
	TIME [epoch: 1.83 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495079768919325		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.06495079768919325 | validation: 0.06112113908608714]
	TIME [epoch: 1.83 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636358539625021		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.0636358539625021 | validation: 0.06455111227165171]
	TIME [epoch: 1.83 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464814690993842		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.06464814690993842 | validation: 0.0604570639186077]
	TIME [epoch: 1.83 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06339658395089895		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.06339658395089895 | validation: 0.059597984676629724]
	TIME [epoch: 1.83 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322236796944082		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.06322236796944082 | validation: 0.05848649372097491]
	TIME [epoch: 1.83 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336279611354431		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.06336279611354431 | validation: 0.05583883611680227]
	TIME [epoch: 1.83 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399088457581546		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.06399088457581546 | validation: 0.06322136672920951]
	TIME [epoch: 1.83 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638682586584091		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.0638682586584091 | validation: 0.05587736364325862]
	TIME [epoch: 1.83 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292045832057803		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.06292045832057803 | validation: 0.061532875734357444]
	TIME [epoch: 1.83 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218981533613436		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.06218981533613436 | validation: 0.05772212269492019]
	TIME [epoch: 1.83 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061064184710491055		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.061064184710491055 | validation: 0.057388592537994325]
	TIME [epoch: 1.83 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061333629080484314		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.061333629080484314 | validation: 0.05712243743478686]
	TIME [epoch: 1.83 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06273389736452532		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.06273389736452532 | validation: 0.060784465315655456]
	TIME [epoch: 1.83 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336362524431839		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.06336362524431839 | validation: 0.056382170844425844]
	TIME [epoch: 1.83 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06175264848925334		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.06175264848925334 | validation: 0.056552684470811604]
	TIME [epoch: 1.83 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06185501693572484		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.06185501693572484 | validation: 0.05728256981346217]
	TIME [epoch: 1.83 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140488272583275		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.06140488272583275 | validation: 0.058944163092382235]
	TIME [epoch: 1.83 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06204689604004248		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.06204689604004248 | validation: 0.05687400715370549]
	TIME [epoch: 1.83 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06162788644668517		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.06162788644668517 | validation: 0.05633199375832936]
	TIME [epoch: 1.85 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06278464223267582		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.06278464223267582 | validation: 0.05535295698353199]
	TIME [epoch: 1.83 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061117263278804526		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.061117263278804526 | validation: 0.0629566460611267]
	TIME [epoch: 1.83 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06155505534089745		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.06155505534089745 | validation: 0.05908926980184831]
	TIME [epoch: 1.83 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642893865867629		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.0642893865867629 | validation: 0.051576744543347375]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_986.pth
	Model improved!!!
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728996020497188		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.06728996020497188 | validation: 0.057967290814026476]
	TIME [epoch: 1.83 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06350778736186609		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06350778736186609 | validation: 0.06261191378632136]
	TIME [epoch: 1.83 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06239462349511594		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.06239462349511594 | validation: 0.06399064405734771]
	TIME [epoch: 1.82 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061556355179368874		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.061556355179368874 | validation: 0.06384530398300726]
	TIME [epoch: 1.82 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0607073421836823		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0607073421836823 | validation: 0.05531072467489744]
	TIME [epoch: 1.82 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06234485344912482		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.06234485344912482 | validation: 0.0580209462532007]
	TIME [epoch: 1.82 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0600590121268038		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.0600590121268038 | validation: 0.053389705328695004]
	TIME [epoch: 1.82 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347453483403773		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.06347453483403773 | validation: 0.0635969235516629]
	TIME [epoch: 1.82 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366676103424715		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.06366676103424715 | validation: 0.05796211894819493]
	TIME [epoch: 1.83 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06492381794156427		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06492381794156427 | validation: 0.05808974654538284]
	TIME [epoch: 1.82 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061973218489226545		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.061973218489226545 | validation: 0.05881325140489993]
	TIME [epoch: 1.82 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218954946567751		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.06218954946567751 | validation: 0.05832703711589951]
	TIME [epoch: 1.82 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061258333272126556		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.061258333272126556 | validation: 0.06022786127273697]
	TIME [epoch: 1.82 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06113903325866355		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06113903325866355 | validation: 0.05665328393971791]
	TIME [epoch: 1.82 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062045509125660495		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.062045509125660495 | validation: 0.0593762315268375]
	TIME [epoch: 143 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06393122284310968		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.06393122284310968 | validation: 0.07131283918810925]
	TIME [epoch: 3.96 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926281958556535		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.06926281958556535 | validation: 0.056858198458175815]
	TIME [epoch: 3.95 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748367500794006		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.06748367500794006 | validation: 0.05905039929179343]
	TIME [epoch: 3.95 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060862184923075484		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.060862184923075484 | validation: 0.05956290601644683]
	TIME [epoch: 3.95 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626260081835177		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.0626260081835177 | validation: 0.054776691320649634]
	TIME [epoch: 3.95 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062015448239459675		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.062015448239459675 | validation: 0.06052009400905618]
	TIME [epoch: 3.95 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05956532535107874		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.05956532535107874 | validation: 0.05988807122546413]
	TIME [epoch: 3.95 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06061925672180607		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.06061925672180607 | validation: 0.05651016323068782]
	TIME [epoch: 3.95 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06066522595884434		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.06066522595884434 | validation: 0.061626106542744835]
	TIME [epoch: 3.95 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986893175733874		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.05986893175733874 | validation: 0.05896301676951798]
	TIME [epoch: 3.95 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061986161921325736		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.061986161921325736 | validation: 0.05751507964579562]
	TIME [epoch: 3.95 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061139200755210225		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.061139200755210225 | validation: 0.0647473742698795]
	TIME [epoch: 3.95 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403202086447485		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.06403202086447485 | validation: 0.05567163796871291]
	TIME [epoch: 3.95 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435048395373735		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.06435048395373735 | validation: 0.05544329804137049]
	TIME [epoch: 3.95 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06020726509059827		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06020726509059827 | validation: 0.06018107273424359]
	TIME [epoch: 3.95 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06254491323942606		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.06254491323942606 | validation: 0.060395244101428686]
	TIME [epoch: 3.95 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06231651846260032		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.06231651846260032 | validation: 0.054816218215512115]
	TIME [epoch: 3.95 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379935936848098		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.06379935936848098 | validation: 0.06264133375481876]
	TIME [epoch: 3.95 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060319876834045306		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.060319876834045306 | validation: 0.05658178081144932]
	TIME [epoch: 3.95 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294819962766396		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.06294819962766396 | validation: 0.06290004186955801]
	TIME [epoch: 3.95 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303631192313724		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.06303631192313724 | validation: 0.05887978553772]
	TIME [epoch: 3.95 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06045600675134617		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.06045600675134617 | validation: 0.0529976221567646]
	TIME [epoch: 3.95 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0614177142690955		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.0614177142690955 | validation: 0.058840611820486645]
	TIME [epoch: 3.95 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060366816815409625		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.060366816815409625 | validation: 0.058402252631026066]
	TIME [epoch: 3.95 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903529849009609		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05903529849009609 | validation: 0.05667148834541324]
	TIME [epoch: 3.95 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060756993237501326		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.060756993237501326 | validation: 0.0494024840282472]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1027.pth
	Model improved!!!
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0601987072538612		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.0601987072538612 | validation: 0.05742228221951743]
	TIME [epoch: 3.95 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06352826044536057		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.06352826044536057 | validation: 0.05722708203276339]
	TIME [epoch: 3.95 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06120392778135589		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.06120392778135589 | validation: 0.057731335965863655]
	TIME [epoch: 3.95 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06299853991501272		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.06299853991501272 | validation: 0.06107935627963512]
	TIME [epoch: 3.95 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06192123282286051		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.06192123282286051 | validation: 0.054647748594409275]
	TIME [epoch: 3.95 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06168522926016272		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.06168522926016272 | validation: 0.06259273679146514]
	TIME [epoch: 3.95 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062349107213431586		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.062349107213431586 | validation: 0.05299003740075645]
	TIME [epoch: 3.95 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061216915501437014		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.061216915501437014 | validation: 0.05892458932458581]
	TIME [epoch: 3.95 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330518271080149		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.06330518271080149 | validation: 0.06082807497104821]
	TIME [epoch: 3.95 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06068125108667877		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.06068125108667877 | validation: 0.054412360159854734]
	TIME [epoch: 3.95 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060306095953383015		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.060306095953383015 | validation: 0.0569659816315665]
	TIME [epoch: 3.96 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838914224939467		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.05838914224939467 | validation: 0.06108390634348485]
	TIME [epoch: 3.95 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055915859507094		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06055915859507094 | validation: 0.05769063396585076]
	TIME [epoch: 3.95 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05982757536166619		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.05982757536166619 | validation: 0.052413869513632284]
	TIME [epoch: 3.95 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05932450595569053		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.05932450595569053 | validation: 0.05641484425349994]
	TIME [epoch: 3.95 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06270626769845326		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.06270626769845326 | validation: 0.05468211871429721]
	TIME [epoch: 3.95 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060135960926792825		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.060135960926792825 | validation: 0.055588751675013676]
	TIME [epoch: 3.95 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059284884517658405		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.059284884517658405 | validation: 0.0552147826159071]
	TIME [epoch: 3.95 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06182859532697248		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.06182859532697248 | validation: 0.060470392369567486]
	TIME [epoch: 3.95 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061216191864896324		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.061216191864896324 | validation: 0.05529231827625363]
	TIME [epoch: 3.95 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06089058054203919		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06089058054203919 | validation: 0.05245110878869735]
	TIME [epoch: 3.95 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06195637670108791		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.06195637670108791 | validation: 0.05724275312655426]
	TIME [epoch: 3.96 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05822901610960829		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05822901610960829 | validation: 0.06020972394491462]
	TIME [epoch: 3.96 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061439768178930175		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.061439768178930175 | validation: 0.05814311406049477]
	TIME [epoch: 3.95 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261441909524963		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.06261441909524963 | validation: 0.054312165538122625]
	TIME [epoch: 3.95 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060506854906375356		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.060506854906375356 | validation: 0.05789179212214564]
	TIME [epoch: 3.96 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0578501603664205		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.0578501603664205 | validation: 0.053512236909801614]
	TIME [epoch: 3.95 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984337202756752		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.05984337202756752 | validation: 0.054128394115640015]
	TIME [epoch: 3.95 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0602455599881206		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.0602455599881206 | validation: 0.056959107549004906]
	TIME [epoch: 3.95 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06197278510452569		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.06197278510452569 | validation: 0.05760659566064974]
	TIME [epoch: 3.95 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913789208724978		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.05913789208724978 | validation: 0.06042704021573391]
	TIME [epoch: 3.95 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05774138686512235		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.05774138686512235 | validation: 0.05220820987533057]
	TIME [epoch: 3.95 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131799732466159		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.06131799732466159 | validation: 0.05551346192947496]
	TIME [epoch: 3.95 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060443399187563945		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.060443399187563945 | validation: 0.05401963908733522]
	TIME [epoch: 3.95 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06035102075266632		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.06035102075266632 | validation: 0.05480931209714788]
	TIME [epoch: 3.95 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845185295959926		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.05845185295959926 | validation: 0.05524586392260758]
	TIME [epoch: 3.95 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06047432780967367		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.06047432780967367 | validation: 0.057736626243026716]
	TIME [epoch: 3.95 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058023918462699664		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.058023918462699664 | validation: 0.05046510305752612]
	TIME [epoch: 3.95 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0603849888215475		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.0603849888215475 | validation: 0.06137991422065488]
	TIME [epoch: 3.95 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060624313950291546		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.060624313950291546 | validation: 0.04853773762066749]
	TIME [epoch: 3.97 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06075599763483057		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06075599763483057 | validation: 0.05560018964759005]
	TIME [epoch: 3.95 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06039047198375311		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.06039047198375311 | validation: 0.05416585813018537]
	TIME [epoch: 3.95 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980868348673607		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.05980868348673607 | validation: 0.055036861086712144]
	TIME [epoch: 3.95 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05976380087675863		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.05976380087675863 | validation: 0.060777888379107796]
	TIME [epoch: 3.95 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059444113475854474		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.059444113475854474 | validation: 0.05707693465354094]
	TIME [epoch: 3.95 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013530266247304		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.06013530266247304 | validation: 0.05472370600450208]
	TIME [epoch: 3.95 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060634756026832035		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.060634756026832035 | validation: 0.05144112923442088]
	TIME [epoch: 3.95 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0605674797083093		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.0605674797083093 | validation: 0.055546461445645845]
	TIME [epoch: 3.95 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06200311027439552		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.06200311027439552 | validation: 0.05268014419179959]
	TIME [epoch: 3.95 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060033369768613176		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.060033369768613176 | validation: 0.06052852794168271]
	TIME [epoch: 3.95 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06019403088301958		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.06019403088301958 | validation: 0.05962747031676581]
	TIME [epoch: 3.95 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854039918100575		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.05854039918100575 | validation: 0.052919698355131266]
	TIME [epoch: 3.95 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060519204491800926		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.060519204491800926 | validation: 0.05751969762415287]
	TIME [epoch: 3.95 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797188582908376		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.05797188582908376 | validation: 0.054889227290121315]
	TIME [epoch: 3.95 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764406088508789		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.05764406088508789 | validation: 0.05164856384557533]
	TIME [epoch: 3.95 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05886515868589269		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.05886515868589269 | validation: 0.05347778452565896]
	TIME [epoch: 3.95 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05893777626454631		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.05893777626454631 | validation: 0.05003751452632064]
	TIME [epoch: 3.95 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05969249504372431		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.05969249504372431 | validation: 0.06302681846789542]
	TIME [epoch: 3.95 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06270049074718446		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.06270049074718446 | validation: 0.049928824827633805]
	TIME [epoch: 3.95 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958979980828001		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.05958979980828001 | validation: 0.05168131118375524]
	TIME [epoch: 3.95 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789461742771022		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05789461742771022 | validation: 0.05347414698155545]
	TIME [epoch: 3.95 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930295181892027		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.05930295181892027 | validation: 0.05214666992862989]
	TIME [epoch: 3.95 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962350514405928		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.05962350514405928 | validation: 0.056159610098844716]
	TIME [epoch: 3.95 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060962775672017265		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.060962775672017265 | validation: 0.054425112362127326]
	TIME [epoch: 3.95 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06003513037207843		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.06003513037207843 | validation: 0.054031402013279786]
	TIME [epoch: 3.95 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061789657435191825		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.061789657435191825 | validation: 0.05257421921073394]
	TIME [epoch: 3.95 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901061936216431		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.05901061936216431 | validation: 0.05724673163459594]
	TIME [epoch: 3.95 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05993784732249895		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.05993784732249895 | validation: 0.05589350304743568]
	TIME [epoch: 3.95 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970963518636486		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.05970963518636486 | validation: 0.05179142700019575]
	TIME [epoch: 3.95 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745314317902981		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.05745314317902981 | validation: 0.05726551184068027]
	TIME [epoch: 3.95 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05890814816098518		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05890814816098518 | validation: 0.058443310808065134]
	TIME [epoch: 3.95 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05771026236740329		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.05771026236740329 | validation: 0.05049302719961395]
	TIME [epoch: 3.95 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05724131883733957		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.05724131883733957 | validation: 0.05641690898327206]
	TIME [epoch: 3.95 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059710582986952546		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.059710582986952546 | validation: 0.05847658911034098]
	TIME [epoch: 3.95 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0593889128869831		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.0593889128869831 | validation: 0.05471982021801791]
	TIME [epoch: 3.95 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057626900051932294		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.057626900051932294 | validation: 0.04984824745823769]
	TIME [epoch: 3.95 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059400351608963606		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.059400351608963606 | validation: 0.054017635290319214]
	TIME [epoch: 3.95 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060483457095064		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.06060483457095064 | validation: 0.054090283233613125]
	TIME [epoch: 3.95 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06084694185394266		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.06084694185394266 | validation: 0.055989816992958776]
	TIME [epoch: 3.95 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580167791777267		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.0580167791777267 | validation: 0.06035527610199385]
	TIME [epoch: 3.95 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05963961654552934		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.05963961654552934 | validation: 0.051826764029023024]
	TIME [epoch: 3.95 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05696936837297173		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.05696936837297173 | validation: 0.058113287434767846]
	TIME [epoch: 3.95 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05875098431748727		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.05875098431748727 | validation: 0.05271039350428526]
	TIME [epoch: 3.95 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06063273510235604		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.06063273510235604 | validation: 0.05335380072973184]
	TIME [epoch: 3.95 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05817175533782946		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.05817175533782946 | validation: 0.05598686134707153]
	TIME [epoch: 3.95 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06045078474970232		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.06045078474970232 | validation: 0.06040768176121195]
	TIME [epoch: 3.95 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059549641557895557		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.059549641557895557 | validation: 0.0506057720427224]
	TIME [epoch: 3.95 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059629275571757026		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.059629275571757026 | validation: 0.050825210127119935]
	TIME [epoch: 3.95 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05866743368529072		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.05866743368529072 | validation: 0.04923051674730474]
	TIME [epoch: 3.95 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860100715635202		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.05860100715635202 | validation: 0.05223610748973744]
	TIME [epoch: 3.96 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058012762395211366		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.058012762395211366 | validation: 0.053616981285192294]
	TIME [epoch: 3.95 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924472255035028		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.05924472255035028 | validation: 0.04916057092113544]
	TIME [epoch: 3.95 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059525916851799886		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.059525916851799886 | validation: 0.05315748121785923]
	TIME [epoch: 3.95 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05755302006285538		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.05755302006285538 | validation: 0.06135604561781535]
	TIME [epoch: 3.95 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584448330810961		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.0584448330810961 | validation: 0.05089233342905915]
	TIME [epoch: 3.95 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056946956235280004		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.056946956235280004 | validation: 0.056410006206004205]
	TIME [epoch: 3.95 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057995326057934234		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.057995326057934234 | validation: 0.06039562767878928]
	TIME [epoch: 3.95 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060282890030620005		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.060282890030620005 | validation: 0.050605628974348574]
	TIME [epoch: 3.95 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813021817053758		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.05813021817053758 | validation: 0.056278835765001325]
	TIME [epoch: 3.95 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058773883498218174		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.058773883498218174 | validation: 0.04918259364125361]
	TIME [epoch: 3.95 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973812185075259		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.05973812185075259 | validation: 0.05613305294460394]
	TIME [epoch: 3.95 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05784199877481667		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.05784199877481667 | validation: 0.05266476308818573]
	TIME [epoch: 3.95 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05850663236364367		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05850663236364367 | validation: 0.0561947551167282]
	TIME [epoch: 3.95 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05850306832757065		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.05850306832757065 | validation: 0.05836247686042245]
	TIME [epoch: 3.95 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05766196152666		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.05766196152666 | validation: 0.05116664169074369]
	TIME [epoch: 3.95 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057803693884721924		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.057803693884721924 | validation: 0.04872403637597794]
	TIME [epoch: 3.95 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099230010306512		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.06099230010306512 | validation: 0.05588104063044126]
	TIME [epoch: 3.98 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057937623330626686		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.057937623330626686 | validation: 0.05074786189784837]
	TIME [epoch: 3.95 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05713969052321066		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.05713969052321066 | validation: 0.052590995978831934]
	TIME [epoch: 3.95 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707799731743326		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.05707799731743326 | validation: 0.048181087082738494]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1137.pth
	Model improved!!!
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05524207234479416		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.05524207234479416 | validation: 0.05206157046657903]
	TIME [epoch: 3.95 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06031775650460031		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.06031775650460031 | validation: 0.051656352791973274]
	TIME [epoch: 3.95 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060666756594879065		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.060666756594879065 | validation: 0.05614648822542269]
	TIME [epoch: 3.95 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058110031825139234		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.058110031825139234 | validation: 0.05006273360600334]
	TIME [epoch: 3.95 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0579765412301278		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.0579765412301278 | validation: 0.05346530855406067]
	TIME [epoch: 3.95 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746752198676667		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.05746752198676667 | validation: 0.059463043866137426]
	TIME [epoch: 3.95 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058052618281957805		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.058052618281957805 | validation: 0.047553509663140564]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1144.pth
	Model improved!!!
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058689546396150385		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.058689546396150385 | validation: 0.051406042162964294]
	TIME [epoch: 3.95 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0585481541267208		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.0585481541267208 | validation: 0.057915619629590256]
	TIME [epoch: 3.95 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05760714668845327		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.05760714668845327 | validation: 0.0503675059554138]
	TIME [epoch: 3.95 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055103660834007244		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.055103660834007244 | validation: 0.048593197243139444]
	TIME [epoch: 3.95 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05726812230403427		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.05726812230403427 | validation: 0.04668010626524206]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745027305868253		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05745027305868253 | validation: 0.053032710000714946]
	TIME [epoch: 3.95 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05708640773825894		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.05708640773825894 | validation: 0.05075464397763601]
	TIME [epoch: 3.95 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058142835337773126		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.058142835337773126 | validation: 0.056239321159054424]
	TIME [epoch: 3.95 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057507598581872356		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.057507598581872356 | validation: 0.046933587705633946]
	TIME [epoch: 3.95 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700326914255352		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.05700326914255352 | validation: 0.055709112938807585]
	TIME [epoch: 3.95 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575704552879054		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.05575704552879054 | validation: 0.05633834223450938]
	TIME [epoch: 3.95 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567690274235707		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.05567690274235707 | validation: 0.055908414828222286]
	TIME [epoch: 3.95 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05559490733241566		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.05559490733241566 | validation: 0.047603138704780085]
	TIME [epoch: 3.94 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05738869956741701		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.05738869956741701 | validation: 0.04965232950390411]
	TIME [epoch: 3.95 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777837084159347		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.05777837084159347 | validation: 0.048545720488738474]
	TIME [epoch: 3.95 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057526028957866904		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.057526028957866904 | validation: 0.04788095113482425]
	TIME [epoch: 3.95 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056923613890595595		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.056923613890595595 | validation: 0.05257534175155526]
	TIME [epoch: 3.95 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789203710153138		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05789203710153138 | validation: 0.05371455449776154]
	TIME [epoch: 3.95 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057729160168464695		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.057729160168464695 | validation: 0.04563576505303418]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05891628934992691		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.05891628934992691 | validation: 0.055219010582613304]
	TIME [epoch: 3.95 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732528310315309		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.05732528310315309 | validation: 0.052824518070526016]
	TIME [epoch: 3.95 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057102641304988426		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.057102641304988426 | validation: 0.050239111616787846]
	TIME [epoch: 3.95 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05992057951850824		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.05992057951850824 | validation: 0.05018330349948799]
	TIME [epoch: 3.95 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058423868396789834		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.058423868396789834 | validation: 0.04935020227701887]
	TIME [epoch: 3.95 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824937269348159		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.05824937269348159 | validation: 0.05771298699167748]
	TIME [epoch: 3.95 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05856921062914111		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.05856921062914111 | validation: 0.05000940078160981]
	TIME [epoch: 3.95 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05773223942700023		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.05773223942700023 | validation: 0.04999819233814474]
	TIME [epoch: 3.95 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05524016007555347		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.05524016007555347 | validation: 0.05297654360534803]
	TIME [epoch: 3.95 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777649507578123		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.05777649507578123 | validation: 0.05162057495380412]
	TIME [epoch: 3.95 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626471890611737		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.05626471890611737 | validation: 0.0553768878058222]
	TIME [epoch: 3.95 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05686074264509701		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.05686074264509701 | validation: 0.048254173917548965]
	TIME [epoch: 3.95 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057872801905709065		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.057872801905709065 | validation: 0.05259298692739842]
	TIME [epoch: 3.95 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056819784696160495		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.056819784696160495 | validation: 0.04700770589900316]
	TIME [epoch: 3.95 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847989734624149		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.05847989734624149 | validation: 0.056716438913853466]
	TIME [epoch: 3.95 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543214414323732		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.05543214414323732 | validation: 0.05540547350298092]
	TIME [epoch: 3.95 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056397842337169365		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.056397842337169365 | validation: 0.04861392786866858]
	TIME [epoch: 3.95 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058902431839386316		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.058902431839386316 | validation: 0.05505834182416709]
	TIME [epoch: 3.95 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056714970133408635		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.056714970133408635 | validation: 0.05292598552134532]
	TIME [epoch: 3.95 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05574465869982033		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.05574465869982033 | validation: 0.049326642802142884]
	TIME [epoch: 3.95 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057920383644689974		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.057920383644689974 | validation: 0.04960976732001174]
	TIME [epoch: 3.95 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903484929079343		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.05903484929079343 | validation: 0.05327404079155376]
	TIME [epoch: 3.95 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873011620496246		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.05873011620496246 | validation: 0.052188733223661056]
	TIME [epoch: 3.95 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744426434185888		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.05744426434185888 | validation: 0.052374804713611445]
	TIME [epoch: 3.95 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054951014333132396		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.054951014333132396 | validation: 0.048394345039304856]
	TIME [epoch: 3.95 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746912914721075		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.05746912914721075 | validation: 0.0503800792000299]
	TIME [epoch: 3.95 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057381416407920605		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.057381416407920605 | validation: 0.05243048898673526]
	TIME [epoch: 3.95 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058698647865750464		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.058698647865750464 | validation: 0.04888754087372734]
	TIME [epoch: 3.95 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05769241197345716		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.05769241197345716 | validation: 0.05246608723159756]
	TIME [epoch: 3.95 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055747296364783434		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.055747296364783434 | validation: 0.049852139084811786]
	TIME [epoch: 3.95 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569045584073454		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.0569045584073454 | validation: 0.046348531744623944]
	TIME [epoch: 3.94 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056529575296121064		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.056529575296121064 | validation: 0.05320809379117965]
	TIME [epoch: 3.95 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05849720864765558		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.05849720864765558 | validation: 0.05180563022751383]
	TIME [epoch: 3.95 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797657508508637		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.05797657508508637 | validation: 0.050267843147737114]
	TIME [epoch: 3.95 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060140729478748746		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.060140729478748746 | validation: 0.05356300264272739]
	TIME [epoch: 3.95 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056134705785216295		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.056134705785216295 | validation: 0.05127621902083704]
	TIME [epoch: 3.95 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825749527419637		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.05825749527419637 | validation: 0.04740984393778176]
	TIME [epoch: 3.99 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05578516182522588		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.05578516182522588 | validation: 0.048168910662488226]
	TIME [epoch: 3.95 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055999640090330906		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.055999640090330906 | validation: 0.052083819066862616]
	TIME [epoch: 3.95 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803797096013034		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.05803797096013034 | validation: 0.056619598661162424]
	TIME [epoch: 3.95 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639998181356037		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.05639998181356037 | validation: 0.04794231971962932]
	TIME [epoch: 3.95 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660680115144409		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.05660680115144409 | validation: 0.04899144271363006]
	TIME [epoch: 3.95 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056640602815872416		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.056640602815872416 | validation: 0.04654537419291141]
	TIME [epoch: 3.95 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739935011552851		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.05739935011552851 | validation: 0.048665961842403654]
	TIME [epoch: 3.95 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055725641129096816		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.055725641129096816 | validation: 0.0491695515246041]
	TIME [epoch: 3.95 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05550441418645262		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.05550441418645262 | validation: 0.05080933431297061]
	TIME [epoch: 3.95 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587887104090601		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.05587887104090601 | validation: 0.05019859617169351]
	TIME [epoch: 3.95 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055525077394215215		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.055525077394215215 | validation: 0.048006304728666516]
	TIME [epoch: 3.95 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057060096012077406		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.057060096012077406 | validation: 0.04654686913878874]
	TIME [epoch: 3.95 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563384669903923		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.0563384669903923 | validation: 0.049346798510704726]
	TIME [epoch: 3.95 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892697128999133		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.05892697128999133 | validation: 0.05156922416324181]
	TIME [epoch: 3.95 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055537013447801285		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.055537013447801285 | validation: 0.05389950969860219]
	TIME [epoch: 3.95 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057736585649853386		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.057736585649853386 | validation: 0.04715014955210442]
	TIME [epoch: 3.95 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05550122100696786		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.05550122100696786 | validation: 0.048950729652365355]
	TIME [epoch: 3.95 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055088411141574374		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.055088411141574374 | validation: 0.04990475873038631]
	TIME [epoch: 3.95 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05613775886607218		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.05613775886607218 | validation: 0.050203391071476804]
	TIME [epoch: 3.95 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635948513597759		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.05635948513597759 | validation: 0.04854138158432279]
	TIME [epoch: 3.95 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056025841307623545		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.056025841307623545 | validation: 0.04549501966517599]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1221.pth
	Model improved!!!
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597647712155087		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.05597647712155087 | validation: 0.05353060702585047]
	TIME [epoch: 3.95 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055408239619126005		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.055408239619126005 | validation: 0.05182307242239288]
	TIME [epoch: 3.95 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05640538344042138		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05640538344042138 | validation: 0.049471430648433734]
	TIME [epoch: 3.95 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05605371778789897		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.05605371778789897 | validation: 0.04690208985272529]
	TIME [epoch: 3.95 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055329415564011414		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.055329415564011414 | validation: 0.05049233016033586]
	TIME [epoch: 3.95 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05698539352481637		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.05698539352481637 | validation: 0.047966050287428806]
	TIME [epoch: 3.95 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057311127682345746		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.057311127682345746 | validation: 0.05247088587611651]
	TIME [epoch: 3.95 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057463884922500624		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.057463884922500624 | validation: 0.04817560619492824]
	TIME [epoch: 3.95 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529907437046779		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.05529907437046779 | validation: 0.049726479845732516]
	TIME [epoch: 3.95 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057379079828113036		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.057379079828113036 | validation: 0.05476077089702507]
	TIME [epoch: 3.95 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05553661725243947		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.05553661725243947 | validation: 0.049028555087197]
	TIME [epoch: 3.95 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057781009936461354		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.057781009936461354 | validation: 0.048538269839612354]
	TIME [epoch: 3.97 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05517582354927651		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.05517582354927651 | validation: 0.05483349354590314]
	TIME [epoch: 3.95 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678511444298261		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.05678511444298261 | validation: 0.049361745046370295]
	TIME [epoch: 3.95 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587968674513137		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.05587968674513137 | validation: 0.049536067318382615]
	TIME [epoch: 3.95 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658225603492906		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.05658225603492906 | validation: 0.04753014393481908]
	TIME [epoch: 3.95 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681918964718152		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.05681918964718152 | validation: 0.05241788081237732]
	TIME [epoch: 3.95 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059971696148413786		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.059971696148413786 | validation: 0.04496140104966313]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1239.pth
	Model improved!!!
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594467595051695		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.0594467595051695 | validation: 0.05847845050191025]
	TIME [epoch: 3.95 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583605164593192		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.0583605164593192 | validation: 0.05001356431644732]
	TIME [epoch: 3.95 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053931421474565075		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.053931421474565075 | validation: 0.04785833927332243]
	TIME [epoch: 3.95 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05659530206143691		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.05659530206143691 | validation: 0.050090601634990566]
	TIME [epoch: 3.95 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580911554569823		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0580911554569823 | validation: 0.046421267282779]
	TIME [epoch: 3.95 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655922116324488		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.05655922116324488 | validation: 0.054262278597013096]
	TIME [epoch: 3.95 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05666362397406305		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05666362397406305 | validation: 0.050571008068110615]
	TIME [epoch: 3.95 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622383640061283		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.05622383640061283 | validation: 0.05203641288522808]
	TIME [epoch: 3.95 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05374575627395418		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.05374575627395418 | validation: 0.0475868795271068]
	TIME [epoch: 3.95 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558332162504006		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.05558332162504006 | validation: 0.05262651011874963]
	TIME [epoch: 3.96 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05757562998834022		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.05757562998834022 | validation: 0.05011489467799815]
	TIME [epoch: 3.95 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595175479345902		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.05595175479345902 | validation: 0.04932230053393212]
	TIME [epoch: 3.95 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056100105824212314		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.056100105824212314 | validation: 0.049594388622129615]
	TIME [epoch: 3.95 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05672963735668618		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.05672963735668618 | validation: 0.04716287396818307]
	TIME [epoch: 3.95 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627423267281053		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.05627423267281053 | validation: 0.04936616358577435]
	TIME [epoch: 3.95 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684989863820079		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.05684989863820079 | validation: 0.04894748068112978]
	TIME [epoch: 3.95 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057196476833065946		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.057196476833065946 | validation: 0.0485156296325581]
	TIME [epoch: 3.95 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057315245416942585		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.057315245416942585 | validation: 0.04336785818146617]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1257.pth
	Model improved!!!
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056493704242072625		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.056493704242072625 | validation: 0.052374035733436186]
	TIME [epoch: 3.95 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591675909914118		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.05591675909914118 | validation: 0.04962715846248082]
	TIME [epoch: 3.95 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05661116440825335		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05661116440825335 | validation: 0.04801830509383414]
	TIME [epoch: 3.95 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055372904472123764		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.055372904472123764 | validation: 0.049056284975687744]
	TIME [epoch: 3.95 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05938513880205063		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.05938513880205063 | validation: 0.04904128500785858]
	TIME [epoch: 3.95 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05620673665937043		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.05620673665937043 | validation: 0.05185122256974322]
	TIME [epoch: 3.95 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05623771982460886		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.05623771982460886 | validation: 0.051878700320469574]
	TIME [epoch: 3.95 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054316535821712524		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.054316535821712524 | validation: 0.04731396275857774]
	TIME [epoch: 3.95 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054305811496873524		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.054305811496873524 | validation: 0.05263337438508362]
	TIME [epoch: 3.97 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052931202672701336		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.052931202672701336 | validation: 0.04809225832656777]
	TIME [epoch: 3.95 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054613899909382424		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.054613899909382424 | validation: 0.05256409996998063]
	TIME [epoch: 3.95 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055657060265751686		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.055657060265751686 | validation: 0.04874447899538581]
	TIME [epoch: 3.95 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693132630182667		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.05693132630182667 | validation: 0.04411116880554302]
	TIME [epoch: 3.95 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056350610423949975		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.056350610423949975 | validation: 0.0484557476717229]
	TIME [epoch: 3.95 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626921792424424		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.05626921792424424 | validation: 0.05008795041861827]
	TIME [epoch: 3.95 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550383555002333		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.0550383555002333 | validation: 0.04900418224921667]
	TIME [epoch: 3.95 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557960883541897		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.05557960883541897 | validation: 0.048024358653298356]
	TIME [epoch: 3.95 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534958301290935		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.05534958301290935 | validation: 0.05070377055919313]
	TIME [epoch: 3.95 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057058705579110865		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.057058705579110865 | validation: 0.04733864221398497]
	TIME [epoch: 3.95 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715329405122153		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.05715329405122153 | validation: 0.053669175836490904]
	TIME [epoch: 3.95 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05910923473908611		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.05910923473908611 | validation: 0.05244020726831211]
	TIME [epoch: 3.95 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056874763147647056		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.056874763147647056 | validation: 0.051698242302317635]
	TIME [epoch: 3.95 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567318945516997		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0567318945516997 | validation: 0.04632781373050568]
	TIME [epoch: 3.95 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509555289039349		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.05509555289039349 | validation: 0.049303687163359006]
	TIME [epoch: 3.95 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05405794452851706		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.05405794452851706 | validation: 0.048300994699650324]
	TIME [epoch: 3.95 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056123547264515836		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.056123547264515836 | validation: 0.049003740692377694]
	TIME [epoch: 3.95 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05570436064731759		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.05570436064731759 | validation: 0.05314599534540047]
	TIME [epoch: 3.95 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055270065686386614		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.055270065686386614 | validation: 0.0527197722027841]
	TIME [epoch: 3.95 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0557679601135431		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.0557679601135431 | validation: 0.046698527575692546]
	TIME [epoch: 3.95 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684711130989868		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.05684711130989868 | validation: 0.04715662774466745]
	TIME [epoch: 3.95 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054991963541839654		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.054991963541839654 | validation: 0.04735185867534165]
	TIME [epoch: 3.95 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055152927019515165		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.055152927019515165 | validation: 0.052589929236047266]
	TIME [epoch: 3.95 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056646530293342934		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.056646530293342934 | validation: 0.05136077205273311]
	TIME [epoch: 3.95 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056366378747239876		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.056366378747239876 | validation: 0.04521881854683725]
	TIME [epoch: 3.95 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05546393114025683		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.05546393114025683 | validation: 0.046343523234717784]
	TIME [epoch: 3.95 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05682432949619688		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.05682432949619688 | validation: 0.04937625199976688]
	TIME [epoch: 3.95 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806368791925287		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.05806368791925287 | validation: 0.047677445377289546]
	TIME [epoch: 3.95 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746416967448715		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.05746416967448715 | validation: 0.047449347419408205]
	TIME [epoch: 3.95 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05510754981333539		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.05510754981333539 | validation: 0.050609371972889665]
	TIME [epoch: 3.95 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545937977574372		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.05545937977574372 | validation: 0.04781869344382591]
	TIME [epoch: 3.95 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055080534137545316		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.055080534137545316 | validation: 0.04950283465011682]
	TIME [epoch: 3.95 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056446867339000235		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.056446867339000235 | validation: 0.04862264237737621]
	TIME [epoch: 3.96 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05537087245617686		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.05537087245617686 | validation: 0.05003211890117176]
	TIME [epoch: 3.96 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463498708930937		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.05463498708930937 | validation: 0.04942268825082351]
	TIME [epoch: 3.95 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05412013412996805		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.05412013412996805 | validation: 0.04935840866528876]
	TIME [epoch: 3.95 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055988452470235255		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.055988452470235255 | validation: 0.053214723136089596]
	TIME [epoch: 3.95 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05477367631657845		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.05477367631657845 | validation: 0.047754601443258875]
	TIME [epoch: 3.95 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05624454481812571		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.05624454481812571 | validation: 0.047282858219237556]
	TIME [epoch: 3.95 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054877215182887		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.054877215182887 | validation: 0.047857706769701164]
	TIME [epoch: 3.95 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662492243510285		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.05662492243510285 | validation: 0.05623268532592332]
	TIME [epoch: 3.95 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054700575286075574		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.054700575286075574 | validation: 0.05388620224195008]
	TIME [epoch: 3.95 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05535428606642528		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.05535428606642528 | validation: 0.048580399499090196]
	TIME [epoch: 3.95 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540049208579413		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.05540049208579413 | validation: 0.04908006858986853]
	TIME [epoch: 3.95 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05483949687648385		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.05483949687648385 | validation: 0.049945664036260086]
	TIME [epoch: 3.95 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05367861764798362		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.05367861764798362 | validation: 0.0536246528593054]
	TIME [epoch: 3.95 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053589471717149166		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.053589471717149166 | validation: 0.04700392618916143]
	TIME [epoch: 3.95 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05680973144527395		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.05680973144527395 | validation: 0.047087076490189755]
	TIME [epoch: 3.95 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360381854914781		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.05360381854914781 | validation: 0.04947850460230709]
	TIME [epoch: 3.95 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567172701650698		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.05567172701650698 | validation: 0.05030737468692419]
	TIME [epoch: 3.96 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05385527987236888		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.05385527987236888 | validation: 0.05315411868672654]
	TIME [epoch: 3.95 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554451753792532		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.05554451753792532 | validation: 0.04978824650823227]
	TIME [epoch: 3.95 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05518635855759678		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.05518635855759678 | validation: 0.04994078680475886]
	TIME [epoch: 3.95 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511712086916418		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.05511712086916418 | validation: 0.053651775014869764]
	TIME [epoch: 3.95 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794129751031969		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.05794129751031969 | validation: 0.05127387373479773]
	TIME [epoch: 3.95 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054288636779833865		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.054288636779833865 | validation: 0.050401539980098935]
	TIME [epoch: 3.95 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05334556097243425		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.05334556097243425 | validation: 0.045232388905702074]
	TIME [epoch: 3.95 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05656692275483054		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.05656692275483054 | validation: 0.04925189930527761]
	TIME [epoch: 3.95 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558282730837355		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.05558282730837355 | validation: 0.04855080280744778]
	TIME [epoch: 3.95 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540863603182075		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.05540863603182075 | validation: 0.048372246902193605]
	TIME [epoch: 3.95 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05636061319976435		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.05636061319976435 | validation: 0.047155844936484365]
	TIME [epoch: 3.95 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05427904470267016		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.05427904470267016 | validation: 0.05203877351446687]
	TIME [epoch: 3.95 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054897556351828886		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.054897556351828886 | validation: 0.04712578948382637]
	TIME [epoch: 3.95 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0555929538683464		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.0555929538683464 | validation: 0.04696609361482389]
	TIME [epoch: 3.95 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055378665999762426		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.055378665999762426 | validation: 0.048968295222508104]
	TIME [epoch: 3.95 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055614948886162446		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.055614948886162446 | validation: 0.046181513796700956]
	TIME [epoch: 3.95 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05697469028261403		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.05697469028261403 | validation: 0.0482646065279379]
	TIME [epoch: 3.95 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05713780597553994		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.05713780597553994 | validation: 0.04951687927540005]
	TIME [epoch: 3.96 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05327587020894014		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.05327587020894014 | validation: 0.05184886237920101]
	TIME [epoch: 3.95 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054744850718680294		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.054744850718680294 | validation: 0.047700053051237326]
	TIME [epoch: 3.95 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054506704319123966		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.054506704319123966 | validation: 0.050102900659017215]
	TIME [epoch: 3.95 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05451858235543575		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.05451858235543575 | validation: 0.04947918828488413]
	TIME [epoch: 3.95 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053428550035915595		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.053428550035915595 | validation: 0.051008429390822445]
	TIME [epoch: 3.95 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358518913714434		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.05358518913714434 | validation: 0.05001713071446277]
	TIME [epoch: 3.95 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055227862260016476		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.055227862260016476 | validation: 0.05281110040424297]
	TIME [epoch: 3.95 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05418104038341535		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.05418104038341535 | validation: 0.05177127224389642]
	TIME [epoch: 3.95 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05426924939741011		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.05426924939741011 | validation: 0.05110518372287587]
	TIME [epoch: 3.95 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610107471041678		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.05610107471041678 | validation: 0.047924747431944255]
	TIME [epoch: 3.95 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057147363852603626		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.057147363852603626 | validation: 0.04863237295344329]
	TIME [epoch: 3.95 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053980333279809604		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.053980333279809604 | validation: 0.054029916448779594]
	TIME [epoch: 3.95 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05484371101816528		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.05484371101816528 | validation: 0.051655823024970675]
	TIME [epoch: 3.95 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054807417207298016		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.054807417207298016 | validation: 0.047191194622128146]
	TIME [epoch: 3.94 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05539113853293005		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.05539113853293005 | validation: 0.05323065357905963]
	TIME [epoch: 3.95 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05495196022031587		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.05495196022031587 | validation: 0.046289823219846156]
	TIME [epoch: 3.97 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446751249299698		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.05446751249299698 | validation: 0.051264188604444844]
	TIME [epoch: 3.97 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055406682165908605		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.055406682165908605 | validation: 0.0474549788327335]
	TIME [epoch: 3.95 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057068602024046		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.057068602024046 | validation: 0.049933793780652165]
	TIME [epoch: 3.95 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0581489914840347		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.0581489914840347 | validation: 0.04909819278666667]
	TIME [epoch: 3.95 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583185035185511		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.05583185035185511 | validation: 0.050856820442867615]
	TIME [epoch: 3.95 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053335834324329826		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.053335834324329826 | validation: 0.04658160297521802]
	TIME [epoch: 3.95 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05556136765651635		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.05556136765651635 | validation: 0.04600194093205945]
	TIME [epoch: 3.95 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561194747997061		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.05561194747997061 | validation: 0.04817890181473652]
	TIME [epoch: 3.95 sec]
	Saving model to: out/model_training/model_phi1_4a_distortion_v1_2_v_mmd1_20250421_142949/states/model_phi1_4a_distortion_v1_2_v_mmd1_1358.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3338.408 seconds.
